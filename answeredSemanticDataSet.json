[
  {
    "content": "Hi,\r\n\r\nI'm recently trying to load the T5-11B model on GPUs using the Deepspeed framework. I'm using 5 Quadro RTX 8000 GPUs (48GB GPU memory each, node has 100 cores and 500GB RAM). I also varied the GPU numbers between [2, 10]. I've been getting an error \"RuntimeError: CUDA error: an illegal memory access was encountered\". This is different from the OOM errors I was getting initially. I don't know if there is some issue with my usage of the API or something else. I tried with T5-large and the code works fine. But I don't see what's the error with T5-11B, especially when I'm allocating so many GPUs already. A HuggingFace thread shows that T5-11B model can be loaded even on one GPU. So, I feel it should be doable here for sure. Any direction to debug further/ help would be super helpful. Below is a code sketch of how I used `deepspeed_stage_3`.\r\n\r\n```\r\nclass MyModel(LightningModule):\r\n\tdef __init__(self):\r\n\t\tsuper().__init__()\r\n\t\tself.ptlm = T5ForConditionalGeneration.from_pretrained('t5-11b', low_cpu_mem_usage=True, torch_dtype=\"auto\")\r\n\t\t# Also tried without low_cpu_mem_usage and torch_dtype\r\n\r\n\tdef configure_sharded_model(self):\r\n\t\t# ptlm = auto_wrap(self.ptlm)\t# Also tried the auto_wrap function here\r\n\t\tself.reasoner = self.ptlm\r\n\r\n\tdef forward(self, x):\r\n\t\treturn self.reasoner(x)\r\n\r\n\tdef configure_optimizers(self):\r\n\t\treturn FusedAdam(self.reasoner.parameters(), lr=1e-5)\r\n\r\ndef run(args):\r\n\tmodel = MyModel()\r\n\ttrainer\t= Trainer.from_argparse_args(\r\n\t\targs,\r\n\t\tstrategy=\"deepspeed_stage_3\",\t# Also tried \"ddp_sharded\"\r\n\t\taccelerator=\"gpu\",\r\n\t\tprecision=16,\r\n\t)\r\n\r\n```\r\n\r\n\r\nThe most recent error trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/soumya/git/lr_dataset/src/main.py\", line 231, in <module>\r\n    trainer.fit(model, dm)\r\n  File \"/home/soumya/miniconda3/envs/robustlr/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 768, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/home/soumya/miniconda3/envs/robustlr/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 736, in _call_and_handle_interrupt\r\n    self._teardown()\r\n  File \"/home/soumya/miniconda3/envs/robustlr/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1298, in _teardown\r\n    self.strategy.teardown()\r\n  File \"/home/soumya/miniconda3/envs/robustlr/lib/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 476, in teardown\r\n    torch.cuda.empty_cache()\r\n  File \"/home/soumya/miniconda3/envs/robustlr/lib/python3.9/site-packages/torch/cuda/memory.py\", line 114, in empty_cache\r\n    torch._C._cuda_emptyCache()\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\n``` Hi, any help/example script that shows how T5-11B can be used on (single/multiple) GPUs would be highly appreciated. Thanks\r\n\r\ncc: @SeanNarenA more whole answer here, we've added a function to `lightning-transformers` to help signal to HF that we'd like to enable sharding when loading their pre-trained weights (works only for DeepSpeed). Here is the code you need to add to your LightningModule! Note that the model has to be made in the `setup` function after the environment has been setup by Lightning.\r\n\r\n```bash\r\npip install lightning-transformers\r\n```\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom transformers import T5ForConditionalGeneration\r\nfrom lightning_transformers.utilities.deepspeed import enable_transformers_pretrained_deepspeed_sharding\r\n\r\n\r\nclass MyModel(pl.LightningModule):\r\n\r\n    def setup(self, stage: Optional[str] = None) -> None:\r\n        if not hasattr(self, \"ptlm\"):\r\n            enable_transformers_pretrained_deepspeed_sharding(self)\r\n            self.ptlm = T5ForConditionalGeneration.from_pretrained(\"t5-11b\")\r\n```",
    "meta": {
      "name": "Loading a large HuggingFace LM (T5-11B) using DeepSpeed"
    },
    "answer": "A more whole answer here, we've added a function to `lightning-transformers` to help signal to HF that we'd like to enable sharding when loading their pre-trained weights (works only for DeepSpeed). Here is the code you need to add to your LightningModule! Note that the model has to be made in the `setup` function after the environment has been setup by Lightning.\r\n\r\n```bash\r\npip install lightning-transformers\r\n```\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom transformers import T5ForConditionalGeneration\r\nfrom lightning_transformers.utilities.deepspeed import enable_transformers_pretrained_deepspeed_sharding\r\n\r\n\r\nclass MyModel(pl.LightningModule):\r\n\r\n    def setup(self, stage: Optional[str] = None) -> None:\r\n        if not hasattr(self, \"ptlm\"):\r\n            enable_transformers_pretrained_deepspeed_sharding(self)\r\n            self.ptlm = T5ForConditionalGeneration.from_pretrained(\"t5-11b\")\r\n```"
  },
  {
    "content": "Would appreciate if someone has an example of passing a list of transforms with arguments to a Lightning DataModule from the LightningCLI.\r\n\r\nI'm thinking of a syntax similar to [this](https://pytorch-lightning.readthedocs.io/en/stable/cli/lightning_cli_advanced_3.html#trainer-callbacks-and-arguments-with-class-type) where one can pass multiple callables from the CLI. \r\n\r\nEDIT: I managed to pass a single transform by adding the `transforms: Optional[Callable] = None,` typing hint to my datamodule `__init__()` but I get an unrecognized argument error when using the `data.transforms+=my.transforms.Transform` syntax in the CLI.\r\n\r\nThanks!You should use list, for more details, see https://jsonargparse.readthedocs.io/en/stable/#type-hintsI'm also trying to initiate transformations and pass them to the data module. I use the following config for the data module:\r\n```\r\ndata:\r\n  class_path: pfp.data.data_loader.DataLoaderIter\r\n  init_args:\r\n    batch_size: 2\r\n    pyg_file_dir: ./pyg_dataset\r\n    transformations:\r\n      - class_path: torch_geometric.transforms.center.Center\r\n```\r\nWhen debugging this is parsed as :\r\n\r\n`transformations = [{'class_path': 'torch_geometric.tran...ter.Center'}] `\r\n\r\nThe object is not initiated like the callbacks that are passed to the trainer. Is this how it should be or I'm doing something wrong? \r\n",
    "meta": {
      "name": "Passing transforms to datamodule from LightningCLI example"
    },
    "answer": "You should use list, for more details, see https://jsonargparse.readthedocs.io/en/stable/#type-hints"
  },
  {
    "content": "I need to be able to preserve the order in which the data is fed to the model when training in multiple GPUS.\r\nAccording to https://github.com/Lightning-AI/lightning/discussions/13342 each GPU gets a consecutive fraction of the dataset, so if I have 2GPUs, the first one will get the first half of the dataset and the other one will get the second half of the dataset.\r\nI need to preserve the order and don't know how to overwrite the dataset-splitting logic. Any advice?for that you'd need to write a custom [DistributedSampler](https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler) and pass it to the dataloader and set `Trainer(replace_sampler_ddp=False)`",
    "meta": { "name": "How to preserve dataset order when using DDP?" },
    "answer": "for that you'd need to write a custom [DistributedSampler](https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler) and pass it to the dataloader and set `Trainer(replace_sampler_ddp=False)`"
  },
  {
    "content": "\r\n```py\r\nfollowing is the error: NotImplementedError: `val_dataloader` must be implemented to be used with the Lightning Trainer\r\n\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-11-263e8be26564> in <module>()\r\n      3     tft,\r\n      4     train_dataloader=train_dataloader,\r\n----> 5     val_dataloaders=val_dataloader,\r\n      6 )\r\n\r\n11 frames\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/hooks.py in val_dataloader(self)\r\n    590             will have an argument ``dataloader_idx`` which matches the order here.\r\n    591         \"\"\"\r\n--> 592         raise NotImplementedError(\"`val_dataloader` must be implemented to be used with the Lightning Trainer\")\r\n    593 \r\n    594     def predict_dataloader(self) -> EVAL_DATALOADERS:\r\n\r\nNotImplementedError: `val_dataloader` must be implemented to be used with the Lightning Trainer\r\n```\r\n\r\n```py\r\ntrainer.fit(\r\n    tft,\r\n    train_dataloader=train_dataloader,\r\n    val_dataloaders=val_dataloader,\r\n)\r\n```I think it's a problem with pytorch-lightning==1.5.0.\r\nHad this problem too with code that worked before recreating my venv and the difference was version 1.5.0 release on 2. Nov.\r\nSwitched to 1.4.9 and it worked again.\r\nChecked 1.5.0rc1 and it did not work either. I realized that you cannot use `self.x_dataloader()` methods inside the `LightningModule` implementation anymore. I need to call `self.val_dataloader()` method to post-process the model predictions. I guess this has been changed with the new version. Right now, you can access the datamodule object with `datamodule` field within the lightning module implementation (e.g. `self.datamodule`, `self.datamodule.val_dataloader()` or `self.datamodule.val_data`).@geeksouvik @paapu88 @ilkerkesen : \r\n\r\nthe `self.x_dataloader()` are optional methods defined in the LightningModule, in case your module and data are not separable. The default implementation of these methods was changed from logging a warning message that these funtions should be implemented to raising a `NotImplementedError` to be explicit in case you calling something which you haven't defined. \r\n\r\nThough the naming is similar, this is very different from accessing an already instantiated dataloader which you've passed to the trainer outside of the LightningModule's definition. If your model depends on the data, I'd recommend making it an input to your LightningModule constructor, and then pass that to the Trainer for `fit` and `test`",
    "meta": {
      "name": "Issue in fitting model and finding optimal learning rate parameter"
    },
    "answer": "I think it's a problem with pytorch-lightning==1.5.0.\r\nHad this problem too with code that worked before recreating my venv and the difference was version 1.5.0 release on 2. Nov.\r\nSwitched to 1.4.9 and it worked again.\r\nChecked 1.5.0rc1 and it did not work either. "
  },
  {
    "content": "## Summary\r\nLoss does not decrease and accuracy/F1-score is not improving during training HuggingFace Transformer BertForSequenceClassification with Pytorch-Lightning\r\n\r\n## Issue\r\nHello PTL team, previously, I trained huggingface bert model with my own trainer code. To improve code quality and implement MLOps system, I\u2019m trying to train huggingface\u2019s transformers Bert with pytorch lightning. \r\n\r\nWhen I train BertForSequenceClassification in the transformers with PTL, however, Loss, accuracy, and even f1 score seems to not improve during a training phase. I think there are some bugs in the optimizer or back-propagation in my code, but I can\u2019t find any problems. my question is, what is the problem with my code?\r\n\r\n## my assumtions:\r\n1. configure_optimizer is not correctly configured\r\n2. huggingface sequence classification module\r\n3. segment_ids should not be passes during training step\r\n4. etc.\r\n\r\n## package versions\r\npython==3.7\r\npytorch==1.11.0\r\npytorch-lightning == 1.7.7\r\ntransformers == 4.2.2\r\ntorchmetrics == up-to-date\r\n\r\n## code snippet\r\n```python\r\nimport os\r\nimport csv\r\nimport tqdm\r\nfrom typing import *\r\nimport pandas as pd\r\n\r\nimport torch\r\nfrom torch import nn\r\nimport pytorch_lightning as pl\r\nimport torchmetrics.functional as F\r\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\r\nfrom transformers.modeling_outputs import SequenceClassifierOutput\r\nfrom transformers import BertForSequenceClassification, BertTokenizer, InputExample, InputFeatures, AutoConfig, get_linear_schedule_with_warmup\r\n\r\n\r\ndef get_input_example(guid: int, text_a: str, label: str) -> Tuple[InputExample, str]:\r\n    input_example = InputExample(\r\n        guid=guid,\r\n        text_a=text_a,\r\n        text_b=None,\r\n        label=label\r\n    )\r\n    return input_example, label\r\n\r\n\r\ndef add_examples(\r\n        texts_or_text_and_labels: Union[List[str], str],\r\n        text_index: int,\r\n        label_index: int,\r\n        label_dict: Dict[str, int] = None,\r\n        remove_top: bool = False\r\n) -> Tuple[List[InputExample], Dict[str, int]]:\r\n    examples = list()\r\n    labels = list()\r\n\r\n    tmp = []\r\n    if isinstance(texts_or_text_and_labels, str):\r\n        with open(texts_or_text_and_labels, 'r') as f:\r\n            if texts_or_text_and_labels.endswith('csv'):\r\n                delimiter = ','\r\n            elif texts_or_text_and_labels.endswith('tsv'):\r\n                delimiter='\\t'\r\n\r\n            reader = csv.reader(f, delimiter=delimiter, quotechar='\"')\r\n            for idx, line in enumerate(tqdm.tqdm(reader)):\r\n                if remove_top is True and idx == 0:\r\n                    pass\r\n                else:\r\n                    tmp.append(line)\r\n        texts_or_text_and_labels = tmp\r\n\r\n    for line in tqdm.tqdm(texts_or_text_and_labels):\r\n        text_a = line[text_index]\r\n        label = line[label_index]\r\n\r\n        input_example, label = get_input_example(guid=line[0], text_a=text_a, label=label)\r\n        examples.append(input_example)\r\n        if label_dict is None:\r\n            labels.append(label)\r\n    if label_dict is None:\r\n        label_dict = {i: idx for idx, i in enumerate(list(set(labels)))}\r\n    return examples, label_dict\r\n\r\n\r\nclass BertDataset(Dataset):\r\n    def __init__(self, examples, tokenizer, label_dict, max_length):\r\n        self.examples = examples\r\n        self.tokenizer = tokenizer\r\n        self.label_dict = label_dict\r\n        self.max_length = max_length\r\n\r\n    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length) -> None:\r\n        \"\"\"\r\n        Truncates a sequence pair in place to the maximum length.\r\n        This is a simple heuristic which will always truncate the longer sequence\r\n        one token at a time. This makes more sense than truncating an equal percent\r\n        of tokens from each, since if one sequence is very short then each token\r\n        that's truncated likely contains more information than a longer sequence.\r\n        \"\"\"\r\n        while True:\r\n            total_length = len(tokens_a) + len(tokens_b)\r\n            if total_length <= max_length - 3:\r\n                break\r\n            if len(tokens_a) > len(tokens_b):\r\n                tokens_a.pop()\r\n            else:\r\n                tokens_b.pop()\r\n\r\n    def __len__(self):\r\n        return len(self.examples)\r\n\r\n    def __getitem__(\r\n            self,\r\n            idx: int,\r\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\r\n\r\n        def _tokens_and_segment_id(token_a: List[str], token_b: List[str] = None) -> Tuple[Any, List[int]]:\r\n            tokens = ['[CLS]'] + token_a + ['[SEP]']  # See in 1-1. Section in /docs/Appendix.md\r\n            token_type_ids = [0] * len(tokens)  # for more information of 138-145 lines\r\n            if token_b:\r\n                tokens += token_b + ['[SEP]']\r\n                token_type_ids += [1] * (len(token_b) + 1)\r\n            return tokens, token_type_ids\r\n\r\n        text_a = self.examples[idx].text_a\r\n        text_b = self.examples[idx].text_b\r\n        label = self.examples[idx].label\r\n\r\n            #   Convert texts into tokens\r\n        tokens_a = self.tokenizer.tokenize(text_a)\r\n        tokens_b = None\r\n        if text_b:\r\n            tokens_b = self.tokenizer.tokenize(text_b)\r\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\r\n            # length is less than the specified length.\r\n            # Account for [CLS], [SEP], [SEP] with '- 3'\r\n            self._truncate_seq_pair(tokens_a, tokens_b, self.max_length)\r\n        else:\r\n            if len(tokens_a) > self.max_length - 2:\r\n                tokens_a = tokens_a[:(self.max_length - 2)]\r\n\r\n        tokens, token_type_ids = _tokens_and_segment_id(tokens_a, tokens_b)\r\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\r\n        label_ids = self.label_dict[label]\r\n\r\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\r\n        # tokens are attended to.\r\n        attention_mask = [1] * len(input_ids)\r\n\r\n        # Zero-pad up to the sequence length.\r\n        padding = [0] * (self.max_length - len(input_ids))\r\n        input_ids += padding\r\n        attention_mask += padding\r\n        token_type_ids += padding\r\n\r\n        assert len(input_ids) == self.max_length\r\n        assert len(attention_mask) == self.max_length\r\n        assert len(token_type_ids) == self.max_length\r\n\r\n        input_ids = torch.tensor(input_ids, dtype=torch.long)\r\n        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\r\n        token_type_ids = torch.tensor(token_type_ids, dtype=torch.long)\r\n        labels = torch.tensor(label_ids, dtype=torch.long)\r\n        return input_ids, attention_mask, token_type_ids, labels\r\n\r\n\r\nclass BertAccTestModel(pl.LightningModule):\r\n    def __init__(self, num_classes: int):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.config = AutoConfig.from_pretrained('klue/bert-base', num_labels=num_classes)\r\n        self.model = BertForSequenceClassification.from_pretrained('klue/bert-base', config=self.config)\r\n        self.tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\r\n        self.num_classes = num_classes\r\n\r\n    def forward(self, init_ids, input_mask, segment_ids) -> SequenceClassifierOutput:\r\n        outputs = self.model(init_ids, input_mask, segment_ids)\r\n        return outputs\r\n\r\n    def info(self, dictionary: dict) -> None:\r\n        r\"\"\"\r\n        Logging information from dictionary.\r\n        Args:\r\n            dictionary (dict): dictionary contains information.\r\n        \"\"\"\r\n        for key, value in dictionary.items():\r\n            self.log(key, value, prog_bar=True, sync_dist=True)\r\n\r\n    def training_step(self, batch: Tuple, batch_idx: int) -> Dict[str, torch.Tensor]:\r\n        init_ids, input_mask, segment_ids, label_ids = batch\r\n        outputs = self.model(init_ids, input_mask, segment_ids, labels=label_ids)\r\n        top1_acc = F.accuracy(outputs.logits, label_ids)\r\n        top1_f1 = F.f1_score(outputs.logits, label_ids, average='macro', num_classes=self.num_classes)\r\n        loss = outputs.loss\r\n        self.info({\r\n            'train_loss': loss,\r\n            'train_acc': top1_acc,\r\n            'train_f1': top1_f1,\r\n        })\r\n        return loss\r\n\r\n    def validation_step(self, batch: Tuple, batch_idx: int) -> Dict[str, torch.Tensor]:\r\n        init_ids, input_mask, segment_ids, label_ids = batch\r\n        outputs = self.model(init_ids, input_mask, segment_ids, labels=label_ids)\r\n        top1_acc = F.accuracy(outputs.logits, label_ids)\r\n        top1_f1 = F.f1_score(outputs.logits, label_ids, average='macro', num_classes=self.num_classes)\r\n        loss = outputs.loss\r\n        self.info({\r\n            'val_loss': loss,\r\n            'val_acc': top1_acc,\r\n            'val_f1': top1_f1,\r\n        })\r\n        return loss\r\n\r\n    def test_step(self, batch: Tuple, batch_idx: int) -> Dict[str, torch.Tensor]:\r\n        init_ids, input_mask, segment_ids, label_ids = batch\r\n        outputs = self.model(init_ids, input_mask, segment_ids, labels=label_ids)\r\n        top1_acc = F.accuracy(outputs.logits, label_ids)\r\n        top1_f1 = F.f1_score(outputs.logits, label_ids, average='macro', num_classes=self.num_classes)\r\n        loss = outputs.loss\r\n        self.info({\r\n            'test_loss': loss,\r\n            'test_acc': top1_acc,\r\n            'test_f1': top1_f1,\r\n        })\r\n        return loss\r\n\r\n    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Any:\r\n        init_ids, input_mask, segment_ids, label_ids = batch\r\n        outputs = self(init_ids, input_mask, segment_ids, label_ids)\r\n        return torch.argmax(outputs.logits)\r\n\r\n    def configure_optimizers(self):\r\n        model = self.model\r\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\r\n        optimizer_grouped_parameters = [\r\n            {\r\n                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\r\n                \"weight_decay\": 0.0,\r\n            },\r\n            {\r\n                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\r\n                \"weight_decay\": 0.0,\r\n            },\r\n        ]\r\n        optim = torch.optim.AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\r\n        scheduler = get_linear_schedule_with_warmup(\r\n            optim,\r\n            num_warmup_steps=500,\r\n            num_training_steps=self.trainer.estimated_stepping_batches,\r\n        )\r\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\r\n        return [optim], [scheduler]\r\n\r\n\r\ndef main():\r\n    root = '/'.join(os.getcwd().split('/'))\r\n    print(root)\r\n    tokenizer = BertTokenizer.from_pretrained('klue/bert-base', do_lower_case=False)\r\n\r\n    train_examples, label_dict = add_examples(os.path.join(root, 'data/training_merged_1d.tsv'), text_index=2, label_index=7,  remove_top=True)\r\n    eval_examples, _ = add_examples(os.path.join(root, 'data/validation.tsv'), text_index=2, label_index=7, label_dict=label_dict)\r\n    test_examples, _ = add_examples(os.path.join(root, 'data/test.tsv'), text_index=2, label_index=7, label_dict=label_dict, remove_top=True)\r\n    train_dataset = BertDataset(train_examples, tokenizer, max_length=256, label_dict=label_dict)\r\n    eval_dataset = BertDataset(eval_examples, tokenizer, max_length=256, label_dict=label_dict)\r\n    test_dataset = BertDataset(test_examples, tokenizer, max_length=256, label_dict=label_dict)\r\n\r\n    print(len(train_dataset), len(eval_dataset), len(test_dataset))\r\n    trn_dataloader = DataLoader(train_dataset, batch_size=64, num_workers=4, sampler=RandomSampler(train_dataset))\r\n    eval_dataloader = DataLoader(eval_dataset, batch_size=64, num_workers=4, sampler=SequentialSampler(eval_dataset))\r\n    test_dataloader = DataLoader(test_dataset, batch_size=4, num_workers=4, sampler=SequentialSampler(test_dataset))\r\n    inference_label_dict = {v: k for k, v in label_dict.items()}\r\n\r\n    model = BertAccTestModel(num_classes= len(label_dict))\r\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\r\n        save_last=True,\r\n        save_weights_only=True,\r\n        monitor='val_f1',\r\n        mode='max',\r\n        dirpath=os.path.join(root, 'weights'),\r\n        filename='pytorch_model'\r\n    )\r\n    trainer = pl.Trainer(gpus=4, max_epochs=10, accelerator='cuda', strategy='ddp', precision=32, callbacks=[checkpoint_callback])\r\n    trainer.fit(model, train_dataloaders=trn_dataloader, val_dataloaders=eval_dataloader)\r\n    trainer.test(model, test_dataloader)\r\n\r\n    with open(os.path.join(root, 'weights', 'labels.dict'), 'w') as f:\r\n        import json\r\n        json.dump(inference_label_dict, f)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```## Updates\r\nI found that DDP was the problem. Accuracy and loss improve when I train my model on single-GPU, It seems to add_examples code was a problem because they make a label list which is not a fixed index. \r\n",
    "meta": {
      "name": "Accuracy/loss is not improving when train huggingface transformer bert"
    },
    "answer": "## Updates\r\nI found that DDP was the problem. Accuracy and loss improve when I train my model on single-GPU, It seems to add_examples code was a problem because they make a label list which is not a fixed index. \r\n"
  },
  {
    "content": "When using LARS optimizer, usually the batch size is scale linearly with the learning rate.\r\nSuppose I set the **base_lr** to be **0.1 * batch_size / 256.**\r\nNow for 1 GPU training with batch size 512, the learning rate should be 0.1 * 2 = **0.2**\r\n\r\nHowever when I use **2 GPUs** with **DDP** backend and **batch size** of **512** on each GPU. Should my learning rate be:\r\n\r\n- 0.1 * 2 = **0.2**\r\n- or 0.1 * 2 *  2 (no. GPUs) = **0.4**\r\nJust to clarify if you use batch_size=512 in DDP backend, each GPU will train on 512 batch_size in lightning. Do you want 512 on each or 256 on each GPU?Hi I want each GPUs to has batch size of **512**. So two GPUs will have a total batch size of **1024**. I don't know if I should set the learinng rate base on the **total batch size** or batch size on **each GPU**in DDP the gradients are averaged and synced across each device before `optimzer_step`, so I don't think `lr` should be changed here.As far as I know, learning rate is scaled with the batch size so that the sample variance of the gradients is kept approx. constant. \r\n\r\nSince DDP averages the gradients from all the devices, I think the LR should be scaled in proportion to the effective batch size, namely, batch_size * num_accumulated_batches * num_gpus * num_nodes\r\n\r\nIn this case, assuming batch_size=512, num_accumulated_batches=1, num_gpus=2 and num_noeds=1  the effective batch size is 1024, thus the LR should be scaled by sqrt(2), compared to a single gpus with effective batch size 512.Thank you all for your answers. I'll scale the LR with the total effective batch size.This is mentioned very briefly in the DDP documentation, perhaps it should also be mentioned in the TPU section in the docs since TPU uses DDP. This was my case, and I understood it as needing to scale batch size to match effective learning rate, but it was hard to find confirmation on this even with several threads on the subject in various places.@itsikad Hi, I read your explanation and it makes sense to me, but when I ran an experiment with Lightning DDP I got:\r\n\r\n```\r\n# Simple CNN Autoencoder using MSE loss on MNIST\r\n\r\n# baseline\r\n1 GPU, bs=32, lr=1e-4: MSE=0.016281472519040108\r\n\r\n# sqrt scaling\r\n2 GPU, bs=32, lr=1e-4 * sqrt(2): MSE=0.01761590503156185\r\n3 GPU, bs=32, lr=1e-4 * sqrt(3): MSE=0.01891041174530983\r\n4 GPU, bs=32, lr=1e-4 * sqrt(4): MSE=0.01945831999182701\r\n\r\n# linear scaling\r\n2 GPU, bs=32, lr=1e-4 * 2: MSE=0.016594046726822853\r\n3 GPU, bs=32, lr=1e-4 * 3: MSE= 0.016459766775369644\r\n4 GPU, bs=32, lr=1e-4 * 4: MSE=0.016359923407435417\r\n```\r\n\r\nWhich seems to show that linear scaling instead of scaling with the sqrt() is what actually enables scaling GPUs while maintaining performance for me. I assume I'm missing something, but can you help me understand how to unite your theoretical explanation with my results? I think this paper also suggests linear scaling: https://arxiv.org/abs/1706.02677",
    "meta": {
      "name": "How to scale learning rate with batch size for DDP training?"
    },
    "answer": "As far as I know, learning rate is scaled with the batch size so that the sample variance of the gradients is kept approx. constant. \r\n\r\nSince DDP averages the gradients from all the devices, I think the LR should be scaled in proportion to the effective batch size, namely, batch_size * num_accumulated_batches * num_gpus * num_nodes\r\n\r\nIn this case, assuming batch_size=512, num_accumulated_batches=1, num_gpus=2 and num_noeds=1  the effective batch size is 1024, thus the LR should be scaled by sqrt(2), compared to a single gpus with effective batch size 512."
  },
  {
    "content": "I use the early stopping callback like this \r\n\r\n```python\r\n    trainer = pl.Trainer(\r\n        strategy=strategy,\r\n        accelerator=accelerator,\r\n        devices=devices,\r\n        deterministic=True,\r\n        max_epochs=max_epochs,\r\n        callbacks=[EarlyStopping(monitor=\"rougeL_fmeasure\", mode=\"max\", patience=5)],\r\n    )\r\n```\r\n\r\nBut this just creates a single checkpoint like this `'epoch=8-step=4500.ckpt'` epoch 8 is when the training was stopped, and epoch 3 is actually the best epoch. How can I access the checkpoint with the best metric?Hello @vikigenius,\r\n\r\nI usually specify how my PL model checkpoints, as shown in the code snippet below:\r\n```python\r\ntrainer = pl.Trainer(\r\n            [...] # remain trainer params\r\n            callbacks=[\r\n\r\n                self.get_model_checkpoint_callback(),  # checkpoint_callback\r\n                self.get_early_stopping_callback(),  # early_stopping_callback\r\n                [...] # other callbacks\r\n            ],\r\n            deterministic=True\r\n        )\r\n\r\n\r\n\r\ndef get_model_checkpoint_callback(self):\r\n    return ModelCheckpoint(\r\n        monitor=self.params.val_metric,\r\n        dirpath=self.params.model_checkpoint.dir,\r\n        filename=f\"{self.params.model.name}_{self.params.data.name}_{self.params.fold}\",\r\n        save_top_k=self.params.save_top_k, # the best k models according to the monitor\r\n        save_weights_only=self.params.save_weights_only,\r\n        mode=self.params.mode\r\n    )\r\n\r\n\r\ndef get_early_stopping_callback(self,):\r\n    return EarlyStopping(\r\n        monitor=self.params.val_metric,\r\n        patience=self.params.trainer.patience,\r\n        min_delta=self.params.trainer.min_delta,\r\n        mode=self.params.mode\r\n    )\r\n```\r\n\r\nI hope this could help you.",
    "meta": { "name": "Access best checkpoint when using early stopping" },
    "answer": "Hello @vikigenius,\r\n\r\nI usually specify how my PL model checkpoints, as shown in the code snippet below:\r\n```python\r\ntrainer = pl.Trainer(\r\n            [...] # remain trainer params\r\n            callbacks=[\r\n\r\n                self.get_model_checkpoint_callback(),  # checkpoint_callback\r\n                self.get_early_stopping_callback(),  # early_stopping_callback\r\n                [...] # other callbacks\r\n            ],\r\n            deterministic=True\r\n        )\r\n\r\n\r\n\r\ndef get_model_checkpoint_callback(self):\r\n    return ModelCheckpoint(\r\n        monitor=self.params.val_metric,\r\n        dirpath=self.params.model_checkpoint.dir,\r\n        filename=f\"{self.params.model.name}_{self.params.data.name}_{self.params.fold}\",\r\n        save_top_k=self.params.save_top_k, # the best k models according to the monitor\r\n        save_weights_only=self.params.save_weights_only,\r\n        mode=self.params.mode\r\n    )\r\n\r\n\r\ndef get_early_stopping_callback(self,):\r\n    return EarlyStopping(\r\n        monitor=self.params.val_metric,\r\n        patience=self.params.trainer.patience,\r\n        min_delta=self.params.trainer.min_delta,\r\n        mode=self.params.mode\r\n    )\r\n```\r\n\r\nI hope this could help you."
  },
  {
    "content": "Hi, I was wondering what is the proper way of logging metrics when using DDP. I noticed that if I want to print something inside `validation_epoch_end` it will be printed twice when using 2 GPUs. I was expecting `validation_epoch_end` to be called only on rank 0 and to receive the outputs from all GPUs, but I am not sure this is correct anymore. Therefore I have several questions:\r\n\r\n1. `validation_epoch_end(self, outputs)` - When using DDP does every subprocess receive the data processed from the current GPU or data processed from all GPUs, i.e. does the input parameter `outputs` contains the outputs of the entire validation set, from all GPUs?\r\n2. If `outputs` is GPU/process specific what is the proper way to calculate any metric on the entire validation set in `validation_epoch_end` when using DDP?\r\n\r\nI understand that I can solve the printing by checking `self.global_rank == 0` and printing/logging only in that case, however I am trying to get a deeper understanding of what I am printing/logging in this case. \r\n\r\nHere is a code snippet from my use case. I would like to be able to report f1, precision and recall on the entire validation dataset and I am wondering what is the correct way of doing it when using DDP.\r\n\r\n    \r\n```Python\r\n    def _process_epoch_outputs(self,\r\n                               outputs: List[Dict[str, Any]]\r\n                               ) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"Creates and returns tensors containing all labels and predictions\r\n\r\n        Goes over the outputs accumulated from every batch, detaches the\r\n        necessary tensors and stacks them together.\r\n\r\n        Args:\r\n            outputs (List[Dict])\r\n        \"\"\"\r\n        all_labels = []\r\n        all_predictions = []\r\n\r\n        for output in outputs:\r\n            for labels in output['labels'].detach():\r\n                all_labels.append(labels)\r\n\r\n            for predictions in output['predictions'].detach():\r\n                all_predictions.append(predictions)\r\n\r\n        all_labels = torch.stack(all_labels).long().cpu()\r\n        all_predictions = torch.stack(all_predictions).cpu()\r\n\r\n        return all_predictions, all_labels\r\n\r\n    def validation_epoch_end(self, outputs: List[Dict[str, Any]]) -> None:\r\n        \"\"\"Logs f1, precision and recall on the validation set.\"\"\"\r\n\r\n        if self.global_rank == 0:\r\n            print(f'Validation Epoch: {self.current_epoch}')\r\n\r\n        predictions, labels = self._process_epoch_outputs(outputs)\r\n        for i, name in enumerate(self.label_columns):\r\n\r\n            f1, prec, recall, t = metrics.get_f1_prec_recall(predictions[:, i],\r\n                                                             labels[:, i],\r\n                                                             threshold=None)\r\n            self.logger.experiment.add_scalar(f'{name}_f1/Val',\r\n                                              f1,\r\n                                              self.current_epoch)\r\n            self.logger.experiment.add_scalar(f'{name}_Precision/Val',\r\n                                              prec,\r\n                                              self.current_epoch)\r\n            self.logger.experiment.add_scalar(f'{name}_Recall/Val',\r\n                                              recall,\r\n                                              self.current_epoch)\r\n\r\n            if self.global_rank == 0:\r\n                print((f'F1: {f1}, Precision: {prec}, '\r\n                       f'Recall: {recall}, Threshold {t}'))\r\n```I have the same question, and have not been able to get sufficient clarity from the docs about how logging works during distributed training. \r\n\r\nI found the suggestion to [use the `sync_dist` flag when logging during validation/testing in distributed training](https://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html#synchronize-validation-and-test-logging), but it's unclear exactly what this does, and whether I should or shouldn't use `sync_dist=True` when logging in the training step as well. If not, why not?I have the same problem.  I managed to log the synced metric by calling metric.compute(). But the value is not identical with the Checkpoint callback.\r\n\r\nDetails please find in https://github.com/PyTorchLightning/pytorch-lightning/discussions/6352\r\n\r\nAnd there is a related issue: \r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/issues/6122\r\n  @williamFalcon any chance someone can help us with this?@edenafek  could you please take a look at the above issue?Hi all,\r\nSorry we have not got back to you in time, let me try to answer some of your questions:\r\n1. Is `validation_epoch_end` only called on rank 0?\r\n\r\nNo, it is called by all processes\r\n\r\n2. What does the `sync_dist` flag do:\r\n\r\nHere is the essential code:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/a72a7992a283f2eb5183d129a8cf6466903f1dc8/pytorch_lightning/core/step_result.py#L108-L115\r\nIf `sync_dist=True` then it will as default call the `sync_ddp` function which will sum the value across all processes using `torch.distributed.all_reduce` \r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/a72a7992a283f2eb5183d129a8cf6466903f1dc8/pytorch_lightning/utilities/distributed.py#L120\r\nUse this flag if you want to synchronize the value between different processes. \r\n\r\n3. How to print stuff in distributed lightning:\r\n\r\nRecommended is using either the `rank_zero_info` function. Import as:\r\n```python\r\nfrom pytorch_lightning.utilities import rank_zero_info\r\n```\r\nor use the `rank_zero_only` decorator (imported from the same module) which can be wrapped around any function such that it only gets called on `rank=0`. Each logger experiment is decorated with `rank_zero_experiment` which internally calls `rank_zero_only`\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/a72a7992a283f2eb5183d129a8cf6466903f1dc8/pytorch_lightning/loggers/base.py#L31-L43\r\n\r\n4. What about `pytorch_lightning.metrics` (now known as `torchmetrics`)\r\n\r\nOur own metrics have custom synchronization going on. Any metric will automatically synchronize between different processes whenever `metric.compute()` is called. Metrics calculated this way should therefore not be logged using `sync_dist=True`.\r\n\r\n5. Recommended way of logging:\r\n\r\nUsing `self.log` in your lightning module \r\n\r\nNot sure this answers all questions.",
    "meta": { "name": "Proper way to log things when using DDP" },
    "answer": "Hi all,\r\nSorry we have not got back to you in time, let me try to answer some of your questions:\r\n1. Is `validation_epoch_end` only called on rank 0?\r\n\r\nNo, it is called by all processes\r\n\r\n2. What does the `sync_dist` flag do:\r\n\r\nHere is the essential code:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/a72a7992a283f2eb5183d129a8cf6466903f1dc8/pytorch_lightning/core/step_result.py#L108-L115\r\nIf `sync_dist=True` then it will as default call the `sync_ddp` function which will sum the value across all processes using `torch.distributed.all_reduce` \r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/a72a7992a283f2eb5183d129a8cf6466903f1dc8/pytorch_lightning/utilities/distributed.py#L120\r\nUse this flag if you want to synchronize the value between different processes. \r\n\r\n3. How to print stuff in distributed lightning:\r\n\r\nRecommended is using either the `rank_zero_info` function. Import as:\r\n```python\r\nfrom pytorch_lightning.utilities import rank_zero_info\r\n```\r\nor use the `rank_zero_only` decorator (imported from the same module) which can be wrapped around any function such that it only gets called on `rank=0`. Each logger experiment is decorated with `rank_zero_experiment` which internally calls `rank_zero_only`\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/a72a7992a283f2eb5183d129a8cf6466903f1dc8/pytorch_lightning/loggers/base.py#L31-L43\r\n\r\n4. What about `pytorch_lightning.metrics` (now known as `torchmetrics`)\r\n\r\nOur own metrics have custom synchronization going on. Any metric will automatically synchronize between different processes whenever `metric.compute()` is called. Metrics calculated this way should therefore not be logged using `sync_dist=True`.\r\n\r\n5. Recommended way of logging:\r\n\r\nUsing `self.log` in your lightning module \r\n\r\nNot sure this answers all questions."
  },
  {
    "content": "I am using two optimizers and two schedules in my PL model:\r\n```python\r\ndef configure_optimizers(self):\r\n    # optimizers\r\n    opt1 = torch.optim.AdamW(\r\n        self.encoder_1.parameters(), \r\n        lr=self.hparams.lr_1,\r\n        weight_decay=self.hparams.weight_decay\r\n    )\r\n\r\n    opt2 = torch.optim.AdamW(\r\n        self.encoder_2.parameters(),\r\n        lr=self.hparams.lr_2,\r\n        weight_decay=self.hparams.weight_decay\r\n    )\r\n\r\n    step_size_up = round(0.07 * self.trainer.estimated_stepping_batches)\r\n\r\n    schdlr_1 = torch.optim.lr_scheduler.CyclicLR(opt1, mode='triangular2',\r\n                                                       base_lr=self.hparams.base_lr,\r\n                                                       max_lr=self.hparams.max_lr, step_size_up=step_size_up\r\n                                                       )\r\n    schdlr_2 = torch.optim.lr_scheduler.CyclicLR(opt2, mode='triangular2',\r\n                                                       base_lr=self.hparams.base_lr,\r\n                                                       max_lr=self.hparams.max_lr, step_size_up=step_size_up\r\n                                                       )\r\n\r\n    return (\r\n        {\"optimizer\": opt1, \"lr_scheduler\": schdlr_1, \"frequency\": self.hparams.frequency_1},\r\n        {\"optimizer\": opt2, \"lr_scheduler\": schdlr_2, \"frequency\": self.hparams.frequency_2},\r\n    )\r\n```\r\nHowever, inspecting the loss over the training steps revealed that only one learning rate was updated:\r\n\r\n![lr_scheduler](https://user-images.githubusercontent.com/11181748/193428975-fdd09902-ecd5-4277-aa4e-db17505f7dd3.png)\r\n\r\nIt happend because I've forgotten about specifying the scheduler's interval.\r\nThe correct desired config is something like:\r\n\r\n```python\r\nreturn (\r\n            {\"optimizer\": opt_1, \"lr_scheduler\": {\"scheduler\": schdlr_1, \"interval\": \"step\", \"name\": \"LRS-1\"}, \"frequency\": 1},\r\n            {\"optimizer\": opt_2, \"lr_scheduler\": {\"scheduler\": schdlr_1, \"interval\": \"step\", \"name\": \"LRS-2\"}, \"frequency\": 1}\r\n)\r\n```",
    "meta": {
      "name": "Training with dual (optimizer+scheduler) only one learning rate is updated over training steps"
    },
    "answer": "It happend because I've forgotten about specifying the scheduler's interval.\r\nThe correct desired config is something like:\r\n\r\n```python\r\nreturn (\r\n            {\"optimizer\": opt_1, \"lr_scheduler\": {\"scheduler\": schdlr_1, \"interval\": \"step\", \"name\": \"LRS-1\"}, \"frequency\": 1},\r\n            {\"optimizer\": opt_2, \"lr_scheduler\": {\"scheduler\": schdlr_1, \"interval\": \"step\", \"name\": \"LRS-2\"}, \"frequency\": 1}\r\n)\r\n```"
  },
  {
    "content": "I have max batch size of 4 in single gpu. If 2 gpus are used, should I increase the batch size to 8 such that each gpu gets 4 batches. Or I just keep it as 4 and PL will load 2 four-batches data to 2 gpus? \r\nBased on the doc, \r\n```\r\nIn DDP, DDP_SPAWN, Deepspeed, DDP_SHARDED, or Horovod your effective batch size will be 7 * devices * num_nodes.\r\n```\r\nI think it is the second case?\r\n\r\nI also have another problem related to ddp training, which is posted on this link below. \r\nhttps://forums.pytorchlightning.ai/t/how-to-initialize-tensors-that-are-in-the-right-device-when-ddp-are-used/1708\r\n\r\nI post it here for convenience.\r\n\r\nI am incorporating  a pytorch based model into the pl framework for ddp training. \r\nI have a lightning model \r\n```python\r\nclass ZfoldLightning(pl.LightningModule):\r\n    def __init__(self, hparams):\r\n        ...\r\n        self.model = XFold(MODEL_PARAM)\r\n```\r\nwhich initializes the `XFold` model in `__init__`.\r\nHowever, the XFold model contains many 'to device' code like `b = torch.randn(1).to(a.device)`, which is not recommended by PL. \r\nI tried to increase the batch size and train this model on two device. this does not work. OOM error appears. Turns out even DDP is used, I can only use the same batch size as that of single gpu. I think the reason is that all the tensors are stored in one gpu no matter how many gpus are ultized.\r\n\r\nOne solution is to refactor those **to device code** and use the recommended usage `a.type_as(b)`. But there are to many of code to refactor.\r\nI am wondering if there are better solutions?\r\nAny helps?I have solved my problem and find out that the answer is: each gpu get #batch_size batches. If you have batch_size of 2 and 2 gpus are utilized,  each gpu gets 2 batches and 4 batches in total are feed into a forward pass.",
    "meta": { "name": "Questions about effective batch size in ddp context" },
    "answer": "I have solved my problem and find out that the answer is: each gpu get #batch_size batches. If you have batch_size of 2 and 2 gpus are utilized,  each gpu gets 2 batches and 4 batches in total are feed into a forward pass."
  },
  {
    "content": "Hello pytorch-lightning community,\r\n\r\nmy training hangs when training on multi-nodes; on single node with multiple GPUs runs fine :/\r\nIt baffles me that although the global rank ID seems right, the member output has 4 instead of 8 in the denominator.\r\nSince I run in a slurm environment, do I have to add the SLURMEnvironment plugin in the Trainer? I tried to add it alongside the DDPPlugin but it was not accepted (Found invalid type for plugin <class 'pytorch_lightning.plugins.environments.slurm_environment.SLURMEnvironment'>. Expected a precision or training type plugin)\r\n\r\nThe job submission file has the corresponding lines:\r\n#SBATCH --gres=gpu:4\r\n#SBATCH --nodes=2\r\n#SBATCH --exclusive\r\n\r\nsrun --ntasks=8 python3 coolModel.py 2>&1 | tee log.train\r\n\r\nI attach the output and the code below...\r\npytorch version:  1.8.1\r\npytorch-lightning version:  1.2.4\r\n\r\nCheers,\r\nNikos\r\n\r\n[multNodeTraining.txt](https://github.com/PyTorchLightning/pytorch-lightning/files/6398914/multNodeTraining.txt)\r\n\r\n###########python code\r\nimport os\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nimport torchvision.transforms as transforms\r\n\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning import Trainer\r\n#from test_tube import Experiment\r\n\r\n\r\nclass database(pl.LightningDataModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    #def setup(self, stage=None):\r\n    def train_dataloader(self):\r\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\r\n\r\n\r\n\r\nclass CoolModel(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super(CoolModel, self).__init__()\r\n        # not the best model...\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def my_loss(self, y_hat, y):\r\n        return F.cross_entropy(y_hat, y)\r\n\r\n    def training_step(self, batch, batch_nb):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return {'loss': self.my_loss(y_hat, y)}\r\n\r\n    def validation_step(self, batch, batch_nb):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return {'val_loss': self.my_loss(y_hat, y)}\r\n\r\n    def validation_end(self, outputs):\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        return {'avg_val_loss': avg_loss}\r\n\r\n    def configure_optimizers(self):\r\n        return [torch.optim.Adam(self.parameters(), lr=0.02)]\r\n\r\n\r\n\r\nif __name__=='__main__':\r\n\r\n    dm = database()\r\n    model = CoolModel()\r\n    #exp = Experiment(save_dir=os.getcwd())\r\n    #checkp1 = pl.callbacks.ModelCheckpoint(\r\n    #    monitor='loss', save_top_k=2, dirpath='./', mode='min', save_last=True\r\n    #    ) \r\n    trainer = Trainer(\r\n            max_epochs=10,\r\n            gpus=4, num_nodes=2, accelerator='ddp',\r\n            plugins= pl.plugins.DDPPlugin(find_unused_parameters=False), #pl.plugins.SLURMEnvironment],\r\n            #progress_bar_refresh_rate=0\r\n            )\r\n    # train on 32 gpus across 4 nodes (make sure to submit appropriate SLURM job)\r\n    # trainer = Trainer(experiment=exp, max_nb_epochs=1, gpus=[0, 1, 2, 3, 4, 5, 6, 7], nb_gpu_nodes=4)\r\n    trainer.fit(model, dm)\r\n    # view tensorflow logs\r\n    print(f'View tensorboard logs by running\\ntensorboard --logdir {os.getcwd()}')\r\n    print('and going to http://localhost:6006 on your browser')\r\nHello Nikos\r\n\r\nDo you have 8 gpus in the node? I think it must match gres. \r\nDon't you also need to specify how many tasks per node in the SBATCH directive? [1]\r\nAlso, I notice some unsupported Trainer arguments in your script. It should be:\r\n`trainer = Trainer(max_epochs=1, gpus=[0, 1, 2, 3, 4, 5, 6, 7], num_nodes=4)\r\n`\r\nMake sure this script actually runs on CPU first before going to the cluster \ud83d\ude05 \r\n\r\nTotally no slurm expert here, just looking at your script with one eye closed. \r\n\r\n\r\n[1] https://pytorch-lightning.readthedocs.io/en/latest/clouds/cluster.html#slurm-managed-clusterI want to revisit this discussion because I find my self in a similar situation that I can't get out of.\r\nI submit a SLURM job with 2 nodes, 4 gpus per node and 4 tasks per node (1 task per gpu) as suggested by the lightning docs [here](https://pytorch-lightning.readthedocs.io/en/latest/clouds/cluster_advanced.html). \r\n\r\n```\r\n#SBATCH --nodes=2\r\n#SBATCH --ntasks-per-node=4\r\n#SBATCH --gpus-per-node=A100:4\r\n```\r\n\r\nwith `devices=4` and `num_nodes=2` in the Trainer.\r\n\r\nI get the following error although the first 4 ranks (gpus) have been initialized as follows\r\n`Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8`\r\n`Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8`\r\n`Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8`\r\n`Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8`\r\n\r\nCan you help me?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/local/tmp.581850/9.train.py\", line 156, in <module>\r\n    main(args, config)\r\n  File \"/local/tmp.581850/9.train.py\", line 138, in main\r\n    trainer.fit(model, datamodule=data, ckpt_path=None)\r\n  File \"/envs/pytorch-env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 696, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/envs/pytorch-env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/envs/pytorch-env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 735, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/envs/pytorch-env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1102, in _run\r\n    self.strategy.setup_environment()\r\n  File \"/envs/pytorch-env/lib/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 157, in setup_environment\r\n    self.setup_distributed()\r\n  File \"/envs/pytorch-env/lib/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 209, in setup_distributed\r\n    self._process_group_backend = self._get_process_group_backend()\r\n  File \"/envs/pytorch-env/lib/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 216, in _get_process_group_backend\r\n    or get_default_process_group_backend_for_device(self.root_device)\r\n  File \"/envs/pytorch-env/lib/python3.9/site-packages/pytorch_lightning/strategies/ddp.py\", line 123, in root_device\r\n    return self.parallel_devices[self.local_rank]\r\nIndexError: list index out of range\r\n```",
    "meta": { "name": "Problem in multi-node training" },
    "answer": "Hello Nikos\r\n\r\nDo you have 8 gpus in the node? I think it must match gres. \r\nDon't you also need to specify how many tasks per node in the SBATCH directive? [1]\r\nAlso, I notice some unsupported Trainer arguments in your script. It should be:\r\n`trainer = Trainer(max_epochs=1, gpus=[0, 1, 2, 3, 4, 5, 6, 7], num_nodes=4)\r\n`\r\nMake sure this script actually runs on CPU first before going to the cluster \ud83d\ude05 \r\n\r\nTotally no slurm expert here, just looking at your script with one eye closed. \r\n\r\n\r\n[1] https://pytorch-lightning.readthedocs.io/en/latest/clouds/cluster.html#slurm-managed-cluster"
  },
  {
    "content": "Hello, \r\n\r\nI have a pre-trained model that I had trained from scratch using Densenet Architecture with size output  2048. \r\nI am trying to use my pretrained model for finetuning/transfer learning on a downstream task.\r\nHowever, I keep having size mismatch issues at the loss computation line and loss explosion. I am thinking it has something to do with batch size not in tune with each other.\r\nPlease see a minimal reproduction of my code below:\r\n\r\n\r\n```python\r\nclass DownstreamTask(pl.LightningModule):\r\n    def __init__(self, pre_model, lr=LR):\r\n        super().__init__()\r\n        self.network = pre_model\r\n        self.fc = nn.Sequential(nn.Linear(2048,22)) \r\n        self.learning_rate = lr\r\n\r\n    def forward(self, x):\r\n        features = self.gaze_network(x)\r\n        features = features.view(features.size(0), -1)\r\n        gaze = self.fc(features)\r\n        return gaze\r\n\r\n    def training_step(self, batch):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.l1_loss(y_hat, y)\r\n        self.log(\"my_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\r\n        return {'loss': loss}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\r\n\r\nmodel = DenseNet()\r\nmodel.load_state_dict(torch.load(PATH))\r\nmodel.eval()\r\n\r\ntrain_loader = DataLoader(TrainLoader(data_dir, batch_size, num_workers), batch_size=batch_size, shuffle=True, num_workers=int(num_workers))\r\n\r\nlearner = learner = DownstreamTask(model)\r\ntrainer = pl.Trainer(accelerator='gpu', devices=num_gpus, max_epochs=epochs, strategy='ddp', num_nodes=num_nodes,)\r\n\r\ntrainer.fit(learner, train_loader)\r\n```\r\n\r\nHere is the error I got:\r\n\r\n```\r\n/trainer.py:44: UserWarning: Using a target size (torch.Size([10, 2])) that is different to the input size (torch.Size([490, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\r\n  loss = F.l1_loss(y_hat, y)\r\n\r\nFile \"/usersDownstream/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py\", line 79, in forward\r\n    output = self.module.training_step(*inputs, **kwargs)\r\n  File \"/users/trainer.py\", line 44, in training_step\r\n    loss = F.l1_loss(y_hat, y)\r\n  File \"/users/Downstream/lib/python3.10/site-packages/torch/nn/functional.py\", line 3248, in l1_loss\r\n    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\r\n  File \"/users/Downstream/lib/python3.10/site-packages/torch/functional.py\", line 73, in broadcast_tensors\r\n    return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]\r\nRuntimeError: The size of tensor a (490) must match the size of tensor b (10) at non-singleton dimension 0\r\n```\r\n\r\nI have tried to resize/reshape but nothing seems to work. It looks like there is a batch size mismatch. \r\nI am seriously confused, I will appreciate any help. Many thanks.\r\n@Estabi I believe this is irrelevant to PL. Have you checked the sizes of `y_hat` and `y` match? See PyTorch docs:\u3000https://pytorch.org/docs/1.12/generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss",
    "meta": { "name": "Batch size mismatch during loss computation" },
    "answer": "@Estabi I believe this is irrelevant to PL. Have you checked the sizes of `y_hat` and `y` match? See PyTorch docs:\u3000https://pytorch.org/docs/1.12/generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss"
  },
  {
    "content": "Hello!\r\n\r\nI'm attempting to do some simple adversarial training with lightning but I'm running in some issues for the testing part.\r\nMy model is a lightning module with a base model and an attack model (from torchattacks).\r\nDuring adversarial training adversarial images are first computed with the attack and then used for the training.\r\nDuring validation and test both clean and adversarial images are used.\r\nSince gradients are required to compute adversarial images I simply enable gradients in validation_step / testing_step:\r\n```python\r\n        with torch.enable_grad():\r\n            adv_img = self.atk(imgs, labels)\r\n```\r\n\r\nThis works fine during training (when doing trainer.fit(model), but fails during testing (trainer.test(model)), with \r\n```python\r\n\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n```\r\n\r\nI checked similar problems and the solutions was to enable gradients (which I did) or remove automatic optimization but I would like to retain the possibility of accumulating gradients, and since trainer.fit works fine I don't see why I would need to do manual optimization.\r\n\r\nI did some digging and the enabling of gradients seems to work:\r\nAll the model parameters.requires_grad and adv_images.requires_grad are True in the attack code:\r\n[https://github.com/Harry24k/adversarial-attacks-pytorch/blob/master/torchattacks/attacks/pgd.py](https://github.com/Harry24k/adversarial-attacks-pytorch/blob/712144154f580ff8a1f492bde6c154f14e8556a6/torchattacks/attacks/pgd.py#L62)\r\nbut outputs.requires_grad is False. even if I had a second (probably redundant) with torch.enable_grad() in the attack when computing outputs.\r\nUsing torch.set_grad_enabled(True) does not change this.\r\n\r\n\r\nSame thing happens if I do trainer.validate(model). Also when using other attacks than PGD.\r\n\r\nAny idea why is that and how I can fix it?\r\nWhat is intriguing is that it works when doing trainer.fit...\r\n\r\nReproducible in [colab](https://colab.research.google.com/drive/1GKriw-4OB0vNndW2pmb8Man-zMvL0FzV#scrollTo=LU6Nj3Vh_KeS) \r\n\r\nOr full script to reproduce:\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom torch import nn\r\nimport torch\r\nfrom torchvision.datasets import MNIST\r\nfrom torch.utils.data import DataLoader\r\nimport torchvision.transforms as T\r\nimport torchattacks\r\n\r\nclass adv_model(pl.LightningModule):\r\n    def __init__(self,\r\n                 model,\r\n                 attack=None,\r\n                 loaders=None,\r\n                 loss_fn=nn.CrossEntropyLoss(),\r\n                 optim=\"AdamW\",\r\n                 clean=False,\r\n                 lr=0.01\r\n                 ):\r\n        super().__init__()\r\n        self.model = model\r\n        self.loss_fn = loss_fn\r\n        self.loaders = loaders\r\n        self.atk = attack\r\n        self.clean = clean\r\n        self.lr = lr\r\n        if optim is None:\r\n            self.optim = torch.optim.AdamW\r\n        elif optim == \"AdamW\":\r\n            self.optim = torch.optim.AdamW\r\n        elif optim == \"Adam\":\r\n            self.optim = torch.optim.Adam\r\n        elif optim == \"SGD\":\r\n            self.optim = torch.optim.SGD\r\n        else:\r\n            raise ValueError(f\"Optim should be in '[AdamW, Adam, SGD]', not {optim}\")\r\n\r\n    def forward(self, x, clean=None):\r\n        return self.model(x)\r\n\r\n    def training_step(self, batch, batch_nb):\r\n        imgs, labels = batch\r\n        if not self.clean:\r\n            imgs = self.atk(imgs, labels)\r\n        logits = self.model(imgs)\r\n        loss = self.loss_fn(logits, labels)\r\n        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\r\n        acc = (logits.argmax(dim=1)).eq(labels).sum().item() / len(imgs)\r\n        self.log(\"train_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\r\n\r\n        return {\"loss\": loss, \"acc\": acc}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        imgs, labels = batch\r\n\r\n        clean_logits = self.model(imgs)\r\n        clean_loss = self.loss_fn(clean_logits, labels)\r\n        clean_acc = (clean_logits.argmax(dim=1)).eq(labels).sum().item() / len(imgs)\r\n        self.log(\"clean_val_loss\", clean_loss, prog_bar=True)\r\n        self.log('clean_val_acc', clean_acc, prog_bar=True)\r\n        if self.clean:\r\n            return clean_loss, clean_acc\r\n        # computing adversarial accuracy and loss\r\n        with torch.enable_grad():\r\n            adv_img = self.atk(imgs, labels)\r\n        adv_logits = self.model(adv_img)\r\n        adv_loss = self.loss_fn(adv_logits, labels)\r\n        self.log(\"adv_val_loss\", adv_loss, prog_bar=True)\r\n        adv_acc = (adv_logits.argmax(dim=1)).eq(labels).sum().item() / len(imgs)\r\n        self.log('adv_val_acc', adv_acc, prog_bar=True)\r\n\r\n        return clean_loss, clean_acc, adv_loss, adv_acc\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        imgs, labels = batch\r\n\r\n        clean_logits = self.model(imgs)\r\n        clean_loss = self.loss_fn(clean_logits, labels)\r\n        clean_acc = (clean_logits.argmax(dim=1)).eq(labels).sum().item() / len(imgs)\r\n\r\n        self.log(\"clean_test_loss\", clean_loss, prog_bar=True)\r\n        self.log('clean_test_acc', clean_acc, prog_bar=True)\r\n        if self.clean:\r\n            return clean_loss, clean_acc\r\n        # computing adversarial accuracy and loss\r\n        with torch.enable_grad():\r\n            adv_img = self.atk(imgs, labels)\r\n        adv_logits = self.model(adv_img)\r\n        adv_loss = self.loss_fn(adv_logits, labels)\r\n        self.log(\"adv_test_loss\", adv_loss, prog_bar=True)\r\n        adv_acc = (adv_logits.argmax(dim=1)).eq(labels).sum().item() / len(imgs)\r\n        self.log('adv_test_acc', adv_acc, prog_bar=True)\r\n\r\n        return clean_loss, clean_acc, adv_loss, adv_acc\r\n\r\n    def configure_optimizers(self):\r\n        optim = self.optim\r\n        if issubclass(optim, torch.optim.SGD):\r\n            if self.lr is not None:\r\n                return optim(self.model.parameters(), lr=self.lr, momentum=0.9, weight_decay=1e-4)\r\n            else:\r\n                return optim(self.model.parameters(), momentum=0.9, weight_decay=1e-4)\r\n        elif issubclass(optim, (torch.optim.Adam, torch.optim.AdamW)):\r\n            if self.lr is not None:\r\n                return optim(self.model.parameters(), lr=self.lr, weight_decay=1e-4)\r\n            else:\r\n                return optim(self.model.parameters(), weight_decay=1e-4)\r\n        else:\r\n            return self.optim\r\n\r\n    def train_dataloader(self):\r\n        return self.loaders[0]\r\n\r\n    def val_dataloader(self):\r\n\r\n        return self.loaders[1]\r\n\r\n    def test_dataloader(self):\r\n        return self.loaders[2]\r\n\r\n\r\ntrainer = pl.Trainer(accelerator=\"gpu\",\r\n                     max_epochs=3,\r\n                     val_check_interval=1.0,\r\n\r\n                     )\r\n\r\nbase_model = torch.nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(),\r\n                                 nn.Linear(256, 256), nn.ReLU(),\r\n                                 nn.Linear(256, 10))\r\n\r\ntrain_set = MNIST(root=\"./\",\r\n                  transform=T.ToTensor(),\r\n                  download=True,\r\n                  train=True\r\n                  )\r\n\r\ntest_set = MNIST(root=\"./\",\r\n                 transform=T.ToTensor(),\r\n                 download=True,\r\n                 train=False\r\n                 )\r\n\r\ntrain_loader = DataLoader(train_set, batch_size=100, shuffle=True, num_workers=2)\r\ntest_loader = DataLoader(test_set, batch_size=1000, shuffle=False, num_workers=2)\r\nval_loader = DataLoader(test_set, batch_size=1000, shuffle=False, num_workers=2)\r\nloaders = (train_loader, val_loader, test_loader)\r\n\r\natk = torchattacks.PGD(model=base_model.cuda(), steps=10)\r\nmodel = adv_model(base_model,\r\n                  loaders=loaders,\r\n                  attack=atk,\r\n                  clean=False,\r\n                  optim=\"Adam\")\r\n\r\n\r\ntrainer.fit(model)\r\n\r\ntrainer.test(model)\r\n```\r\nAnswering to myself:\r\nAfter more digging, it seems that it is the use of torch.inference_mode that is the cause of the issue.\r\nUsing torch.no_grad is not enough to get out of inference_mode.\r\nIn fact getting out of inference_mode with e.g with torch.inference_mode(mode=False) or a decorator is not enough, I then have a problem\r\n with Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.\r\n \r\n For now the solution I have is to change the function\r\n ```python\r\n @contextmanager\r\ndef _evaluation_context(accelerator: Accelerator) -> Generator:\r\n    # inference mode is not supported with gloo backend (#9431),\r\n    # and HPU & TPU accelerators.\r\n    context_manager_class = (\r\n        torch.inference_mode\r\n        if not (dist.is_initialized() and dist.get_backend() == \"gloo\")\r\n        and not isinstance(accelerator, HPUAccelerator)\r\n        and not isinstance(accelerator, TPUAccelerator)\r\n        else torch.no_grad\r\n    )\r\n    with context_manager_class():\r\n        yield\r\n```\r\nto always use torch.no_grad (l2794 in trainer.py). \r\nor use older pytorch/lightning versions...\r\n\r\nIf there is a simple alternative to use in the test_step or some parameters to force the use of no_grad instead of inference_mode I'm all ears.",
    "meta": { "name": "Adversarial training with lightning" },
    "answer": "Answering to myself:\r\nAfter more digging, it seems that it is the use of torch.inference_mode that is the cause of the issue.\r\nUsing torch.no_grad is not enough to get out of inference_mode.\r\nIn fact getting out of inference_mode with e.g with torch.inference_mode(mode=False) or a decorator is not enough, I then have a problem\r\n with Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.\r\n \r\n For now the solution I have is to change the function\r\n ```python\r\n @contextmanager\r\ndef _evaluation_context(accelerator: Accelerator) -> Generator:\r\n    # inference mode is not supported with gloo backend (#9431),\r\n    # and HPU & TPU accelerators.\r\n    context_manager_class = (\r\n        torch.inference_mode\r\n        if not (dist.is_initialized() and dist.get_backend() == \"gloo\")\r\n        and not isinstance(accelerator, HPUAccelerator)\r\n        and not isinstance(accelerator, TPUAccelerator)\r\n        else torch.no_grad\r\n    )\r\n    with context_manager_class():\r\n        yield\r\n```\r\nto always use torch.no_grad (l2794 in trainer.py). \r\nor use older pytorch/lightning versions...\r\n\r\nIf there is a simple alternative to use in the test_step or some parameters to force the use of no_grad instead of inference_mode I'm all ears."
  },
  {
    "content": "use this code, i can get test data. But when i use pl data module to fit train model, i got dataloader returned 0 length error\r\n```Python\r\nimport os\r\nfrom typing import Optional\r\nimport PIL\r\nimport cv2\r\nimport json\r\nimport copy\r\nimport numpy as np\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom torchvision import transforms\r\nfrom torch.utils.data import Dataset, random_split, DataLoader\r\n\r\nfrom det.det_modules import ResizeShortSize, IaaAugment, EastRandomCropData, MakeBorderMap, MakeShrinkMap\r\n\r\n\r\ndef load_json(file_path: str):\r\n    with open(file_path, 'r', encoding='utf8') as f:\r\n        content = json.load(f)\r\n    return content\r\n\r\n\r\nclass ICDARDataset(Dataset):\r\n    def __init__(self, json_path, img_path, is_train=True):\r\n        self.ignore_tags = ['*', '###']\r\n        self.load_char_annotation = False\r\n        self.data_list = self.load_data(json_path, img_path)\r\n        self.transform = transforms.Compose(\r\n            [\r\n                transforms.ToTensor(),\r\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n            ]\r\n        )\r\n        self.iaa_augment = IaaAugment()\r\n        self.east_random_crop_data = EastRandomCropData()\r\n        self.make_border_map = MakeBorderMap()\r\n        self.make_shrink_map = MakeShrinkMap()\r\n        self.resize = ResizeShortSize(short_size=736, resize_text_polys=False)\r\n        self.is_train = is_train\r\n\r\n    def load_data(self, json_path: str, img_path) -> list:\r\n        data_list = []\r\n        content = load_json(json_path)\r\n        for item in content:\r\n            p = os.path.join(img_path, item + '.jpg')\r\n            polygons = []\r\n            texts = []\r\n            illegibility_list = []\r\n            for annotation in content[item]:\r\n                if len(annotation['points']) == 0 or len(annotation['transcription']) == 0:\r\n                    continue\r\n                polygons.append(annotation['points'])\r\n                texts.append(annotation['transcription'])\r\n                illegibility_list.append(annotation['illegibility'])\r\n            data_list.append(\r\n                {\r\n                    'img_path': p,\r\n                    'text_polys': np.array(polygons, dtype=object),\r\n                    'texts': texts,\r\n                    'ignore_tags': illegibility_list\r\n                }\r\n            )\r\n        return data_list\r\n\r\n    def __getitem__(self, index):\r\n        data = self.data_list[index]\r\n        im = cv2.imread(data['img_path'])\r\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\r\n\r\n        data['img'] = im\r\n        data['shape'] = [im.shape[0], im.shape[1]]\r\n        if self.is_train:\r\n            data = self.iaa_augment(data)\r\n            data = self.east_random_crop_data(data)\r\n            data = self.make_border_map(data)\r\n            data = self.make_shrink_map(data)\r\n        else:\r\n            data = self.resize(data)\r\n        # resize = ResizeShortSize(short_size=736, resize_text_polys=False)\r\n        # data = resize(data)\r\n        data['img'] = self.transform(data['img'])\r\n        data['text_polys'] = data['text_polys']\r\n        return copy.deepcopy(data)\r\n\r\n    def __len__(self):\r\n        return len(self.data_list)\r\n\r\n\r\nclass DetCollectFN:\r\n    def __init__(self, *args, **kwargs):\r\n        pass\r\n\r\n    def __call__(self, batch):\r\n        data_dict = {}\r\n        to_tensor_keys = []\r\n        for sample in batch:\r\n            for k, v in sample.items():\r\n                if k not in data_dict:\r\n                    data_dict[k] = []\r\n                if isinstance(v, (np.ndarray, torch.Tensor, PIL.Image.Image)):\r\n                    if k not in to_tensor_keys:\r\n                        to_tensor_keys.append(k)\r\n                    if isinstance(v, np.ndarray):\r\n                        v = torch.tensor(v)\r\n                    if isinstance(v, PIL.Image.Image):\r\n                        v = transforms.ToTensor()(v)\r\n                data_dict[k].append(v)\r\n        for k in to_tensor_keys:\r\n            data_dict[k] = torch.stack(data_dict[k], 0)\r\n        return data_dict\r\n\r\n\r\nclass DBDataModule(pl.LightningDataModule):\r\n    def __init__(self, train_json_path, train_img_path, val_json_path, val_img_path):\r\n        super(DBDataModule, self).__init__()\r\n        self.train = ICDARDataset(train_json_path, train_img_path, is_train=True)\r\n        self.val = ICDARDataset(val_json_path, val_img_path, is_train=False)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.train, batch_size=32, num_workers=0, shuffle=True, collate_fn=DetCollectFN)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.val, batch_size=32, num_workers=0, collate_fn=DetCollectFN)\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    import torch\r\n    from torch.utils.data import DataLoader\r\n\r\n    from matplotlib import pyplot as plt\r\n\r\n\r\n    def show_img(imgs: np.ndarray, title='img'):\r\n        from matplotlib import pyplot as plt\r\n        color = (len(imgs.shape) == 3 and imgs.shape[-1] == 3)\r\n        imgs = np.expand_dims(imgs, axis=0)\r\n        for i, img in enumerate(imgs):\r\n            plt.figure()\r\n            plt.title('{}_{}'.format(title, i))\r\n            plt.imshow(img, cmap=None if color else 'gray')\r\n\r\n\r\n    def draw_bbox(img_path, result, color=(255, 0, 0), thickness=2):\r\n        import cv2\r\n        if isinstance(img_path, str):\r\n            img_path = cv2.imread(img_path)\r\n            # img_path = cv2.cvtColor(img_path, cv2.COLOR_BGR2RGB)\r\n        img_path = img_path.copy()\r\n        for point in result:\r\n            # point = point.astype(int)\r\n            cv2.polylines(img_path, [point], True, color, thickness)\r\n        return img_path\r\n\r\n\r\n    dataset = ICDARDataset('/home/data/OCRData/icdar2019/train/train.json', '/home/data/OCRData/icdar2019/train/images')\r\n    print(len(dataset))\r\n    train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True, num_workers=0)\r\n    for i, data in enumerate(train_loader):\r\n        img = data['img']\r\n        shrink_label = data['shrink_map']\r\n        threshold_label = data['threshold_map']\r\n\r\n        print(threshold_label.shape, threshold_label.shape, img.shape)\r\n        show_img(img[0].numpy().transpose(1, 2, 0), title='img')\r\n        show_img((shrink_label[0].to(torch.float)).numpy(), title='shrink_label')\r\n        show_img((threshold_label[0].to(torch.float)).numpy(), title='threshold_label')\r\n        # img = draw_bbox(img[0].numpy().transpose(1, 2, 0), np.array(data['text_polys']))\r\n        # show_img(img, title='draw_bbox')\r\n        plt.show()\r\n\r\n        break\r\n\r\n```Dear @morestart,\r\n\r\nWould you mind unit-testing your code ? Can you check your `DBDataModule` train and `val` ICDARDataset length aren't 0 ?\r\nLightning doesn't manipulate your dataset / dataloaders, so maybe your dataset are empty.\r\n\r\nBest,\r\nT.C",
    "meta": {
      "name": "ValueError: `Dataloader` returned 0 length. Please make sure that it returns at least 1 batch"
    },
    "answer": "Dear @morestart,\r\n\r\nWould you mind unit-testing your code ? Can you check your `DBDataModule` train and `val` ICDARDataset length aren't 0 ?\r\nLightning doesn't manipulate your dataset / dataloaders, so maybe your dataset are empty.\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "Hi, I started noticing the following warning message after setting up a new conda environment with Pytorch 1.8.1, which is an update from my previous environment that uses Pytorch 1.7.0.\r\n\r\n> Epoch 0:   0%|     \r\n[W reducer.cpp:1050] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance. If your model indeed never has any unused parameters, consider turning this flag off. Note that this warning may be a false positive your model has flow control causing later iterations to have unused parameters. (function operator())\r\n\r\nAny idea if this is a real concern? How can we disable `find_unused_parameters`?\r\n\r\n\r\n```py\r\ntrainer = pl.Trainer(\r\n    val_check_interval=0.1,\r\n    gpus=-1,\r\n    accelerator=\"ddp\",\r\n    callbacks=[checkpoint_callback, early_stop_callback],\r\n    precision=16,\r\n)\r\n```\r\n\r\nPackages:\r\n- pytorch 1.8.1\r\n- pytorch-lightning 1.2.6\r\n- cudatoolkit 11.1.1\r\n- cudnn 8.0.5\r\n- python 3.8\r\nHi @athenawisdoms the docs here cover how you can disable `find_unused_parameters` and speed up your DDP training\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/benchmarking/performance.html#when-using-ddp-set-find-unused-parameters-falseHi! I added this warning in native PyTorch as a way to remind users to disable this flag if performance is critical and there are no unused parameters. One note is - as the warning indicates it can be a false positive if your model has flow control that results in unused params in future iterations. Hi. The warning could disappear by adding plugins as below.\r\n\r\n```python\r\nfrom pytorch_lightning.plugins import DDPPlugin\r\n\r\ntrainer = pl.Trainer(\r\n    val_check_interval=0.1,\r\n    gpus=-1,\r\n    accelerator=\"ddp\",\r\n    callbacks=[checkpoint_callback, early_stop_callback],\r\n    plugins=DDPPlugin(find_unused_parameters=False),\r\n    precision=16,\r\n)\r\n```Hello. For those who are using pl 1.6.0, use `DDPStrategy` instead of `DDPPlugin`, which is deprecated after 1.6.0:\r\n```python\r\nfrom pytorch_lightning.strategies.ddp import DDPStrategy\r\ntrainer = pl.Trainer(\r\n  strategy = DDPStrategy(find_unused_parameters=False),\r\n  accelerator = 'gpu',\r\n  devices = 3\r\n)\r\n```",
    "meta": {
      "name": "Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance."
    },
    "answer": "Hi @athenawisdoms the docs here cover how you can disable `find_unused_parameters` and speed up your DDP training\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/benchmarking/performance.html#when-using-ddp-set-find-unused-parameters-false"
  },
  {
    "content": "Hey! I have a question regarding this library. Really, like how it forces me to structure my code better. I encountered one problem I did not know how to solve based on the documentation.\r\n\r\nLet's say I have two optimizers for two parts of the network, e.g. my `configure_optimizers()` looks like this:\r\n```\r\n    def configure_optimizers(self):\r\n        optimizer_encoder = optim.Adam(self.encoder.parameters(), ...)\r\n        optimizer_decoder = optim.Adam(self.decoder.parameters(), ...)\r\n        return [optimizer_encoder, optimizer_decoder]\r\n```\r\n\r\nnow in the training loop I forward pass the encoder, then the decoder and compute my loss based on the output: \r\n```\r\n    def training_step(self, batch, batch_nb, optimizer_idx):\r\n        inp, gt = ...\r\n\r\n        encoding = self.encoder(inp)\r\n        pred = self.decoder(encoding)\r\n\r\n        loss = F.mse_loss(pred, gt)\r\n        return {'loss': loss}\r\n```\r\n Since I have two optimizers I have to respect that this function is called two times with different `optimizer_idx` however I just have one loss to backprop. How would I go about this?  \r\n\r\n#### What have you tried? \r\nI tried something like this\r\n```\r\n    def training_step(self, batch, batch_nb, optimizer_idx):\r\n        if optimizer_idx == 1:\r\n              return {}\r\n        inp, gt = ...\r\n\r\n        encoding = self.encoder(inp)\r\n        pred = self.decoder(encoding)\r\n\r\n        loss = F.mse_loss(pred, gt)\r\n        return {'loss': loss}\r\n```\r\nHowever, this leads to an error since no loss key is present in `trainer.py:1392`.\r\nin that case, just pass in both sets of params to a single optimizerBut I explicitly want two different learning rates for different parts of the network. That is not really possible with a single optimizer AFAIK. One possibility could be to scale gradients on the weights for which I want lower learning rate before running the optimizer but that is really not a a clean solution. It is possible with parameter groups using a single optimizer. Your use-case is actually the example in the docs: https://pytorch.org/docs/stable/optim.html#per-parameter-optionsThat\u2019s nice. Thank you for hint!Is it possible #14728?",
    "meta": { "name": "Multiple optimizers but only one loss" },
    "answer": "in that case, just pass in both sets of params to a single optimizer"
  },
  {
    "content": "I am trying to train a big styleGAN model on 4 v100, I used the `ddp_sharded` strategy,\r\nAt the end of the first epoch, the training ends with an error.\r\nI think it came from the model checkpointing at the end of each epoch, do you have maybe a solution to perform the checkpointing without this error?\r\nI also have issues visualizing how the model is shared between the multiple gpus.\r\n\r\n```\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\r\n    results = self._run_stage()\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\r\n    return self._run_train()\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\", line 205, in run\r\n    self.on_advance_end()\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py\", line 294, in on_advance_end\r\n    self.trainer._call_callback_hooks(\"on_train_epoch_end\")\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1636, in _call_callback_hooks\r\n    fn(self, self.lightning_module, *args, **kwargs)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 308, in on_train_epoch_end\r\n    self._save_topk_checkpoint(trainer, monitor_candidates)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 381, in _save_topk_checkpoint\r\n    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 661, in _save_none_monitor_checkpoint\r\n    self._save_checkpoint(trainer, filepath)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 384, in _save_checkpoint\r\n    trainer.save_checkpoint(filepath, self.save_weights_only)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 2467, in save_checkpoint\r\n    self._checkpoint_connector.save_checkpoint(filepath, weights_only=weights_only, storage_options=storage_options)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 444, in save_checkpoint\r\n    _checkpoint = self.dump_checkpoint(weights_only)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 380, in dump_checkpoint\r\n    optimizer_state = self.trainer.strategy.optimizer_state(optimizer)\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/pytorch_lightning/strategies/sharded.py\", line 117, in optimizer_state\r\n    optimizer.consolidate_state_dict()\r\n  File \"/home/2022022/02/packages_latent/lib/python3.9/site-packages/fairscale/optim/oss.py\", line 364, in consolidate_state_dict\r\n    dist.broadcast_object_list(\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 1840, in broadcast_object_list\r\n    object_list[i] = _tensor_to_object(obj_view, obj_size)\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 1532, in _tensor_to_object\r\n    return _unpickler(io.BytesIO(buf)).load()\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/storage.py\", line 161, in _load_from_bytes\r\n    return torch.load(io.BytesIO(b))\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/serialization.py\", line 608, in load\r\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/serialization.py\", line 787, in _legacy_load\r\n    result = unpickler.load()\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/serialization.py\", line 743, in persistent_load\r\n    deserialized_objects[root_key] = restore_location(obj, location)\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/serialization.py\", line 175, in default_restore_location\r\n    result = fn(storage, location)\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/serialization.py\", line 155, in _cuda_deserialize\r\n    return storage_type(obj.size())\r\n  File \"/soft/Python3-DL/conda/2010_4.8.3/envs/pydl-2112/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 606, in _lazy_new\r\n    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\r\nRuntimeError: CUDA error: out of memory\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n```> optimizer.consolidate_state_dict()\r\n\r\nthis seems to be consolidating state dicts from all the ranks on rank 0, causing memory issues. If you just need the model's weights to be saved in the checkpoint, you can set `ModelCheckpoint(..., save_weights_only=True)`.\r\n\r\nNote that if you do that, you will not be able to resume the training using such a checkpoint.Hi,\r\nI am still interested to know if there is a native solution in PL to perform the checkpointing on the CPU to save this issue, or if the only solution is to write my own ModelCheckpoint?Ok, DeepSpeed with one optimizer is working quite well with only little changes in the code.\r\n\r\nI still have an issue, when training on 1 node with 4 GPUs everything works fine. When switching to 2 nodes with 4 GPUs per node, I get CUDA out of memory which is interesting?\r\n\r\nIf anyone has the same problem with 2 optimizers, I just did the simple solution to have 1 optimizer and set either the Generator or the Discriminator `requires_grad` to True or False.\r\nThe other limitation of DeepSpeed is that, I think, you can only do one `.step()` per training step on the optimizer.\r\n```\r\n\tdef configure_optimizers(self):\r\n\t\toptimizer = torch.optim.Adam(\r\n\t\t\t[\r\n\t\t\t\t{'params': self.generators.parameters(), 'lr': self.lr * g_reg_ratio, 'betas': (0 ** g_reg_ratio, 0.99 ** g_reg_ratio)},\r\n\t\t\t\t{'params': self.discriminator.parameters(), 'lr': self.lr * d_reg_ratio, 'betas': (0 ** d_reg_ratio, 0.99 ** d_reg_ratio)},\r\n\t\t\t],\r\n\t\t)\r\n\t\treturn optimizer\r\n\r\n\tdef _set_grad_step(self, is_generator: bool):\r\n\t\tfor p in self.generators.parameters():\r\n\t\t\tp.requires_grad = is_generator\r\n\t\tfor p in self.discriminator.parameters():\r\n\t\t\tp.requires_grad = not is_generator\r\n\r\n\tdef single_training_step(self, batch, batch_idx):\r\n\t\tif batch_idx % 2 == 0:\r\n\t\t\tself._set_grad_step(is_generator=False)\r\n\t\t\treturn self.training_step_discriminator(reals=batch, batch_idx=batch_idx)\r\n\t\telse:\r\n\t\t\tself._set_grad_step(is_generator=True)\r\n\t\t\treturn self.training_step_generator(reals=batch, batch_idx=batch_idx)\r\n```",
    "meta": { "name": "ddp_sharded crash during model save" },
    "answer": "Ok, DeepSpeed with one optimizer is working quite well with only little changes in the code.\r\n\r\nI still have an issue, when training on 1 node with 4 GPUs everything works fine. When switching to 2 nodes with 4 GPUs per node, I get CUDA out of memory which is interesting?\r\n\r\nIf anyone has the same problem with 2 optimizers, I just did the simple solution to have 1 optimizer and set either the Generator or the Discriminator `requires_grad` to True or False.\r\nThe other limitation of DeepSpeed is that, I think, you can only do one `.step()` per training step on the optimizer.\r\n```\r\n\tdef configure_optimizers(self):\r\n\t\toptimizer = torch.optim.Adam(\r\n\t\t\t[\r\n\t\t\t\t{'params': self.generators.parameters(), 'lr': self.lr * g_reg_ratio, 'betas': (0 ** g_reg_ratio, 0.99 ** g_reg_ratio)},\r\n\t\t\t\t{'params': self.discriminator.parameters(), 'lr': self.lr * d_reg_ratio, 'betas': (0 ** d_reg_ratio, 0.99 ** d_reg_ratio)},\r\n\t\t\t],\r\n\t\t)\r\n\t\treturn optimizer\r\n\r\n\tdef _set_grad_step(self, is_generator: bool):\r\n\t\tfor p in self.generators.parameters():\r\n\t\t\tp.requires_grad = is_generator\r\n\t\tfor p in self.discriminator.parameters():\r\n\t\t\tp.requires_grad = not is_generator\r\n\r\n\tdef single_training_step(self, batch, batch_idx):\r\n\t\tif batch_idx % 2 == 0:\r\n\t\t\tself._set_grad_step(is_generator=False)\r\n\t\t\treturn self.training_step_discriminator(reals=batch, batch_idx=batch_idx)\r\n\t\telse:\r\n\t\t\tself._set_grad_step(is_generator=True)\r\n\t\t\treturn self.training_step_generator(reals=batch, batch_idx=batch_idx)\r\n```"
  },
  {
    "content": "For example, if I wanted to set the option for `write_interval` in the [BasePredictionWriter](https://pytorch-lightning.readthedocs.io/en/stable/_modules/pytorch_lightning/callbacks/prediction_writer.html#BasePredictionWriter) from the LightningCLI's `trainer.callbacks` flag, would there be a syntax for this? As far as I can tell, one can only pass the name of the callback class.The syntax is described in https://pytorch-lightning.readthedocs.io/en/stable/cli/lightning_cli_advanced_3.html#trainer-callbacks-and-arguments-with-class-type. As command line arguments would be:\r\n```shell\r\n$ python ... \\\r\n    --trainer.callbacks+=BasePredictionWriter \\\r\n    --trainer.callbacks.write_interval=batch \\\r\n    ...\r\n```\r\nOr in a config file:\r\n```yaml\r\ntrainer:\r\n  callbacks:\r\n    - class_path: BasePredictionWriter\r\n      init_args:\r\n        write_interval: batch\r\n```",
    "meta": {
      "name": "Is it possible to pass arguments to callbacks in the LightningCLI?"
    },
    "answer": "The syntax is described in https://pytorch-lightning.readthedocs.io/en/stable/cli/lightning_cli_advanced_3.html#trainer-callbacks-and-arguments-with-class-type. As command line arguments would be:\r\n```shell\r\n$ python ... \\\r\n    --trainer.callbacks+=BasePredictionWriter \\\r\n    --trainer.callbacks.write_interval=batch \\\r\n    ...\r\n```\r\nOr in a config file:\r\n```yaml\r\ntrainer:\r\n  callbacks:\r\n    - class_path: BasePredictionWriter\r\n      init_args:\r\n        write_interval: batch\r\n```"
  },
  {
    "content": "I have a hard time understanding how logging value through metric object works when using `on_step=True`.\r\n\r\nI have these metrics in my lightning module:\r\n```python\r\n# for calculating and averaging accuracy across batches\r\nself.train_acc = Accuracy()\r\n\r\n# for averaging loss across batches\r\nself.train_loss = MeanMetric()\r\n```\r\n\r\nI'm logging metrics on step like this:\r\n```python\r\ndef training_step(self, batch: Any, batch_idx: int):\r\n    loss, preds, targets = self.step(batch)\r\n\r\n    # update metrics\r\n    self.train_loss(loss)\r\n    self.train_acc(preds, targets)\r\n    \r\n    self.log(\"train/loss\", self.train_loss, on_step=True, on_epoch=False, prog_bar=True)\r\n    self.log(\"train/acc\", self.train_acc, on_step=True, on_epoch=False, prog_bar=True)\r\n```\r\n\r\nQuestion: I'm seeing metric values on the progress bar on each step. Are these values actually **accuracy and loss only from current step**, or maybe these values are **average loss and average accuracy from all steps so far** in current epoch?> Question: I'm seeing metric values on the progress bar on each step. Are these values actually **accuracy and loss only from current step**,\r\n\r\nYes, the value of `train/loss=...` shown in the progress bar is calculated based on each step. However, note that the value of `loss=...` (shown by default) at the very left of your progress bar (not `train/loss=...`) is smoothened across 20 steps. (Related to this RFC #9372)",
    "meta": {
      "name": "Question about logging metric object with `on_step=True`"
    },
    "answer": "> Question: I'm seeing metric values on the progress bar on each step. Are these values actually **accuracy and loss only from current step**,\r\n\r\nYes, the value of `train/loss=...` shown in the progress bar is calculated based on each step. However, note that the value of `loss=...` (shown by default) at the very left of your progress bar (not `train/loss=...`) is smoothened across 20 steps. (Related to this RFC #9372)"
  },
  {
    "content": "I was wondering if there is some way to save a WandB config file using the `save_hyperparameters()` call in the Trainer. I am using WandB for logging purposes and to perform parameter sweeps etc. I initialize a WandB run with a config file, which is passed on to the Trainer. And I would like to save that config to the checkpoints I save using ModelCheckpoint callback. I know WandB saves the config that is used for a certain run with it in the same folder, but I would like them to be in the checkpoint as well. Is there some way to do this? Because it looks like the WandB config isn't allowed in the `save_hyperparameter()` at the moment...\r\nSo shortened in pseudo code I have something like this:\r\n\r\nA main file with:\r\n```\r\ndef main(default_config): # default_config is read from a file\r\n    ...\r\n    wandb.init(config=default_config)\r\n    ...\r\n    config = wandb.config\r\n    ...\r\n    model = LitTrainer(netG=generator, netD=discriminator, config=config)\r\n    trainer = pl.Trainer(...)\r\n    trainer.fit(model)\r\n```\r\n\r\nAnd then a trainer file with:\r\n```\r\nclass LitTrainer(pl.LightningModule):\r\n    def __init__(self,\r\n                 netG, netD,\r\n                 config,\r\n                 ):\r\n    super().__init__()\r\n    self.save_hyperparameters(config)\r\n    ...\r\n```\r\nAnd in the rest of the trainer all kinds of parameters are an attribute of that config. But when I try to save that config using the `save_hyperparameters`, it returns the following: `ValueError: Unsupported config type of <class 'wandb.sdk.wandb_config.Config'>.`\r\nAnd first trying to convert the config back to a dict before passing it to `save_hyperparameters` doesn't seem to work as well.\r\nDoes anyone know how to do this?Apparently it has something to do with the config from WandB. The issue is that WandB config is a managed object so it really can't cleanly be deepcopy'ed. Using the static_config from WandB is serializable and works just fine!",
    "meta": { "name": "Save wandb config file as hyperparameter" },
    "answer": "Apparently it has something to do with the config from WandB. The issue is that WandB config is a managed object so it really can't cleanly be deepcopy'ed. Using the static_config from WandB is serializable and works just fine!"
  },
  {
    "content": "I want to pass a type(not an object) to a subclass of LightningDataModule.  I did the following, but it seemed lightningcli tried to initialize an object and complain required fileds were missing. How can I do that?\r\n\r\n```\r\nclass TxtDataModule(LightningDataModule):\r\n    def \\__init\\__(\r\n        self,\r\n        domain_dataset: Dataset,\r\n        ...\r\n    ):\r\n       dataset_obj = domain_dataset(....)\r\n       ...\r\n```\r\n\r\nin yaml\r\n```\r\ndata:\r\n  class_path: src.training.data_module.data_module_txt.TxtDataModule\r\n  init_args:\r\n    domain_dataset: src.training.semantic_sim.dataset_txt.TxtDataset_LoadFromFile  # a subclass of Dataset\r\n```\r\nI don't know why I couldn't set label for my discussion.You can change `domain_dataset` type hint from `Dataset` to `Type`:\r\n\r\nHere is a demo:\r\n```python\r\nfrom typing import Type\r\nfrom jsonargparse import CLI\r\n\r\nclass Class1:\r\n    def __init__(self) -> None:\r\n        print(\"Class1\")\r\n\r\nclass Class2:\r\n    def __init__(self) -> None:\r\n        print(\"Class2\")\r\n\r\ndef test_fn(\r\n    class_type: Type = Class1,\r\n):\r\n    print(class_type)\r\n    class_type()\r\n\r\nif __name__ == \"__main__\":\r\n    CLI(test_fn)\r\n```\r\n\r\n```console\r\npython test.py --class_type '__main__.Class2'\r\n```",
    "meta": {
      "name": "How to pass a type(not an object) to a class with lightningcli?"
    },
    "answer": "You can change `domain_dataset` type hint from `Dataset` to `Type`:\r\n\r\nHere is a demo:\r\n```python\r\nfrom typing import Type\r\nfrom jsonargparse import CLI\r\n\r\nclass Class1:\r\n    def __init__(self) -> None:\r\n        print(\"Class1\")\r\n\r\nclass Class2:\r\n    def __init__(self) -> None:\r\n        print(\"Class2\")\r\n\r\ndef test_fn(\r\n    class_type: Type = Class1,\r\n):\r\n    print(class_type)\r\n    class_type()\r\n\r\nif __name__ == \"__main__\":\r\n    CLI(test_fn)\r\n```\r\n\r\n```console\r\npython test.py --class_type '__main__.Class2'\r\n```"
  },
  {
    "content": "I want to implement a custom callback which calculates a custom metric and needs all of the outputs from the complete epoch. Is there any way to pass all the outputs to `on_validation_epoch_end` hook of the callback ?\r\n\r\nHere's the pseudo-code of the setup\r\n\r\n```python\r\nclass FeedBackPrize(pl.LightningModule):\r\n    def __init__(\r\n        self,\r\n        num_train_steps,\r\n        steps_per_epoch,\r\n        model_name: str = \"allenai/longformer-base-4096\",\r\n        lr: float = 1e-5,\r\n        num_labels: int = 16,\r\n        multi_sample_dropout=True,\r\n        step_scheduler_after: str = \"step\",\r\n    ):\r\n        super().__init__()\r\n        self.learning_rate = lr\r\n        self.model_name = model_name\r\n        self.multi_sample_dropout = multi_sample_dropout\r\n        self.num_train_steps = num_train_steps\r\n        self.num_labels = num_labels\r\n        self.steps_per_epoch = steps_per_epoch\r\n        self.step_scheduler_after = step_scheduler_after\r\n\r\n        hidden_dropout_prob: float = 0.1\r\n        layer_norm_eps: float = 1e-7\r\n\r\n        config = AutoConfig.from_pretrained(model_name)\r\n\r\n        config.update(\r\n            {\r\n                \"output_hidden_states\": True,\r\n                \"hidden_dropout_prob\": hidden_dropout_prob,\r\n                \"layer_norm_eps\": layer_norm_eps,\r\n                \"add_pooling_layer\": False,\r\n                \"num_labels\": self.num_labels,\r\n            }\r\n        )\r\n\r\n        self.transformer = AutoModel.from_pretrained(model_name, config=config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.dropout1 = nn.Dropout(0.1)\r\n        self.dropout2 = nn.Dropout(0.2)\r\n        self.dropout3 = nn.Dropout(0.3)\r\n        self.dropout4 = nn.Dropout(0.4)\r\n        self.dropout5 = nn.Dropout(0.5)\r\n\r\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\r\n\r\n    def forward(self, ids, mask, token_type_ids=None):\r\n        transformer_out = self.transformer(ids, mask)\r\n        sequence_output = transformer_out.last_hidden_state\r\n        sequence_output = self.dropout(sequence_output)\r\n\r\n        if self.multi_sample_dropout:\r\n            logits1 = self.output(self.dropout1(sequence_output))\r\n            logits2 = self.output(self.dropout2(sequence_output))\r\n            logits3 = self.output(self.dropout3(sequence_output))\r\n            logits4 = self.output(self.dropout4(sequence_output))\r\n            logits5 = self.output(self.dropout5(sequence_output))\r\n\r\n            logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\r\n            logits = torch.softmax(logits, dim=-1)\r\n            return logits\r\n        else:\r\n            return sequence_output\r\n\r\n    def configure_optimizers(self):\r\n        param_optimizer = list(self.named_parameters())\r\n        no_decay = [\"bias\", \"LayerNorm.bias\"]\r\n        optimizer_parameters = [\r\n            {\r\n                \"params\": [\r\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\r\n                ],\r\n                \"weight_decay\": 0.01,\r\n            },\r\n            {\r\n                \"params\": [\r\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\r\n                ],\r\n                \"weight_decay\": 0.0,\r\n            },\r\n        ]\r\n        optimizer = AdamW(optimizer_parameters, lr=self.learning_rate)\r\n\r\n        scheduler = get_cosine_schedule_with_warmup(\r\n            optimizer,\r\n            num_warmup_steps=int(0.1 * self.num_train_steps),\r\n            num_training_steps=self.num_train_steps,\r\n            num_cycles=1,\r\n            last_epoch=-1,\r\n        )\r\n        scheduler = {\r\n            \"scheduler\": scheduler,\r\n            \"interval\": self.step_scheduler_after,\r\n            \"frequency\": 1,\r\n        }\r\n\r\n        return [optimizer], [scheduler]\r\n\r\n    def _calculate_loss(self, outputs, targets, attention_mask):\r\n        loss_fct = nn.CrossEntropyLoss()\r\n\r\n        active_loss = attention_mask.view(-1) == 1\r\n        active_logits = outputs.view(-1, self.num_labels)\r\n        true_labels = targets.view(-1)\r\n        outputs = active_logits.argmax(dim=-1)\r\n        idxs = np.where(active_loss.cpu().numpy() == 1)[0]\r\n        active_logits = active_logits[idxs]\r\n        true_labels = true_labels[idxs].to(torch.long)\r\n\r\n        loss = loss_fct(active_logits, true_labels)\r\n\r\n        return loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        ids, mask, targets = batch['ids'], batch['mask'], batch['targets']\r\n        outputs = self(ids, mask)\r\n        loss = self._calculate_loss(outputs, targets, mask)\r\n        return loss\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        ids, mask, targets = batch['ids'], batch['mask'], batch['targets']\r\n        outputs = self(ids, mask)\r\n        loss = self._calculate_loss(outputs, targets, mask)\r\n\r\n        return {\r\n            \"loss\": loss,\r\n            \"preds\": outputs,\r\n            \"targets\": targets\r\n        }\r\n    \r\n    def validation_epoch_end(self, validation_step_outputs):\r\n        preds = []\r\n        targets = []\r\n\r\n        for output in validation_step_outputs:\r\n            preds += output['preds']\r\n            targets += output['targets']\r\n        \r\n        targets = torch.stack(targets) #torch.Size([2, 1536])\r\n        preds = torch.stack(preds) # torch.Size([2, 1536, 15])\r\n\r\n        return {\r\n            \"targets\": targets,\r\n            \"preds\": preds\r\n        }\r\n```\r\n\r\n### Custom callback\r\n```python\r\nclass CompMetricEvaluator(Callback):\r\n    def __init__(self):\r\n        pass\r\n    def on_validation_epoch_end(self, trainer, pl_module):\r\n\r\n        print(\"After validation epoch [custom metric evaluation]\")\r\n        # calculate custom metric here....\r\n```hey @Gladiator07!\r\n\r\nyou can either override `on_validation_batch_end` hook and cache the outputs in some variable of the callback use that.\r\n\r\n```py\r\nclass CustomCallback(Callback):\r\n    def __init__(self):\r\n        self.val_outs = []\r\n    def on_validation_batch_end(self, trainer, pl_module, outputs, ...):\r\n        self.val_outs.append(outputs)\r\n\r\n    def on_validation_epoch_end(self, trainer, pl_module):\r\n        self.val_outs  # <- access them here\r\n```\r\nor cache the val outputs in pl_module inside `validation_epoch_end`\r\n```py\r\nclass LitModel(LightningModule):\r\n    def validation_epoch_end(self, outputs):\r\n       new_outputs = ...\r\n       self.val_outs = new_outputs\r\n\r\n\r\nclass CustomCallback(Callback):\r\n    def on_validation_epoch_end(self, trainer, pl_module):\r\n        pl_module.val_outs  # <- access them here\r\n```\r\nnote that the trainer and pl_module passed inside callbacks are passed by reference so that ever changes in the original lightningmodule will reflect in this referred instance here too.",
    "meta": {
      "name": "How to access validation step outputs of complete epoch in a `on_validation_epoch_end` hook for a custom callback ?"
    },
    "answer": "hey @Gladiator07!\r\n\r\nyou can either override `on_validation_batch_end` hook and cache the outputs in some variable of the callback use that.\r\n\r\n```py\r\nclass CustomCallback(Callback):\r\n    def __init__(self):\r\n        self.val_outs = []\r\n    def on_validation_batch_end(self, trainer, pl_module, outputs, ...):\r\n        self.val_outs.append(outputs)\r\n\r\n    def on_validation_epoch_end(self, trainer, pl_module):\r\n        self.val_outs  # <- access them here\r\n```\r\nor cache the val outputs in pl_module inside `validation_epoch_end`\r\n```py\r\nclass LitModel(LightningModule):\r\n    def validation_epoch_end(self, outputs):\r\n       new_outputs = ...\r\n       self.val_outs = new_outputs\r\n\r\n\r\nclass CustomCallback(Callback):\r\n    def on_validation_epoch_end(self, trainer, pl_module):\r\n        pl_module.val_outs  # <- access them here\r\n```\r\nnote that the trainer and pl_module passed inside callbacks are passed by reference so that ever changes in the original lightningmodule will reflect in this referred instance here too."
  },
  {
    "content": "Suppose I have a subclass of LightningDataModule\r\n\r\nclass TxtDataModule(LightningDataModule):\r\n    def \\__init\\__(\r\n        self,\r\n        line_processor,  # line_processor is a function \r\n        ...\r\n    ):\r\n       line_processor(....)\r\n       ....\r\n\r\nHow can I configure TxtDataModule to pass in a function in yaml configuration?\r\nYou may find [this](https://github.com/Lightning-AI/lightning/discussions/13613#discussioncomment-3174735) helpful.",
    "meta": {
      "name": "How to pass in a function with lightningcli configration?"
    },
    "answer": "You may find [this](https://github.com/Lightning-AI/lightning/discussions/13613#discussioncomment-3174735) helpful."
  },
  {
    "content": "This is very confusing that if the train batch size is set to 1 and Automatic Optimization mode is used, accumulate_grad_batches=32, train loss can converge normally. However, using Manual Optimization mode with automatic_optimization=False, the train loss fails to converge. Is there any other code I'm ignoring in PyTorch Lightning?\r\n \r\nThe code used is as follows:\r\n```python\r\n     def training_step(self, batch, batch_idx, optimizer_idx=None):\r\n            opt = self.optimizers()\r\n            loss = gen_helper()\r\n            self.log('train/loss', loss, prog_bar=True, logger=True)\r\n            self.manual_backward(loss)\r\n            if (batch_idx+1) % self.hparams.accumulate_grad_batches == 0:\r\n                opt.step()\r\n                opt.zero_grad()\r\n```@OPilgrim Your `training_step` looks good to me.\r\n\r\nHowever, please note that your logged value `\"train/loss\"` is being logged regardless of whether the weights are being updated in the iteration, which will make it look like a convergence issue.",
    "meta": {
      "name": "In Manual Optimization mode, loss does not converge when the Train Batch Size is 1"
    },
    "answer": "@OPilgrim Your `training_step` looks good to me.\r\n\r\nHowever, please note that your logged value `\"train/loss\"` is being logged regardless of whether the weights are being updated in the iteration, which will make it look like a convergence issue."
  },
  {
    "content": "I'm creating a classifier that first trains a VAE then passes it into a convolutional network. The psudo code below kind of describes it:\r\n```python\r\nclass VAE(pl.LightningModule):\r\n# ...\r\n\r\nclass ConvNetwork(pl.LightningModule):\r\n    def __init__(self, vae):\r\n        # Trying both ways: pass in entire model vs loading checkpoint\r\n        # self.vae = vae\r\n        # self.vae = VAE.load_from_checkpoint(vae)\r\n        freeze_training(self.vae) # sets all params to requries_grad=False\r\n\r\n        self.sub_network = nn.Sequential(\r\n            # Mix of convolutional layers, ReLU activations, and Batch Normalization\r\n        )\r\n\r\n    def forward(self, data):\r\n         vae_decoded_results = self.vae(data)\r\n         results_that_differ_wildly = self.sub_network(vae_decoded_results)\r\n        \r\n```\r\nIf I train the VAE and pass in the entire model before training the convolutional network, I get good training/validation results. What I would prefer, however, is to train the VAE in a separate script, save off checkpoints, then pass the path of the checkpoint into the convolutional network. Then in the convolutional network's init I load the vae network, freeze training on it, and proceed to train the convolutional network. When I do this, my training results seem okay, but my validation results are all over the place. Some things I've checked:\r\n\r\n- After loading the VAE from a checkpoint, I verified the model parameters perfectly match the VAE that produced the checkpoint.\r\n- In my forward function for the convolutional network I call the VAE's forward function. The results at this step differ by less than 1% between loading a checkpoint and passing in an entire model.\r\n- After passing the VAE forward() results into the first stage of my Convolution network (consists of some convolution layers, ReLU activations, and batch normalization) I get very different results. \r\n\r\nI can't for the life of me figure out why using the results from a loaded model would so wildly differ from the results of a model I train and pass in all in one script, especially when the parameters and vae output appear to match. I'm sure I'm just missing something stupid. Just a wild guess, but maybe the model is in `train`-mode after loading from a checkpoint. Have you tried `model.eval()` in addition to setting the `requires_grad`? I'm thinking about BN layers and so on, where this is important (see [here](https://stackoverflow.com/a/55627781/10429267)).",
    "meta": {
      "name": "load_from_checkpoint giving different validation results"
    },
    "answer": "Just a wild guess, but maybe the model is in `train`-mode after loading from a checkpoint. Have you tried `model.eval()` in addition to setting the `requires_grad`? I'm thinking about BN layers and so on, where this is important (see [here](https://stackoverflow.com/a/55627781/10429267))."
  },
  {
    "content": "Hi,\r\nIt seems there is duplicate computation for [gan example](https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/basic-gan.html).\r\nOne here:\r\n```python\r\n# train generator\r\n        if optimizer_idx == 0:\r\n\r\n            # generate images\r\n            self.generated_imgs = self(z) ## \uff0c<-- first\r\n```\r\nThe other here:\r\n```\r\n# train discriminator\r\n        if optimizer_idx == 1:\r\n....\r\n\r\nfake_loss = self.adversarial_loss(self.discriminator(self(z).detach()), fake) <--- repeat\r\n```\r\n\r\nAnd also the sampling seems also duplicate:\r\n```python\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        imgs, _ = batch\r\n\r\n        # sample noise\r\n        z = torch.randn(imgs.shape[0], self.hparams.latent_dim)\r\n        z = z.type_as(imgs)\r\n```\r\nI do think this is unreasonable.\r\n\r\nSo how to avoid this unnecessary computation in lightning, I know there are some tricks to achieve, such as using a `self.some_state` to hold the values and the reuse them. But they seems no elegant. @wztdream Yes, it is redundant. I see a few options:\r\n\r\n1. Store the computed result (the noise and generated images) in some attribute and reuse it for the next optimizer indexed `optimizer_idx==1`.\r\n2. Use manual optimization.\r\n   - Docs: https://pytorch-lightning.readthedocs.io/en/stable/model/manual_optimization.html?highlight=manual%20optimization#manual-optimization\r\n   - Example: https://github.com/akihironitta/gist/blob/5f29d7aa8712e2f16c929b1384b555dd2e53ae9f/pl_gan_manual_optimization/main.py\r\n3. (Use the loop API. There's an example of yielding training step using the loop API: https://github.com/Lightning-AI/lightning/blob/master/examples/pl_loops/yielding_training_step.py, but I'd recommend the first two above because this one needs a bit more work.)",
    "meta": { "name": "Isn't here duplicate computation for gan example?" },
    "answer": "@wztdream Yes, it is redundant. I see a few options:\r\n\r\n1. Store the computed result (the noise and generated images) in some attribute and reuse it for the next optimizer indexed `optimizer_idx==1`.\r\n2. Use manual optimization.\r\n   - Docs: https://pytorch-lightning.readthedocs.io/en/stable/model/manual_optimization.html?highlight=manual%20optimization#manual-optimization\r\n   - Example: https://github.com/akihironitta/gist/blob/5f29d7aa8712e2f16c929b1384b555dd2e53ae9f/pl_gan_manual_optimization/main.py\r\n3. (Use the loop API. There's an example of yielding training step using the loop API: https://github.com/Lightning-AI/lightning/blob/master/examples/pl_loops/yielding_training_step.py, but I'd recommend the first two above because this one needs a bit more work.)"
  },
  {
    "content": "I'm running a metric learning model, and I'd like to use embeddings from the training step for KNN lookups in the validation step. I tried to do so with the following code. \r\n\r\n```\r\ndef training_epoch_end(self, outputs):\r\n        embeddings = []\r\n        for output in outputs:\r\n            embeddings.append(output[\"embeddings\"])\r\n        self.training_embeddings = torch.cat(embeddings, dim=0)\r\n        print(\"Finished training epoch\")\r\n        return super().training_epoch_end(outputs)\r\n\r\ndef validation_epoch_end(self, outputs):\r\n      train_embeddings = self.train_embeddings\r\n      #Use train embeddings...\r\n      self.train_embeddings = None #Delete embeddings for future use.\r\n```\r\n\r\nHowever, self.train_embeddings is either None, or uninitialized, when validation_epoch_end executes. Is there a way to communicate between these two methods, or send information from both to a final callback? \r\n\r\n\r\nJust saw other answers explaining the order of train_epoch_end and val_epoch_end hooks. This issue was solved by moving any code needing both validation and training data to \"on_train_epoch_end\". ",
    "meta": {
      "name": "Using data from training_epoch_end in validation_epoch_end."
    },
    "answer": "Just saw other answers explaining the order of train_epoch_end and val_epoch_end hooks. This issue was solved by moving any code needing both validation and training data to \"on_train_epoch_end\". "
  },
  {
    "content": "I want to use DDP and experiment with contrastive losses. Since DDP processes each subset of the data independently, negative examples that could be used to increase the contrastive power cannot be taken into account using automatic optimization. Suppose I am training with 2 GPU's and each GPU sees a mini-batch of size 4. This leads to missing signal between (x1, x5), (x1, x6), (x1, x7), etc... since x1-x4 are on GPU1 and x5-x8 are on GPU2. \r\n\r\nWhat is the recommended method to account for this in PT-Lightning?\r\nOne approach seems to be to use the `on_train_batch_end()` callback, and 1) gather the outputs from all GPUs, 2) compute the loss on rank=0, and 3) distribute that loss back to each GPU.\r\n\r\nAfter computing loss, I'm unclear as to the mechanics for how to distribute that loss computed on rank=0 back to all of the GPU's so that the gradients are synced. Is this something that happens automatically under the hood, or do I need to do something w.r.t. manual optimization?@awaelchli @williamFalcon This is, in fact, the same tutorial I wanted for sync'ing stuff using `torch.gather` etc for VICreg!@kkarrancsu You are definitely on the right track here. In the LightningModule, you have [this method](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html?highlight=all_gather#all-gather) for gathering a tensor from all processes:\r\n\r\n```py\r\ntensors_from_all = self.all_gather(my_tensor)\r\n```\r\n\r\nWhat you want is to back-propagate through this all_gather function, and this is possible if you set \r\n\r\n```py\r\ntensors_from_all = self.all_gather(my_tensor, sync_grad=True)\r\n```\r\n\r\nIn your case, your `training_step` method could look something like this:\r\n```py\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        outputs = self(batch)\r\n        ...\r\n\r\n        all_outputs = self.all_gather(outputs, sync_grads=True)\r\n\r\n        loss = contrastive_loss_fn(all_outputs, ...)\r\n        return loss\r\n```Follow-up question, to do contrastive learning for DP, I believe that we need to implement the loss computation in the `training_step_end()` function. But for DDP, it looks like we can do it directly in the `training_step()` function with the code example above.\r\n\r\nDoes this mean we should have flags in our `training_step()` function to be able to support both?\r\n\r\nHere is a stub:\r\n\r\n```\r\ndef training_step(self, batch, batch_idx):\r\n    if self.strategy.lower() == 'ddp':\r\n        outputs = self.model(batch)\r\n        all_outputs = self.all_gather(outputs, sync_grads=True)\r\n        loss = self.loss(all_outputs)\r\n        return loss\r\n    elif self.strategy.lower() == 'dp':\r\n        return {'model_output': model_output, 'labels': labels}\r\n\r\ndef training_step_end(self, batch_parts):\r\n    if self.strategy.lower() == 'ddp':\r\n        pass\r\n    elif self.strategy.lower() == 'dp':\r\n        model_outputs_all = batch_parts['model_output']\r\n        labels_all = batch_parts['labels']\r\n        l = self.loss(model_outputs_all, labels_all)\r\n        return l\r\n```\r\n\r\nTwo follow-up questions:\r\n1 - would doing this prevent me from using TPUs in the future?\r\n2 - do I return batch_parts for strategy=='ddp' in `train_step_end()` or just `pass`",
    "meta": {
      "name": "Proper way to do contrastive learning with DDP & PT-Lightning"
    },
    "answer": "@kkarrancsu You are definitely on the right track here. In the LightningModule, you have [this method](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html?highlight=all_gather#all-gather) for gathering a tensor from all processes:\r\n\r\n```py\r\ntensors_from_all = self.all_gather(my_tensor)\r\n```\r\n\r\nWhat you want is to back-propagate through this all_gather function, and this is possible if you set \r\n\r\n```py\r\ntensors_from_all = self.all_gather(my_tensor, sync_grad=True)\r\n```\r\n\r\nIn your case, your `training_step` method could look something like this:\r\n```py\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        outputs = self(batch)\r\n        ...\r\n\r\n        all_outputs = self.all_gather(outputs, sync_grads=True)\r\n\r\n        loss = contrastive_loss_fn(all_outputs, ...)\r\n        return loss\r\n```"
  },
  {
    "content": "I have 10,000s of images for training a semantic segmentation model.\r\n\r\nI am therefore using `limit_val_batches` (along with `val_check_interval`) to limit the number of valuation batches per epoch\r\n\r\nI however note (i.e [Pytorch Lightning limit_val_batches and val_check_interval behavior - Stack Overflow](https://stackoverflow.com/questions/68658917/pytorch-lightning-limit-val-batches-and-val-check-interval-behavior)) that when using `limit_val_batches=N`, the first `N` batches from the dataloader are returned for each validation iteration. Training therefore only every sees the first \u2018N\u2019 validation batches when using `limit_val_batches` and never the rest that makes up the bulk of validation examples\r\n\r\nRather than using the same \u2018N\u2019 batches starting at index 0 for each validation epoch, I would like the dataloader when using `limit_val_batches` to sequentially chunk through all the validation data (e.g 0-19 batches the first epoch and then 20-39 the next... and so on)\r\n\r\nHow would I go about implementing this behavior with limit_val_batches? or is this not the expected thing to do?`limit_val_batches` is not the expected thing to do.Thanks @tshu-w. \r\n\r\nIn that case, what is the strategy when your validation dataset (or training set) has 10,000 plus images in it and you don't want validation to validate against all these images every epoch, but rather chunk through the dataset each epoch a small bit at a time in a sequential manner? For anyone with a similar problem, [aniketmaurya](https://forums.pytorchlightning.ai/u/aniketmaurya) provided an answer at over at the now defunct [pytorch forums](https://forums.pytorchlightning.ai/t/iterate-over-all-validation-data-while-using-limit-val-batches/1823) which also solves this issue. \r\n\r\nit is as follows:\r\n> Hey [@aeolian](https://forums.pytorchlightning.ai/u/aeolian), you can shuffle your validation dataloader so that for each epoch random images would be validated.\r\n>\r\n>Also, we have recently migrated to [GH Discussions](https://github.com/Lightning-AI/lightning/discussions) for answering community questions. Feel free to ask there if you\u2019re still having some issues.\r\n\r\nIf you are using this method and want to compare results across training you will want to use the `worker_init_fn()` to preserve reproducibility:",
    "meta": {
      "name": "Iterate over all validation data while using limit_val_batches"
    },
    "answer": "Thanks @tshu-w. \r\n\r\nIn that case, what is the strategy when your validation dataset (or training set) has 10,000 plus images in it and you don't want validation to validate against all these images every epoch, but rather chunk through the dataset each epoch a small bit at a time in a sequential manner? "
  },
  {
    "content": "Hi,\r\n\r\nI have two datasets and each dataset will have a corresponding optimizer.\r\n\r\nRight now I am updating the model parameters per batch per dataset. But how can I iterate `datasetA` for an epoch first and then iterate an epoch for `datasetB`? \r\n\r\n- Train an epoch on the datasetA using `optimizer A`\r\n- Train an epoch on the datasetB using `optimizer B`\r\n\r\nThis is what I have for iterating multiple datasets per batch.\r\n```python\r\nclass Mymodel(LightningModule):\r\n  def configure_optimizers(self):\r\n      optimizer = optim.Adam(\r\n          filter(\r\n              lambda p: p.requires_grad,\r\n              self.parameters()),\r\n          lr=self.lr,)\r\n      scheduler = optim.lr_scheduler.ReduceLROnPlateau(\r\n          optimizer,\r\n      )\r\n      scheduler_config = {\r\n          \"scheduler\": scheduler,\r\n          \"monitor\": \"val/rec/epoch_loss\",\r\n          \"interval\": \"epoch\",\r\n          \"frequency\": 1,\r\n      }\r\n      pre_optimizer = optim.SGD(\r\n            filter(\r\n                lambda p: p.requires_grad,\r\n                self.parameters()),\r\n            lr=self.lr_for_other_modules,)\r\n      optimizers = [optimizer, pre_optimizer]\r\n      return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}\r\n  \r\n  def training_step(self, batch, batch_idx, optimizer_idx):\r\n       if optimizer_idx == 0:\r\n          # do forward on datasetA\r\n          inputs = batch[\"datasetA\"]\r\n        if self.is_use_some_modules and optimizer_idx == 1:\r\n          # do something else \r\n          inputs = batch[\"datasetB\"]\r\n        return ...\r\n\r\nclass Datamodule(LightningDataModule):\r\n  def train_dataloader(self):\r\n      ...\r\n      return {\"datasetA\": A_loader, \"datasetB\": B_loader}\r\n\r\n```\r\n\r\nI found a similar question here #3336 and the suggested solution is reloading data loaders every epoch. \r\nI wonder if it is safe to do this as I have multiple optimizers.\r\n\r\n```python\r\nclass Datamodule(LightningDataModule):\r\n  def train_dataloader(self):\r\n      if self.current_epoch % 2 == 0:\r\n          A_loader = ...\r\n          return A_loader\r\n      else:\r\n          B_loader = ...\r\n          return B_loader\r\n\r\nclass Mymodel(LightningModule):\r\n  def training_step(self, batch, batch_idx, optimizer_idx):\r\n      if self.current_epoch % 2 == 0 and optimizer_idx == 0:\r\n           inputs = batch[\"datasetA\"]\r\n      elif self.current_epoch % 2 == 1 and optimizer_idx == 1:\r\n          inputs = batch[\"datasetB\"]\r\n      return ...\r\n``` \r\n\r\n\r\n\r\none simple solution is to use `reload_dataloaders_every_epoch=1` and configure `training_step` based on `current_epoch`.\r\n\r\n```py\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n    if (self.current_epoch % 2 == 0 and optimizer_idx == 0) or (self.current_epoch % 2 == 1 and optimizer_idx == 1):\r\n        ...\r\n        return loss\r\n```\r\n\r\nin other cases of `training_step`, it will return `None`, which will not make any updates to the other optimizer. You might get a warning, but that's fine.\r\nyour solution is fine as well, but it will load the data from both the dataloader which is not required in each epoch.",
    "meta": {
      "name": "How to switch data loaders between epochs while using multiple optimizer"
    },
    "answer": "one simple solution is to use `reload_dataloaders_every_epoch=1` and configure `training_step` based on `current_epoch`.\r\n\r\n```py\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n    if (self.current_epoch % 2 == 0 and optimizer_idx == 0) or (self.current_epoch % 2 == 1 and optimizer_idx == 1):\r\n        ...\r\n        return loss\r\n```\r\n\r\nin other cases of `training_step`, it will return `None`, which will not make any updates to the other optimizer. You might get a warning, but that's fine.\r\nyour solution is fine as well, but it will load the data from both the dataloader which is not required in each epoch."
  },
  {
    "content": "![image](https://user-images.githubusercontent.com/29114869/187025003-ffbc1742-7c8d-4635-9a37-d26842e0b0eb.png)\r\npytorch-lightning             1.7.3I found this error when I did the lightning package import. I followed other solutions and tried to import all packages before importing lightning and it didn't solve my problem. For anyone seeing this issue, see #11663.",
    "meta": { "name": "Segmentation Fault While Import pytorch_lightning" },
    "answer": "For anyone seeing this issue, see #11663."
  },
  {
    "content": "I would like to define some new subcommands along with the default ones and be able to attach some function calls to it. The documentation does not seem to show this use case so I'm wondering if this is possible.\r\n\r\nFor example, to visualize the output of some model.\r\n\r\n```\r\npython main.py visualize --checkpoint=/my/checkpoing.ckpt\r\n```\r\n\r\nOr would it be best to just customize the behaviour of some existing subcommands as described [here](https://pytorch-lightning.readthedocs.io/en/stable/cli/lightning_cli_expert.html#customize-the-lightningcli)?To define new subcommands I would recommend the following. First you subclass `Trainer` and add methods for each of the subcommands you want. These new methods should be usable independent of the CLI. Then subclass `LightningCLI`, in the `__init__` make your new trainer the default and to add the subcommands simply override/extend https://github.com/Lightning-AI/lightning/blob/7a617ec90e1566c763be8ac7a200af1e4025412c/src/pytorch_lightning/cli.py#L425-L434",
    "meta": { "name": "Does LightningCLI support user-defined subcommands?" },
    "answer": "To define new subcommands I would recommend the following. First you subclass `Trainer` and add methods for each of the subcommands you want. These new methods should be usable independent of the CLI. Then subclass `LightningCLI`, in the `__init__` make your new trainer the default and to add the subcommands simply override/extend https://github.com/Lightning-AI/lightning/blob/7a617ec90e1566c763be8ac7a200af1e4025412c/src/pytorch_lightning/cli.py#L425-L434"
  },
  {
    "content": "RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation.\r\n\r\nmy model is transformer.fine i solved this question\r\n\r\nthis is the solution:\r\n\r\nchange PositionalEncoding to this:\r\n\r\n```Ptyhon\r\nclass PositionalEncoding(nn.Module):\r\n\r\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\r\n        super(PositionalEncoding, self).__init__()\r\n        self.dropout = nn.Dropout(p=dropout)\r\n\r\n        pe = torch.zeros(max_len, d_model)\r\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\r\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\r\n        pe[:, 0::2] = torch.sin(position * div_term)\r\n        pe[:, 1::2] = torch.cos(position * div_term)\r\n        pe = pe.unsqueeze(0).transpose(0, 1)\r\n        self.register_parameter('pe', nn.Parameter(pe, requires_grad=False))\r\n\r\n    def forward(self, x):\r\n        x = x + self.pe[:x.size(0), :]\r\n        return self.dropout(x)\r\n```",
    "meta": {
      "name": "`RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location`"
    },
    "answer": "fine i solved this question\r\n\r\nthis is the solution:\r\n\r\nchange PositionalEncoding to this:\r\n\r\n```Ptyhon\r\nclass PositionalEncoding(nn.Module):\r\n\r\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\r\n        super(PositionalEncoding, self).__init__()\r\n        self.dropout = nn.Dropout(p=dropout)\r\n\r\n        pe = torch.zeros(max_len, d_model)\r\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\r\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\r\n        pe[:, 0::2] = torch.sin(position * div_term)\r\n        pe[:, 1::2] = torch.cos(position * div_term)\r\n        pe = pe.unsqueeze(0).transpose(0, 1)\r\n        self.register_parameter('pe', nn.Parameter(pe, requires_grad=False))\r\n\r\n    def forward(self, x):\r\n        x = x + self.pe[:x.size(0), :]\r\n        return self.dropout(x)\r\n```"
  },
  {
    "content": "It seems that validation_epoch_end gets called and only after that train_epoch_end gets called. Why is that?When you run `trainer.fit(...)`, there're two calls to `validation_step()` followed by a call to `validation_epoch_end()` before any calls to `training_step()` by default.\r\n\r\nThe order of all hooks are documented at: https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\r\n```python\r\n# the sanity check runs here <--- mind that here's **SANITY CHECK**\r\n\r\non_train_start()\r\nfor epoch in epochs:\r\n    fit_loop()\r\non_train_end()\r\n```\r\n\r\nYou can disable it by specifying `Trainer(num_sanity_val_steps=0)`: https://pytorch-lightning.readthedocs.io/en/1.7.2/common/trainer.html#num-sanity-val-steps\r\n\r\n",
    "meta": { "name": "Why does validation epoch end before train epoch?" },
    "answer": "When you run `trainer.fit(...)`, there're two calls to `validation_step()` followed by a call to `validation_epoch_end()` before any calls to `training_step()` by default.\r\n\r\nThe order of all hooks are documented at: https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\r\n```python\r\n# the sanity check runs here <--- mind that here's **SANITY CHECK**\r\n\r\non_train_start()\r\nfor epoch in epochs:\r\n    fit_loop()\r\non_train_end()\r\n```\r\n\r\nYou can disable it by specifying `Trainer(num_sanity_val_steps=0)`: https://pytorch-lightning.readthedocs.io/en/1.7.2/common/trainer.html#num-sanity-val-steps\r\n\r\n"
  },
  {
    "content": "I cant import lightning package:  ImportError: cannot import name 'Mapping' from 'collections'. In my Python 3.10 Mapping accessible by 'collections.abc' i.e 'from collections.abc import Mapping' not 'from collections import Mapping'. May be  someone knows how fix this issue. Import lightining package trying run next command 'from collections import Mapping'Would you mind providing the full error message and your environment detail?IDLE Shell has analog problem\r\n<img width=\"536\" alt=\"image\" src=\"https://user-images.githubusercontent.com/109548445/185793058-4ea28f0e-882f-4133-8125-41cd38f9dfe8.png\">\r\n",
    "meta": {
      "name": "`ImportError: cannot import name 'Mapping' from 'collections'`"
    },
    "answer": "Would you mind providing the full error message and your environment detail?"
  },
  {
    "content": "I am trying to understand how the `predict` subcommand works with the LightningCLI.\r\n\r\nAs far as I can tell, it would be calling the lightning module's `predict_step` method. However, I don't know what happens after this. I see there is a `return_predictions` flag but not sure where or whether the output is stored somewhere. I am not finding it in the checkpoint directory.\r\n\r\nThanks in advance!Hi, the return of subcommand is not saved in LightningCLI.\r\nhttps://github.com/Lightning-AI/lightning/blob/0ca3b5aa1b16667cc2d006c3833f4953b5706e72/src/pytorch_lightning/cli.py#L623-L630\r\n\r\nYou can instance CLI with `run=False` or Hack the `LightningCLI` directly to make sure it return results.\r\n```python\r\ncli = LightningCLI(run=False)\r\ncli.trainer.predict(...)\r\n```For more context about this topic, read https://github.com/Lightning-AI/lightning/issues/12438",
    "meta": { "name": "Where are model predictions saved by LightningCLI?" },
    "answer": "Hi, the return of subcommand is not saved in LightningCLI.\r\nhttps://github.com/Lightning-AI/lightning/blob/0ca3b5aa1b16667cc2d006c3833f4953b5706e72/src/pytorch_lightning/cli.py#L623-L630\r\n\r\nYou can instance CLI with `run=False` or Hack the `LightningCLI` directly to make sure it return results.\r\n```python\r\ncli = LightningCLI(run=False)\r\ncli.trainer.predict(...)\r\n```"
  },
  {
    "content": "hi, I just tried mix precision training with precision=16 set in the trainer. I found the training speed does increase by around 30%, but the GPU memory merely decreases. Should it be half of the raw memory?Hi @icoz69 AFAIK, the memory usage depends on your model architecture, specifically, the ratio of the model size to the size of activations. This is because, with amp, your model always stays in fp32 while some operations in your model are done in fp16.",
    "meta": { "name": "FP16 does not decrease much GPU memory" },
    "answer": "Hi @icoz69 AFAIK, the memory usage depends on your model architecture, specifically, the ratio of the model size to the size of activations. This is because, with amp, your model always stays in fp32 while some operations in your model are done in fp16."
  },
  {
    "content": "As the pic is shown below, `validation_step` started before training epoch 0 ended.\r\n\r\n![image](https://user-images.githubusercontent.com/49632327/185279513-7f376958-79aa-45e4-b5ea-940623d9d88e.png)\r\n\r\nIt seems that the validation progress bar rolls forward together with the training progress bar. How could I separate them (i.e. the training epoch first ends, and then validation starts) ?\r\nHi, the global process bar contains training + validation, this doesn't mean validation_step started before training epoch 0 ended.\r\nhttps://github.com/Lightning-AI/lightning/issues/12623@tshu-w \r\nThanks for your clear explanation!@tshu-w \r\nDoes pytorch-lightning offer an API to change global progress bar to separated training and validation progress bar?",
    "meta": {
      "name": "Why validation_step starts before training epoch ends?"
    },
    "answer": "Hi, the global process bar contains training + validation, this doesn't mean validation_step started before training epoch 0 ended.\r\nhttps://github.com/Lightning-AI/lightning/issues/12623"
  },
  {
    "content": "`self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)`\r\nIf the calculated `loss` is averaged on a batch,\r\n\r\nthen, is `val_loss` with `on_epoch` an average of the averaged loss?\r\nSo, double-averaged? Or does it take `batch_size` into account for averaging?it takes batch_size into account for averaging.",
    "meta": { "name": "About average in progress bar with on_epoch" },
    "answer": "it takes batch_size into account for averaging."
  },
  {
    "content": "I would like to change the names of the logging directories from the default \"version_{n}\" to something of my own choosing. How can I do this using command-line arguments to LightningCLI?\r\n\r\nI know I can set the logger using `trainer.logger` but setting logger args e.g. `trainer.logger.version` does not work (unrecognized argument). So how can I pass args to the logger?cc @carmocca See my reply here: https://github.com/PyTorchLightning/pytorch-lightning/issues/10574#issuecomment-1015864152\r\n\r\nWe'll be adding support for shorthand notation shortly too: #11533 ",
    "meta": {
      "name": "LightningCLI: how to configure logger using cmd-line args?"
    },
    "answer": "See my reply here: https://github.com/PyTorchLightning/pytorch-lightning/issues/10574#issuecomment-1015864152\r\n\r\nWe'll be adding support for shorthand notation shortly too: #11533 "
  },
  {
    "content": "```python\r\ndef validation_step(self, batch, batch_nb, dataloader_nb, vis=False, save=True, mesh_save_dir=None):\r\n```\r\n\r\nMy pytorch-lightning version is 1.6.0, but the code is written with pytorch-lightning 1.1.8. And when I ran the code, I met the bug `validation_step() missing 1 required positional argument: 'dataloader_nb'`.\r\n\r\nHow can I  fix it and pass parameters to `validation_step() `?if you are using a single val dataloader:\r\n```py\r\ndef validation_step(self, batch, batch_idx):\r\n```\r\n\r\nif you are using multiple val dataloader:\r\n```py\r\ndef validation_step(self, batch, batch_idx, dataloader_idx):\r\n```\r\n\r\nfor consistency, you can just do\r\n```py\r\ndef validation_step(self, batch, batch_idx, dataloader_idx=0):\r\n```",
    "meta": {
      "name": "TypeError: validation_step() missing 1 required positional argument: 'dataloader_nb'"
    },
    "answer": "if you are using a single val dataloader:\r\n```py\r\ndef validation_step(self, batch, batch_idx):\r\n```\r\n\r\nif you are using multiple val dataloader:\r\n```py\r\ndef validation_step(self, batch, batch_idx, dataloader_idx):\r\n```\r\n\r\nfor consistency, you can just do\r\n```py\r\ndef validation_step(self, batch, batch_idx, dataloader_idx=0):\r\n```"
  },
  {
    "content": "I am currently working on a code where the dataset has one property that\r\n\r\n1. has a custom Dataset which has an attribute at `self.items_served` that counts how many items have been served. It is initialized at 0 in the `init() `method and uptadted every time `get_item()` is called\r\n2. on my LightningModule's ` init()`, I initialize one instance of this custom Dataset and assign it to an attribute of the class: d= `Dataset(args)` and then `self.train_dataset=d`\r\n3. on my LightningModule's `train_dataloader()`, I return a dataloader which has` self.train_dataset` as dataset and numworkers = 10\r\n4. on my LightningModule's `training_step()`, I try to check the attribute at `self.train_dataset.items_served` and it is always 0, it does not update.\r\n\r\nHowever, everything works fine if I use only one worker (numworkers=0), so I guess I a doing something wrong  and/or I don't understand how dataloader works with multiple workers\r\n\r\nAny suggestion on how to tackle this problem?It's because when you use `num_workers>  0`, the data is loaded, and the `collate_fn` is called inside the worker and that fetched data is pushed to the main process. So all the updates are happening inside the sub-worker process, but `training_step` runs on the main process, so no effect is reflected there.\r\n\r\npotential sol you can try: https://stackoverflow.com/questions/63460992/how-do-i-fix-the-dataset-to-return-desired-output-pytorch",
    "meta": {
      "name": "does each worker (num_workers>1) has its own instance of the Dataset object?"
    },
    "answer": "It's because when you use `num_workers>  0`, the data is loaded, and the `collate_fn` is called inside the worker and that fetched data is pushed to the main process. So all the updates are happening inside the sub-worker process, but `training_step` runs on the main process, so no effect is reflected there.\r\n\r\npotential sol you can try: https://stackoverflow.com/questions/63460992/how-do-i-fix-the-dataset-to-return-desired-output-pytorch"
  },
  {
    "content": "Hi,\r\nI am training a classifier with a pretrained backbone. I froze the backbone and only trained a linear classifier for some epochs. Now I want to continue training with an unfrozen backbone and a smaller learning rate. \r\n\r\nCan this be done by just just using the `resume_from_checkpoint` argument in the trainer or will the encoder still be frozen?\r\nIf it is not possible what would the simplest way to achieve my goal be?\r\n\r\nThanks  > Can this be done by just just using the resume_from_checkpoint argument in the trainer or will the encoder still be frozen?\r\n\r\nno, since Trainer doesn't know what your backbone is and it doesn't freeze any module manually.\r\nyou can check out [BackboneFinetuning](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.BackboneFinetuning.html) for your usecase.",
    "meta": { "name": "resume_form_checkpoint with different hyperparameters" },
    "answer": "> Can this be done by just just using the resume_from_checkpoint argument in the trainer or will the encoder still be frozen?\r\n\r\nno, since Trainer doesn't know what your backbone is and it doesn't freeze any module manually.\r\nyou can check out [BackboneFinetuning](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.BackboneFinetuning.html) for your usecase."
  },
  {
    "content": "My Dataset returns additional `doc_id` key which is passed to the model to log loss on each document individually within a batch. The problem is that `doc_id` has some large numbers over > 1000. Due to numerical precision issue with fp16, some `doc_id` will have wrong values when I access them during `validation_step`. When running with full-precision, the `doc_id`s are all correct.\r\n\r\nIs there anyway I can avoid applying fp16 to `doc_id`? This shouldn't be a problem with the model as `doc_id` is never used to update the model's parameter. With vanilla PyTorch I think this should be pretty straight forward, however with Lightning it is unclear exactly when the fp16 is getting applied.\r\n\r\nThank you in advance.@MattYoon Is each `doc_id` converted to float16 at the very beginning of your `training_step`?I just found out that this happened because I tried to stack float16 `doc_id` with float32 `loss` to a single tensor. The `doc_id` values were actually correct in `validation_step`, it was corrupted when the stack operation was applied.",
    "meta": { "name": "Do not apply fp16 to certain key returned by Dataset" },
    "answer": "I just found out that this happened because I tried to stack float16 `doc_id` with float32 `loss` to a single tensor. The `doc_id` values were actually correct in `validation_step`, it was corrupted when the stack operation was applied."
  },
  {
    "content": "Hi, \r\n\r\nI tried to set up my model training with 4 gpus using Lightning DDPPlugin. During my debugging process, I found that the data loaded are different on different devices, but a variable I printed out in the loss calculation seems to have the same value on different gpus devices (same up to 4 digits). Is this normal? If so, what is the reason? If not, what might go wrong with my network/dataloader? I really appreciate your comments and help!you can manually check out the targets and outputs used to compute the loss.",
    "meta": { "name": "get same loss on different GPU device" },
    "answer": "you can manually check out the targets and outputs used to compute the loss."
  },
  {
    "content": "I need to train a model multi-phases with a pre-trained backbone.\r\n\r\nFor the first 10 epoch, I want to have the backbone frozen and train the classifier only. After epoch 10, I want to start training the whole network. After a certain point (e.g. 100 epochs), I need to enable certain blocks in the network. In regular PyTorch, I would instantiate a new optimizer adding the backbone params, additional required blocks params that I want to train. Then I\u2019d swap both optimizer and lr_scheduler.\r\n\r\nI know I can make a multi `trainer.fit()`. But, what\u2019s the recommended way to do something like this in PL in a callback like BaseFinetuninig?\r\n\r\nHere\u2019s my sample code in a callback\r\n\r\n```python\r\n    def freeze_before_training(self, pl_module):\r\n        # Here, we are freezing `backbone`\r\n        self.freeze(pl_module.net.encoder)\r\n        if not pl_module.shared_weights:\r\n            self.freeze(pl_module.net.left_encoder)\r\n            self.freeze(pl_module.net.right_encoder)\r\n\r\n    def on_train_start(self, trainer, pl_module) -> None:\r\n\r\n        if trainer.current_epoch == self._unfreeze_at_epoch:\r\n            print(\"unfreeze and add param group...\")\r\n            pl_module.net.freeze_backbone(False)\r\n            new_optimizer = optim.Adam(\r\n                filter(\r\n                    lambda p: p.requires_grad,\r\n                    pl_module.net.parameters()),\r\n                lr=pl_module.lr,\r\n                weight_decay=pl_module.weight_decay)\r\n            new_schedulers = optim.lr_scheduler.ReduceLROnPlateau(\r\n                new_optimizer,\r\n                mode=\"min\",\r\n                factor=0.1,\r\n                patience=pl_module.scheduler_patience,\r\n                cooldown=3,\r\n            )\r\n            # not sure if its correct or safe to do this\r\n            trainer.optimizers = [new_optimizer]\r\n            trainer.lr_schedulers = [new_schedulers]\r\n    if not pl_module.shared_weights and current_epoch == self._enable_left_view_at_epoch:\r\n        # do the same process\r\n        # unfreeze, and change opt and scheduler\r\n    if not pl_module.shared_weights and current_epoch == self._enable_right_view_at_epoch:\r\n       # do the same process\r\n       # unfreeze, and change opt and scheduler\r\n   # and more conditions and blocks\r\n```@icedpanda Lightning supports multiple optimizers. You can define multiple optimizers and LR_schedules in LightningModule.configure_optimizers(), for example:\r\n```\r\nclass yourModule(LightningModule):\r\n    def __init_(..):\r\n\r\n    def configure_optimizers(self):\r\n        optimizer1 = torch.optim.Adam(params, lr)\r\n        optimizer2 = torch.optim.Adam(params, lr)\r\n        lr_scheduler1 = torch.optim.lr_scheduler.StepLR(optimizer1)\r\n        lr_scheduler2 = torch.optim.lr_scheduler. ExponentialLR(optimizer2)\r\n        return [optimizer1, optimizer2], [lr_scheduler1, lr_scheduler2]\r\n\r\n   def training_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            # do training_step when freeze\r\n            ...\r\n        if optimizer_idx == 1:\r\n            # do training_step when unfreeze\r\n            ...\r\n```",
    "meta": {
      "name": "How to change optimizer and lr scheduler in the middle of training"
    },
    "answer": "@icedpanda Lightning supports multiple optimizers. You can define multiple optimizers and LR_schedules in LightningModule.configure_optimizers(), for example:\r\n```\r\nclass yourModule(LightningModule):\r\n    def __init_(..):\r\n\r\n    def configure_optimizers(self):\r\n        optimizer1 = torch.optim.Adam(params, lr)\r\n        optimizer2 = torch.optim.Adam(params, lr)\r\n        lr_scheduler1 = torch.optim.lr_scheduler.StepLR(optimizer1)\r\n        lr_scheduler2 = torch.optim.lr_scheduler. ExponentialLR(optimizer2)\r\n        return [optimizer1, optimizer2], [lr_scheduler1, lr_scheduler2]\r\n\r\n   def training_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            # do training_step when freeze\r\n            ...\r\n        if optimizer_idx == 1:\r\n            # do training_step when unfreeze\r\n            ...\r\n```"
  },
  {
    "content": "In my code, with each batch, I sample a random tensor. However, when I try porting to GPU, I get an error that I'm trying to multiply a tensor on CPU with a tensor on GPU. The [documentation](https://pytorch-lightning.readthedocs.io/en/latest/accelerators/accelerator_prepare.html) states\r\n\r\n> The LightningModule knows what device it is on. You can access the reference via self.device. Sometimes it is necessary to store tensors as module attributes. However, if they are not parameters they will remain on the CPU even if the module gets moved to a new device. To prevent that and remain device agnostic, register the tensor as a buffer in your modules\u2019 __init__ method with register_buffer().\r\n\r\nThe example given is:\r\n\r\n```\r\nclass LitModel(LightningModule):\r\n    def __init__(self):\r\n        ...\r\n        self.register_buffer(\"sigma\", torch.eye(3))\r\n        # you can now access self.sigma anywhere in your module\r\n```\r\n\r\nUsing this example, suppose I want to randomly sample a 3x3 matrix `sigma` with each batch. How do I properly register this tensor?I just found this answer from 2020: https://stackoverflow.com/questions/63660624/normal-distribution-sampling-in-pytorch-lightning\r\n\r\nIs the best solution currently to specify the device?That's the 2020 solution. Is there not a more modern approach?\n\nOn Tue, Aug 9, 2022, 8:05 PM Akihiro Nitta ***@***.***> wrote:\n\n> In my code, with each batch, I sample a random tensor. However, when I try\n> porting to GPU, I get an error that I'm trying to multiply a tensor on CPU\n> with a tensor on GPU.\n>\n> As the thread suggests, you need to move your tensor to GPU manually with\n> self.device in each iteration.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Lightning-AI/lightning/discussions/14131#discussioncomment-3362881>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACEHLCYYBOTRQSDD7EPBGPLVYMMBJANCNFSM56C3FX2Q>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***\n> com>\n>\nOk cool thank you!\n\nCheers,\nRylan Schaeffer\n\n\nOn Tue, Aug 9, 2022 at 8:58 PM Akihiro Nitta ***@***.***>\nwrote:\n\n> There's no \"modern\" approach I could think of, and I'd rather say that\n> PyTorch and PyTorch Lightning are stable enough to keep the same APIs for a\n> few years now :)\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Lightning-AI/lightning/discussions/14131#discussioncomment-3363050>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACEHLCZYJOVFMVCBXAKFUILVYMSFJANCNFSM56C3FX2Q>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***\n> com>\n>\n",
    "meta": { "name": "How to register a (repeatedly) sampled random tensor?" },
    "answer": "I just found this answer from 2020: https://stackoverflow.com/questions/63660624/normal-distribution-sampling-in-pytorch-lightning\r\n\r\nIs the best solution currently to specify the device?"
  },
  {
    "content": "I am seeing the following message in the Tensorboard:\r\n    **No dashboards are active for the current data set.** \r\n\r\nI use the following code\r\n\r\n        def training_step(self, batch, batch_idx):\r\n                \r\n                # computes loss\r\n        \r\n                tensorboard_logs = {'train_loss': loss}\r\n        \r\n                return {\"loss\": loss, 'log': tensorboard_logs}\r\n\r\nI do the same in validation_epoch_end() as well.\r\n\r\nIs this functionality of logging through the 'log' keyword not supported anymore? Should we log explicitly using self.log() or something similar?@kochark1 No, it isn't supported since 1.0.0 (#3681), and yes, you need to use `self.log(...)`. See details here: https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.htmlsaved my day...",
    "meta": {
      "name": "Why is 'log' keyword in the return parameter of training_step of LightningModule is not working?"
    },
    "answer": "@kochark1 No, it isn't supported since 1.0.0 (#3681), and yes, you need to use `self.log(...)`. See details here: https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html"
  },
  {
    "content": "When I use \u201cresume from checkpoint\u201d, \r\nthere is a \u201cCUDA out of memory\u201d problem, \r\nwhen using torch.load(), set \"map location\" to \"cpu\" can solve this problem, \r\nin \"resume from checkpoint\" scenario, what should I do?@Defiler24 Could you share which training strategy(plugin) are you using? Or could you share your code hereI solved the problem after setting the strategy to 'ddp'.I have exactly the same issue, the only difference is that my model is trained on single GPU. I did not specify any stratgey as well, is there any solution to solve it? Thanks",
    "meta": { "name": "\"resume from checkpoint\" lead to CUDA out of memory" },
    "answer": "I solved the problem after setting the strategy to 'ddp'."
  },
  {
    "content": "I\u2019m facing an issue where training a lightning module with DDP on >4 GPUs gets stuck at end of first training epoch (I made sure there is no validation epoch). This doesn\u2019t occur with 2 GPUs.\r\n\r\nI made sure that the dataset is balanced, and that the total batch size is equal to number of GPUs.\r\n\r\nDetecting unused parameters is on. There are unused parameters (and that\u2019s intentional).\r\n\r\nI obtained a stack traces with TORCH_CPP_LOG_LEVEL=INFO and TORCH_DISTRIBUTED_DEBUG=DETAIL.\r\n\r\nI\u2019m having difficulty understanding these stack traces, since they include >10 layers of PyTorch Lightning calls, and I don\u2019t have a good enough understanding of Lighting\u2019s internals. Perhaps someone can glance at this and get a sense for what are the top possible causes?\r\n\r\nStack trace from rank 7:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/solr/app/main_lightning.py\", line 164, in <module>\r\n    trainer.fit(model, datamodule=datamodule)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\r\n    results = self._run_stage()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\r\n    return self._run_train()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py\", line 205, in run\r\n    self.on_advance_end()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 294, in on_advance_end\r\n    self.trainer._call_callback_hooks(\"on_train_epoch_end\")\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1636, in _call_callback_hooks\r\n    fn(self, self.lightning_module, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 308, in on_train_epoch_end\r\n    self._save_topk_checkpoint(trainer, monitor_candidates)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 381, in _save_topk_checkpoint\r\n    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 658, in _save_none_monitor_checkpoint\r\n    filepath = self._get_metric_interpolated_filepath_name(monitor_candidates, trainer)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 619, in _get_metric_interpolated_filepath_name\r\n    while self.file_exists(filepath, trainer) and filepath != del_filepath:\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 720, in file_exists\r\n    return trainer.strategy.broadcast(exists)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/ddp.py\", line 319, in broadcast\r\n    torch.distributed.broadcast_object_list(obj, src, group=_group.WORLD)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/distributed_c10d.py\", line 1869, in broadcast_object_list\r\n    broadcast(object_sizes_tensor, src=src, group=group)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/distributed_c10d.py\", line 1187, in broadcast\r\n    work = default_pg.broadcast([tensor], opts)\r\nRuntimeError: Detected mismatch between collectives on ranks. Rank 7 is running inconsistent collective: CollectiveFingerPrint(OpType=BROADCAST, TensorShape=[1], TensorDtypes=Long, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))\r\n```\r\n\r\nStack trace from rank 2 (ranks 0,1,3,4,5,6 are also similar):\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/solr/app/main_lightning.py\", line 164, in <module>\r\n    trainer.fit(model, datamodule=datamodule)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\r\n    results = self._run_stage()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\r\n    return self._run_train()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 266, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 208, in advance\r\n    batch_output = self.batch_loop.run(batch, batch_idx)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 88, in advance\r\n    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 203, in advance\r\n    result = self._run_optimization(\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 256, in _run_optimization\r\n    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 369, in _optimizer_step\r\n    self.trainer._call_lightning_module_hook(\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1595, in _call_lightning_module_hook\r\n    output = fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/core/lightning.py\", line 1646, in optimizer_step\r\n    optimizer.step(closure=optimizer_closure)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/core/optimizer.py\", line 168, in step\r\n    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/ddp.py\", line 286, in optimizer_step\r\n    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/strategy.py\", line 193, in optimizer_step\r\n    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 155, in optimizer_step\r\n    return optimizer.step(closure=closure, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\", line 88, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/optim/rmsprop.py\", line 96, in step\r\n    loss = closure()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 140, in _wrap_closure\r\n    closure_result = closure()\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 148, in __call__\r\n    self._result = self.closure(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 143, in closure\r\n    self._backward_fn(step_output.closure_loss)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 311, in backward_fn\r\n    self.trainer._call_strategy_hook(\"backward\", loss, optimizer, opt_idx)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1765, in _call_strategy_hook\r\n    output = fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/strategy.py\", line 168, in backward\r\n    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 80, in backward\r\n    model.backward(closure_loss, optimizer, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/core/lightning.py\", line 1391, in backward\r\n    loss.backward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\", line 363, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 173, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\nRuntimeError: Detected mismatch between collectives on ranks. Rank 2 is running inconsistent collective: CollectiveFingerPrint(OpType=ALLREDUCE, TensorShape=[283125], TensorDtypes=Float, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))\r\n```Found the issue. Even with find_unused_parameters=True, there needs to be at least one used parameter every training step.\r\n\r\nI had a unique case where for some batches no parameters were used. This caused ranks to lose sync. My guess for why this happens is as follows: The ranks with used parameters would get stuck on allreduce in the backwards hook, waiting for the rank with no used parameters to catch up. However, the rank with no used parameters doesn't hit a backwards hook, and instead proceeds to the next training step, when it eventually joins up with the allreduce operation. Since it has now done one more training step than the other ranks, it will run out of data at the end of the epoch earlier. When this happens, it proceeds to save the model checkpoint, while the other ranks are still waiting on gradient allreduce.\r\n\r\nThe quick workaround was to add a dummy parameter to the model and simply return it in place of the loss in this special case.",
    "meta": { "name": "Collective mismatch at end of training epoch" },
    "answer": "Found the issue. Even with find_unused_parameters=True, there needs to be at least one used parameter every training step.\r\n\r\nI had a unique case where for some batches no parameters were used. This caused ranks to lose sync. My guess for why this happens is as follows: The ranks with used parameters would get stuck on allreduce in the backwards hook, waiting for the rank with no used parameters to catch up. However, the rank with no used parameters doesn't hit a backwards hook, and instead proceeds to the next training step, when it eventually joins up with the allreduce operation. Since it has now done one more training step than the other ranks, it will run out of data at the end of the epoch earlier. When this happens, it proceeds to save the model checkpoint, while the other ranks are still waiting on gradient allreduce.\r\n\r\nThe quick workaround was to add a dummy parameter to the model and simply return it in place of the loss in this special case."
  },
  {
    "content": "If I want to use CLI to initialize datamodule object I should use:\r\n\r\n`python train.py  --config data.yaml`\r\n\r\n```\r\ndata:\r\n  class_path: foo.datasets.my_data\r\n  init_args:\r\n    data_dir: data\r\n```\r\n\r\nBut what if one the `init_arguments` is a function? How can I pass a reference to it like this? \r\n\r\n```\r\ndata:\r\n  class_path: foo.datasets.my_data\r\n  init_args:\r\n    data_dir: data\r\n    filter: foo.datasets.filter1\r\n```\r\n\r\n`foo.datasets.filter1` is a function.\r\n\r\nThank you for attention.This is supported in jsonargparse. Just use `Callable` as type hint, see its mention in the [docs](https://jsonargparse.readthedocs.io/en/stable/#type-hints). Basically you would have:\r\n```python\r\nclass MyData:\r\n    def __init__(self, data_dir: str, filter: Callable):\r\n        ...\r\n```\r\nThen the config could be as you had it:\r\n```yaml\r\nclass_path: foo.datasets.my_data\r\ninit_args:\r\n  data_dir: data\r\n  filter: foo.datasets.filter1\r\n```\r\nOr `filter` could be a class that once instantiated becomes callable, like:\r\n```yaml\r\nclass_path: foo.datasets.my_data\r\ninit_args:\r\n  data_dir: data\r\n  filter:\r\n    class_path: path.to.callable.class\r\n    init_args:\r\n      param1: val1\r\n      ...\r\n```",
    "meta": {
      "name": "How to pass a reference to a function in yaml config file"
    },
    "answer": "This is supported in jsonargparse. Just use `Callable` as type hint, see its mention in the [docs](https://jsonargparse.readthedocs.io/en/stable/#type-hints). Basically you would have:\r\n```python\r\nclass MyData:\r\n    def __init__(self, data_dir: str, filter: Callable):\r\n        ...\r\n```\r\nThen the config could be as you had it:\r\n```yaml\r\nclass_path: foo.datasets.my_data\r\ninit_args:\r\n  data_dir: data\r\n  filter: foo.datasets.filter1\r\n```\r\nOr `filter` could be a class that once instantiated becomes callable, like:\r\n```yaml\r\nclass_path: foo.datasets.my_data\r\ninit_args:\r\n  data_dir: data\r\n  filter:\r\n    class_path: path.to.callable.class\r\n    init_args:\r\n      param1: val1\r\n      ...\r\n```"
  },
  {
    "content": "I just trying to use `StepLR`  in my `.yaml` config. \r\n```yaml\r\nlr_scheduler:\r\n  class_path: StepLR\r\n  init_args:\r\n    step_size : 10\r\n    gamma : 0.5\r\n```\r\nI guess the CLI will set the `interval` as \"epoch\", but I want to use the \"step\" to control my learning rate.  I do not find anything about it in the docs. How can achieve it just in my `.yaml` file?cc: @carmocca This is not possible from the yaml file. If you need to customize the learning rate scheduler configuration, you can do so by overriding:\r\n\r\n\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    @staticmethod\r\n    def configure_optimizers(lightning_module, optimizer, lr_scheduler=None):\r\n        if lr_scheduler is None:\r\n            return optimizer\r\n        return {\r\n            \"optimizer\": optimizer,\r\n            \"lr_scheduler\": {\"scheduler\": lr_scheduler, \"interval\": \"step\"},\r\n        }\r\n```",
    "meta": { "name": "Change the scheduler interval in CLI" },
    "answer": "This is not possible from the yaml file. If you need to customize the learning rate scheduler configuration, you can do so by overriding:\r\n\r\n\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    @staticmethod\r\n    def configure_optimizers(lightning_module, optimizer, lr_scheduler=None):\r\n        if lr_scheduler is None:\r\n            return optimizer\r\n        return {\r\n            \"optimizer\": optimizer,\r\n            \"lr_scheduler\": {\"scheduler\": lr_scheduler, \"interval\": \"step\"},\r\n        }\r\n```"
  },
  {
    "content": "Hi I have trained the model using trainer and was trying to use trainer.predict() method to predict on the datamodule. But it throws the following error:\r\n\r\n```\r\nMisconfigurationException                 Traceback (most recent call last)\r\nInput In [200], in <cell line: 1>()\r\n----> 1 trainer.predict(dataloaders=datamodule)\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1025, in Trainer.predict(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\r\n   1000 r\"\"\"\r\n   1001 Run inference on your data.\r\n   1002 This will call the model forward function to compute predictions. Useful to perform distributed\r\n   (...)\r\n   1022     Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\r\n   1023 \"\"\"\r\n   1024 self.strategy.model = model or self.lightning_module\r\n-> 1025 return self._call_and_handle_interrupt(\r\n   1026     self._predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\r\n   1027 )\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:723, in Trainer._call_and_handle_interrupt(self, trainer_fn, *args, **kwargs)\r\n    721         return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)\r\n    722     else:\r\n--> 723         return trainer_fn(*args, **kwargs)\r\n    724 # TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\r\n    725 except KeyboardInterrupt as exception:\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1072, in Trainer._predict_impl(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\r\n   1066 self._ckpt_path = self.__set_ckpt_path(\r\n   1067     ckpt_path, model_provided=model_provided, model_connected=self.lightning_module is not None\r\n   1068 )\r\n   1070 self._predicted_ckpt_path = self.ckpt_path  # TODO: remove in v1.8\r\n-> 1072 results = self._run(model, ckpt_path=self.ckpt_path)\r\n   1074 assert self.state.stopped\r\n   1075 self.predicting = False\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1160, in Trainer._run(self, model, ckpt_path)\r\n   1157 self._callback_connector._attach_model_callbacks()\r\n   1158 self._callback_connector._attach_model_logging_functions()\r\n-> 1160 verify_loop_configurations(self)\r\n   1162 # hook\r\n   1163 log.detail(f\"{self.__class__.__name__}: preparing data\")\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:48, in verify_loop_configurations(trainer)\r\n     46     __verify_eval_loop_configuration(trainer, model, \"test\")\r\n     47 elif trainer.state.fn == TrainerFn.PREDICTING:\r\n---> 48     __verify_eval_loop_configuration(trainer, model, \"predict\")\r\n     50 __verify_dp_batch_transfer_support(trainer, model)\r\n     51 _check_add_get_queue(model)\r\n\r\nFile /opt/conda/envs/pytorch-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:197, in __verify_eval_loop_configuration(trainer, model, stage)\r\n    193 # -----------------------------------\r\n    194 # verify model has an eval_dataloader\r\n    195 # -----------------------------------\r\n    196 if not has_loader:\r\n--> 197     raise MisconfigurationException(f\"No `{loader_name}()` method defined to run `Trainer.{trainer_method}`.\")\r\n    199 # predict_step is not required to be overridden\r\n    200 if stage == \"predict\":\r\n\r\nMisconfigurationException: No `predict_dataloader()` method defined to run `Trainer.predict`.\r\n```\r\nI have the following dataloader:\r\n```\r\nclass ProductDatasetModule(pl.LightningDataModule):\r\n    def __init__(self,train_data,test_data,tokenizer,train_transforms,test_transforms,batch_size):\r\n        super().__init__()\r\n        self.train_data = train_data\r\n        self.test_data = test_data\r\n        self.tokenizer = tokenizer\r\n        self.batch_size= batch_size\r\n        self.train_transforms = train_transforms\r\n        self.test_transforms = test_transforms\r\n        \r\n    def setup(self,stage=None):\r\n        self.train_dataset = ImageTextDataset(self.train_data,tokenizer=self.tokenizer,transforms=self.train_transforms)\r\n        self.test_dataset = ImageTextDataset(self.test_data,tokenizer=self.tokenizer,transforms=self.test_transforms)\r\n        \r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(\r\n        self.train_dataset,\r\n        batch_size=self.batch_size,\r\n        num_workers=4,\r\n        shuffle=True)\r\n    \r\n    def test_dataloader(self):\r\n        \r\n        return torch.utils.data.DataLoader(\r\n        self.test_dataset,\r\n        batch_size=self.batch_size,\r\n        num_workers=4,\r\n        shuffle=False)\r\n\r\n    def val_dataloader(self):\r\n        return torch.utils.data.DataLoader(\r\n        self.test_dataset,\r\n        batch_size=self.batch_size,\r\n        num_workers=4,\r\n        shuffle=False)\r\n        \r\n```\r\n\r\nI have following model defined:\r\n\r\n```\r\nclass MLP(pl.LightningModule):\r\n    \"\"\"\r\n    Creates a MTL model with the encoder from \"arch\" and with dropout multiplier ps.\r\n    \"\"\"\r\n    def __init__(self, model,input_embedding_dim,output_dim,learning_rate,batch_size):\r\n        super(MLP,self).__init__()\r\n        self.encoder = model\r\n        for param in model.parameters():\r\n            param.requires_grad = False\r\n        self.batch_size=batch_size\r\n        self.projection = nn.Linear(input_embedding_dim, 256)\r\n        # self.layernorm = nn.LayerNorm()\r\n        self.gelu = nn.ReLU()\r\n        self.fc = nn.Linear(256, 256)\r\n        self.dropout = nn.Dropout(0.15)\r\n        self.fc2 = nn.Linear(256, 128)\r\n        self.fc3 = nn.Linear(128, 64)\r\n        # self.fc2 = nn.Linear(128, 128)\r\n        self.fc4 = nn.Linear(64,output_dim)\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        self.learning_rate = learning_rate\r\n        self.train_acc = torchmetrics.Accuracy()\r\n        self.train_f1 = torchmetrics.F1Score(number_classes=25,\r\n        average=\"micro\")\r\n        self.train_auroc = torchmetrics.AUROC(number_classes=25,\r\n        average=\"micro\")\r\n        self.val_acc = torchmetrics.Accuracy()\r\n        self.val_f1 = torchmetrics.F1Score(number_classes=25,\r\n        average=\"micro\")\r\n        self.val_auroc = torchmetrics.AUROC(number_classes=25,\r\n        average=\"micro\")\r\n        #self.layer_norm = nn.LayerNorm(output_dim)\r\n\r\n    def forward(self,x):\r\n        image_features = self.encoder.image_encoder(x[\"image\"].to(CFG.device))\r\n        image_embeddings = self.encoder.image_projection(image_features)\r\n        text_features = self.encoder.text_encoder(\r\n        input_ids=x[\"input_ids\"], attention_mask=x[\"attention_mask\"])\r\n        text_embeddings = self.encoder.text_projection(text_features)\r\n        img_txt_embeddings = torch.cat((image_embeddings,text_embeddings),dim=1)\r\n        projected = self.projection(img_txt_embeddings)\r\n        # z = self.layernorm(z)\r\n        z= self.gelu(projected)\r\n        z= self.fc(z)\r\n        # z=self.layernorm(z)\r\n        z= self.gelu(z)\r\n        z= self.dropout(z)\r\n        z= self.fc2(z)\r\n        # z=self.layernorm(z)\r\n        z= self.gelu(z)\r\n        z= self.dropout(z)\r\n        z= self.fc3(z)\r\n        z= self.gelu(z)\r\n        z= self.fc4(z)\r\n        \r\n        \r\n        loss = self.criterion(z,x[\"label\"])\r\n        return loss,z\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        y=batch[\"label\"]\r\n        loss,y_pred= self(batch)\r\n        y_pred = y_pred.softmax(dim=-1)\r\n        # accumulate and return metrics for logging\r\n        acc = self.train_acc(y_pred, y)\r\n        f1 = self.train_f1(y_pred, y)\r\n        # just accumulate\r\n        self.train_auroc.update(y_pred, y)\r\n        self.log(\"train_accuracy\", acc,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"train_f1\", f1,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"train_loss\",loss, prog_bar=True, on_step=True, on_epoch=True,logger=True,batch_size=self.batch_size)\r\n        return loss\r\n    def validation_step(self, batch, batch_idx):\r\n        y=batch[\"label\"]\r\n        loss,y_pred= self(batch)\r\n        y_pred = y_pred.softmax(dim=-1)\r\n        self.val_acc.update(y_pred, y)\r\n        self.val_f1.update(y_pred, y)\r\n        self.val_auroc.update(y_pred, y)\r\n        self.log(\"val_accuracy\",self.val_acc,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"val_f1\",self.val_f1,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"val_loss\", loss, prog_bar=True,on_step=True, on_epoch=True, logger=True,batch_size=self.batch_size)\r\n        return loss\r\n    def test_step(self, batch, batch_idx):\r\n        y=batch[\"label\"]\r\n        loss,y_pred= self(batch)\r\n        y_pred = y_pred.softmax(dim=-1)\r\n        self.val_acc.update(y_pred, y)\r\n        self.val_f1.update(y_pred, y)\r\n        self.val_auroc.update(y_pred, y)\r\n        self.log(\"test_accuracy\",self.val_acc,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"test_f1\",self.val_f1,prog_bar=True, on_step=True, on_epoch=True,logger=True)\r\n        self.log(\"test_loss\", loss, prog_bar=True, on_step=True, on_epoch=True,logger=True,batch_size=self.batch_size)\r\n        return loss\r\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\r\n        y=batch[\"label\"]\r\n        loss,y_pred= self(x)\r\n        return y_pred.softmax(dim=-1)\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.AdamW(self.parameters(),lr= self.learning_rate)\r\n        return optimizer\r\n```\r\n\r\nPlease, help to predict on the testdata. How I can leverage trainer to predict on the test data and get classification report for the predicted output?\r\n\r\nThanks@karndeepsingh To use `Trainer.predict()`, You must have `predict_dataloader()` defined in your LightningModule or LightningDataModule as the error message states:\r\n```\r\nMisconfigurationException: No `predict_dataloader()` method defined to run `Trainer.predict`.\r\n```\r\n\r\nIf you'd like to run inference on your test set, you just need to define `predict_dataloader()` with your test set:\r\n```python\r\n    def predict_dataloader(self):\r\n        return torch.utils.data.DataLoader(\r\n        self.test_dataset,\r\n        batch_size=self.batch_size,\r\n        num_workers=4,\r\n        shuffle=False)\r\n```",
    "meta": {
      "name": "How to predict on the test dataset using trainer.predict()?"
    },
    "answer": "@karndeepsingh To use `Trainer.predict()`, You must have `predict_dataloader()` defined in your LightningModule or LightningDataModule as the error message states:\r\n```\r\nMisconfigurationException: No `predict_dataloader()` method defined to run `Trainer.predict`.\r\n```\r\n\r\nIf you'd like to run inference on your test set, you just need to define `predict_dataloader()` with your test set:\r\n```python\r\n    def predict_dataloader(self):\r\n        return torch.utils.data.DataLoader(\r\n        self.test_dataset,\r\n        batch_size=self.batch_size,\r\n        num_workers=4,\r\n        shuffle=False)\r\n```"
  },
  {
    "content": "Hi all, I migrated my code to the latest version of lightnining and tried using `accelerator=auto` and `devices=-1` to use all available GPUs but I get:\r\n\r\nUserWarning: The flag `devices=-1` will be ignored, instead the device specific number 1\r\n\r\nWhat is the recommended way to use, by default, all GPUs (when present)?Hi @mnslarcher! To use all of available devices, use:\r\n```python\r\nTrainer(..., devices=\"auto\")\r\n```\r\n\r\nThe doc page is available here: https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#devicesFor anyone seeing this discussion, have a look at https://github.com/Lightning-AI/lightning/issues/12756#issuecomment-1106629824.",
    "meta": { "name": "How to use all the available GPUs" },
    "answer": "For anyone seeing this discussion, have a look at https://github.com/Lightning-AI/lightning/issues/12756#issuecomment-1106629824."
  },
  {
    "content": "I'm working on a model with multiple losses that I add together: an autoencoder reconstruction loss (mse) and a prediction loss (cross entropy).\r\n\r\nI now want to weight them based on the `reconstruction loss / prediction loss` ratio that is derived from a random initialization of the network (prior to training). I've thought about calculating these ratios using the `on_train_start` hook and then storing them in self. However, as far as I'm aware I do not have access to/cannot pass my dataloaders to `on_train_start`?\r\n\r\nAlternatively, I've also thought about creating a custom method in my model class that calculates the weights, stores them in self and then I would call that method manually after instantiating my model but prior to calling fit. Something like:\r\n```python\r\nclass MyModelClass(pl.LightningModule):\r\n    def __init__(self):\r\n        ...\r\n    \r\n    def calculate_loss_weights(self):\r\n        # calculate my ratio\r\n        self.reconstruction_prediction_ratio = ratio\r\n\r\n    def training_step(self, loaders):\r\n        # all my training code\r\n        return self.reconstruction_prediction_ratio * reconstruction loss + prediction_loss\r\n\r\ndata = MyDataClass()\r\nmodel = MyModelClass()\r\nmodel.calculate_loss_weights()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(model, data)\r\n```\r\n\r\nHowever, I've realized that here too I don't know how to access my data loaders. \r\n\r\nIs there any way to implement this, and are there any best practices in terms of implementation I should stick to?\r\n\r\nTwo things to note:\r\n1. these weights would not change during training but remain fixed\r\n2. I have about 4 different dataloaders at the moment, most of which need to be passed to calculate the loss ratiosHi @Michael-Geuenich, I think #8114 will help.",
    "meta": {
      "name": "Weighting different losses based on random initialization of network"
    },
    "answer": "Hi @Michael-Geuenich, I think #8114 will help."
  },
  {
    "content": "Hi all, \r\n\r\nI am using the following code to start a trainer with multrigpu.\r\n```python\r\npl.Trainer(accelerator=\"gpu\", devices=get_num_gpus(), strategy=\"ddp\")\r\n```\r\n\r\nand then I have this line of code: \r\n```python3\r\ninference_outputs = self.trainer.predict(self.embedding_model, inference_dataloader)\r\n\r\nprint(\"abc\")\r\n```\r\n\r\nWhat I am seeing is that the print(\"abc\" is being printed to the number of available devices while I would hav expect only the predict function to run on multiple gpu and processes and then finish before running the next line and gather all results into `inference_outputs`. \r\n\r\nAm I missing something? Is there a way to achieve what I just described?@hfaghihi15 That's how it is! With DDP, Lightning runs the whole script in its subprocesses as described in the doc here: https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu.html#distributed-data-parallel\r\n\r\n\r\n> This Lightning implementation of DDP calls your script under the hood multiple times with the correct environment variables:\r\n> ```shell\r\n> # example for 3 GPUs DDP\r\n> MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=0 LOCAL_RANK=0 python my_file.py --accelerator 'gpu' --devices 3 --etc\r\n> MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=1 LOCAL_RANK=0 python my_file.py --accelerator 'gpu' --devices 3 --etc\r\n> MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=2 LOCAL_RANK=0 python my_file.py --accelerator 'gpu' --devices 3 --etc\r\n> ```\r\n\r\n\r\n---\r\n\r\nIn case you want to run something in only one process, you can utilise the trainer property:\r\n```python\r\nif trainer.is_global_zero:\r\n    print(\"abc\")\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#is-global-zero@akihironitta Thanks for your answer, what if I run this with `DP` because I saw the exact same issue even there. \r\n",
    "meta": {
      "name": "Why does DDP mode continue the program in multiple process for longer than intended?"
    },
    "answer": "@hfaghihi15 That's how it is! With DDP, Lightning runs the whole script in its subprocesses as described in the doc here: https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu.html#distributed-data-parallel\r\n\r\n\r\n> This Lightning implementation of DDP calls your script under the hood multiple times with the correct environment variables:\r\n> ```shell\r\n> # example for 3 GPUs DDP\r\n> MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=0 LOCAL_RANK=0 python my_file.py --accelerator 'gpu' --devices 3 --etc\r\n> MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=1 LOCAL_RANK=0 python my_file.py --accelerator 'gpu' --devices 3 --etc\r\n> MASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=2 LOCAL_RANK=0 python my_file.py --accelerator 'gpu' --devices 3 --etc\r\n> ```\r\n\r\n\r\n---\r\n\r\nIn case you want to run something in only one process, you can utilise the trainer property:\r\n```python\r\nif trainer.is_global_zero:\r\n    print(\"abc\")\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#is-global-zero"
  },
  {
    "content": "Hi,\r\n\r\nI have a model with multiple models inside the object (similar to a GAN) except that i want to use a single loss function with multiple optimizers.  I am disabling automatic_optimization using the **automatic_optimization** flag. The code has been implemented and it works for a single GPU configuration.\r\n\r\nIn order to accelerate the training process, i need to use DDP and across 3 GPU devices (1 node, multiple devices). The distributed training succeeds but i am not sure if it is working the way it is supposed to. Is there a way to figure out things?\r\n\r\nMy psuedo code is something like below, cant share the full code due to NDAs:\r\n[sample_model.py.txt](https://github.com/PyTorchLightning/pytorch-lightning/files/7441363/sample_model.py.txt)\r\n\r\nMy query is, for distributed training with multiple optimizers, will the above code work in the INTENDED way? What should **training_step_end** function contain then? and how does multiple optimizers update across different devices?\r\n\r\nThank you\r\n\r\n---\r\nEDIT by @akihironitta\r\n\r\n<details><summary>provided script</summary>\r\n\r\n```python\r\nimport <standard imports>\r\n\r\nclass some_model1(nn.Module):\r\n\r\n    def __init__(self):\r\n        some layers\r\n    def forward(self, x, labels):\r\n        some calcuation on x and labels\r\n        return feat1\r\n        \r\n\r\nclass some_model2(nn.Module):\r\n\r\n    def __init__(self):\r\n        some layers\r\n\r\n    def forward(self, x, labels):\r\n        some calcuation on x and labels\r\n        return feat2\r\n\r\n\r\n\r\nclass some_model3(nn.Module):\r\n\r\n    def __init__(self):\r\n        some layers\r\n\r\n    def forward(self, x, labels):\r\n        some calcuation on x and labels\r\n        return feat3\r\n\r\n\r\nclass some_model4(nn.Module):\r\n\r\n    def __init__(self):\r\n        some layers\r\n\r\n    def forward(self, x, labels):\r\n        some calcuation on x and labels\r\n        return loss\r\n\r\n\r\n\r\nclass Model(pl.LightningModule):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n\r\n        self.model1 = some_model1()\r\n        self.model2 = some_model2()\r\n        self.model3 = some_model3()\r\n        self.sub_model = some_model4()\r\n\r\n        self.common = nn.Sequential(some_layers)\r\n\r\n        self.sofmax_layer = nn.Softmax(dim=-1)   \r\n        self.automatic_optimization=False\r\n\r\n    def forward(self, input_dict, output_dict, *args, **kwargs):\r\n\r\n        data1, data2, data3 = input_dict['data1'], input_dict['data2'],input_dict['data3']\r\n\r\n        feat1 = self.model1(data1)\r\n        pred1 = self.common(feat1)\r\n\r\n        feat2 = self.model2(data2)\r\n        pred3 = self.common(feat2)\r\n\r\n        feat3 = self.model3(data3)\r\n        pred3 = self.common(feat3)\r\n\r\n        combined_metrics2 = some metric_calculation based on output_dict\r\n\r\n        return {'combined_metrics1':combined_metrics1, 'combined_metrics2':combined_metrics2}\r\n\r\n    def training_step(self, batch, batch_idx, *args, **kwargs): \r\n        input_dict, output_dict = batch\r\n\r\n        self.optimizers()[0].zero_grad()\r\n        self.optimizers()[1].zero_grad()\r\n\r\n        combined_metrics1, combined_metrics2 = self.forward(input_dict, output_dict)\r\n\r\n        final_loss = some_loss_fn(combined_metrics1, combined_metrics2, output_dict)\r\n\r\n        self.optimizers()[0].zero_grad()\r\n        self.optimizers()[1].zero_grad()\r\n\r\n        self.manual_backward(final_loss)\r\n\r\n\r\n        self.optimizers()[0].step()\r\n        self.optimizers()[1].step()\r\n\r\n        return combined_metrics1, combined_metrics2\r\n\r\n    def training_step_end(self, outputs):\r\n\r\n        return ??\r\n\r\n    def configure_optimizers(self):\r\n        normal_params = list(self.model1.parameters()) + list(self.model2.parameters()) + list(self.model3.parameters())\r\n        self.normal_opt = optim.Adam(\r\n            normal_params,\r\n            lr = 0.001\r\n        )\r\n        self.opt2 = optim.Adam(\r\n            self.sub_model.parameters(),\r\n            lr= 0.001 \r\n        )\r\n        return self.normal_opt, self.opt2 \r\n\r\nif __name__ == '__main__':\r\n    from xyz import TrainDataloader\r\n    from xyz import TestDataloader\r\n\r\n    train_set = TrainDataloader(params, partition='train')\r\n    test_set = TestDataloader(params, partition='test')\r\n\r\n    data_loader_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True,num_workers=1)\r\n    data_loader_test = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True,num_workers=1)\r\n\r\n    class DataModule(pl.LightningDataModule):\r\n        def __init__(self, batch_size: int = 32):\r\n            super().__init__()\r\n            self.batch_size = batch_size\r\n\r\n        def setup(self, stage=None):\r\n            print('Setting up the data loader')\r\n\r\n        def train_dataloader(self):\r\n            return data_loader_loader \r\n\r\n        def val_dataloader(self):\r\n            return data_loader_test \r\n\r\n        def test_dataloader(self):\r\n            return data_loader_test \r\n\r\n        def teardown(self):\r\n            # Used to clean-up when the run is finished\r\n            ...\r\n    dm = DataModule()\r\n\r\n    from pytorch_lightning.loggers import TensorBoardLogger\r\n\r\n    logger = TensorBoardLogger(\"logs\", name=\"Custom\")\r\n\r\n    model = Model(params)\r\n    trainer = pl.Trainer(max_epochs=50, log_every_n_steps=2, gpus=2, accelerator='ddp', logger=logger)\r\n    trainer.fit(model, datamodule=dm) \r\n```\r\n\r\n</detaills>> My query is, for distributed training with multiple optimizers, will the above code work in the INTENDED way?\r\n\r\nYour code looks good to me.\r\n\r\n> What should training_step_end function contain then?\r\n\r\nNo need to do anything if you don't need to run anything at the end of `training_step`.\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/common/lightning_module.html#training-step-end\r\n\r\n> how does multiple optimizers update across different devices?\r\n\r\nDDP syncs gradients across different devices overlapping backprop, and each device updates the weights with gradients synced across devices. See the PyTorch documentation for details: https://pytorch.org/docs/1.12/notes/ddp.html",
    "meta": { "name": "Distributed training with multiple optimizers" },
    "answer": "> My query is, for distributed training with multiple optimizers, will the above code work in the INTENDED way?\r\n\r\nYour code looks good to me.\r\n\r\n> What should training_step_end function contain then?\r\n\r\nNo need to do anything if you don't need to run anything at the end of `training_step`.\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/common/lightning_module.html#training-step-end\r\n\r\n> how does multiple optimizers update across different devices?\r\n\r\nDDP syncs gradients across different devices overlapping backprop, and each device updates the weights with gradients synced across devices. See the PyTorch documentation for details: https://pytorch.org/docs/1.12/notes/ddp.html"
  },
  {
    "content": "Hello and thanks for this amazing library. \r\n\r\nWhen training models I find useful checking which parameters change after backpropagation. \r\n\r\nIn vanilla Pytorch I would do something like\r\n\r\n```\r\nparameters_before = model.parameters()\r\n\r\n...\r\n\r\nloss.backward()\r\noptimizer.step()\r\n...\r\nparameters_after = model.parameters()\r\n\r\n[print( (parameter_before != parameter_after).mean() ) for parameter_before, parameter_after\r\nin zip(parameters_before, parameters_after)]\r\n\r\n```\r\n\r\nAnd I should see a bunch of ones. Some zeroes can indicate branches of the network where the gradients are not flowing back. \r\n\r\nI was thinking of a callback that does the same within PL. After reading the docs, I have came up with this:\r\n\r\n```\r\nclass CheckParamsUpdatedCallback(Callback):\r\n    def __init__(self):\r\n        self.prms_for_check = dict.fromkeys(('before', 'after'))\r\n\r\n    def on_train_epoch_start(self, trainer, pl_module):\r\n        self.prms_for_check['before'] = pl_module.parameters()\r\n\r\n    def on_validation_epoch_start(self, trainer, pl_module):\r\n        self.prms_for_check['after'] = pl_module.parameters()\r\n        for v in self.prms_for_check.values():\r\n            if v is None:  # handles validation sanity check\r\n                return None\r\n        print([(pbef != paft).float().mean().item()\r\n               for pbef, paft in zip(self.prms_for_check['before'], self.prms_for_check['after'])])\r\n```\r\n\r\n\r\nNevertheless, I believe that pl_module.parameters() is not returning the actual parameters used in the optimization, as I am always getting zeroes printed out, even if other indicators (loss/metrics) suggest the model is being trained well.\r\n\r\nHow can I access the model parameters during training process?\r\n\r\nThanks a lot and best wishes,\r\n\r\nVictor> And I should see a bunch of ones.\r\n\r\nyou should see or you did see. I think the parameters saved in the dict are references to the same object hence you can't see any difference there.\r\n\r\n> pl_module.parameters()\r\n\r\nalso, this is a generator.\r\n\r\nyou can deepcopy the state_dict instead to verify it.",
    "meta": { "name": "Access model weights during training" },
    "answer": "> And I should see a bunch of ones.\r\n\r\nyou should see or you did see. I think the parameters saved in the dict are references to the same object hence you can't see any difference there.\r\n\r\n> pl_module.parameters()\r\n\r\nalso, this is a generator.\r\n\r\nyou can deepcopy the state_dict instead to verify it."
  },
  {
    "content": "Hi,\r\nwhen I use the `limit_train_batches` flag in the trainer and set it to `0.1` does it use the same batches in each epoch or does it randomly sample 10% of the images (in my case) from the training data?\r\n\r\nThxIt completely depends on your dataloader's arg `shuffle=True|False` (or your sampler specifically).So this means that when I set `shuffle=True` it loads different samples in every epoch, right?\r\n(I use the standard torch `DataLoader`, without any fancy sampler.)  ",
    "meta": { "name": "limit_train_batches" },
    "answer": "It completely depends on your dataloader's arg `shuffle=True|False` (or your sampler specifically)."
  },
  {
    "content": "I'm looking to create a callback that, given some external \"signal\", will stop training & save a checkpoint so that training can be resumed later.\r\n\r\nHere is the code so far. The \"signal\" in this case is a file that i create.\r\n\r\n```python\r\nimport os\r\nfrom pathlib import Path\r\nfrom pytorch_lightning.callbacks import Callback\r\n\r\nclass SignalStopCallback(Callback):\r\n    '''Given signal, will stop training'''\r\n\r\n    def __init__(self, signal_fpath: Path, chkpoint_dir: Path):\r\n        super().__init__()\r\n        self.signal_fpath = signal_fpath\r\n        self.chkpoint_dir = chkpoint_dir\r\n\r\n    def on_train_epoch_end(self, trainer, pl_module):\r\n        if self.signal_fpath.exists():\r\n            print(f'Signal stop found, stopping at epoch {trainer.current_epoch}')\r\n            trainer.save_checkpoint(self.chkpoint_dir.joinpath('last.ckpt'))\r\n            trainer.should_stop = True\r\n            os.remove(self.signal_fpath)\r\n```\r\n\r\nI believe this should work in a single GPU training setting. However, in DDP (one node, 4 GPUs), I believe this will cause undefined behaviour as once the file is removed by one process, `self.signal_fpath.exists()` will be evaluated as `False` in the others.\r\n\r\nI could remedy this situation by having this function not delete the file, and delete it manually myself. But is there a simple way to have the desired functionality here? For example is there a way to set `trainer.should_stop` only on the rank 0 process (by using `@rank_zero_only`, but have all other processes end safely?\r\n\r\nThanksHi @m-lyon,\r\n\r\nUse `trainer.strategy.barrier()` to ensure all processes are at the same line. It is strategy-agnostic, so when you're using a single device, for example, it will be just no-op.\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.strategies.Strategy.html#pytorch_lightning.strategies.Strategy.barrier",
    "meta": { "name": "Custom callback to stop training in DDP." },
    "answer": "Hi @m-lyon,\r\n\r\nUse `trainer.strategy.barrier()` to ensure all processes are at the same line. It is strategy-agnostic, so when you're using a single device, for example, it will be just no-op.\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.strategies.Strategy.html#pytorch_lightning.strategies.Strategy.barrier"
  },
  {
    "content": "\r\n![image](https://user-images.githubusercontent.com/29114869/180907549-d7619959-d6c8-4cd3-8417-04ab91dacccc.png)\r\n\r\nThe loss in the red box indicates? \r\nMy understanding: loss_step denotes the loss of the current step (batch-level), loss_epoch denotes the loss of an epoch. So, the loss in the red box denotes ? ideally it is just the step loss, but you might notice that it's different from `loss_step`. The reason is `loss` is a running loss with certain window.\r\n\r\nThere is a [tracking issue](https://github.com/Lightning-AI/lightning/issues/9372), that will remove the averaging window.",
    "meta": { "name": "About the loss on the progress bar" },
    "answer": "ideally it is just the step loss, but you might notice that it's different from `loss_step`. The reason is `loss` is a running loss with certain window.\r\n\r\nThere is a [tracking issue](https://github.com/Lightning-AI/lightning/issues/9372), that will remove the averaging window."
  },
  {
    "content": "I am trying to save my Callback state along with my model at the end of each epoch.\r\nI tried reimplementing the example [here](https://pytorch-lightning.readthedocs.io/en/stable/extensions/callbacks.html#persisting-callback-state).\r\n\r\nI put print statement/breakpoint at the state_dict and load_state_dict method, and they are never called during my training. And the Counter state is not recovered.\r\n\r\nI tried to use checkpointing both with a callback ModelCheckpoint in the callback list of the trainer and with enable_checkpointing set to True (with or without the ModelCheckpoint) without it working.\r\n\r\nFor reference, other states are well loaded (for example the right epoch, the model weights etc).\r\n\r\nHere is a minimum working example:\r\n1) First train a model from scratch and save it\r\n2) put `loading = True`\r\n3) Rerun the script, in the training_step the value self.current_epoch et self.cpt are recovered but not the state of the Callback Counter.\r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom pytorch_lightning import Callback, Trainer\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom torch.nn import Parameter\r\nfrom torch.optim import Adam\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\n\r\nclass Dummy_dataset(Dataset):\r\n\tdef __init__(self, length):\r\n\t\tself.length = length\r\n\r\n\tdef __len__(self):\r\n\t\treturn self.length\r\n\r\n\tdef __getitem__(self, item):\r\n\t\treturn 1\r\n\r\n\r\nclass Dummy_Model(pl.LightningModule):\r\n\tdef __init__(self):\r\n\t\tsuper(Dummy_Model, self).__init__()\r\n\t\tself.cpt = Parameter(torch.tensor(0.), requires_grad=False)\r\n\r\n\tdef configure_optimizers(self):\r\n\t\treturn Adam([\r\n\t\t\t{'params': self.cpt},\r\n\t\t])\r\n\r\n\tdef forward(self, data):\r\n\t\treturn data\r\n\r\n\tdef training_step(self, data, batch_idx):\r\n\t\tself.cpt += batch_idx\r\n\t\tprint(f'\\n{self.current_epoch=} {self.cpt=}')\r\n\t\treturn None\r\n\r\n\tdef validation_step(self, data, batch_idx):\r\n\t\t\tpass\r\n\r\n\tdef test_step(self, data, batch_idx):\r\n\t\tpass\r\n\r\n\r\nclass Counter(Callback):\r\n\tdef __init__(self):\r\n\t\tself.state = {\"epochs\": 0, \"batches\": 0}\r\n\r\n\tdef on_train_epoch_end(self, *args, **kwargs):\r\n\t\tself.state['epochs'] += 1\r\n\t\tprint(self.state)\r\n\r\n\tdef on_train_batch_end(self, *args, **kwargs):\r\n\t\tself.state['batches'] += 1\r\n\r\n\tdef load_state_dict(self, state_dict):\r\n\t\tprint('load_state_dict')\r\n\t\tself.state.update(state_dict)\r\n\r\n\tdef state_dict(self):\r\n\t\tprint('state_dict')\r\n\t\treturn self.state.copy()\r\n\r\n\r\ntrain_dataloader = DataLoader(Dummy_dataset(length=10), batch_size=2)\r\nvalid_dataloader = DataLoader(Dummy_dataset(length=10), batch_size=2)\r\n\r\nmodel = Dummy_Model()\r\ncheckpoint_callback = ModelCheckpoint(\r\n\tdirpath='./',\r\n\tmonitor=None,\r\n\tverbose=True,\r\n\tsave_last=True,\r\n\tevery_n_epochs=1,\r\n)\r\nloading = False\r\ntrainer = Trainer(\r\n\tmax_epochs=300 if loading else 50,\r\n\tcallbacks=[Counter(), checkpoint_callback],\r\n\tenable_checkpointing=True,\r\n)\r\ntrainer.fit(\r\n\tmodel=model,\r\n\ttrain_dataloader=train_dataloader,\r\n\tval_dataloaders=valid_dataloader,\r\n\tckpt_path='last.ckpt' if loading else None,\r\n)\r\n```Your `training_step` returns None, hence no parameter updates and thus no parameter change. Lightning only saves a checkpoint if the parameters change. Try again by returning a loss with gradient.what's your lightning version??",
    "meta": { "name": "Callback Checkpointing never called" },
    "answer": "what's your lightning version??"
  },
  {
    "content": "I have 1 generator and 2 discriminators (`d_a` and `d_b`) in my `LightningModule` for *GAN* training. `d_a` will be updated every 2 steps and discriminator `d_b` will be updated every 3 steps. The generator will be pruned at step 10000.\r\n\r\nThe following code work in PyTorch Lightning < 1.6,\r\n\r\n```python\r\n\r\nclass PrunedModule(pl.LightningModule):\r\n    \"\"\"Apply pruning when self.global_step == 10000\"\"\"\r\n\r\n    def training_step_end(self, _):\r\n        if self.global_step == 10000:\r\n            # apply pruning\r\n\r\n\r\nclass PrunedGAN(PruningLightningModule):\r\n\r\n    def training_step(batch, batch_idx):\r\n        # ...\r\n        optimizer_g.step()\r\n\r\n        if self.global_step % 2 == 0:\r\n            # ...\r\n            # update d_a\r\n            optimizer_d_a.step()\r\n\r\n        if self.global_step % 3 == 0:\r\n            # ...\r\n            # update d_b\r\n            optimizer_d_b.step()\r\n```\r\n\r\nIn PyTorch Lightning 1.6, the `self.global_step` of `PrunedGAN` will be increased by 1, 2 or 3 in a single `training_step`.\r\n\r\nIf I use `self.register_buffer(\"my_global_step\", torch.tensor(0))` in `PrunedModule` and increase `self.my_global_step` by 1 in `training_step_end`, I still need to re-implement all classes inherited from `PrunedModule` and `ModelCheckpoint` to make sure that they use `self.my_global_step` instead of `self.global_step` or `trainer.global_step`. Besides, `my_global_step` will be moved to `cuda` in GPU training mode. It seems to be a bad idea to re-implement global step tracking by myself.\r\n\r\nIs there a simple way to use old (PyTorch Lightning < 1.6) `global_step` update behavior when using multiple optimizers?\r\n\r\nhttps://github.com/Lightning-AI/lightning/issues/13752#issuecomment-1190509385 might be relevant to your question.",
    "meta": {
      "name": "Is there a simple way to use old (PyTorch Lightning < 1.6) `global_step` update behavior?"
    },
    "answer": "https://github.com/Lightning-AI/lightning/issues/13752#issuecomment-1190509385 might be relevant to your question."
  },
  {
    "content": "Automatically move data to GPU is fine.\r\nBut, not knowing what moves and what doesn't, When the batch in ```training_step(self, batch, batch_idx)``` is a dict, no movement will occur.\r\n\r\nAlso don't know when to move and how to move to GPU?\r\nI didn't find where to implement in the package!!Hi @FutureWithoutEnding\r\n\r\n**When the batch is transferred to device**\r\nI think the pseudocode in the docs explains when a batch is transferred to each device very well:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/common/lightning_module.html#hooks\r\n\r\n**What is transferred to device**\r\nSee the list of supported data structures in the documentation. If you have custom data structure, you need to override this hook in your `LightningModule`:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/common/lightning_module.html#transfer-batch-to-device\r\n\r\nAlso, if you need to manually transfer tensors to device, you can utilise `self.device` so that your code stays hardware-agnostic. `your_tensor.to(self.device)`:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/accelerators/gpu.html#init-tensors-using-type-as-and-register-buffer",
    "meta": {
      "name": "when and how the trainer or module move the data to gpu?"
    },
    "answer": "Hi @FutureWithoutEnding\r\n\r\n**When the batch is transferred to device**\r\nI think the pseudocode in the docs explains when a batch is transferred to each device very well:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/common/lightning_module.html#hooks\r\n\r\n**What is transferred to device**\r\nSee the list of supported data structures in the documentation. If you have custom data structure, you need to override this hook in your `LightningModule`:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/common/lightning_module.html#transfer-batch-to-device\r\n\r\nAlso, if you need to manually transfer tensors to device, you can utilise `self.device` so that your code stays hardware-agnostic. `your_tensor.to(self.device)`:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.5/accelerators/gpu.html#init-tensors-using-type-as-and-register-buffer"
  },
  {
    "content": "Hi! I am currently working on a project where, for a given trained model, I perform inference batches of inputs and compute (and store) the gradients of the output with respect to the inputs.\r\nMy code is something like this\r\n```py\r\ndef test_step(self, batch,batch_idx):\r\n   with torch.set_grad_enabled(True):\r\n      gradients_list=[]\r\n      for batch_of_inputs in batches:\r\n         batch_of_inputs.requires_grad_()\r\n         output=self(batch_of_inputs)\r\n         gradients = torch.autograd.grad(\r\n            outputs=output,\r\n            inputs=batch_of_inputs,\r\n            grad_outputs=torch.ones_like(output),\r\n            retain_graph=False,\r\n           )\r\n         gradients_list.append(gradients.detach_())\r\n```\r\nThe thing is that the used memory increases and increases until OOM error rises. I have tried to use\r\n`del gradients,batch_of_inputs,output`\r\nand the problem persisted.\r\nWhat would you suggest?\r\nThanks in advance> The thing is that the used memory increases and increases until OOM error rises\r\n\r\nif the OOM error is on GPU, you can move your gradients from GPU to CPU while storing them to release some GPU memory.",
    "meta": { "name": "How to delete all the gradients in between operations" },
    "answer": "> The thing is that the used memory increases and increases until OOM error rises\r\n\r\nif the OOM error is on GPU, you can move your gradients from GPU to CPU while storing them to release some GPU memory."
  },
  {
    "content": "The default process bar like this, \r\nEpoch 1:   4%|\u258e         | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10]\r\n\r\nIf i want to display acc metric on process bar like this, \r\nEpoch 1:   4%|\u258e         | 40/1095 [00:03<01:37, 10.84it/s, loss=4.501, v_num=10, acc=0.99]\r\n\r\nWhat should I do\uff1f thx~I find it~ \r\nself.log(\"accuracy\", accuracy, prog_bar=True)",
    "meta": {
      "name": "How can I show my information on the progress bar\uff1f"
    },
    "answer": "I find it~ \r\nself.log(\"accuracy\", accuracy, prog_bar=True)"
  },
  {
    "content": "Hi, \r\n\r\nis there a way to pass objects from a **LightningDataModule** to a **LightningModule** in the **LightningCLI** using **link_arguments**? I tried the following\r\n\r\n```\r\nclass TestModel(LightningModule):\r\n\r\n    def __init__(self, scaler: StandardScaler, *args: Any, **kwargs: Any):\r\n        super().__init__(*args, **kwargs)\r\n        print(scaler)\r\n\r\n\r\nclass TestDataModule(LightningDataModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.scaler = StandardScaler()\r\n\r\n\r\nclass TestCLI(LightningCLI):\r\n\r\n    def add_arguments_to_parser(self, parser):\r\n        parser.link_arguments('data.scaler', 'model.scaler', apply_on='instantiate')\r\n\r\n\r\nTestCLI(TestModel, TestDataModule)\r\n```\r\n\r\nbut got a ` ValueError: Target key \"model.scaler\" must be for an individual argument`@mauvilsa posted here as discussed in slackThe support for this has been implemented in [jsonargparse](https://github.com/omni-us/jsonargparse). There is no need for any change in pytorch-lightning. Just update the package, i.e. `pip install -U jsonargparse`, and your example should work.",
    "meta": { "name": "LightningCLI: Passing objects via link_arguments" },
    "answer": "The support for this has been implemented in [jsonargparse](https://github.com/omni-us/jsonargparse). There is no need for any change in pytorch-lightning. Just update the package, i.e. `pip install -U jsonargparse`, and your example should work."
  },
  {
    "content": "My training code is as such:\r\n\r\nself.trainer.fit(self, train_loader, dev_loader)\r\ntorch.save(self.model.state_dict(), path_model)\r\n\r\nHowever, when I press ctrl+C during training I get a \"attempting graceful shutdown\", and then the model is saved. I'd like to not save the model or perform any of the code after trainer.fit if keyboard interrupt is performed. \r\n\r\nThe doc says that \"The trainer will catch the KeyboardInterrupt and attempt a graceful shutdown, including running accelerator callback on_train_end to clean up memory. The trainer object will also set an attribute interrupted to True in such cases. If you have a callback which shuts down compute resources, for example, you can conditionally run the shutdown logic for only uninterrupted runs.\"\r\n\r\nHowever, when I try to access the \"interrupted\" attribute, I get \"no attribute 'interrupted' \". \r\n\r\nIs there a way to force the model to perform ungraceful shutdown or to quit after having release resources?\r\n\r\nThank you> I'd like to not ... perform any of the code after trainer.fit if keyboard interrupt is performed.\r\n\r\n1. Press C-c many times, or\r\n2. Override `on_exception` and reraise the exception there\r\n   - docs: https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html?highlight=on_exception#on-exception\r\n   - the error handling all happens at:\r\nhttps://github.com/Lightning-AI/lightning/blob/1d59b3f0ce185702d69eedef752363f63bc48012/src/pytorch_lightning/trainer/trainer.py#L649-L670\r\n\r\n> when I try to access the \"interrupted\" attribute, I get \"no attribute 'interrupted' \".\r\n\r\nHere's the trainer's property:\r\n```python\r\n# status in (\"initializing\", \"running\", \"finished\", \"interrupted\")\r\ntrainer.state.status\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#state\r\n",
    "meta": { "name": "How to perform ungraceful shutdown" },
    "answer": "> I'd like to not ... perform any of the code after trainer.fit if keyboard interrupt is performed.\r\n\r\n1. Press C-c many times, or\r\n2. Override `on_exception` and reraise the exception there\r\n   - docs: https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html?highlight=on_exception#on-exception\r\n   - the error handling all happens at:\r\nhttps://github.com/Lightning-AI/lightning/blob/1d59b3f0ce185702d69eedef752363f63bc48012/src/pytorch_lightning/trainer/trainer.py#L649-L670\r\n\r\n> when I try to access the \"interrupted\" attribute, I get \"no attribute 'interrupted' \".\r\n\r\nHere's the trainer's property:\r\n```python\r\n# status in (\"initializing\", \"running\", \"finished\", \"interrupted\")\r\ntrainer.state.status\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#state\r\n"
  },
  {
    "content": "Hello, I am switching my code to use LightningCLI for easier config/reproducibility. I want to access dataset examples when creating my model, but I don't want to have to make my data loading code explicitly available to both `LightningDataModule` and `LightningModule`.\r\n\r\nCurrently my project uses code similar to this:\r\n\r\n```python\r\ndata = MyLightningDataModule(...)\r\nmodel = MyLightningModule(data.input_dim, ...)\r\ntrainer.fit(model, datamodule=data)\r\n```\r\n\r\nwhere `data.input_dim` is roughly equivalent to `data.train_dataloader().dataset[0].shape[0]`. Basically, the width of the first layer of the model depends on the data generated by the DataModule. This is modeled after the API usage suggested in https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html#using-a-datamodule. I posted pseudocode to make it simpler but I can post the real thing if I didn't get the point across.\r\n\r\nThis is how I use LightningCLI:\r\n\r\n```python\r\ncli = LightningCLI(MyLightningModule, MyLightningDataModule)\r\n```\r\n\r\nBut it seems that when using LightningCLI, the LightningModule is instantiated before the LightningDataModule, and I can't figure out how to configure LightningCLI to make the LightningDataModule available to the LightningModule constructor.\r\n\r\nI've tried to make this happen by supplying datamodule as a parameter to my `LightningModule` following this API: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#models-with-multiple-submodules.\r\n\r\n```python\r\nclass MyLightningModule(pl.LightningModule):\r\n    def __init__(self, data: BigVulDatasetLineVDDataModule):\r\n        self.data = data\r\n        # instantiate model...\r\n\r\n    def train_dataloader(self):\r\n        return self.data.train_dataloader()\r\n\r\n    def val_dataloader(self):\r\n        return self.data.train_dataloader()\r\n\r\n    def test_dataloader(self):\r\n        return self.data.train_dataloader()\r\n\r\n    # other LightningModule related methods...\r\n```\r\n\r\nAnd my config looks like this:\r\n```yaml\r\nmodel:\r\n  # model args...\r\n  data:\r\n    class_path: project.MyLightningDataModule\r\n    init_args:\r\n      # datamodule args...\r\n```\r\n\r\n...but it would be nice if I could keep these two classes independent. Is there any better way to achieve the functionality I want?For this you use [link_arguments](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#argument-linking) which requires to subclass `LightningCLI`. Something like:\r\n```python\r\nclass MyLightningModule(LightningModule):\r\n    def __init__(self, input_dim: int):\r\n        ...\r\n\r\nclass MyCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        parser.link_arguments('data.input_dim', 'model.input_dim', apply_on='instantiate')\r\n```\r\nIt might also be possible like your second example linking the entire data object. Though this needs the latest version of [jsonargparse](https://github.com/omni-us/jsonargparse):\r\n```python\r\nclass MyLightningModule(pl.LightningModule):\r\n    def __init__(self, data: BigVulDatasetLineVDDataModule):\r\n        self.data = data\r\n        # instantiate model...\r\n\r\nclass MyCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        parser.link_arguments('data', 'model.data', apply_on='instantiate')\r\n```",
    "meta": {
      "name": "LightningCLI access DataModule methods in model constructor"
    },
    "answer": "For this you use [link_arguments](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#argument-linking) which requires to subclass `LightningCLI`. Something like:\r\n```python\r\nclass MyLightningModule(LightningModule):\r\n    def __init__(self, input_dim: int):\r\n        ...\r\n\r\nclass MyCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        parser.link_arguments('data.input_dim', 'model.input_dim', apply_on='instantiate')\r\n```\r\nIt might also be possible like your second example linking the entire data object. Though this needs the latest version of [jsonargparse](https://github.com/omni-us/jsonargparse):\r\n```python\r\nclass MyLightningModule(pl.LightningModule):\r\n    def __init__(self, data: BigVulDatasetLineVDDataModule):\r\n        self.data = data\r\n        # instantiate model...\r\n\r\nclass MyCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        parser.link_arguments('data', 'model.data', apply_on='instantiate')\r\n```"
  },
  {
    "content": "I am working on my first Lightning project and having an issue when I attempt to train on GPUs. When I train on the CPU using the accelerator='cpu' argument, the training and validation occurs with no problem. My workstation has two GPUs, so I set the accelerator='gpu', devices=2, and strategy='dp' (I've also tried 'ddp' with the same result). The data is being provided by a LightningDataModule that is pulling a custom Torch Dataset, and the Dataset is using a Pandas Dataframe. The Dataframe contains file names to Numpy files which are being loaded as follows:\r\n\r\n    def __getitem__(self, idx):\r\n        \r\n        x = self.df.ct[idx]\r\n        y = self.df.label[idx]\r\n\r\n        x = np.load(x)['arr_0']\r\n        y = np.loadtxt(y).flatten()\r\n\r\n        x = torch.from_numpy(x).type(torch.FloatTensor)\r\n        y = torch.from_numpy(y).type(torch.FloatTensor)\r\n\r\n        return x, y\r\n\r\nHowever, when I try to switch to GPUs for training, I receive the following error with strategy='dp':\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\r\n\r\nWith the strategy='ddp' this error is displayed:\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking argument for argument mat1 in method wrapper_addmm)\r\n\r\nAny help would be appreciated.I figured it out. I had a mistake in my model generation where I was using a List for modules rather than a Pytorch class.\r\n\r\nI changed:\r\n`self.fc = []`\r\nto\r\n`self.fc = nn.Sequential()`\r\n",
    "meta": {
      "name": "`RuntimeError: Expected all tensors to be on the same device`"
    },
    "answer": "I figured it out. I had a mistake in my model generation where I was using a List for modules rather than a Pytorch class.\r\n\r\nI changed:\r\n`self.fc = []`\r\nto\r\n`self.fc = nn.Sequential()`\r\n"
  },
  {
    "content": "**What I got is**: \r\nI run the horovod for the multi-gpus using the following command for the pytorch lightning 1.6.5.\r\n\r\n\r\n```python\r\npython pl_examples/basic_examples/mnist_examples/image_classifier_5_lightning_datamodule.py --trainer.accelerator 'gpu' --trainer.devices 4 --trainer.strategy 'horovod'\r\n```\r\n\r\n\r\nthe output is \r\n```python\r\nThu Jul 14 14:26:11 2022       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            On   | 00000000:00:1B.0 Off |                    0 |\r\n| N/A   35C    P0    34W /  70W |   1547MiB / 15360MiB |     37%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla T4            On   | 00000000:00:1C.0 Off |                    0 |\r\n| N/A   31C    P8    16W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla T4            On   | 00000000:00:1D.0 Off |                    0 |\r\n| N/A   31C    P8    16W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   32C    P8    16W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A     39649      C                                    1545MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n\r\nthe command line output is \r\n\r\n```python\r\n(base) ray@ip-172-31-36-78:~/horovod-gpu/lightning$ python pl_examples/basic_examples/mnist_examples/image_classifier_5_lightning_datamodule.py --trainer.accelerator 'gpu' --trainer.devices 4 --trainer.strategy 'horovod'\r\n\r\n\r\n                    ####\r\n                ###########\r\n             ####################\r\n         ############################\r\n    #####################################\r\n##############################################\r\n#########################  ###################\r\n#######################    ###################\r\n####################      ####################\r\n##################       #####################\r\n################        ######################\r\n#####################        #################\r\n######################     ###################\r\n#####################    #####################\r\n####################   #######################\r\n###################  #########################\r\n##############################################\r\n    #####################################\r\n         ############################\r\n             ####################\r\n                  ##########\r\n                     ####\r\n\r\nGlobal seed set to 42\r\n/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:92: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\r\n  rank_zero_warn(\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\n/home/ray/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\r\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n\r\n  | Name     | Type     | Params\r\n--------------------------------------\r\n0 | model    | Net      | 1.2 M \r\n1 | test_acc | Accuracy | 0     \r\n--------------------------------------\r\n1.2 M     Trainable params\r\n0         Non-trainable params\r\n1.2 M     Total params\r\n4.800     Total estimated model params size (MB)\r\n/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\r\n  rank_zero_warn(\r\nEpoch 0:   0%|                               \r\n```\r\n\r\n\r\n**What should be**: \r\n\r\nIt should be multiple gpus. How should I run? \r\n\r\n\r\nTry using `gpus` to specify number of devices to train on `--trainer.gpus=4`@Taehee-K \r\n\r\n```python\r\nThu Jul 14 17:43:52 2022       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            On   | 00000000:00:1B.0 Off |                    0 |\r\n| N/A   34C    P0    34W /  70W |   1547MiB / 15360MiB |     38%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla T4            On   | 00000000:00:1C.0 Off |                    0 |\r\n| N/A   32C    P8    16W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla T4            On   | 00000000:00:1D.0 Off |                    0 |\r\n| N/A   32C    P8    16W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   31C    P8    16W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A     20520      C                                    1545MiB |\r\n+-----------------------------------------------------------------------------+\r\n```still see this \r\n\r\n```python\r\n(base) ray@ip-172-31-93-242:~/horovod-gpu/lightning$ python pl_examples/basic_examples/mnist_examples/image_classifier_5_lightning_datamodule.py --trainer.accelerator 'gpu' --trainer.gpus 4 --trainer.strategy 'horovod'\r\n\r\n\r\n                    ####\r\n                ###########\r\n             ####################\r\n         ############################\r\n    #####################################\r\n##############################################\r\n#########################  ###################\r\n#######################    ###################\r\n####################      ####################\r\n##################       #####################\r\n################        ######################\r\n#####################        #################\r\n######################     ###################\r\n#####################    #####################\r\n####################   #######################\r\n###################  #########################\r\n##############################################\r\n    #####################################\r\n         ############################\r\n             ####################\r\n                  ##########\r\n                     ####\r\n\r\nGlobal seed set to 42\r\n/home/ray/horovod-gpu/lightning/pytorch_lightning/loops/utilities.py:92: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\r\n  rank_zero_warn(\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\r\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw/train-images-idx3-ubyte.gz\r\n9913344it [00:00, 23040462.18it/s]                                                                                                                                                                                           \r\nExtracting /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw/train-images-idx3-ubyte.gz to /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw\r\n\r\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\r\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw/train-labels-idx1-ubyte.gz\r\n29696it [00:00, 119648464.54it/s]                                                                                                                                                                                            \r\nExtracting /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw/train-labels-idx1-ubyte.gz to /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw\r\n\r\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\r\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\r\n1649664it [00:00, 10574307.41it/s]                                                                                                                                                                                           \r\nExtracting /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw\r\n\r\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\r\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\r\n5120it [00:00, 48475928.85it/s]                                                                                                                                                                                              \r\nExtracting /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/ray/horovod-gpu/lightning/Datasets/MNIST/raw\r\n\r\n/home/ray/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\r\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\r\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\r\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\r\n9913344it [00:00, 35476547.86it/s]                                                                                                                                                                                           \r\nExtracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\r\n\r\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\r\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\r\n29696it [00:00, 118510039.57it/s]                                                                                                                                                                                            \r\nExtracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\r\n\r\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\r\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\r\n1649664it [00:00, 10670010.40it/s]                                                                                                                                                                                           \r\nExtracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\r\n\r\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\r\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\r\n5120it [00:00, 49941480.19it/s]                                                                                                                                                                                              \r\nExtracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\r\n\r\nMissing logger folder: /home/ray/horovod-gpu/lightning/lightning_logs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n\r\n  | Name     | Type     | Params\r\n--------------------------------------\r\n0 | model    | Net      | 1.2 M \r\n1 | test_acc | Accuracy | 0     \r\n--------------------------------------\r\n1.2 M     Trainable params\r\n0         Non-trainable params\r\n1.2 M     Total params\r\n4.800     Total estimated model params size (MB)\r\n/home/ray/horovod-gpu/lightning/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\r\n  rank_zero_warn(\r\nEpoch 0:   0%|                                                                                                                                                                                      | 0/1875 [00:00<?, ?it/s]/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\r\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\r\nEpoch 2:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                  | 1243/1875 [00:11<00:05, 107.94it/s, loss=0.103, v_num=0]^C/home/ray/horovod-gpu/lightning/pytorch_lightning/trainer/trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\r\n  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\r\nRestoring states from the checkpoint path at /home/ray/horovod-gpu/lightning/lightning_logs/version_0/checkpoints/epoch=1-step=3750.ckpt\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nLoaded model weights from checkpoint at /home/ray/horovod-gpu/lightning/lightning_logs/version_0/checkpoints/epoch=1-step=3750.ckpt\r\n/home/ray/horovod-gpu/lightning/pytorch_lightning/trainer/connectors/data_connector.py:330: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\r\n  rank_zero_warn(\r\n/home/ray/horovod-gpu/lightning/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\r\n  rank_zero_warn(\r\nTesting DataLoader 0:   0%|                                                                                                                                                                          | 0/313 [00:00<?, ?it/s]/home/ray/anaconda3/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\r\n                not been set for this class (_ResultMetric). The property determines if `update` by\r\n                default needs access to the full metric state. If this is not the case, significant speedups can be\r\n                achieved and we recommend setting this to `False`.\r\n                We provide an checking function\r\n                `from torchmetrics.utilities import check_forward_no_full_state`\r\n                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\r\n                default for now) or if `full_state_update=False` can be used safely.\r\n                \r\n  warnings.warn(*args, **kwargs)\r\nTesting DataLoader 0:   5%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                       | 17/313 [00:00<00:02, 143.89it/s]^C^C\r\n```i am using pytorch lightning 1.6.5",
    "meta": { "name": "how to run horovod strategy?" },
    "answer": "Try using `gpus` to specify number of devices to train on `--trainer.gpus=4`"
  },
  {
    "content": "\r\n\r\nI try to understand / rectify a warning about saving my hyper parameters and would need some assistance please.\r\n\r\nI build a model this way:\r\n\r\n```py\r\nfrom pytorch_lightning.core.mixins import HyperparametersMixin\r\n\r\nclass MyModel(nn.Module, HyperparametersMixin):\r\n    def __init__(...):\r\n        super().__init__()\r\n        self.save_hyperparameters()  # Logs to self.hparams only, not to the logger (since there isn't any yet)\r\n\r\nclass MyModule(pl.LightningModule):\r\n    def __init__(model: nn.Module):\r\n        super().__init__()\r\n        self.save_hyperparameters(\"model\", logger=False)\r\n        self.save_hyperparameters()\r\n        self.save_hyperparameters(model.hparams)\r\n\r\nmodel = MyModel(...)\r\nmodule = MyModule(model)\r\n```\r\n\r\nThis works, and I can load a checkpoint with `model = MyModule.load_from_checkpoint(model_path_to_load_from)`\r\n\r\nBut I also get this warning during the initialization of `MyModule`: `Attribute 'model' is an instance of 'nn.Module' and is already saved during checkpointing. It is recommended to ignore them using 'self.save_hyperparameters(ignore=['model'])'`.\r\n\r\nSo I change the corresponding code in `MyModule` to:\r\n\r\n```py\r\nself.save_hyperparameters(ignore=[\"model\"])\r\nself.save_hyperparameters(model.hparams)\r\n```\r\n\r\nThe created checkpoint is marginally reduced by ~3KB, the checkpoint size is ~1MB.\r\n\r\nBut when I want to load the  checkpoint I get this error:\r\n\r\n```py\r\nFile \"/Users/stephan/Library/Caches/pypoetry/virtualenvs/molgen-6oMP0hTK-py3.9/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 161, in load_from_checkpoint\r\n    model = cls._load_model_state(checkpoint, strict=strict, **kwargs)\r\n  File \"/Users/stephan/Library/Caches/pypoetry/virtualenvs/molgen-6oMP0hTK-py3.9/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 203, in _load_model_state\r\n    model = cls(**_cls_kwargs)\r\nTypeError: __init__() missing 1 required keyword-only argument: 'model'\r\n```\r\n\r\nWhich seems to indicate that I need to save `model` as a hyper parameter.\r\n\r\nWhat am I missing? What is the correct way to save the hyper parameters / model in the `pl.LightningModule`?the attributes that are not saved as hparams need to be passed explicitly. Considering you are using `load_from_checkpoint` API, you can use `model = MyModule.load_from_checkpoint(ckpt_path, model=model)`.\r\n\r\nIf you include it in the hparams, your checkpoints will be unnecessarily big and can create issues if you have large models.\r\n\r\nBy\r\n```\r\nis already saved during checkpointing.\r\n```\r\nit means the model weights are already saved in the checkpoint and are loaded using PyTorch API, not as hparams.Hi @rohitgr7,\r\n\r\nThank you very much for your swift response. Understood, I think.\r\n\r\nWould you mind checking my understanding? I have 2 concerns\r\n\r\n(1) I changed the code to load the checkpoint as follows, please refer to the assumption in the comment at the end of the code snippet.\r\n\r\n```py\r\n    # Load checkpoint and move it to CPU\r\n    checkpoint = torch.load(model_path_to_load_from, map_location=torch.device(\"cpu\"))\r\n\r\n    # Leverage the logged hyper params to build an (untrained) model\r\n    # The function build_model returns an instance of nn.Module\r\n    hparams = checkpoint[\"hyper_parameters\"]\r\n    model_config = {\r\n        \"vocab_size\": hparams[\"vocab_size\"],\r\n        \"embed_dim\": hparams[\"embed_dim\"],\r\n        ...\r\n    }\r\n    model = build_model(\r\n        hparams[\"architecture\"], hparams[\"variant\"], **model_config\r\n    )\r\n\r\n    # ***** I think I do not need to manually load the state_dict, True? *****\r\n    # If I need to, the issue is that the state_dict contains keys such as \"model.linear.bias\",\r\n    # but load_state_dict() expects \"linear.bias\"\r\n    # model.load_state_dict(checkpoint[\"state_dict\"])\r\n\r\n   # Finally, load the pl module\r\n    pl_model = MoleculeGenerator.load_from_checkpoint(model_path_to_load_from, model=model)\r\n```\r\n\r\n(2) Besides the model, I pass a loss function to the pl Module, which is also an nn.Module. Do I need to / do you recommend treating it as the model, i.e., load / configure it manually from the checkpoint? I guess the question is, should I exclude nn.Module just as a precaution if it gets too large (and the loss function parameters probably don't) or is it sort of \u201cforbidden\u201d to log nn.Modules via save_hyperparameters()?> I changed the code to load the checkpoint as follows, please refer to the assumption in the comment at the end of the code snippet.\r\n\r\nI'd not recommend this way. Since you are loading the actual model using the hparams, you should load it from within the LightningModule's init\r\n```py\r\nclass MyModule(LightningModule):\r\n    def __init__(...):\r\n        super().__init__()\r\n        self.save_hyperparameters(...)\r\n        model = build_model(self.hparams...)\r\n```\r\njust wondering, how did you load the model without the checkpoint if you need hparams to load them.\r\n\r\n> Besides the model, I pass a loss function to the pl Module, which is also an `nn.Module`. Do I need to / do you recommend treating it as the model, i.e., load / configure it manually from the checkpoint? I guess the question is, should I exclude nn.Module just as a precaution if it gets too large (and the loss function parameters probably don't) or is it sort of \u201cforbidden\u201d to log nn.Modules via save_hyperparameters()?\r\n\r\nit's actually hard for us to determine which one is a loss module or which one is a model. So we check if an object is an instance of `nn.Module` and raise a warning accordingly. Personally, I'd exclude an instance of the Loss function from the hparam since it's not an hparam (rather the type of loss or loss name should be considered as an hparam), but again it's optional and we are raising a warning, not an error, since loss functions don't have any params, it should be fine too, but again, I won't recommend it.I hope you still have some time/energy to stay with me, I really would like to do it properly.\r\n\r\nThat's getting trickier than I thought. For context: I am doing experiments and need to combine different datasets, models, encodings, \u2026 to assess the overall/combined performance. So, I am trying to make a generic wrapper that I can plug those components into. Which is the background for my comment:\r\n\r\n> Since you are loading the actual model using the hparams, you should load it from within the LightningModule's init\r\n\r\nI think I understand your reasoning, but since I have different model architectures, I have a different set of hparams for each architecture. So, I would need to pass the superset of those hparams to the `LightningModule` and I am/was trying to avoid that. My `build_model` has a bunch of `if architecture == ...` statements, and the `LightningModule` (until now) is not aware of the model specifics. I just have to make sure that the models' input and output shapes stay the same (or at least can be dynamically assessed).\r\n\r\n> just wondering, how did you load the model without the checkpoint if you need hparams to load them.\r\n\r\nMaybe not correctly until now? Until now, I saved the `nn.Module`s with `save_hyperparameters()` and loaded the `LightningModule` with `load_from_checkpoint()`. But since I am still building the pipeline, I did not do predictions with the loaded model.\r\n\r\nAs a summary of my understanding, I have the following options:\r\n\r\n- Save the `nn.Module`s as hparams and be done with a `load_checkpoint()`. Easy but not recommended due to potential (model parameter) size issues\r\n- Build the model (and the loss function) during `__init__()` of the `LightningModule`. Recommended way, but \"blows up\" the parameter/arguments to `__init__()`\r\n- Load the checkpoint with `torch.load()`, build the model and loss function \"outside\" of the `LightningModule` and pass them as arguments to the `LightningModule`s `__init__()`. Not recommended (but not sure why) and I probably need to \"transcribe\" the model's state_dict from e.g. `model.linear.bias` to `linear.bias`\r\n\r\nIf you can think of a fourth \"best of all worlds\" I am happy to hear it of course. Thanks for your insights.personally I'd recommend this.\r\n\r\n> Build the model (and the loss function) during __init__() of the LightningModule. Recommended way, but \"blows up\" the parameter/arguments to __init__()\r\n\r\nif you are concerned with the huge number of arguments, you can use namespaces.\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/hyperparameters.html#argparser-best-practices\r\n\r\nbecause in all other cases, you might have to reload the checkpoint manually to initialize the `model` using the saved hparams.",
    "meta": {
      "name": "Warning during save_hyperparameter() gives misleading advice?"
    },
    "answer": "the attributes that are not saved as hparams need to be passed explicitly. Considering you are using `load_from_checkpoint` API, you can use `model = MyModule.load_from_checkpoint(ckpt_path, model=model)`.\r\n\r\nIf you include it in the hparams, your checkpoints will be unnecessarily big and can create issues if you have large models.\r\n\r\nBy\r\n```\r\nis already saved during checkpointing.\r\n```\r\nit means the model weights are already saved in the checkpoint and are loaded using PyTorch API, not as hparams."
  },
  {
    "content": "https://github.com/Lightning-AI/lightning/blob/master/src/pytorch_lightning/trainer/callback_hook.py\r\n\r\nthese callbacks are removed but where are the new locations for them? Hi @JiahaoYao These hooks were not meant to be called by users. (related to #10575)\r\n\r\nCan I ask what you're trying to find? Maybe, what you're looking for is user-facing `Callback`'s hooks? If so, they are located here in the codebase: https://github.com/Lightning-AI/lightning/blob/b59f80224843886459d54c828325683d770da746/src/pytorch_lightning/callbacks/callback.py",
    "meta": { "name": "where the new callbaclks located?" },
    "answer": "Hi @JiahaoYao These hooks were not meant to be called by users. (related to #10575)\r\n\r\nCan I ask what you're trying to find? Maybe, what you're looking for is user-facing `Callback`'s hooks? If so, they are located here in the codebase: https://github.com/Lightning-AI/lightning/blob/b59f80224843886459d54c828325683d770da746/src/pytorch_lightning/callbacks/callback.py"
  },
  {
    "content": "In pytorch-lightning,  we often monitor the metric at the current batch level\uff0c**validation_step**. Does this mean that the model parameters we save are optimal for the current batch rather than the whole validation set? Does this mean that in order to get the best model for the validation set, I have to monitor the metric in     **validation_epoch_end**?\r\n\r\n```\r\n    def share_val_step(self, batch):\r\n        graph_label = batch.y\r\n        graph_pred = self(data=batch)\r\n        y_pred_label = graph_pred.argmax(dim=-1)\r\n        f1, p, r, acc = envaulation(y_true=graph_label, y_pred=y_pred_label)\r\n        return torch.tensor(acc, device=self.device), torch.tensor(f1, device=self.device)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        f1, acc = self.share_val_step(batch)\r\n        metrics = {'val_f1': f1, 'val_acc': acc}\r\n        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True)\r\n\r\n    def validation_step_end(self, val_step_outputs):\r\n        pass\r\n\r\n    def validation_epoch_end(self, validation_step_outputs):\r\n        pass\r\n\r\n    def test_step(self, batch, batch_idx, dataloader_idx):\r\n        f1, acc = self.share_val_step(batch)\r\n        metrics = {'test_f1': f1, 'test_acc': acc}\r\n        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True)\r\n\r\n``````py\r\nf1, acc = self.share_val_step(batch)\r\nmetrics = {'val_f1': f1, 'val_acc': acc}\r\nself.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True)\r\n```\r\nhere you are using `on_epoch=True`, which means the metric will be aggregated across the validation set and will be used to monitor the checkpoints if you set `ModelCheckpoint(..., monitor='val_acc')`.",
    "meta": { "name": "monitoring metric and model saving" },
    "answer": "```py\r\nf1, acc = self.share_val_step(batch)\r\nmetrics = {'val_f1': f1, 'val_acc': acc}\r\nself.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True)\r\n```\r\nhere you are using `on_epoch=True`, which means the metric will be aggregated across the validation set and will be used to monitor the checkpoints if you set `ModelCheckpoint(..., monitor='val_acc')`."
  },
  {
    "content": "The current PL's logic seems to only ask people to first define and model, and then send it to the Trainer which automatically handles multi-gpus. However, right now I have a custom model which needs to be initialized on each GPUs. It means that I need to define the model after DDP is initialized on each GPU. Is there any way I can handle this in pytorch-lighting?https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#setup",
    "meta": {
      "name": "Are we able to construct the model after DDP is initialized?"
    },
    "answer": "https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#setup"
  },
  {
    "content": "When I use it like the following, does it use the best model by default? I remember that it defaults to the best model, but I can't find the documentation.\r\n\r\n```python\r\ntrainer.fit(model)\r\n\r\nresult = trainer.test(model)\r\n```have done",
    "meta": {
      "name": "Are model parameters after fitting the best or the last one?"
    },
    "answer": "have done"
  },
  {
    "content": "A question: when I configure callbacks in the model instead of in the Train function, will the model look for the best model or the current model when testing?\r\n\r\nFor example:  configure the   configure_callbacks in my model\r\n`\r\n```\r\n    def configure_callbacks(self):\r\n        early_stop_callback = EarlyStopping(monitor=\"val_f1\", min_delta=0.00, patience=self.args.patience,\r\n                                            verbose=False, mode=\"max\")\r\n        checkpoint_callback = ModelCheckpoint(monitor='val_f1',\r\n                                              dirpath=\"{}\".format(self.save_log_path),\r\n                                              filename='best_{}'.format(self.index_times),\r\n                                              save_top_k=1,\r\n                                              mode='max',\r\n                                              save_last=False)\r\n        return [checkpoint_callback, early_stop_callback]\r\n\r\n    def on_train_start(self):\r\n        self.print(\"Training is started!\")\r\n\r\n    def on_train_end(self):\r\n        self.print(\"Training is done!\")\r\n```\r\n\r\n`\r\n\r\nNot  configure the   configure_callbacks in the Trainer\r\n`\r\n        trainer = Trainer(devices=\"auto\", accelerator=\"auto\", logger=False, multiple_trainloader_mode='max_size_cycle',\r\n                          terminate_on_nan=True,\r\n                          logger=tb_logger, log_every_n_steps=1,\r\n                          flush_logs_every_n_steps=5,\r\n                          max_epochs=args.epochs)\r\n`\r\n\r\nWhen i perform the trainer  like this:\r\n`\r\n\r\n        trainer.fit(model)\r\n\r\n        result = trainer.test(model)\r\n`\r\n\r\nMy question is: Does the model use the parameters at the last epoch when testing, or the best parameters loaded?> Does the model use the parameters at the last epoch when testing, or the best parameters loaded?\r\n\r\nHi @Struggle-Forever, yes, the `model` is in the last state, but not in the best state. I hope the following example in the documentation will explain it enough:\r\n\r\n```python\r\n# run full training\r\ntrainer.fit(model)\r\n\r\n# (1) load the best checkpoint automatically (lightning tracks this for you)\r\ntrainer.test(ckpt_path=\"best\")\r\n\r\n# (2) test using a specific checkpoint\r\ntrainer.test(ckpt_path=\"/path/to/my_checkpoint.ckpt\")\r\n\r\n# (3) test with an explicit model (will use this model and not load a checkpoint)\r\ntrainer.test(model)\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.4/common/evaluation.html#test-after-fitthank you very much!",
    "meta": {
      "name": "when I configure callbacks in the model instead of in the Trainer function, will the model look for the best model or the current model when testing?"
    },
    "answer": "> Does the model use the parameters at the last epoch when testing, or the best parameters loaded?\r\n\r\nHi @Struggle-Forever, yes, the `model` is in the last state, but not in the best state. I hope the following example in the documentation will explain it enough:\r\n\r\n```python\r\n# run full training\r\ntrainer.fit(model)\r\n\r\n# (1) load the best checkpoint automatically (lightning tracks this for you)\r\ntrainer.test(ckpt_path=\"best\")\r\n\r\n# (2) test using a specific checkpoint\r\ntrainer.test(ckpt_path=\"/path/to/my_checkpoint.ckpt\")\r\n\r\n# (3) test with an explicit model (will use this model and not load a checkpoint)\r\ntrainer.test(model)\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/1.6.4/common/evaluation.html#test-after-fit"
  },
  {
    "content": "Generally, in pytorch-lightning, we use the step method for metric calculation, and the callback function monitors the metric of the step. I am curious how pytorch-lightning gets the metric of the epoch? Is it obtained by averaging?Suppose I now have three batches of data: 20, 20, 10 (the last batch is not enough), and their accurate predictions are: 10 (10/20=0.5), 15 (15/20=0.75), and 5 (5/10=0.5). If we use the average method, the metric at this time: (0.5+0.75+0.5)/3 = 0.58.\r\nHowever, in reality their accuracy should be: (10+15+5)/(20+20+10) = 0.6.I'm not sure if I'm misunderstanding, or if pytorch-lightning has an additional calculation?it takes the weighted average using the current batch size.",
    "meta": { "name": "the metric calculation for pytorch-lightning" },
    "answer": "it takes the weighted average using the current batch size."
  },
  {
    "content": "- [ ] When I perform adversarial training, I find that the output of BERT is always NAN.\r\n- [ ] \r\n![\u5fae\u4fe1\u622a\u56fe_20220705202316](https://user-images.githubusercontent.com/29114869/177455365-d167b314-5776-496a-8414-8485a40ac04a.png)\r\nAs far as I know, training with AMP (`precision=16`) can sometimes be unstable and lead to nan as you report.",
    "meta": {
      "name": "When I perform adversarial training, I find that the output of BERT is always NAN."
    },
    "answer": "As far as I know, training with AMP (`precision=16`) can sometimes be unstable and lead to nan as you report."
  },
  {
    "content": "I want to change weight decay during training, which hook should I overwrite, optimizer_step(), on_train_batch_end() or ... ?\r\n@FredMushZhao I think either works if you want to schedule it on each step. Alternatively, you could use `on_train_epoch_start` if you want to update it on each epoch.\r\n\r\nFor reference on all available hooks: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#hooks",
    "meta": { "name": "where to add weight decay scheduler" },
    "answer": "@FredMushZhao I think either works if you want to schedule it on each step. Alternatively, you could use `on_train_epoch_start` if you want to update it on each epoch.\r\n\r\nFor reference on all available hooks: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#hooks"
  },
  {
    "content": "I'm training a model across two GPUs on patient data (id). In my test steps, I output dictionaries, which contain the id, as well as all the metrics. I store these (a list with a dict per id) at the end of the test epoch, so I can later on statistically evaluate model performances.\r\n\r\nI'm experiencing a problem with the test step, however. \r\n\r\n    # Test step\r\n    def test_step(self, batch, batch_idx):\r\n\r\n        # Get new input and predict, then calculate loss\r\n        x, y, id = batch[\"input\"], batch[\"target\"], batch[\"id\"]\r\n\r\n        # Infer and time inference\r\n        start = time()\r\n        y_hat = self.test_inference(x, self, **self.test_inference_params)\r\n        end = time()\r\n\r\n        # Calculate metrics\r\n        id = id[0] if len(id) == 1 else tuple(id)\r\n\r\n        # Output dict with duration of inference\r\n        output = {\"id\": id, \"time\": end - start}\r\n\r\n        # Add other metrics to output dict\r\n        for m, pars in zip(self.metrics, self.metrics_params):\r\n\r\n            metric_value = m(y_hat, y, **pars)\r\n\r\n            if hasattr(metric_value, \"item\"):\r\n                metric_value = metric_value.item()\r\n\r\n            output[f\"test_{m.__name__}\"] = metric_value\r\n\r\n        return output\r\n\r\n    # Test epoch end (= test end)\r\n    def test_epoch_end(self, outputs):\r\n\r\n        # Go over outputs and gather\r\n        self.test_results = outputs     #self.all_gather(outputs)\r\n\r\nI hadn't considered this before (as I'm used to training on a single GPU), but the test_results attribute now only contains half of the outputs (one half per process). So when my main script reaches this section, only half the output is effectively stored:\r\n\r\n    log(\"Evaluating model.\")\r\n    trainer.test(model=model,\r\n                 dataloaders=brats.val_dataloader())\r\n    results = model.test_results\r\n\r\n    # Save test results\r\n    log(\"Saving results.\")\r\n    np.save(file=join(result_dir, f'{model_name}_v{version}_fold{fold_index}.npy'), arr=results)\r\n\r\nI have read about the `self.all_gather` method, but I'm not sure it suits my needs. I want to merge the lists, not reduce anything. Also, they're not Tensors, but dicts. How can I store all dicts across both DDP processes?all_gather is different from all_reduce. It doesn't do any math operation here.\r\nsort of like:\r\n```\r\nall_gather -> collect outputs from all devices\r\nall_reduce -> in general, collect outputs from all devices and reduce (apply a math op)\r\n```\r\nall_gather isn't working for you?Hi @WouterDurnez,\r\nI am in a very similar situation and I have a follow up. How did you solve the problem of keeping track of the patient ID?\r\nDid you map every ID (string) to a unique integer?\r\n\r\nThanks for your hints\r\nI would think all_gather_object would be optimal given the data types, no?",
    "meta": { "name": "Combine outputs in test epochs when using DDP" },
    "answer": "all_gather is different from all_reduce. It doesn't do any math operation here.\r\nsort of like:\r\n```\r\nall_gather -> collect outputs from all devices\r\nall_reduce -> in general, collect outputs from all devices and reduce (apply a math op)\r\n```\r\nall_gather isn't working for you?"
  },
  {
    "content": "Is there a way to make pytorch lightning **not** intercept ctrl-c, I would like to handle it myself.See #13560.",
    "meta": {
      "name": "How to prevent pytorch lightning from intercepting Ctrl-C?"
    },
    "answer": "See #13560."
  },
  {
    "content": "how to get the \"epoch\" value in the \"validate_epoch_end\" \r\n\r\nBecause I wanna use the \"epoch\" value as a  output datadir parameter.\r\n\r\nthank a lot.you can get  epoch using self.current_epoch",
    "meta": {
      "name": "how to get the \"epoch\" value in the \"validate_epoch_end\""
    },
    "answer": "you can get  epoch using self.current_epoch"
  },
  {
    "content": "I'm not certain whether this is user error or a PyTorch/Lightning issue, so am posting a discussion instead.\r\n\r\nAdding the line `os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL'` while using multiple GPUs and DDP causes the program to hang indefinitely.\r\n\r\nTo reproduce:\r\n\r\n```python\r\nimport argparse\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run(cl_args):\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n\r\n    # Start changed code\r\n    import os\r\n    os.environ[\r\n        \"TORCH_DISTRIBUTED_DEBUG\"\r\n    ] = \"DETAIL\"  # set to DETAIL for runtime logging.\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser = Trainer.add_argparse_args(parser)\r\n    args = parser.parse_args(cl_args.split() if cl_args else None)\r\n    trainer = Trainer.from_argparse_args(args)\r\n    # End changed code\r\n\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run('--gpus 2 --strategy ddp')\r\n```I confirmed the hang with my script, too. https://github.com/akihironitta/gist/blob/repro/13503-torch-dist-debug-detail/pl_boring_model/main.py\r\n\r\n<details><summary>env</summary>\r\n\r\n```console\r\n$ pip list|grep torch\r\ntorch                   1.12.0+cu116\r\ntorchaudio              0.12.0+cu116\r\ntorchmetrics            0.9.2\r\ntorchvision             0.13.0+cu116\r\n```\r\n\r\n</details>\r\n\r\nGiving [the doc page](https://pytorch.org/docs/stable/distributed.html#torch-distributed-debug) a read and tying out a few runs, I think the env var is supposed to be set on rank 0 only, so instead, you might want to set the env var outside the script `TORCH_DISTRIBUTED_DEBUG=DETAIL python your_script.py`, which worked in my case.Reading @akihironitta 's response and looking at the [documentation](https://pytorch.org/docs/stable/distributed.html#torch-distributed-debug) again, I noticed that they set the environment variable prior to calling `mp.spawn`. Moving the `os.environ['TORCH_DISTRIBUTED_DEBUG] = 'DETAIL'` line outside of the main function prevented hanging.\r\n\r\n```python\r\nimport argparse\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n# Start changed code\r\nimport os\r\nos.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL'\r\n# End changed code\r\n\r\ndef run(cl_args):\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n\r\n    # Start changed code\r\n    parser = argparse.ArgumentParser()\r\n    parser = Trainer.add_argparse_args(parser)\r\n    args = parser.parse_args(cl_args.split() if cl_args else None)\r\n    trainer = Trainer.from_argparse_args(args)\r\n    # End changed code\r\n\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run('--gpus 2 --strategy ddp')\r\n```\r\nI presume this has to do with where the Trainer is forking the process. In summary, it seems one can \r\n- Set the environment variable via `os.environ` outside of the main function\r\n- Set the environment variable in the shell",
    "meta": { "name": "DDP Hangs with TORCH_DISTRIBUTED_DEBUG = DETAIL" },
    "answer": "Reading @akihironitta 's response and looking at the [documentation](https://pytorch.org/docs/stable/distributed.html#torch-distributed-debug) again, I noticed that they set the environment variable prior to calling `mp.spawn`. Moving the `os.environ['TORCH_DISTRIBUTED_DEBUG] = 'DETAIL'` line outside of the main function prevented hanging.\r\n\r\n```python\r\nimport argparse\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n# Start changed code\r\nimport os\r\nos.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL'\r\n# End changed code\r\n\r\ndef run(cl_args):\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n\r\n    # Start changed code\r\n    parser = argparse.ArgumentParser()\r\n    parser = Trainer.add_argparse_args(parser)\r\n    args = parser.parse_args(cl_args.split() if cl_args else None)\r\n    trainer = Trainer.from_argparse_args(args)\r\n    # End changed code\r\n\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run('--gpus 2 --strategy ddp')\r\n```\r\nI presume this has to do with where the Trainer is forking the process. In summary, it seems one can \r\n- Set the environment variable via `os.environ` outside of the main function\r\n- Set the environment variable in the shell"
  },
  {
    "content": "When using `accumulate_grad_batches=K` with Lightning and training for a fixed number of steps `n` (instead of epochs), does Lightning either:\r\n- execute `K` forward passes and then one optimization step per step, resulting in a total of `n*K` batches and `n` optimization steps\r\n- or execute one forward pass per step for a total of `n` batches with an optimization step only every `K` steps (therefore only `n/K` optimization steps in total)?if you mean `max_steps=n`, then 1.",
    "meta": {
      "name": "Gradient accumulation and total number of training steps"
    },
    "answer": "if you mean `max_steps=n`, then 1."
  },
  {
    "content": "I'm trying to fine-tune a Transformer model (XLM-R) on multi-gpu, using the `ddp_sharded` strategy. The train works, but at the end of the first epoch I got this error\r\n\r\n```\r\nRuntimeError: Trying to create tensor with negative dimension -2061635393: [-2061635393]\r\n```\r\n\r\nI'm running the latest PyTorch Lightning, PyTorch 1.10, and I'm using two V100 on a Power9 based architecture. I've tried both with 16bit and 32bit precision. The optimizer I'm using is RAdam, from PyTorch.\r\n\r\nI can provide the code if needed.\r\n\r\n<details>\r\n  <summary>Here the complete stack trace</summary>\r\n  \r\n```\r\nTraceback (most recent call last):\r\n  File \"transformers_ner/train.py\", line 186, in main\r\n    train(conf)\r\n  File \"transformers_ner/train.py\", line 103, in train\r\n    trainer.fit(pl_module, datamodule=pl_data_module)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 736, in\r\n_call_and_handle_interrupt\r\n    self.strategy.reconciliate_processes(traceback.format_exc())\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py\", line 451, in\r\nreconciliate_processes\r\n    raise DeadlockDetectedException(f\"DeadLock detected from rank: {self.global_rank} \\n {trace}\")\r\npytorch_lightning.utilities.exceptions.DeadlockDetectedException: DeadLock detected from rank: 0\r\n Traceback (most recent call last):\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 721, in\r\n_call_and_handle_interrupt\r\n    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 93,\r\nin launch\r\n    return function(*args, **kwargs)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\r\n    results = self._run_stage()\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\r\n    return self._run_train()\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 205, in run\r\n    self.on_advance_end()\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 297, in on_advance_end\r\n    self.trainer._call_callback_hooks(\"on_train_epoch_end\")\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1636, in\r\n_call_callback_hooks\r\n    fn(self, self.lightning_module, *args, **kwargs)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 308, in\r\non_train_epoch_end\r\n    self._save_topk_checkpoint(trainer, monitor_candidates)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 379, in\r\n_save_topk_checkpoint\r\n    self._save_monitor_checkpoint(trainer, monitor_candidates)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 651, in\r\n_save_monitor_checkpoint\r\n    self._update_best_and_save(current, trainer, monitor_candidates)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 702, in\r\n_update_best_and_save\r\n    self._save_checkpoint(trainer, filepath)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 384, in\r\n_save_checkpoint\r\n    trainer.save_checkpoint(filepath, self.save_weights_only)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 2467, in save_checkpoint\r\n    self._checkpoint_connector.save_checkpoint(filepath, weights_only=weights_only, storage_options=storage_options)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 444,\r\nin save_checkpoint\r\n    _checkpoint = self.dump_checkpoint(weights_only)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 380,\r\nin dump_checkpoint\r\n    optimizer_state = self.trainer.strategy.optimizer_state(optimizer)\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/pytorch_lightning/strategies/sharded.py\", line 117, in optimizer_state\r\n    optimizer.consolidate_state_dict()\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/fairscale/optim/oss.py\", line 364, in consolidate_state_dict\r\n    dist.broadcast_object_list(\r\n  File \"/m100/home/usertrain/a08trc0m/.conda/envs/ner/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1823, in\r\nbroadcast_object_list\r\n    object_tensor = torch.empty(\r\nRuntimeError: Trying to create tensor with negative dimension -2061635393: [-2061635393]\r\n```\r\n</details>Duplicate of #13431.",
    "meta": {
      "name": "Trying to create tensor with negative dimension with `ddp_sharded`"
    },
    "answer": "Duplicate of #13431."
  },
  {
    "content": "I am doing feature extraction using an efficientnet_b0 model. The training process works fine but it seems to pause every once in a while. I verified this using `nvidia-smi dmon`. There are spikes of a few seconds where the GPU utilization is anywhere between 50% and 100%, followed by a few seconds where the GPU utilization is 0%. \r\n\r\nRight now I am training with 4 Tesla T4, but I verified the same issue with a single GPU (T4 and V100).\r\nI am using a batch size of 200 (per GPU).  I have 48 CPUs and their usage is pretty low (I'd say 20-40%).\r\n\r\nI noticed the training pausing at epoch 48, 96, 144,... So it pauses every 48 steps.\r\n\r\nI thought that the pause were caused by logging so in my `Trainer` I set ` log_every_n_steps=500` and I also initialize my logger with `TensorBoardLogger(\"tb_logs\", name=\"vehicles\", max_queue=1000, flush_secs=120)`. I can that the processes pauses more frequently than 120 seconds.\r\n\r\nOriginally, I thought it was a PyTorch \"issue\". So I opened a post here https://discuss.pytorch.org/t/gpu-usage-is-not-constant-during-training/154718 . However I am wondering whether this could be caused by torch lightning.\r\n\r\nThank you\r\n@mfoglio It sounds like it's because of logging as you mentioned. To make sure if the issue sits around the logger, have you checked if you see the same behaviour by disabling it completely `Trainer(logger=False)`? Also, it would be easier to look into it if you could provide all of your trainer arguments (or your full script if feasible) for reproduction.\r\n\r\n> I noticed the training pausing at epoch 48, 96, 144,... So it pauses every 48 steps.\r\n\r\nHow are you checking that it happens at these exact steps? From the progress bar?\r\nUPDATE: You mean \"steps\" but not \"epochs\", right?Hi @akihironitta , thank you for your help. Yes, I am talking about steps, not epochs. And I am checking using the progress bar. The sleep is also confirmed by `nvidia-smi dmon`.\r\n\r\nEDIT: while at the beginning the code seems to be stuck at number of steps that are multiples of 48, I also noticed the progress bar getting stuck at step 965 which is obviously not a multiple of 48.\r\n\r\nI disabled both logging and checkpoint but I still see the same issue:\r\n\r\n<details><summary>code</summary>\r\n\r\n```python\r\n    # Trainer\r\n    trainer = Trainer(\r\n        accelerator='auto',\r\n        gpus=-1,\r\n        default_root_dir=checkpoint_path,\r\n        # devices=1,  # automatically inferred\r\n        # num_processes=os.cpu_count(),\r\n        strategy=DDPStrategy(find_unused_parameters=True),\r\n        precision=16 if torch.cuda.is_available() else 32,\r\n        # max_epochs=1000,\r\n        # max_steps=1000,\r\n        logger=False,\r\n        log_every_n_steps=500,\r\n        val_check_interval=1.0,  # must be a float otherwise it's interpreted as the number of batches\r\n        # num_sanity_val_steps=0,\r\n        # callbacks=[\r\n        #     feature_freeze_unfreeze,\r\n        #     checkpoint_epoch_model,\r\n        #     checkpoint_best_model,\r\n        #     RichProgressBar(),\r\n            # LearningRateMonitor(logging_interval='step'),\r\n        # ],\r\n        # profiler='simple',\r\n        max_epochs=-1\r\n    )\r\n\r\n    # Train\r\n    trainer.fit(\r\n        model=model,\r\n        datamodule=vehicles_datamodule,\r\n        ckpt_path=sorted(\r\n            glob.glob(os.path.join(checkpoint_epoch_path, '*.ckpt'))  # todo: edit path\r\n        )[-1] if resume_from_checkpoint else None\r\n    )\r\n```\r\nIt's hard for me to share my entire code but I can share the model and the data module.\r\nModel:\r\n```python\r\nfrom collections import defaultdict\r\nfrom typing import Dict, List\r\n\r\nimport seaborn as sn\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn as nn\r\nimport torchmetrics\r\nimport torchvision\r\n\r\nfrom vehicles.dataset.attributes import Attribute, AttributeValue\r\nfrom vehicles.dataset.splits import DatasetSplit\r\nfrom vehicles.model.modules import Flatten\r\nfrom utils.plots import convert_matplotlib_plot_to_torch_tensor\r\n\r\n\r\nclass MultiOutputClassifier(pl.LightningModule):\r\n\r\n    def __init__(self, attributes: List[Attribute],\r\n                 samples_per_attribute_value: Dict[Attribute, Dict[AttributeValue, int]],\r\n                 batch_size: int = 64,\r\n                 freeze_features=False,\r\n                 verbose=False,\r\n                 ):\r\n        super(MultiOutputClassifier, self).__init__()\r\n        self.attributes = attributes\r\n        self.verbose = verbose\r\n\r\n        # FEATURES\r\n        # Resnet\r\n        # resnet = torchvision.models.resnet50(pretrained=True)\r\n        # torch.nn.Sequential(*(list(resnet.children())[:-1]))\r\n        # EfficientNet\r\n        efficientnet_b0 = torchvision.models.efficientnet_b0(pretrained=True)\r\n        self.features = torch.nn.Sequential(*(list(efficientnet_b0.children())[:-1]))\r\n\r\n        # SqueezeNet\r\n        # self.features = torchvision.models.squeezenet1_1(pretrained=True).features  # output is [n, 512, 13, 13]\r\n\r\n        if freeze_features is True:\r\n            for param in self.features.parameters():\r\n                param.requires_grad = False\r\n\r\n        # MIDDLE LAYERS\r\n        self.channels_fc = 128\r\n        self.middle_layers = nn.Sequential(\r\n            # nn.Conv2d(512, 512, 3),\r\n            # nn.Conv2d(512, 512, 3),\r\n            # nn.Dropout2d(p=0.1),\r\n            # Squeezenet\r\n            # nn.AvgPool2d(13),\r\n            # Flatten(512),\r\n            # EfficientDet\r\n            Flatten(1280),\r\n            nn.Dropout(0.25),\r\n            nn.Linear(1280, self.channels_fc),  # 1280 for efficient det\r\n            nn.Linear(self.channels_fc, self.channels_fc, bias=False),\r\n            # nn.Linear(self.channels_fc, self.channels_fc, bias=False),\r\n        )\r\n\r\n        # OUTPUTS\r\n        self.output_layers = nn.ModuleDict({\r\n            f'output_{attribute.name}': nn.Sequential(\r\n                nn.Linear(self.channels_fc, len(attribute.get_values()), bias=False),\r\n            ) for attribute in self.attributes\r\n        })\r\n\r\n        # INITIALIZE WEIGHTS\r\n        self._initialize_weights()\r\n\r\n        # BATCH SIZE\r\n        self.batch_size = batch_size\r\n\r\n        # LOSSES\r\n        self.losses = self._get_losses(self.attributes, samples_per_attribute_value)\r\n\r\n        # METRICS\r\n        self.accuracy_metrics = nn.ModuleDict({\r\n            dataset_split: nn.ModuleDict({\r\n                attribute.name: torchmetrics.Accuracy(\r\n                    ignore_index=Attribute.ATTRIBUTE_VALUE_NONE.index,\r\n                    average='macro',\r\n                    num_classes=len(attribute.get_values(include_none=False))\r\n                )\r\n                for attribute in self.attributes\r\n            }) for dataset_split in [DatasetSplit.TRAIN, DatasetSplit.VALIDATION, DatasetSplit.TEST]\r\n        })\r\n        self.precision_metrics = nn.ModuleDict({\r\n            dataset_split: nn.ModuleDict({\r\n                attribute.name: torchmetrics.Precision(\r\n                    ignore_index=Attribute.ATTRIBUTE_VALUE_NONE.index,\r\n                    average='macro',\r\n                    num_classes=len(attribute.get_values(include_none=False))\r\n                )\r\n                for attribute in self.attributes\r\n            }) for dataset_split in [DatasetSplit.TRAIN, DatasetSplit.VALIDATION, DatasetSplit.TEST]\r\n        })\r\n        self.recall_metrics = nn.ModuleDict({\r\n            dataset_split: nn.ModuleDict({\r\n                attribute.name: torchmetrics.Recall(\r\n                    ignore_index=Attribute.ATTRIBUTE_VALUE_NONE.index,\r\n                    average='macro',\r\n                    num_classes=len(attribute.get_values(include_none=False))\r\n                )\r\n                for attribute in self.attributes\r\n            }) for dataset_split in [DatasetSplit.TRAIN, DatasetSplit.VALIDATION, DatasetSplit.TEST]\r\n        })\r\n        self.f1_metrics = nn.ModuleDict({\r\n            dataset_split: nn.ModuleDict({\r\n                attribute.name: torchmetrics.F1Score(\r\n                    ignore_index=Attribute.ATTRIBUTE_VALUE_NONE.index,\r\n                    average='macro',\r\n                    num_classes=len(attribute.get_values(include_none=False))\r\n                )\r\n                for attribute in self.attributes\r\n            }) for dataset_split in [DatasetSplit.TRAIN, DatasetSplit.VALIDATION, DatasetSplit.TEST]\r\n        })\r\n        self.confusion_matrix_metrics = nn.ModuleDict({\r\n            dataset_split: nn.ModuleDict({\r\n                attribute.name: torchmetrics.ConfusionMatrix(\r\n                    num_classes=len(attribute.get_values(include_none=False)),\r\n                    ignore_index=Attribute.ATTRIBUTE_VALUE_NONE.index,\r\n                    normalize='true',\r\n                    nan_strategy='ignore'\r\n                )\r\n                for attribute in self.attributes\r\n            }) for dataset_split in [DatasetSplit.TRAIN, DatasetSplit.VALIDATION, DatasetSplit.TEST]\r\n        })\r\n\r\n    def forward(self, x):\r\n        # Features\r\n        x = self.features(x)\r\n        if self.verbose:\r\n            print(f'Features {x.shape}')\r\n        # Middle layers\r\n        x = self.middle_layers(x)\r\n        if self.verbose:\r\n            print(f'Middle layers {x.shape}')\r\n        # Outputs\r\n        outputs = {attribute: output_layer(x) for attribute, output_layer in self.output_layers.items()}\r\n        # outputs = self.output_namedtuple(**outputs)\r\n        if self.verbose:\r\n            print(f'Output layers' + str({attribute: output.shape for attribute, output in outputs.items()}))\r\n        return outputs\r\n\r\n    def configure_optimizers(self):\r\n        def lr_lambda(epoch: int):\r\n            if epoch < 1:\r\n                return 1\r\n            # elif epoch < 2:\r\n            #     return 2\r\n            elif epoch < 25:\r\n                return 1\r\n            else:\r\n                return 0.97 ** (epoch / 2.4)  # official efficientnet decaying from paper\r\n\r\n        # Rescaling learning rate of official paper to the used batch size\r\n        # learning_rate = 0.1 * 0.256 / 4096 * self.batch_size * torch.cuda.device_count()\r\n        # Optimizer used by the paper\r\n        # TODO: check that RMSprop is properly initialized as explained in the paper\r\n        # optimizer = torch.optim.RMSprop(\r\n        #     self.parameters(), lr=learning_rate, alpha=0.9, momentum=0.9, weight_decay=1e-5\r\n        # )\r\n        optimizer = torch.optim.AdamW(self.parameters(), lr=0.1)\r\n        # optimizer_features = torch.optim.AdamW(self.features.parameters(), lr=0.0001)\r\n        # optimizer_middle_layers = torch.optim.AdamW(self.middle_layers.parameters(), lr=0.01)\r\n        # optimizer_outputs = torch.optim.AdamW(self.output_layers.parameters(), lr=0.01)\r\n\r\n        # lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\r\n        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\r\n            optimizer, max_lr=0.25, epochs=100, steps_per_epoch=4,\r\n            pct_start=0.1, anneal_strategy='cos', cycle_momentum=True,\r\n            base_momentum=0.85\r\n        )\r\n\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    @staticmethod\r\n    def _get_losses(attributes: List[Attribute],\r\n                    samples_per_attribute_value: Dict[Attribute, Dict[AttributeValue, int]]):\r\n        # Compute weights\r\n        weights_attributes = dict()\r\n        for attribute in attributes:\r\n            weights_attributes[attribute]: Dict[AttributeValue, float] = dict()\r\n            for attribute_value in attribute.get_values():\r\n                n_samples = samples_per_attribute_value[attribute][attribute_value]\r\n                if attribute_value is not Attribute.ATTRIBUTE_VALUE_NONE:\r\n                    weights_attributes[attribute][attribute_value] = 1 / n_samples if n_samples != 0 else 1\r\n        # Convert to tensors\r\n        weights_attributes = {\r\n            attribute: torch.tensor(list(weights_attribute.values()))\r\n            for attribute, weights_attribute in weights_attributes.items()\r\n        }\r\n        losses = nn.ModuleDict({\r\n            attribute.name: torch.nn.CrossEntropyLoss(\r\n                weight=weights_attribute,\r\n                ignore_index=Attribute.ATTRIBUTE_VALUE_NONE.index\r\n            )\r\n            for attribute, weights_attribute in weights_attributes.items()\r\n        })\r\n        return losses\r\n\r\n    def training_step(self, batch, batch_id):\r\n        inputs, labels = batch\r\n        y_pred_attributes = self.forward(inputs)\r\n        log_metrics = self._get_log_metrics(\r\n            dataset_split=DatasetSplit.TRAIN,\r\n            y_pred_attributes=y_pred_attributes,\r\n            y_target_attributes=labels,\r\n        )\r\n        # self.log_dict(log_metrics, rank_zero_only=True)\r\n        losses = {\r\n            attribute.name: log_metrics[f'{attribute.name} Loss'][DatasetSplit.TRAIN]\r\n            for attribute in self.attributes\r\n        }\r\n        sum_losses = sum([loss for loss in losses.values() if not loss.isnan()])\r\n        return {'loss': sum_losses, 'losses': losses, 'log': log_metrics}\r\n\r\n    def validation_step(self, batch, batch_id):\r\n        inputs, labels = batch\r\n        y_pred_attributes = self.forward(inputs)\r\n        log_metrics = self._get_log_metrics(\r\n            dataset_split=DatasetSplit.VALIDATION,\r\n            y_pred_attributes=y_pred_attributes,\r\n            y_target_attributes=labels,\r\n        )\r\n        # self.log_dict(log_metrics, sync_dist=True, rank_zero_only=True)\r\n        losses = {\r\n            attribute.name: log_metrics[f'{attribute.name} Loss'][DatasetSplit.VALIDATION]\r\n            for attribute in self.attributes\r\n        }\r\n        sum_losses = sum([loss for loss in losses.values() if not loss.isnan()])\r\n        return {'loss': sum_losses, 'log': log_metrics}\r\n\r\n    def test_step(self, batch, batch_id):\r\n        inputs, labels = batch\r\n        y_pred_attributes = self.forward(inputs)\r\n        log_metrics = self._get_log_metrics(\r\n            dataset_split=DatasetSplit.TEST,\r\n            y_pred_attributes=y_pred_attributes,\r\n            y_target_attributes=labels,\r\n        )\r\n        # self.log_dict(log_metrics, sync_dist=True, rank_zero_only=True)\r\n        losses = {\r\n            attribute.name: log_metrics[f'{attribute.name} Loss'][DatasetSplit.TEST]\r\n            for attribute in self.attributes\r\n        }\r\n        sum_losses = sum([loss for loss in losses.values() if not loss.isnan()])\r\n        return {'loss': sum_losses, 'log': log_metrics}\r\n\r\n    def training_epoch_end(self, outputs):\r\n        self._log_confusion_matrix(dataset_split=DatasetSplit.TRAIN)\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        self._log_confusion_matrix(dataset_split=DatasetSplit.VALIDATION)\r\n        # Metric to select best model\r\n        avg_f1 = torch.mean(torch.tensor(\r\n            [f1_metric.compute() for f1_metric in self.f1_metrics[DatasetSplit.VALIDATION].values()]\r\n        ))\r\n        # self.log('val_avg_f1', avg_f1, sync_dist=True, rank_zero_only=True)\r\n\r\n    def test_epoch_end(self, outputs):\r\n        self._log_confusion_matrix(dataset_split=DatasetSplit.TEST)\r\n\r\n    def _get_log_metrics(self, dataset_split: 'DatasetSplit', y_pred_attributes, y_target_attributes) \\\r\n            -> Dict[str, Dict[str, float]]:\r\n        metrics: Dict[str, Dict[str, float]] = defaultdict(dict)\r\n        for attribute, label in zip(self.attributes, y_target_attributes.T):\r\n            metrics[f'{attribute.name} Loss'][dataset_split] = self.losses[attribute.name](\r\n                y_pred_attributes[f'output_{attribute.name}'], label\r\n            )\r\n            if not all(label == Attribute.ATTRIBUTE_VALUE_NONE.index):  # TODO: is it ok to skip all these samples?\r\n                y_pred_attribute = y_pred_attributes[f'output_{attribute.name}']\r\n                # Filter out ignored index ( # TODO: torchmetrics seems to have a bug)\r\n                # TODO: is it correct?\r\n                filtered_label = label[label != Attribute.ATTRIBUTE_VALUE_NONE.index]\r\n                filtered_y_pred_attribute = y_pred_attribute[label != Attribute.ATTRIBUTE_VALUE_NONE.index]\r\n                # Compute metrics\r\n                self.accuracy_metrics[dataset_split][attribute.name](filtered_y_pred_attribute, filtered_label)\r\n                self.precision_metrics[dataset_split][attribute.name](filtered_y_pred_attribute, filtered_label)\r\n                self.recall_metrics[dataset_split][attribute.name](filtered_y_pred_attribute, filtered_label)\r\n                self.f1_metrics[dataset_split][attribute.name](filtered_y_pred_attribute, filtered_label)\r\n                self.confusion_matrix_metrics[dataset_split][attribute.name](filtered_y_pred_attribute, filtered_label)\r\n                metrics[f'{attribute.name} Accuracy'][dataset_split] = self.accuracy_metrics[dataset_split][attribute.name]\r\n                metrics[f'{attribute.name} F-1'][dataset_split] = self.f1_metrics[dataset_split][attribute.name]\r\n                metrics[f'{attribute.name} Precision'][dataset_split] = self.precision_metrics[dataset_split][attribute.name]\r\n                metrics[f'{attribute.name} Recall'][dataset_split] = self.recall_metrics[dataset_split][attribute.name]\r\n        return metrics\r\n\r\n    def _log_confusion_matrix(self, dataset_split: 'DatasetSplit'):\r\n        # Confusion matrix\r\n        for attribute in self.attributes:\r\n            # Compute confusion matrix\r\n            conf_mat = self.confusion_matrix_metrics[dataset_split][attribute.name].compute().detach().cpu().numpy().astype(np.float32)\r\n            # Convert confusion matrix to pandas dataframe\r\n            attribute_labels = [attribute_value.label for attribute_value in attribute.get_values()]\r\n            df_cm = pd.DataFrame(conf_mat, index=attribute_labels, columns=attribute_labels)\r\n            # Generate plot\r\n            figure = plt.figure(figsize=(25, 25))\r\n            # sn.set(font_scale=1.2)\r\n            sn.heatmap(df_cm, annot=True, annot_kws={'size': 7}, square=True, fmt='.2%')\r\n            # Send plot to tensorboard\r\n            image = convert_matplotlib_plot_to_torch_tensor(figure)\r\n            # self.logger.experiment.add_image(\r\n            #     f'{dataset_split} {attribute.name} Confusion Matrix',\r\n            #     image,\r\n            #     global_step=self.current_epoch\r\n            # )\r\n\r\n    def _initialize_weights(self):\r\n        def init_weights(m):\r\n            if type(m) in [nn.Linear, nn.Conv2d]:\r\n                torch.nn.init.kaiming_uniform_(m.weight)\r\n        self.middle_layers.apply(init_weights)\r\n        for output_layer in self.output_layers.values():\r\n            output_layer.apply(init_weights)\r\n\r\n    @torch.jit.export\r\n    def _format_single_output(self, labels: List[str], values: torch.Tensor):\r\n        indexes = torch.argsort(values, descending=True)\r\n        values: List[float] = values.tolist()\r\n        sorted_labels = [labels[i] for i in indexes]\r\n        sorted_values = [values[i] for i in indexes]\r\n        return {label: value for label, value in zip(sorted_labels, sorted_values)}\r\n\r\n    def freeze_feature_layers(self):\r\n        for param in self.features.parameters():\r\n            param.requires_grad = False\r\n\r\n    def unfreeze_feature_layers(self):\r\n        for param in self.features.parameters():\r\n            param.requires_grad = True\r\n\r\n    def freeze_middle_layers(self):\r\n        for param in self.middle_layers.parameters():\r\n            param.requires_grad = False\r\n\r\n    def unfreeze_middle_layers(self):\r\n        for param in self.middle_layers.parameters():\r\n            param.requires_grad = True\r\n\r\n    def to_onnx(self, **kwargs):\r\n        # ONNX settings\r\n        dynamic_axes = {\r\n            'images': {0: 'batch'},\r\n            **{\r\n                str(output): {0: 'batch'}\r\n                for output in self.output_layers.keys()\r\n            }\r\n        }\r\n        torch_onnx_export_kwargs = dict(\r\n            input_names=['images'],\r\n            output_names=[str(output) for output in self.output_layers.keys()],\r\n            dynamic_axes=dynamic_axes,\r\n        )\r\n        kwargs = {\r\n            **kwargs,\r\n            **torch_onnx_export_kwargs\r\n        }\r\n        super().to_onnx(**kwargs)\r\n\r\n    def export_deepstream_label_file(self, label_file_path: str):\r\n        # Create label file\r\n        text = ''\r\n        for attribute in self.attributes:\r\n            text += ';'.join([\r\n                f'{attribute.name}_{attribute_value.label}'\r\n                for attribute_value in attribute.get_values(include_none=False)\r\n            ])\r\n            text += '\\n'\r\n        with open(label_file_path, 'w') as fp:\r\n            fp.write(text)\r\n\r\n\r\nclass FeatureExtractorFreezeUnfreeze(pl.callbacks.BaseFinetuning):\r\n\r\n    def __init__(self, unfreeze_at_epoch: int):\r\n        super().__init__()\r\n        self._unfreeze_at_epoch = unfreeze_at_epoch\r\n\r\n    def freeze_before_training(self, pl_module):\r\n        # freeze any module you want\r\n        # Here, we are freezing `features`\r\n        self.freeze(pl_module.features)\r\n\r\n    def finetune_function(self, pl_module, current_epoch, optimizer, optimizer_idx):\r\n        # When `current_epoch` is self._unfreeze_at_epoch, features layers will start training.\r\n        if current_epoch == self._unfreeze_at_epoch:\r\n            self.unfreeze_and_add_param_group(\r\n                modules=pl_module.features,\r\n                optimizer=optimizer,\r\n                train_bn=True,\r\n            )\r\n\r\n```\r\nData module:\r\n```python\r\nimport os\r\nfrom typing import Any, Dict, List, Optional\r\n\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom pytorch_lightning import LightningDataModule\r\n\r\nfrom vehicles.dataset.attributes import Attribute\r\n\r\n\r\nclass VehiclesDataModule(LightningDataModule):\r\n\r\n    def __init__(self, train_dataset: Dataset, val_dataset: Dataset, test_dataset: Dataset,\r\n                 attributes: List[Attribute], batch_size: int, num_workers: int):\r\n        super().__init__()\r\n        self.train_dataset = train_dataset\r\n        self.val_dataset = val_dataset\r\n        self.test_dataset = test_dataset\r\n        self.attributes = attributes\r\n        self.batch_size = batch_size\r\n        self.num_workers = num_workers\r\n\r\n    def setup(self, stage: Optional[str] = None):\r\n        pass\r\n\r\n    def _get_data_loaders_kwargs(self):\r\n        return dict(\r\n            batch_size=self.batch_size,\r\n            num_workers=self.num_workers,\r\n            pin_memory=True,\r\n            persistent_workers=True,\r\n            # drop_last=True\r\n            prefetch_factor=2\r\n        )\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.train_dataset, **self._get_data_loaders_kwargs())\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.val_dataset, **self._get_data_loaders_kwargs())\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.test_dataset, **self._get_data_loaders_kwargs())\r\n\r\n    def predict_dataloader(self):\r\n        return DataLoader(self.test_dataset, **self._get_data_loaders_kwargs())\r\n\r\n    def teardown(self, stage: Optional[str] = None):\r\n        # Used to clean-up when the run is finished\r\n        pass\r\n\r\n    def state_dict(self) -> Dict[str, Any]:\r\n        return {\r\n            'train_dataset': self.train_dataset.state_dict(),\r\n            'val_dataset': self.val_dataset.state_dict(),\r\n            'test_dataset': self.test_dataset.state_dict(),\r\n        }\r\n\r\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\r\n        self.train_dataset.load_state_dict(state_dict['train_dataset'])\r\n        self.val_dataset.load_state_dict(state_dict['val_dataset'])\r\n        self.test_dataset.load_state_dict(state_dict['test_dataset'])\r\n\r\n```\r\n\r\n</details>As a side question, the following checkpoint will be saved only at the end of every epoch, not every time there is an improvement in the metric, right? Because during the first epochs there is going to be an improvement almost every step.\r\n```\r\n    checkpoint_best_model = ModelCheckpoint(\r\n        dirpath=checkpoint_best_path,\r\n        filename=\"{epoch}-{step}-{val_avg_f1:.3f}\",\r\n        monitor=\"val_avg_f1\",\r\n        mode=\"min\",\r\n        save_top_k=10,\r\n        save_on_train_epoch_end=True,  # to ensure the required metric is being accumulated correctly\r\n                                       # for creating a checkpoint\r\n    )\r\n```\r\nAgain, just to be clear, in my latest test the checkpointing system was disabled.I analyzed the problem a little bit more. I noticed that I have 48 CPUs and 48 workers. That makes the training process pausing every 48 steps. If use 12 workers, the pause happens every 12 steps.\r\nI'd like to increase the number of workers but the RAM usage is crazy high. With 48 workers I am almost using all the 180Gb of RAM available. Is this normal for simply loading images of a few Kbytes?\r\nAny suggestion on how to speed this up?\r\n\r\nEDIT: I think I am facing this issue https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662 even though I am not entirely sure. My memory consumption is of about 100-150 gb right after the training starts. I tried to used a numpy array to store the huge list of integers containing the IDs of the record in the dataset. However, this didn't reduce the RAM usage.\r\nSuppose my dataset has a property `myobject` of type `MyObject`, and that `myobject` internally references a list of integers. Should I convert this list of integers to a numpy array too?",
    "meta": { "name": "Training seems to pause every N steps" },
    "answer": "I analyzed the problem a little bit more. I noticed that I have 48 CPUs and 48 workers. That makes the training process pausing every 48 steps. If use 12 workers, the pause happens every 12 steps.\r\nI'd like to increase the number of workers but the RAM usage is crazy high. With 48 workers I am almost using all the 180Gb of RAM available. Is this normal for simply loading images of a few Kbytes?\r\nAny suggestion on how to speed this up?\r\n\r\nEDIT: I think I am facing this issue https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662 even though I am not entirely sure. My memory consumption is of about 100-150 gb right after the training starts. I tried to used a numpy array to store the huge list of integers containing the IDs of the record in the dataset. However, this didn't reduce the RAM usage.\r\nSuppose my dataset has a property `myobject` of type `MyObject`, and that `myobject` internally references a list of integers. Should I convert this list of integers to a numpy array too?"
  },
  {
    "content": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nI want to test summarization model from [huggingface summarization example](https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/finetune.py) on multiple GPUs . My problem is **how could I collect test results  on different GPUs** , since `test_epoch_end` only processes epoch for a single GPU. \r\nFor more information, the model is trained with ddp backend.\r\n#### Code\r\n```   \r\n def test_epoch_end(self, outputs):\r\n        output_test_predictions_file = os.path.join(self.hparams.output_dir, \"test_predictions.txt\")\r\n        output_test_targets_file = os.path.join(self.hparams.output_dir, \"test_targets.txt\")\r\n        # write predictions and targets for later rouge evaluation.\r\n        with open(output_test_predictions_file, \"w+\") as p_writer, open(output_test_targets_file, \"w+\") as t_writer:\r\n            for output_batch in outputs:\r\n                p_writer.writelines(s + \"\\n\" for s in output_batch[\"preds\"])\r\n                t_writer.writelines(s + \"\\n\" for s in output_batch[\"target\"])\r\n            p_writer.close()\r\n            t_writer.close()\r\n\r\n        return self.test_end(outputs)\r\n```\r\n\r\n#### What have you tried?\r\nFor now, I can only use single GPU to get result of whole dataset.\r\n#### What's your environment?\r\n\r\n - OS: Unbuntu 18.04\r\n - Packaging pip\r\n - Version: 0.7.6\r\nUse `torch.distributed.all_gather` to gather and merge the outputs from all GPUs.\r\nAnd you should remove the redundant examples due to the ddp_sampler adds extra examples to work with multi GPUS. (https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler)\r\n\r\nHere is the workaround snippet used in my own project.\r\n\r\n```\r\ndef gather_distributed(*tensors):\r\n    output_tensors = []\r\n    for tensor in tensors:\r\n        tensor_list = [torch.ones_like(tensor) for _ in range(dist.get_world_size())]\r\n        dist.all_gather(tensor_list, tensor)\r\n        output_tensors.append(torch.cat(tensor_list))\r\n    return output_tensors\r\n\r\n\r\ndef deduplicate_and_sort(index, *tensors):\r\n    reverse_index = torch.zeros_like(index)\r\n    for ri, i in enumerate(index):\r\n        reverse_index[i] = ri\r\n    reverse_index = reverse_index[:index.max() + 1]\r\n    output_tensors = [tensor.index_select(0, reverse_index) for tensor in tensors]\r\n    return output_tensors\r\n```\r\nIn the above code, you need the **index** of each example to remove redundant examples and sort outputs in order.\r\nNotice that the index should consist of consecutive integers (e.g., 0,1,2,...N).\r\nThanks to @haichao592 :+1:\r\n I assume this is resolved now @joe32140 anyway feel free to reopen if needed :raccoon: ",
    "meta": {
      "name": "How to gather results on multiple GPUs while testing? ddp"
    },
    "answer": "Use `torch.distributed.all_gather` to gather and merge the outputs from all GPUs.\r\nAnd you should remove the redundant examples due to the ddp_sampler adds extra examples to work with multi GPUS. (https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler)\r\n\r\nHere is the workaround snippet used in my own project.\r\n\r\n```\r\ndef gather_distributed(*tensors):\r\n    output_tensors = []\r\n    for tensor in tensors:\r\n        tensor_list = [torch.ones_like(tensor) for _ in range(dist.get_world_size())]\r\n        dist.all_gather(tensor_list, tensor)\r\n        output_tensors.append(torch.cat(tensor_list))\r\n    return output_tensors\r\n\r\n\r\ndef deduplicate_and_sort(index, *tensors):\r\n    reverse_index = torch.zeros_like(index)\r\n    for ri, i in enumerate(index):\r\n        reverse_index[i] = ri\r\n    reverse_index = reverse_index[:index.max() + 1]\r\n    output_tensors = [tensor.index_select(0, reverse_index) for tensor in tensors]\r\n    return output_tensors\r\n```\r\nIn the above code, you need the **index** of each example to remove redundant examples and sort outputs in order.\r\nNotice that the index should consist of consecutive integers (e.g., 0,1,2,...N).\r\n"
  },
  {
    "content": "I'm looking at the following example: \r\n\r\n```py\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule\r\nfrom pytorch_lightning.utilities.cli import LightningCLI\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n\r\ndef run():\r\n    LightningCLI(model_class=BoringModel)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nwith config file: \r\n\r\n```\r\ntrainer:\r\n    max_steps: 1\r\n    limit_val_batches: 1\r\n    accelerator: ddp_cpu\r\n    plugins:\r\n      - class_path: \"pytorch_lightning.plugins.training_type.ddp.DDPPlugin\"\r\n        init_args:\r\n          find_unused_parameters: false\r\n\r\n```\r\n\r\nIn Lightning 1.4 I am getting the following error.\r\n\r\n```python\r\nbug_report_model.py: error: Parser key \"trainer.plugins\": Value \"[{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}]\" does not validate against any of the types in typing.Union[typing.List[typing.Union[pytorch_lightning.plugins.base_plugin.Plugin, pytorch_lightning.plugins.environments.cluster_environment.ClusterEnvironment, str]], pytorch_lightning.plugins.base_plugin.Plugin, pytorch_lightning.plugins.environments.cluster_environment.ClusterEnvironment, str, NoneType]:\r\n  - Value \"{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}\" does not validate against any of the types in typing.Union[pytorch_lightning.plugins.base_plugin.Plugin, pytorch_lightning.plugins.environments.cluster_environment.ClusterEnvironment, str]:\r\n    - Problem with given class_path \"pytorch_lightning.plugins.training_type.ddp.DDPPlugin\":\r\n      - 'Configuration check failed :: No action for key \"find_unused_parameters\" to check its value.'\r\n    - \"pytorch_lightning.plugins.training_type.ddp.DDPPlugin\" is not a subclass of ClusterEnvironment\r\n    - Expected a <class 'str'> but got \"{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}\"\r\n  - Type <class 'pytorch_lightning.plugins.base_plugin.Plugin'> expects an str or a Dict with a class_path entry but got \"[{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}]\"\r\n  - Type <class 'pytorch_lightning.plugins.environments.cluster_environment.ClusterEnvironment'> expects an str or a Dict with a class_path entry but got \"[{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}]\"\r\n  - Expected a <class 'str'> but got \"[{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}]\"\r\n  - Expected a <class 'NoneType'> but got \"[{'class_path': 'pytorch_lightning.plugins.training_type.ddp.DDPPlugin', 'init_args': {'find_unused_parameters': True}}]\"\r\n```\r\n\r\nThe relevant error is:\r\n\r\n```python\r\n    - Problem with given class_path \"pytorch_lightning.plugins.training_type.ddp.DDPPlugin\":\r\n      - 'Configuration check failed :: No action for key \"find_unused_parameters\" to check its value.'\r\n```\r\n\r\nThe [DDPPlugin signature has kwargs defined](https://github.com/PyTorchLightning/pytorch-lightning/blob/75e18a52988f760a8b32a3c4fc6f0f3217868e57/pytorch_lightning/plugins/training_type/ddp.py#L74) and I am not sure if jsonargparse is supposed to be able to handle them, I couldn't find any explanation in their [docs](https://jsonargparse.readthedocs.io/en/stable/#class-type-and-sub-classes). \r\n\r\nI would like to pass in `find_unused_parameters=False` as part of these kwargs. \r\n\r\nCurrent workaround: \r\n\r\n```\r\ntrainer:\r\n    ...\r\n    plugins: ddp_find_unused_parameters_false \r\n\r\n```\r\n\r\ncc @mauvilsa @carmocca who might be pleased to hear that I started to really like the CLI <3  \r\n\r\nEDIT\r\n- A similar issue exists here #8262, but it is not the same because what OP posted works with the latest jsonargparse version and is not my exact use case.There are many possible ways in which people could use `**kwargs` in the `__init__` of a class. Having `jsonargparse` try to interpret the source code to see how `**kwargs` is used would be highly complex. Thus it is a design decision that `**kwargs` is used in a single way, and that is for class inheritance. The documentation does mention it \"since the init has the `**kwargs` argument, the keyword arguments from MyBaseClass are also added\". This might be a bit too brief and could be good to extend it. Also one objective of `jsonargparse` has been to validate everything so that input mistakes make the code fail as early as possible with useful messages. Having `**kwargs` mean to accept anything goes against this.\r\n\r\nIs there any class or function that defines the accepted parameters and type hints for the `DDPPlugin` init kwargs?Hi @awaelchli @mauvilsa @carmocca, what about implementing `jsonargparse` to support such kind of input:\r\n```\r\nfit:\r\n  trainer:\r\n    plugins:\r\n      - class_path: pytorch_lightning.plugins.DDPPlugin\r\n        init_args:\r\n          kwargs:\r\n            find_unused_parameters: false\r\n```\r\nThat is to regard `kwargs` as a special `Dict` that doesn't need to check the types.\r\nIt's a good compromise for the type validation and the practical needs for the `**kwargs`.The issues related to plugins instantiation have been resolved with #13283.",
    "meta": { "name": "How to instantiate plugins with LightningCLI?" },
    "answer": "The issues related to plugins instantiation have been resolved with #13283."
  },
  {
    "content": "Hi,\r\n\r\nI'm using some GPUs and the Distributed-Data-Parallel strategy and I want to know how the global dataset is split across all GPUs.\r\n\r\nIs it split iteratively: \r\n  * The N first graphs go-to GPU 0\r\n  * The N next go-to GPU 1\r\n  * ...\r\n  *  Until the end ?\r\n\r\nIs it randomly distributed?\r\n\r\nIs it another way?\r\n\r\nI looked at the DistributedSampler class, but I didn't find the answer.I believe [this line](https://github.com/pytorch/pytorch/blob/63c0bcf887736810dd2ca6e8b671439c67e58ed6/torch/utils/data/distributed.py#L118) in PyTorch code explains it all:\r\n```python\r\nindices = indices[self.rank:self.total_size:self.num_replicas]\r\n```",
    "meta": {
      "name": "Multi-GPUs DDP - How the dataset is distributed accross the GPUs"
    },
    "answer": "I believe [this line](https://github.com/pytorch/pytorch/blob/63c0bcf887736810dd2ca6e8b671439c67e58ed6/torch/utils/data/distributed.py#L118) in PyTorch code explains it all:\r\n```python\r\nindices = indices[self.rank:self.total_size:self.num_replicas]\r\n```"
  },
  {
    "content": "Hi,\r\n\r\nI'm training a model on several GPUs with the DDP strategy, I'm using an Nvidia DGX where GPUs are interconnected with NVLinks. \r\n\r\nI want to know when the gradients are synchronized, does the communication use the NVLinks or GPU-to-GPU or use the CPUs to communicate?This thread in PyTorch forum may be what you find useful:\r\nhttps://discuss.pytorch.org/t/simple-code-example-with-nvlink-support/125304",
    "meta": {
      "name": "DDP - Synchronization on DGX - Use CPUs or GPU-to-GPU interconnect"
    },
    "answer": "This thread in PyTorch forum may be what you find useful:\r\nhttps://discuss.pytorch.org/t/simple-code-example-with-nvlink-support/125304"
  },
  {
    "content": "Using the exact code from the [imagenet template](https://github.com/Lightning-AI/lightning/blob/master/examples/pl_domain_templates/imagenet.py), I add the following to the end of the script:\r\n\r\n```python \r\nif __name__ == \"__main__\":\r\n    import pytorch_lightning\r\n\r\n    model = ImageNetLightningModel(\r\n        data_path=\"path/to/imagenet/data\",\r\n        pretrained=True,\r\n        workers=8\r\n    )\r\n\r\n    trainer = pytorch_lightning.Trainer(\r\n        gpus=4\r\n    )\r\n\r\n    trainer.test(model)\r\n\r\n    trainer.fit(model)\r\n```\r\n\r\nIf I run `trainer.test(model)` I get the performance I expect from a pretrained imagenet model. However, when running `trainer.fit(model)` the training accuracy is zero. \r\n\r\nAre the weights getting reinitialized when training is started? If so, how do I prevent that from happening?> Are the weights getting reinitialized when training is started? If so, how do I prevent that from happening?\r\n\r\nthey are not, can you check the loss and training logic inside the training step. Or maybe there is an issue on how you are calculating the accuracy.",
    "meta": { "name": "Imagnet pretrained model: training accuracy to zero" },
    "answer": "> Are the weights getting reinitialized when training is started? If so, how do I prevent that from happening?\r\n\r\nthey are not, can you check the loss and training logic inside the training step. Or maybe there is an issue on how you are calculating the accuracy."
  },
  {
    "content": "For example: How to save checkpoints ONLY when valset accuracy is greater than some specific threshold?\r\n\r\nIf this is not possible using parameters in ModelCheckpoint callback, how can we extend ModelCheckpoint to achieve this?you can do:\r\n```py\r\nclass CustomModelCheckpoint(ModelCheckpoint):\r\n    def on_validation_end(self, trainer, pl_module):\r\n        score = trainer.callback_metrics[self.monitor]\r\n        if score > some_threshold:\r\n            super().on_validation_end(trainer, pl_module)\r\n\r\n\r\ntrainer = Trainer(callbacks=[CustomModelCheckpoint(...)])\r\n```",
    "meta": { "name": "Placing a condition on ModelCheckpoint Callback" },
    "answer": "you can do:\r\n```py\r\nclass CustomModelCheckpoint(ModelCheckpoint):\r\n    def on_validation_end(self, trainer, pl_module):\r\n        score = trainer.callback_metrics[self.monitor]\r\n        if score > some_threshold:\r\n            super().on_validation_end(trainer, pl_module)\r\n\r\n\r\ntrainer = Trainer(callbacks=[CustomModelCheckpoint(...)])\r\n```"
  },
  {
    "content": "Hi,\r\n\r\nI have successfully implemented the method to log images to tensorboard logger, except I run out of GPU memory soon as I accumulate images during the whole validation_step and by end of the validation round, I randomly select few images to log. This is not the best way to do it. Can someone point me how to do it properly where I don\u2019t consume too much of Memory. Thanks.\r\n\r\nThis is how my validation step looks:\r\n\r\n```\r\ndef validation_step(self, batch, batch_idx):\r\n        imgs, y_true = batch\r\n        y_pred = self(imgs)\r\n        val_loss = self.nn_criterion(y_pred, y_true)\r\n        self.log(\"val_loss\", val_loss)\r\n        \r\n        return {\"val_loss\": val_loss,\r\n                \"images\": imgs,\r\n                \"masks_pred\": y_pred,\r\n                \"true_masks\": y_true}\r\n```\r\n                \r\nOne can clearly see that I am accumulating tensors over the validation step. Since I am working with very large dataset, I run out of memory very soon. Thanks in advance.hey @muaali !\r\n\r\n> by end of the validation round, I randomly select few images to log\r\n\r\nyou can log the images randomly inside `validation_step` itself to avoid accumulation that is creating memory overhead.",
    "meta": {
      "name": "Logging Images during validation using Tensorboard Logger"
    },
    "answer": "hey @muaali !\r\n\r\n> by end of the validation round, I randomly select few images to log\r\n\r\nyou can log the images randomly inside `validation_step` itself to avoid accumulation that is creating memory overhead."
  },
  {
    "content": "Yesterday, I saw the code [\ud83d\udd17](https://colab.research.google.com/github/NVIDIA/NeMo/blob/r1.0.0rc1/tutorials/nlp/Question_Answering_Squad.ipynb#scrollTo=ref1qSonGNhP)\r\n, which given the comment at the last cell.\r\n> To improve the performance of the model, train with multi-GPU and a global batch size of 24. So if you use 8 GPUs with trainer.gpus=8, set model.train_ds.batch_size=3\r\n\r\nIn tensorflow, we need to calculate the global batch size in multi-gpu settings. Is the same concept also applied in the multi-gpu of trainer ?\r\n\r\nIf we setup `trainer = Trainer(**{'devices':4, 'accelerator':'gpu', 'strategy':'ddp'})` with the batch size 64, will the model benefit with the larger batch size (256), or the model only benefit with batch size (64) ?\r\n\r\n```\r\n# sample code snippet :\r\ntra_ld = DataLoader(train_dset, batch_size=32, ... )\r\ntrainer = Trainer(**{'devices':4, 'accelerator':'gpu', 'strategy':'ddp'})\r\ntrainer.fit(model, tra_ld)\r\n```Hi @HuangChiEn, in your case shown in the sample code, the actual batch size will be `128` (`32 * 4`).\r\n\r\nIt behaves differently depending on which strategy is used. You can read more about it\r\n- in our docs: https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu.html#batch-size\r\n- in PyTorch forum: https://discuss.pytorch.org/t/do-dataparallel-and-distributeddataparallel-affect-the-batch-size-and-gpu-memory-consumption/97194",
    "meta": {
      "name": "What's the relationship between number of gpu and batch size (global batch size))"
    },
    "answer": "Hi @HuangChiEn, in your case shown in the sample code, the actual batch size will be `128` (`32 * 4`).\r\n\r\nIt behaves differently depending on which strategy is used. You can read more about it\r\n- in our docs: https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu.html#batch-size\r\n- in PyTorch forum: https://discuss.pytorch.org/t/do-dataparallel-and-distributeddataparallel-affect-the-batch-size-and-gpu-memory-consumption/97194"
  },
  {
    "content": "* `pytorch-lightning` version: `1.6.4`\r\n\r\n---\r\nWhen tuning the learning_rate using `lr_find`, I get:\r\n```\r\nUserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\r\n```\r\nRelated Issue: #5587\r\nCode snippet is given below:\r\n\r\n```python\r\ndef tune_lr(\r\n        model: LightningModule,\r\n        tuning_params: Dict[str, Any] = None,\r\n        trainer_args: Dict[str, Any] = None\r\n):\r\n    if trainer_args is None:\r\n        trainer_args = dict()\r\n    if tuning_params is None:\r\n        tuning_params = dict()\r\n    dummy_trainer = Trainer(\r\n        **trainer_args\r\n    )\r\n    lr_finder = dummy_trainer.tuner.lr_find(model=model, **tuning_params)\r\n    lr = lr_finder.suggestion()\r\n    del dummy_trainer\r\n    return lr\r\n\r\nnet.hparams.lr = tune_lr(\r\n    model=net,\r\n    tuning_params={\r\n        \"mode\": \"exponential\",\r\n        \"datamodule\": data_module,\r\n        \"min_lr\": 1e-08,\r\n        \"max_lr\": 1.0\r\n    },\r\n    trainer_args={\r\n        \"callbacks\": [\r\n            StochasticWeightAveraging(swa_lrs=1e-2),\r\n            EarlyStopping(\r\n                monitor=\"Validation-Mean_Loss\",\r\n                mode=\"min\",\r\n                patience=10,\r\n                strict=True,\r\n                check_finite=True,\r\n                min_delta=1e-3,\r\n                check_on_train_epoch_end=False,\r\n            )\r\n        ],\r\n        \"accumulate_grad_batches\": 1,\r\n        \"check_val_every_n_epoch\": 10,\r\n        \"num_sanity_val_steps\": 0,\r\n        \"detect_anomaly\": False,\r\n        \"log_every_n_steps\": 1,\r\n        \"enable_progress_bar\": True,\r\n        \"precision\": 16,\r\n        \"sync_batchnorm\": False,\r\n        \"enable_model_summary\": False,\r\n        \"max_epochs\": max_epochs,\r\n        \"accelerator\": \"gpu\",\r\n        \"devices\": -1\r\n    }\r\n)\r\n```@digital-idiot Can I see the trainer args as well?",
    "meta": {
      "name": "Detected call of `lr_scheduler.step()` before `optimizer.step()`"
    },
    "answer": "@digital-idiot Can I see the trainer args as well?"
  },
  {
    "content": "Hello. I love PL!\r\n\r\nis there a bibtex version of this citation out there?\r\n\r\nThanks\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/bfa8b7be2d99b980afa62f5cb0433326bcfd2ef0/CITATION.cff#L1\r\n@noamsgl You can copy it from \"Cite this repository\" in the side bar of the top page.\r\n\r\n![Screen Shot 2022-06-15 at 22 42 09](https://user-images.githubusercontent.com/20610905/173842305-2f939fa1-2bf6-4404-8890-fd693b4b3be2.png)",
    "meta": { "name": "Bibtex Citation" },
    "answer": "@noamsgl You can copy it from \"Cite this repository\" in the side bar of the top page.\r\n\r\n![Screen Shot 2022-06-15 at 22 42 09](https://user-images.githubusercontent.com/20610905/173842305-2f939fa1-2bf6-4404-8890-fd693b4b3be2.png)"
  },
  {
    "content": "I am modifying some code taken out of the lightning docs and training a VAE to learn a represenation of time series thought to contain structure.  The validation outputs are reconstructions of the time series.  The following code is written with the intention of grabbing a random output and plotting it.\r\n\r\nTwo things happen that I don't understand. \r\n\r\n1 - It seems the plt object from the previous epoch is still in memory and the new data is plotted alongside the old.  I have tried deleting the data explicitly but to no avail... \r\n2 - Somehow both the input and the output are getting plotted (in the attached plot you see the learned noise, and the well formed actual singal I am trying to reconstruct. I have no idea where the input signal is coming from - is it embedded in the output tensor somewhere?\r\n \r\n    def validation_epoch_end(self, outputs):\r\n        if not self.save_images:\r\n            return\r\n        if not os.path.exists(self.save_path):\r\n            os.makedirs(self.save_path)\r\n        choice = random.choice(outputs)\r\n        output_sample = random.choice(choice[0])\r\n        plt.plot(output_sample.cpu())\r\n        plt.savefig(f\"{self.save_path}/epoch_{self.current_epoch}.png\")\r\n        print('this is def interesting')\r\n\r\n![epoch_0](https://user-images.githubusercontent.com/25708723/173474539-a50d4833-debe-4a69-a777-5aff7b555132.png)\r\n![epoch_1](https://user-images.githubusercontent.com/25708723/173474546-81e00883-e6e1-43eb-b2ac-de025024df34.png)\r\n![epoch_2](https://user-images.githubusercontent.com/25708723/173474554-d2e35885-4080-423c-97d5-1fb51e78e320.png)\r\n\r\nCreating a new figure before plotting?\r\n```python\r\nfig, ax = plt.subplots()\r\nax.set_xlim(0, 10)\r\nax.plot([1, 5], [0, 10])\r\n```",
    "meta": {
      "name": "Interesting behaviour/interaction between matplotlib and validation_epoch_end(self, outputs)"
    },
    "answer": "Creating a new figure before plotting?\r\n```python\r\nfig, ax = plt.subplots()\r\nax.set_xlim(0, 10)\r\nax.plot([1, 5], [0, 10])\r\n```"
  },
  {
    "content": "Hi, I'm trying to use my Pytorch Lightning code in conjunction with [Jukebox](https://github.com/openai/jukebox) which has [its own set of routines for distributed training](https://github.com/openai/jukebox/blob/master/jukebox/utils/dist_utils.py) via the `torch.distributed.run` method.   I have read the [PyTorchLightning docs on torch.distributed](https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_intermediate.html?highlight=torch.distributed.run#torch-distributed-elastic) and followed them as far as I know:\r\n\r\n```bash\r\npython -m torch.distributed.run --nnodes=1 --nproc_per_node=8 --rdzv_id=31459 --rdzv_backend=c10d --rdzv_endpoint=127.0.0.1 ./my_script.py --myarg1=thing  ...etc\r\n```\r\n\r\nIf I only ever run on 1 GPU there's no problem, but when I try to run on more than 1 GPU via DDP, then I get many errors from NCCL such as\r\n\r\n```\r\n[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).\r\n[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\r\n[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.\r\nCaught error during NCCL init (attempt 0 of 5): The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\r\n```\r\n\r\n\"Already in use\":  Presumably Jukebox, which it runs its own MPI initialization in the form of \r\n```Python\r\nfrom jukebox.utils.dist_utils import setup_dist_from_mpi\r\n...\r\nrank, local_rank, device = setup_dist_from_mpi()\r\n```\r\n(^^ This call is inside my TRAINER module, BTW, so it should be AFTER Lightning sets up the `init_process_group()`, right?) \r\n\r\n...Jukebox is trying to setup re-reserve the slots ALREADY setup/reserved by the Lightning Trainer when the [DDP Spawn](https://github.com/PyTorchLightning/pytorch-lightning/blob/c1f05021ff0093f720770a6065ab62a70c535add/pytorch_lightning/strategies/ddp_spawn.py#L160) routine in [PytorchLightning itself already calls ` torch.distributed.init_process_group()`](https://github.com/PyTorchLightning/pytorch-lightning/blob/bfa8b7be2d99b980afa62f5cb0433326bcfd2ef0/pytorch_lightning/utilities/distributed.py#L355) ; and rather than just polling the `os.` environment for keys like RANK and MASTER_ADDR, Jukebox is ignoring those for now. \r\n\r\n## My question:\r\nIf PyTorch Lightning is setting the `torch.distributed.dist` object already, then is there a way I can get access to it?  ( for interfacing with  the Jukebox code?)\r\n\r\n(or can I call `init_process_group()` myself or obtain the result from when PyTorch Lightning called it?)\r\n\r\nBecause right now, if I try NOT calling their MPI initialization routine `setup_dist_from_mpi()` and instead just communicate key values based on environment variables a la:\r\n\r\n```Python\r\n        rank, local_rank, device = os.getenv('RANK'), os.getenv('RANK'), self.device\r\n```\r\n\r\n...then whereever the Jukebox code calls something like `dist.barrier()`, then I get an error about \r\n```\r\nRuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1102, internal error, NCCL version 21.0.3\r\nncclInternalError: Internal check failed. This is either a bug in NCCL or due to memory corruption\r\n\r\n...\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group....\r\n``` \r\nBut I thought [Lightning was supposedly calling init_process_group() already?](https://github.com/PyTorchLightning/pytorch-lightning/blob/bfa8b7be2d99b980afa62f5cb0433326bcfd2ef0/pytorch_lightning/utilities/distributed.py#L355)....? \r\nSo I'm confused.   Any tips?\r\n\r\n**UPDATE:** Before the call to the Jukebox stuff, I did check and the pytorch distributed `dist.is_available()` is True, so it looks like Lightning may have done something already by that point.  But in that case I'm confused about why we're still seeing that RuntimeError about process group not being initialized:\r\n```\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n```Solved it.  ...Not quite sure which of the numerous things I tried was they key fix, but I can come back and post that.  \r\nFor now, just want to save the trouble for anyone who might have devoted time to helping me.  # MY SOLUTION: \r\n\r\nI think I've distilled it to two simple parts. Default values all ended up being ok, and no special environment-variable-setting proved necessary (e.g. I unset all the NCCL flags I'd tried earlier).  Two things:\r\n\r\n1. running as instructed with the `torch.distributed.run` as before.  Although the default values were fine. e.g. \r\n```\r\npython -m torch.distributed.run --nnodes=1 --nproc_per_node=8 ./myscript.py --arg1=thing ...etc\r\n```\r\n\r\n2. Near the top of my Trainer init code (and before the Jukebox stuff), initialize :\r\n```Python\r\ndist.init_process_group(backend=\"nccl\")\r\n```\r\n no other parts were essential.   And I could either have the trainer strategy set to \"ddp\" or \"fsdp\" or nothing at all; made no difference. \r\n\r\n3. ALTHOUGH, one extra other thing that makes it go even faster: For some reason `OMP_NUM_THREADS` is not being set and so you see a warning message that it's getting set to 1 by default.  No need to leave that!   So my final, well-performing invocation looks like:\r\n```\r\nOMP_NUM_THREADS=12 python -m torch.distributed.run --nnodes=1 --nproc_per_node=8 ./myscript.py --arg1=thing ...etc\r\n```\r\none could also permanenty set `export OMP_NUM_THREADS=12` but I haven't bothered to do that yet. \r\n",
    "meta": {
      "name": "DDP: NCCL \" The server socket has failed to bind to...\""
    },
    "answer": "# MY SOLUTION: \r\n\r\nI think I've distilled it to two simple parts. Default values all ended up being ok, and no special environment-variable-setting proved necessary (e.g. I unset all the NCCL flags I'd tried earlier).  Two things:\r\n\r\n1. running as instructed with the `torch.distributed.run` as before.  Although the default values were fine. e.g. \r\n```\r\npython -m torch.distributed.run --nnodes=1 --nproc_per_node=8 ./myscript.py --arg1=thing ...etc\r\n```\r\n\r\n2. Near the top of my Trainer init code (and before the Jukebox stuff), initialize :\r\n```Python\r\ndist.init_process_group(backend=\"nccl\")\r\n```\r\n no other parts were essential.   And I could either have the trainer strategy set to \"ddp\" or \"fsdp\" or nothing at all; made no difference. \r\n\r\n3. ALTHOUGH, one extra other thing that makes it go even faster: For some reason `OMP_NUM_THREADS` is not being set and so you see a warning message that it's getting set to 1 by default.  No need to leave that!   So my final, well-performing invocation looks like:\r\n```\r\nOMP_NUM_THREADS=12 python -m torch.distributed.run --nnodes=1 --nproc_per_node=8 ./myscript.py --arg1=thing ...etc\r\n```\r\none could also permanenty set `export OMP_NUM_THREADS=12` but I haven't bothered to do that yet. \r\n"
  },
  {
    "content": "I usually set seed_everything() at the beginning of my script but this does not always solves the problem.\r\n\r\nWhen I train models on cpu, it is determinstic; but when I switch to gpu, it becomes non-deterministic.\r\n\r\nWhen I am training simple model, like one-layer LSTM, it is deterministic both on cpu and gpu.\r\n\r\nBut when I train a more completed model like LSTM-FCN, it is deterministic on cpu but not on gpu\r\n\r\nCan I get any help on debugging?\r\n\r\nI got my LSTM-FCN model from here (https://github.com/timeseriesAI/tsai/blob/main/tsai/models/RNN_FCN.py) and the LSTM model I tested was simply a nn.LSTMmaybe setting `Trainer(deterministic=True)` might help?Wow, it worked. This is amazing. I thought seed_everything() has done everything pytorch lightning could do. Is there any documentation for the Trainer(deterministic=True) you mentioned? Want to take a look.",
    "meta": { "name": "training on gpu becomes non-deterministic" },
    "answer": "maybe setting `Trainer(deterministic=True)` might help?"
  },
  {
    "content": "Hi, I have a use case where I need to have a single validation dataloader or multiple validation dataloader based on a flag. My data module looks something like \r\n```python\r\nclass CodeXGlueDataModule(pl.LightningDataModule):\r\n    def __init__(self, args):\r\n        ...\r\n\r\n    def train_dataloader(self):\r\n        ...\r\n\r\n    def val_dataloader(self):\r\n        loader1 = ...\r\n\r\n        if flag:\r\n            loader2 = ...\r\n            loaders = [dataloader_ppl, dataloader_bleu]\r\n            return loaders\r\n        loaders = [dataloader_ppl, ]\r\n        return loaders\r\n```\r\nand my lightening model looks like\r\n```python\r\ndef validation_step(self, batch, batch_idx, dataloader_idx):\r\n    if dataloader_idx == 0:\r\n        ...\r\n        return {\"x\":x}\r\n\r\n    elif dataloader_idx == 1:\r\n        ...\r\n        return {\"y\":y}    \r\n```\r\n\r\nWhen I run the code with flag = True, it works fines but when I run it with flag = False, I get an error saying\r\n```\r\nvalidation_step() missing 1 required positional argument: 'dataloader_idx'\r\n```\r\nI understand that a single dataloader is not treated as a list and hence there is no dataloader_idx argument for the validation_step function but is there something I can do to make this work. \r\n\r\nThanks in Advance! @prateeky2806 Making it an optional keyword argument will avoid the error?\r\n```diff\r\n-def validation_step(self, batch, batch_idx, dataloader_idx):\r\n-    if dataloader_idx == 0:\r\n+def validation_step(self, batch, batch_idx, dataloader_idx=None):\r\n+    if dataloader_idx == 0 or dataloader_idx is None:\r\n        ...\r\n\r\n    elif dataloader_idx == 1:\r\n        ...\r\n```",
    "meta": { "name": "Conditionally have a single or multiple data Loaders." },
    "answer": "@prateeky2806 Making it an optional keyword argument will avoid the error?\r\n```diff\r\n-def validation_step(self, batch, batch_idx, dataloader_idx):\r\n-    if dataloader_idx == 0:\r\n+def validation_step(self, batch, batch_idx, dataloader_idx=None):\r\n+    if dataloader_idx == 0 or dataloader_idx is None:\r\n        ...\r\n\r\n    elif dataloader_idx == 1:\r\n        ...\r\n```"
  },
  {
    "content": "When using rich progress bar with shell command like _python train.py > file.log 2>&1_, the file.log don't save any progress bar output. But check rich library [documents](https://rich.readthedocs.io/en/latest/console.html#error-console), it says rich will write to sys.stdout by default. Anyone know how to do that?In fact, it does redirect to file.log and you can see them if you wait until all is over. I think maybe you can submit an issue and see if PL can keep flushing when running.",
    "meta": { "name": "How to redirect output of rich progress bar to file?" },
    "answer": "In fact, it does redirect to file.log and you can see them if you wait until all is over. I think maybe you can submit an issue and see if PL can keep flushing when running."
  },
  {
    "content": "Hello,\r\n\r\nI implemented MoCo in Pytorch lightning. I was surprised to see that my lightning version was slower than Pytorch's and I ran the profiler to check which function is slow. I can't share all my code but here are the relevant parts:\r\n\r\n``` python\r\nclass MoCoModel(LightningModule):\r\n    def __init__(\r\n        ...\r\n    ) -> None:\r\n        ...\r\n\r\n        self.register_buffer('queue', torch.randn(queue.feature_dim, queue.size))\r\n        self.queue = nn.functional.normalize(self.queue, dim=0)\r\n        self.register_buffer('queue_ptr', torch.zeros(1, dtype=torch.long))\r\n    \r\n    @torch.no_grad()\r\n    def _update_queue(self, x: Tensor) -> None:\r\n        x = self.concat_all_gather_without_backprop(x)\r\n\r\n        #batch_size = x.shape[0]\r\n        batch_size = self._get_batch_size(x)\r\n\r\n        # for simplicity\r\n        ptr = self._get_ptr()\r\n        #ptr = int(self.queue_ptr)\r\n\r\n        self._assert(batch_size)\r\n        #assert self.queue_size % batch_size == 0\r\n\r\n        # replace the keys at ptr (dequeue and enqueue)\r\n        x = self._transpose(x)\r\n        self._assign_in_queue(x, ptr, batch_size)\r\n        #self.queue[:, ptr: ptr + batch_size] = x.T\r\n\r\n        # move pointer\r\n        ptr = self._compute_ptr(ptr, batch_size)\r\n        self._assign_ptr(ptr)\r\n        #ptr =  (ptr + batch_size) % self.queue_size\r\n    \r\n    def _get_batch_size(self, x):\r\n        return x.shape[0]\r\n\r\n    def _get_ptr(self):\r\n        return int(self.queue_ptr)\r\n\r\n    def _assert(self, batch_size):\r\n        assert self.queue_size % batch_size == 0\r\n    \r\n    def _assign_ptr(self, ptr):\r\n        self.queue_ptr[0] = ptr\r\n    \r\n    def _compute_ptr(self, batch_size, ptr):\r\n        return (ptr + batch_size) % self.queue_size\r\n\r\n    def _transpose(self, x):\r\n        return x.T\r\n    \r\n    def _assign_in_queue(self, x, ptr, batch_size):\r\n        self.queue[:, ptr: ptr + batch_size] = x\r\n\r\n    def training_step(self, batch):\r\n        ...\r\n        self._update_queue(k)\r\n```\r\n\r\nHere is the output of running simple profiler:\r\n\r\n```\r\nAction                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n--------------------------------------------------------------------------------------------------------------------------------------\r\nTotal                              \t|  -              \t|_              \t|  53.595         \t|  100 %          \t|\r\n--------------------------------------------------------------------------------------------------------------------------------------\r\nrun_training_epoch                 \t|  45.224         \t|1              \t|  45.224         \t|  84.381         \t|\r\nrun_training_batch                 \t|  0.21673        \t|195            \t|  42.262         \t|  78.854         \t|\r\noptimizer_step_with_closure_0      \t|  0.20378        \t|195            \t|  39.738         \t|  74.145         \t|\r\ntraining_step_and_backward         \t|  0.19978        \t|195            \t|  38.957         \t|  72.688         \t|\r\nmodel_forward                      \t|  0.1909         \t|195            \t|  37.225         \t|  69.457         \t|\r\ntraining_step                      \t|  0.19077        \t|195            \t|  37.201         \t|  69.411         \t|\r\nbackward                           \t|  0.0083673      \t|195            \t|  1.6316         \t|  3.0443         \t|\r\non_train_batch_end                 \t|  0.0077772      \t|195            \t|  1.5166         \t|  2.8296         \t|\r\nget_train_batch                    \t|  0.0034326      \t|196            \t|  0.6728         \t|  1.2553         \t|\r\nfetch_next_train_batch             \t|  0.0034203      \t|196            \t|  0.67037        \t|  1.2508         \t|\r\nzero_grad                          \t|  0.00049274     \t|195            \t|  0.096084       \t|  0.17928        \t|\r\nconfigure_optimizers               \t|  0.093719       \t|1              \t|  0.093719       \t|  0.17486        \t|\r\ntraining_batch_to_device           \t|  0.00028381     \t|195            \t|  0.055342       \t|  0.10326        \t|\r\non_train_batch_start               \t|  0.00018134     \t|195            \t|  0.03536        \t|  0.065977       \t|\r\non_train_start                     \t|  0.033906       \t|1              \t|  0.033906       \t|  0.063264       \t|\r\non_pretrain_routine_start          \t|  0.006531       \t|1              \t|  0.006531       \t|  0.012186       \t|\r\non_batch_start                     \t|  3.062e-05      \t|195            \t|  0.0059708      \t|  0.011141       \t|\r\non_after_backward                  \t|  3.0163e-05     \t|195            \t|  0.0058817      \t|  0.010974       \t|\r\non_before_optimizer_step           \t|  2.989e-05      \t|195            \t|  0.0058285      \t|  0.010875       \t|\r\non_batch_end                       \t|  2.9087e-05     \t|195            \t|  0.005672       \t|  0.010583       \t|\r\non_before_zero_grad                \t|  2.8804e-05     \t|195            \t|  0.0056167      \t|  0.01048        \t|\r\non_before_backward                 \t|  2.6982e-05     \t|195            \t|  0.0052616      \t|  0.0098172      \t|\r\non_train_epoch_end                 \t|  0.0014064      \t|1              \t|  0.0014064      \t|  0.0026241      \t|\r\ntraining_step_end                  \t|  4.9198e-06     \t|195            \t|  0.00095937     \t|  0.00179        \t|\r\non_train_epoch_start               \t|  0.00025167     \t|1              \t|  0.00025167     \t|  0.00046957     \t|\r\non_train_end                       \t|  0.00017067     \t|1              \t|  0.00017067     \t|  0.00031844     \t|\r\non_before_accelerator_backend_setup\t|  6.968e-05      \t|1              \t|  6.968e-05      \t|  0.00013001     \t|\r\nsetup                              \t|  5.0209e-05     \t|1              \t|  5.0209e-05     \t|  9.3682e-05     \t|\r\nprepare_data                       \t|  4.4779e-05     \t|1              \t|  4.4779e-05     \t|  8.355e-05      \t|\r\non_fit_end                         \t|  3.892e-05      \t|1              \t|  3.892e-05      \t|  7.2618e-05     \t|\r\non_epoch_start                     \t|  3.332e-05      \t|1              \t|  3.332e-05      \t|  6.2169e-05     \t|\r\non_pretrain_routine_end            \t|  3.009e-05      \t|1              \t|  3.009e-05      \t|  5.6143e-05     \t|\r\non_epoch_end                       \t|  2.741e-05      \t|1              \t|  2.741e-05      \t|  5.1142e-05     \t|\r\non_configure_sharded_model         \t|  2.556e-05      \t|1              \t|  2.556e-05      \t|  4.7691e-05     \t|\r\non_fit_start                       \t|  2.0869e-05     \t|1              \t|  2.0869e-05     \t|  3.8938e-05     \t|\r\nteardown                           \t|  1.9379e-05     \t|1              \t|  1.9379e-05     \t|  3.6158e-05     \t|\r\nconfigure_sharded_model            \t|  6.5197e-06     \t|1              \t|  6.5197e-06     \t|  1.2165e-05     \t|\r\nconfigure_callbacks                \t|  5.16e-06       \t|1              \t|  5.16e-06       \t|  9.6277e-06     \t|\r\non_train_dataloader                \t|  4.2003e-06     \t|1              \t|  4.2003e-06     \t|  7.837e-06      \t|\r\n```\r\n\r\nAs we can see a large time is spent in `training_step` and here is the output of advanced profiler for this function:\r\n```\r\nProfile stats for: training_step rank: 0\r\n         1065072 function calls (862519 primitive calls) in 37.086 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n      195    0.001    0.000   37.082    0.190 accelerator.py:210(training_step)\r\n      195    0.000    0.000   37.079    0.190 ddp.py:438(training_step)\r\n31980/195    0.053    0.000   37.079    0.190 module.py:1096(_call_impl)\r\n      195    0.015    0.000   37.078    0.190 distributed.py:852(forward)\r\n      195    0.002    0.000   36.629    0.188 base.py:76(forward)\r\n      195    0.009    0.000   36.624    0.188 moco.py:201(training_step)\r\n 1170/390    0.006    0.000   34.429    0.088 grad_mode.py:25(decorate_context)\r\n      195    0.002    0.000   32.216    0.165 moco.py:83(_update_queue)\r\n      195   32.171    0.165   32.171    0.165 moco.py:109(_get_ptr)\r\n      390    0.000    0.000    3.942    0.010 resnet.py:268(forward)\r\n     ...\r\n      195    0.008    0.000    0.008    0.000 moco.py:124(_assign_in_queue)\r\n      195    0.005    0.000    0.006    0.000 moco.py:115(_assign_ptr)\r\n      195    0.002    0.000    0.002    0.000 moco.py:121(_transpose)\r\n      195    0.001    0.000    0.001    0.000 gather.py:44(concat_all_gather_without_backprop)\r\n      195    0.000    0.000    0.000    0.000 moco.py:106(_get_batch_size)\r\n     ...\r\n```\r\n\r\nThe function `_update_queue` is very long and the function taking the most time is `_get_ptr` which should be really fast in comparison with forwards or computation of MoCo loss. I watched [lightning bolts implementation](https://github.com/PyTorchLightning/Lightning-Bolts/blob/master/pl_bolts/models/self_supervised/moco/moco2_module.py#L38-L328) that uses the same kind of operations so I don't really understand why it is this slow.\r\n\r\nI tested with DDP and SingleDevice strategy that resulted in the same kind of slow down on a SLURM cluster environment.\r\nFixed it, lightning is now as fast as my previous implementation, the problem was elsewhere but I didn't detect it using the profiler because of the asynchronous computation from GPUs which were not synchronized during profiling.",
    "meta": { "name": "Access a registered buffer is very slow" },
    "answer": "Fixed it, lightning is now as fast as my previous implementation, the problem was elsewhere but I didn't detect it using the profiler because of the asynchronous computation from GPUs which were not synchronized during profiling."
  },
  {
    "content": "I have 8 gpus and during validation loop, I would like to only inference using one single GPU. I tried looking up documentation but I don't see any examples. Thanks!\r\n\r\nEnvironment:\r\n- Single machine\r\n- 8 GPUs\r\n- Pytorch Lightning 1.6.0\r\n- Strategy: DDPthat is not possible.\r\n\r\nbut if you are calling `.validate` then you can configure your Trainer with devices=1\r\n\r\n```py\r\nTrainer(devices=1)\r\ntrainer.validate(...)\r\n```I actually found a hack where by I extend the same list of files by num_of_gpus \r\n\r\nORIGINAL FILES LIST = [1.JPG, 2.JPG,3.JPG,4.JPG]\r\nNUM_OF_GPUS = 2\r\n\r\nFile list that GPU_0 receives = [1.JPG, 3.JPG]\r\nFile list that GPU_1 receives = [2.JPG, 4.JPG]\r\n\r\nAfter the hack:\r\nNew FILES LIST = [1.JPG, 1.JPG,2.JPG,2.JPG,3.JPG,3.JPG,4.JPG,4.JPG)\r\nNUM_OF_GPUS = 2\r\n\r\nFile list that GPU_0 receives = [1.JPG, 2.JPG,3.JPG,4.JPG]\r\nFile list that GPU_1 receives = [1.JPG, 2.JPG,3.JPG,4.JPG]\r\n\r\nso the distributed sampler will distribute each replica exactly once to each gpu",
    "meta": { "name": "How to carry out validation loop on one single GPU" },
    "answer": "that is not possible.\r\n\r\nbut if you are calling `.validate` then you can configure your Trainer with devices=1\r\n\r\n```py\r\nTrainer(devices=1)\r\ntrainer.validate(...)\r\n```"
  },
  {
    "content": "Hi Everyone, \r\n\r\nI'm trying to use the [torchdyn](https://github.com/DiffEqML/) library to solve a problem of mine, code is given here,  when I try to run this code I'm getting an error which says \r\n\r\n## Error:\r\n```\r\n\r\nRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already \r\n\r\nbeen freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify \r\n\r\nretain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after \r\n\r\ncalling backward.\r\n\r\n\r\n```\r\n\r\n\r\n## Code \r\n```\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport torch.nn as nn\r\nimport copy\r\n\r\nfrom torch.autograd import grad\r\n\r\nimport torch\r\n\r\n\r\ndef set_initial_conditions(n_agents):\r\n    if n_agents == 12:\r\n        #px,py,qx,qy\r\n        x0 = torch.tensor([[0], [0.5], [-3], [5],\r\n                           [0], [0.5], [-3], [3],\r\n                           [0], [0.5], [-3], [1],\r\n                           [0], [-0.5], [-3], [-1],\r\n                           [0], [-0.5], [-3], [-3],\r\n                           [0], [-0.5], [-3], [-5],\r\n                           # second column\r\n                           [-0], [0.5], [3], [5],\r\n                           [-0], [0.5], [3], [3],\r\n                           [-0], [0.5], [3], [1],\r\n                           [0], [-0.5], [3], [-1],\r\n                           [0], [-0.5], [3], [-3],\r\n                           [0], [-0.5], [3], [-5],\r\n                           ])\r\n        xbar = torch.tensor([[0], [0], [3], [-5],\r\n                             [0], [0], [3], [-3],\r\n                             [0], [0], [3], [-1],\r\n                             [0], [0], [3], [1],\r\n                             [0], [0], [3], [3],\r\n                             [0], [0], [3], [5],\r\n                             # second column\r\n                             [0], [0], [-3], [-5],\r\n                             [0], [0], [-3], [-3],\r\n                             [0], [0], [-3], [-1],\r\n                             [0], [0], [-3], [1],\r\n                             [0], [0], [-3], [3],\r\n                             [0], [0], [-3], [5.0],\r\n                             ])\r\n    else:\r\n        x0 = (torch.rand(4*n_agents, 1)-0.5)*10\r\n        xbar = (torch.rand(4*n_agents, 1)-0.5)*10\r\n    return x0, xbar\r\n\r\n# X  = (J-R)*dV/dx + Fy + Gu\r\n# Y = G*dV/dx\r\n# forward = return (J-R)*dV/dx\r\n\r\nclass SystemEnv(nn.Module):\r\n    def __init__(self, V, K, n_agents=1, xbar=None, ctls=None, batch_size=1, **kwargs):\r\n        \"\"\" Initialize the environment. Here we represent the system.\r\n        \"\"\"\r\n        super().__init__()\r\n        self.K = K\r\n        self.V = V\r\n        self.k = torch.tensor(1.0)\r\n        self.b = torch.tensor(0.2)\r\n        self.m = torch.tensor(1.0)\r\n        J = torch.tensor([[0, 0, -1, 0],\r\n                          [0, 0, 0, -1],\r\n                          [1., 0, 0, 0],\r\n                          [0, 1., 0, 0]])\r\n        R = torch.tensor([[self.b, 0, 0, 0],\r\n                          [0, self.b, 0, 0],\r\n                          [0, 0, 0, 0],\r\n                          [0, 0, 0, 0]])\r\n        #number of agents\r\n        self.n_agents = n_agents\r\n        #dimension of state space q,p = (2,2)\r\n        self.ni = 4\r\n        self.n = self.ni * n_agents\r\n        if ctls is None:\r\n            ctls = torch.ones(1, n_agents)\r\n            # n_of_inputs x n_masses\r\n        self.interconnection = ctls\r\n        self.J = torch.zeros((self.n, self.n))\r\n        self.R = torch.zeros((self.n, self.n))\r\n        for i in range(0, n_agents):\r\n            self.J[self.ni * i:self.ni * (i + 1), self.ni * i:self.ni * (i + 1)] = J\r\n            self.R[self.ni * i:self.ni * (i + 1), self.ni * i:self.ni * (i + 1)] = R\r\n        if xbar is None:\r\n            xbar = torch.zeros(self.n, 1)\r\n        self.xbar = xbar\r\n        self.B = torch.tensor([[1.0, 0], [0, 1.0]])\r\n        self.batch_size = batch_size\r\n\r\n    def g(self, t, x):\r\n        # g = torch.zeros((self.n, 2*int(self.interconnection.sum())))\r\n        # idx = 0\r\n        # for i, j in self.interconnection.nonzero():\r\n        #     g[(4*j), idx] = 1\r\n        #     g[(4*j)+1, idx+1] = 1\r\n        #     idx += 2\r\n        # return g\r\n        g_agent = torch.tensor([[1.0, 0], [0, 1.0], [0, 0], [0, 0]])\r\n        self.g_agent = copy.deepcopy(g_agent)\r\n        g = torch.zeros(0, 0)\r\n        for i in range(self.n_agents):\r\n            g = torch.block_diag(g, g_agent)\r\n        return g\r\n\r\n    def H(self, t, x):\r\n        delta_x = x - self.xbar\r\n        Q_agent = torch.diag(torch.tensor([1 / self.m, 1 / self.m, self.k, self.k]))\r\n        Q = torch.zeros((self.n, self.n))\r\n        for i in range(self.n_agents):\r\n            Q[self.ni * i:self.ni * (i + 1), self.ni * i:self.ni * (i + 1)] = Q_agent\r\n        R_agent = torch.diag(torch.tensor([1 / self.m, 1 / self.m, self.k, self.k]))\r\n        return 0.5 * F.linear(F.linear(delta_x.T, Q), delta_x.T)\r\n\r\n    def gradH(self, t, x):\r\n        x = x.requires_grad_(True)\r\n        return torch.autograd.grad(self.H(t, x), x, allow_unused=False, create_graph=True)[0]\r\n\r\n    def f(self, t, x):\r\n        dHdx = self.gradH(t, x)\r\n        return F.linear(dHdx.T, self.J - self.R)\r\n\r\n    def _dynamics(self, t, x, u):\r\n        # p = torch.cat(x[0::4], x[1::4], 0)\r\n        q = torch.stack((x[:, 2::4], x[:, 3::4]), dim=2).view(x.shape[0],self.n_agents*2).to(0)\r\n        p = torch.stack((x[:, 0::4], x[:, 1::4]), dim=2).view(x.shape[0], self.n_agents * 2).to(0)\r\n        # [p;q] = [J-R]*delV+[B,0]*u\r\n        delVq = self._energy_shaping(q)\r\n        # delVp = p from formulation\r\n        r1 = torch.stack((delVq, p), dim=2).to(0)\r\n        r1 = r1.view(x.shape[0], self.n_agents*4)\r\n        JR = (self.J - self.R).to(0)\r\n        # input, matrix\r\n        result  = torch.zeros(x.shape[0], self.n_agents*4).to(0)\r\n        u = u.view(x.shape[0], self.n_agents * 2).to(0)\r\n        for i in range(r1.shape[0]):\r\n            par1 = torch.matmul(JR, r1[i, :])\r\n            g = self.g(t, x).to(0)\r\n            # u\u03b8 = \u2212BInv*\u2207qV(q) \u2212 K\u2217(t, q, p)B*qdot\r\n            par2 = F.linear(g, u[i,:])\r\n            result[i,:] = torch.add(par1, par2).to(0)\r\n        return result\r\n\r\n    def _energy_shaping(self,q):\r\n        # dVdx = grad(self.V(q).sum(), q, create_graph=True)[0]\r\n        dVdx = grad(self.V(q).sum(), q, create_graph=True, retain_graph=True)[0]\r\n        return -1*dVdx\r\n\r\n    # def _energy(self,t,x):\r\n    #     Q_agent = torch.diag(torch.tensor([1 / 2*self.m, self.k]))\r\n    #     temp_x = torch.zeros(self.n_agents*2)\r\n    #     temp_p = torch.zeros(self.n_agents*2)\r\n    #     x_temp = x\r\n    #     x_temp = x.view(self.n_agents, 4)\r\n    #     for i in range(self.n_agents):\r\n    #         for j in range(len(x[i])):\r\n    #             temp_x[i]=(0.5*self.m)*()\r\n    #             temp_p[i] = (0.5 * self.m)\r\n    #\r\n    #     F.linear(Q_agent, torch.cat(torch.cdist(x[:,2:...],torch.zeros_like(x[:,2:...])),torch.cdist(x[...,:2]-xbar)))\r\n\r\n    def _damping_injection(self,x):\r\n        # x = [pdot, qdot]\r\n        q = torch.stack((x[:, 2::4], x[:, 3::4]), dim=2).view(x.shape[0],self.n_agents*2).requires_grad_(True)\r\n        Kmat = torch.diag(self.K(x.to(0)).ravel())\r\n        return -1*F.linear(Kmat, q.view(1, x.shape[0]*self.n_agents*2).to(0))\r\n\r\n    def forward(self, t, x):\r\n        # x = [p,q]\r\n        print(\"in forward\")\r\n        x = x.requires_grad_(True)\r\n        #batch_size, n_agents*4, n_agents = x\r\n        q = torch.stack((x[:, 2::4], x[:, 3::4]), dim=2).view(x.shape[0],self.n_agents*2).requires_grad_(True).to(0)\r\n        u1 = self._energy_shaping(q)\r\n        u1 = u1.view(x.shape[0]*self.n_agents*2, 1 )\r\n        u2 = self._damping_injection(x)\r\n        u = u1+u2\r\n        return  (self._dynamics(t,x,u),u)\r\n        # return self.f(t, x).T\r\n\r\nclass AugmentedDynamics(nn.Module):\r\n    # \"augmented\" vector field to take into account integral loss functions\r\n    def __init__(self, f, int_loss):\r\n        super().__init__()\r\n        self.f = f\r\n        self.int_loss = int_loss\r\n        self.nfe = 0.\r\n\r\n    def forward(self, t, x):\r\n        self.nfe += 1\r\n        x = x[:,:f.n_agents*4]\r\n        (dxdt,u) = self.f.forward(t, x)\r\n        dldt =   self.int_loss.int_loss(x, u, self.f)\r\n        return torch.cat([dxdt, dldt], 1).cpu()\r\n\r\n\r\n\r\nclass ControlEffort(nn.Module):\r\n    # control effort integral cost\r\n    def __init__(self, f, x0, xbar, dt):\r\n        super().__init__()\r\n        self.f = f\r\n\r\n        self.x = torch.cat((x0[0::4], x0[1::4]), dim=1)\r\n        self.x = self.x.repeat(f.batch_size, 1)\r\n        self.x = self.x.reshape(f.batch_size,f.n_agents*2)\r\n\r\n        self.xbar = torch.cat((xbar[0::4], xbar[1::4]), dim=1)\r\n        self.xbar = self.xbar.repeat(f.batch_size, 1)\r\n        self.xbar = self.xbar.reshape(batch_size, f.n_agents*2)\r\n\r\n        self.dt = dt\r\n\r\n    def forward(self, t, x):\r\n        with torch.set_grad_enabled(True):\r\n            q = torch.cat((x[2::4],x[3::4]),0).requires_grad_(True)\r\n            # q = torch.transpose(q, 0, 1)\r\n            u1 = torch.transpose(self.f._energy_shaping(q), 0, 1)\r\n            u2 = self.f._damping_injection(x).to(0)\r\n            u = u1+u2\r\n        return u\r\n\r\n    def int_loss(self, x, u, clsys):\r\n        x = x.reshape(x.shape[0],self.f.n_agents,2,2)\r\n        vel = torch.index_select(x, 2, torch.tensor([1]))\r\n        vel = vel.reshape(x.shape[0],self.f.n_agents*2)\r\n        self.x = self.x.cpu()+torch.mul(vel,self.dt)\r\n        self.x = self.x.to(0)\r\n        self.xbar = self.xbar.to(0)\r\n        self.u = u.reshape(self.f.batch_size,self.f.n_agents*2)\r\n        self.clsys = clsys\r\n        lx = self.f_loss_states().reshape(self.f.batch_size,1).to(0)\r\n        lu = self.f_loss_u().reshape(self.f.batch_size,1).to(0)\r\n        lca = self.f_loss_ca()\r\n        loss = lx+lu+lca\r\n        return loss.to(0)\r\n\r\n    def f_loss_states(self, test=False):\r\n        # clsys = SystemEnv\r\n        loss_function = nn.MSELoss(reduction='none')\r\n        xbar = self.clsys.xbar\r\n        # steps = t.shape[0]\r\n        if test:\r\n            gamma = 1\r\n        else:\r\n            gamma = 0.95\r\n        loss = loss_function(self.x, self.xbar)\r\n        loss = loss.view(self.f.batch_size, 2*self.f.n_agents)\r\n        loss = loss.sum(dim=1)\r\n        return loss\r\n\r\n    def f_loss_u(self):\r\n        loss_u = ((self.u*self.clsys.b) ** 2).sum(dim=1)\r\n        return loss_u\r\n\r\n\r\n    def f_loss_ca(self, min_dist=0.5):\r\n        steps = self.x.shape[0]\r\n        min_sec_dist = 1.4 * min_dist\r\n        # for i in range(steps):\r\n        #     for j in range(i+1, steps):\r\n        #         dist = torch.norm(self.x[i, :] - self.x[j, :])\r\n        #         if dist < min_sec_dist:\r\n        #             return torch.tensor(1e10)\r\n        # return torch.tensor(0)\r\n        loss_ca_ = torch.zeros(self.f.batch_size,1).to(0)\r\n        for i in range(self.x.shape[0]):\r\n            x = self.x[i,:].view(self.f.n_agents*2,1).to(0)\r\n            clsys = self.clsys\r\n            # collision avoidance:\r\n            # deltax = x[:, 2::4].repeat(1, 1, clsys.n // 4) - x[:, 2::4].transpose(1, 2).repeat(1, clsys.n // 4, 1)\r\n            deltax = x[0::2].repeat(1, clsys.n // 4) - x[0::2].transpose(0, 1).repeat( clsys.n // 4, 1)\r\n            # deltay = x[:, 3::4].repeat(1, 1, clsys.n // 4) - x[:, 3::4].transpose(1, 2).repeat(1, clsys.n // 4, 1)\r\n            deltay = x[1::2].repeat(1, clsys.n // 4) - x[1::2].transpose(0, 1).repeat(clsys.n // 4, 1)\r\n            distance_sq = deltax ** 2 + deltay ** 2\r\n            mask = torch.logical_not(torch.eye(clsys.n // 4)).unsqueeze(0).repeat(steps, 1, 1)\r\n            mask = mask.to(0)\r\n            loss_ca_[i,:] = (1 / (distance_sq + 1e-3) * (distance_sq.detach() < (min_sec_dist ** 2)) * mask).sum() / 2\r\n        return loss_ca_\r\n\r\n\r\nimport pytorch_lightning as pl\r\nimport torch.utils.data as data\r\n\r\ndef weighted_log_likelihood_loss(x, target, weight):\r\n    # weighted negative log likelihood loss\r\n    log_prob = target.log_prob(x)\r\n    weighted_log_p = weight * log_prob\r\n    return -torch.mean(weighted_log_p.sum(1))\r\n\r\nclass EnergyShapingLearner(pl.LightningModule):\r\n    def __init__(self, model: nn.Module, prior_dist, target_dist, t_span, sensitivity='autograd', n_agents=1):\r\n        super().__init__()\r\n        self.model = model\r\n        self.prior, self.target = prior_dist, target_dist\r\n        self.t_span = t_span\r\n        self.batch_size = batch_size\r\n        self.lr = 5e-3\r\n        self.n_agents = n_agents\r\n        self.weight = torch.ones(n_agents * 4).reshape(1, n_agents * 4)\r\n\r\n    def forward(self, x):\r\n        return self.model.odeint(x, self.t_span)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # sample a batch of initial conditions\r\n        # x0 = self.prior.sample((self.batch_size,))\r\n        n_agents = self.n_agents\r\n        x0 = torch.rand((self.batch_size,n_agents*4))\r\n        # x0, _ = set_initial_conditions(n_agents)\r\n        # Integrate the model\r\n        x0 = torch.cat([x0, torch.zeros(self.batch_size, 1)], -1).to(x0)\r\n        xs, xTl = self(x0)\r\n        xT, l = xTl[-1, :, :2], xTl[-1, :, -1:]\r\n\r\n        # Compute loss\r\n        # terminal_loss = weighted_log_likelihood_loss(xT, self.target, self.weight.to(xT))\r\n        integral_loss = torch.mean(l)\r\n        loss = 0 + 0.01 * integral_loss\r\n        return {'loss': loss.cpu()}\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\r\n        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.999)\r\n        return [optimizer], [scheduler]\r\n\r\n    def train_dataloader(self):\r\n        dummy_trainloader = data.DataLoader(\r\n            data.TensorDataset(torch.Tensor(1, 1), torch.Tensor(1, 1)),\r\n            batch_size=1)\r\n        return dummy_trainloader\r\n\r\n# # # # # # # # Parameters # # # # # # # #\r\nn_agents = 2  # agents are not interconnected (even when having a controller). Each of them acts independently\r\nt_end = 5\r\nsteps = 101\r\nmin_dist = 0.5  # min distance for collision avoidance\r\n# (px, py, qx, qy) - for each agent\r\nhdim = 64\r\nV = nn.Sequential(\r\n    nn.Linear(n_agents*2, hdim),\r\n    nn.Softplus(),\r\n    nn.Linear(hdim, hdim),\r\n    nn.Tanh(),\r\n    nn.Linear(hdim, 1))\r\nK = nn.Sequential(\r\n    nn.Linear(n_agents*4, hdim),\r\n    nn.Softplus(),\r\n    nn.Linear(hdim, (n_agents*2)),\r\n    nn.Softplus())\r\n\r\n\r\nfrom torch.distributions import Uniform, Normal\r\n\r\n\r\ndef prior_dist(q_min, q_max, p_min, p_max, device='cpu'):\r\n    # uniform \"prior\" distribution of initial conditions x(0)=[q(0),p(0)]\r\n    lb = torch.Tensor([q_min, p_min]).to(device)\r\n    ub = torch.Tensor([q_max, p_max]).to(device)\r\n    return Uniform(lb, ub)\r\n\r\n\r\ndef target_dist(mu, sigma, device='cpu'):\r\n    # normal target distribution of terminal states x(T)\r\n    mu, sigma = torch.Tensor(mu).reshape(1, 2).to(device), torch.Tensor(sigma).reshape(1, 2).to(device)\r\n    return Normal(mu, torch.sqrt(sigma))\r\n\r\ndef weighted_log_likelihood_loss(x, target, weight):\r\n    # weighted negative log likelihood loss\r\n    log_prob = target.log_prob(x)\r\n    weighted_log_p = weight * log_prob\r\n    return -torch.mean(weighted_log_p.sum(1))\r\n\r\nfrom torchdyn.models import ODEProblem\r\n\r\n# choose solver and sensitivity method\r\nsolver = 'rk4'\r\nsensitivity = 'autograd'\r\n\r\n# init to zero par.s of the final layer\r\n# for p in V[-1].parameters(): torch.nn.init.zeros_(p)\r\n# for p in K[-2].parameters(): torch.nn.init.zeros_(p)\r\n\r\n# define controlled system dynamics\r\nx0, xbar = set_initial_conditions(n_agents)\r\nbatch_size = 4\r\n\r\nf = SystemEnv(V.to(0), K.to(0), n_agents=2, xbar=xbar, batch_size = batch_size)\r\n\r\nt_span = torch.linspace(0, 3, 30)\r\ndt = t_span[1]-t_span[0]\r\naug_f = AugmentedDynamics(f, ControlEffort(f,x0,xbar,dt))\r\n# define time horizon\r\n\r\n\r\nprob = ODEProblem(aug_f, sensitivity=sensitivity, solver=solver)\r\n\r\n# train (it can be very slow on CPU)\r\n# (don't be scared if the loss starts very high)\r\nprior = prior_dist(-1, 1, -1, 1) # Uniform \"prior\" distribution of initial conditions x(0)\r\ntarget = target_dist([0, 0], [.001, .001]) # Normal target distribution for x(T)\r\nlearn = EnergyShapingLearner(prob, prior, target, t_span, batch_size, n_agents=n_agents)\r\n# trainer = pl.Trainer(accelerator=\"gpu\", devices=1, strategy=\"ddp\",max_epochs=650).fit(learn)\r\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=1, strategy=\"ddp\",max_epochs=650)\r\ntrainer.fit(learn)\r\n\r\n```\r\n\r\nThis can be resolved by using\r\n\r\n`return Variable(dyn).requires_grad_(True)` ,` x = Variable(x.data, requires_grad=True)`",
    "meta": {
      "name": "RuntimeError: Trying to backward through the graph a second time"
    },
    "answer": "This can be resolved by using\r\n\r\n`return Variable(dyn).requires_grad_(True)` ,` x = Variable(x.data, requires_grad=True)`"
  },
  {
    "content": "i want use iter to control multi_scaleok i know the answerJust for anyone looking at this Discussion, here's the list of relevant properties:\r\n- https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#global-step\r\n- https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#current-epoch",
    "meta": { "name": "How can i know the current iter?" },
    "answer": "Just for anyone looking at this Discussion, here's the list of relevant properties:\r\n- https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#global-step\r\n- https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#current-epoch"
  },
  {
    "content": "Hi everyone, I am new to PyTorch lightening and I am currently trying to implement a continual learning model in PyTorch lightening. \r\n\r\nI have multiple data loaders for different tasks and I want to train on all of these data loaders. After training on task1 with dataloader1 I want to update the parameters of the model which are going to be trained for task two. To do this, I have an attribute named current_task in my dataloader which decides the dataset from which the samples are generated for the current task. My datamodule looks something like this.\r\n```python\r\n\r\nclass RandSplitCIFAR100DataModule(LightningDataModule):\r\n    def __init__(self):\r\n        .....\r\n\r\n    def setup(self, stage: Optional[str] = None):\r\n    \r\n        # load datasets only if they're not loaded already\r\n        if not self.data_train and not self.data_val and not self.data_test:\r\n            self.data_train = datasets.CIFAR100(self.hparams.data_dir, train=True, transform=self.train_transforms)\r\n            self.data_val = datasets.CIFAR100(self.hparams.data_dir, train=False, transform=self.val_transforms)\r\n        \r\n        np.random.seed(self.hparams.seed)\r\n        perm = np.random.permutation(self.num_classes)\r\n        print(perm)\r\n\r\n        splits = [\r\n            (self.partition_datasetv4(self.data_train, perm[5 * i:5 * (i+1)]),\r\n            self.partition_datasetv4(self.data_val, perm[5 * i:5 * (i+1)]),)\r\n            for i in range(self.hparams.num_tasks)\r\n        ]\r\n\r\n        kwargs = {\"num_workers\": self.hparams.workers, \"pin_memory\": self.hparams.pin_memory}\r\n        self.loaders = [\r\n            (DataLoader(x[0], batch_size=self.hparams.batch_size, shuffle=True, **kwargs),\r\n            DataLoader(x[1], batch_size=self.hparams.test_batch_size, shuffle=False, **kwargs),)\r\n            for x in splits\r\n        ]\r\n\r\n    def update_task(self, i):\r\n        self.current_task = i\r\n        \r\n    def train_dataloader(self):\r\n        return self.loader[self.current_task][0]\r\n\r\n    def val_dataloader(self):\r\n        return self.loader[self.current_task][1]\r\n```\r\n\r\nNow I want to have a training loop that does something like this. \r\n```python\r\n\r\nfor task in range(num_tasks):\r\n    self.dataloder.update_task(task)\r\n\r\n    for n, p in model.named_parameters():\r\n        # change parameters to update\r\n    for epoch in range(max_epochs):\r\n        for batch in dataloader:\r\n            ....\r\n\r\n```\r\n\r\nI am currently not able to figure out how to go about this, I feel confident that lightening should be able to handle such cases but I am just not sure how to go about this. \r\n\r\nAny help is greatly appreciated!\r\nPrateekwell, there are multiple ways:\r\n\r\n1. if your max_epochs is consistent across all the tasks:\r\n```py\r\n\r\nclass LitModel(LightningModule):\r\n    def on_train_epoch_start(self):\r\n        if current_epoch == 0 or (current_epoch + 1) % self.trainer.reload_dataloaders_every_n_epochs == 0:\r\n            # update model parameters\r\n\r\n\r\nmax_epochs_n_tasks = max_epochs * n_tasks\r\ntrainer = Trainer(max_epochs=max_epochs_n_tasks, reload_dataloaders_every_n_epochs=max_epochs)\r\nmodel = LitModel()\r\n\r\n# inject the update task counter logic inside datamodule\r\ndm = RandSplitCIFAR100DataModule(...)\r\ntrainer.fit(model, datamodule=dm)\r\n```\r\n2. create an explicit loop\r\n\r\n```py\r\ndef init_trainer(...):\r\n    trainer = Trainer(max_epochs=max_epochs, ...)\r\n    return trainer\r\n    \r\ndatamodule = ...\r\nmodel = ...\r\nfor task in range(num_tasks):\r\n    # update params\r\n    datamodule.update_task(task)\r\n    trainer = init_trainer(...)\r\n    trainer.fit(model, datamodule=dm)\r\n```\r\n\r\nAlthough I'd suggest (1), even if your max_epochs differs for each task, it can easily be extended to support that too.",
    "meta": { "name": "Iterating over task for Continual Learning." },
    "answer": "well, there are multiple ways:\r\n\r\n1. if your max_epochs is consistent across all the tasks:\r\n```py\r\n\r\nclass LitModel(LightningModule):\r\n    def on_train_epoch_start(self):\r\n        if current_epoch == 0 or (current_epoch + 1) % self.trainer.reload_dataloaders_every_n_epochs == 0:\r\n            # update model parameters\r\n\r\n\r\nmax_epochs_n_tasks = max_epochs * n_tasks\r\ntrainer = Trainer(max_epochs=max_epochs_n_tasks, reload_dataloaders_every_n_epochs=max_epochs)\r\nmodel = LitModel()\r\n\r\n# inject the update task counter logic inside datamodule\r\ndm = RandSplitCIFAR100DataModule(...)\r\ntrainer.fit(model, datamodule=dm)\r\n```\r\n2. create an explicit loop\r\n\r\n```py\r\ndef init_trainer(...):\r\n    trainer = Trainer(max_epochs=max_epochs, ...)\r\n    return trainer\r\n    \r\ndatamodule = ...\r\nmodel = ...\r\nfor task in range(num_tasks):\r\n    # update params\r\n    datamodule.update_task(task)\r\n    trainer = init_trainer(...)\r\n    trainer.fit(model, datamodule=dm)\r\n```\r\n\r\nAlthough I'd suggest (1), even if your max_epochs differs for each task, it can easily be extended to support that too."
  },
  {
    "content": "Hi, There's a little snippet from the [WandB Logger Docs](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.loggers.WandbLogger.html?highlight=wandblogger) under \"Add other config parameters:\"\r\n\r\n```Python\r\nimport pytorch_lightning as pl\r\nwandb_logger = pl.loggers.WandbLogger(project='my_project')\r\nwandb_logger.experiment.config.update({\"my_key\": \"my_value\"})\r\n\r\n# and then later I run...\r\ntrainer = pl.Trainer(gpus=args.num_gpus,...)\r\n```\r\n\r\nAnd when I use this in my full training code and only run the trainer on 1 GPU, then I have no problems.  It's fine. \r\n\r\nBut if I try to run on more than one GPU, then that config.update line above yields an error:\r\n\r\n```\r\n    wandb_logger.experiment.config.update({\"my_key\": \"my_value\"})\r\nAttributeError: 'function' object has no attribute 'update'\r\n``` \r\n\r\nWhat's going on here, and how do I fix it?   Thanks. Update. Ok so according to [this issue](https://github.com/PyTorchLightning/pytorch-lightning/issues/11380), the higher GPUs don't get \"real\" experiments?  \r\n\r\nSo my stupid hack which seems to work is just to check that the attribute exists! \ud83d\udcaf \r\n\r\n```Python\r\nif hasattr(wandb_logger.experiment.config, 'update'):\r\n    wandb_logger.experiment.config.update({\"my_key\": \"my_value\"})\r\n```",
    "meta": {
      "name": "wandb_logger.experiment.config has no attribute 'update' for GPUs > 1"
    },
    "answer": "Update. Ok so according to [this issue](https://github.com/PyTorchLightning/pytorch-lightning/issues/11380), the higher GPUs don't get \"real\" experiments?  \r\n\r\nSo my stupid hack which seems to work is just to check that the attribute exists! \ud83d\udcaf \r\n\r\n```Python\r\nif hasattr(wandb_logger.experiment.config, 'update'):\r\n    wandb_logger.experiment.config.update({\"my_key\": \"my_value\"})\r\n```"
  },
  {
    "content": "Hi everyone! I am trying to import auto_move_data from pytorch_lightning.core.decorators, but I get an import error:\r\n\r\n```\r\nImportError                               Traceback (most recent call last)\r\n[<ipython-input-3-22381b260381>](https://localhost:8080/#) in <module>()\r\n     26 from pytorch_lightning import LightningDataModule\r\n     27 from pytorch_lightning import LightningModule\r\n---> 28 from pytorch_lightning.core.decorators import auto_move_data\r\n     29 from pytorch_lightning import Trainer\r\n\r\nImportError: cannot import name 'auto_move_data' from 'pytorch_lightning.core.decorators' (/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/decorators.py)\r\n```\r\n\r\nDoes anybody know how to solve this problem?it's no longer present in the package.",
    "meta": { "name": "Cannot import auto_move_data decorator in Colab" },
    "answer": "it's no longer present in the package."
  },
  {
    "content": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nHow to accumulate metrics for multiple validation dataloaders separately? Currently the metrics are accumulated for all dataloaders simultaneously. \r\n#### Code\r\nThe validation step accepts `dataset_idx` parameter when running validation with multiple dataloaders.\r\n```python\r\ndef validation_step(self, batch, batch_idx, dataset_idx: Optional[int] = None):\r\n```\r\nHowever I'm not sure how to update the metrics separately for each dataloader. Would I have to create separate metrics, one for dataset A and second for B? Or maybe my metric could accept the `dataset_idx` parameter to know for which ds it should log given output.\r\n\r\nThis however wouldn't work with pl factory metrics like average precision, since they are dataset agnostic?\r\n```python\r\ndef update(self, preds: torch.Tensor, target: torch.Tensor):\r\n```\r\nNot sure how to approach this.\r\n\r\nYou would have to create seperate metrics per validation dataloader (similar to how you need seperate metrics for train/val/test). Something like this could maybe work for you\r\n``` python\r\ndef __init__(self, ...)\r\n    ...\r\n    self.val_metrics = nn.ModuleList([pl.metrics.Accuracy() for _ in range(n_val_dataloaders)])\r\n\r\ndef validation_step(self, batch, batch_idx, dataset_idx):\r\n    ...\r\n    self.val_metrics[dataset_idx].update(preds, target)\r\n```Maybe it would be a good idea to add it to the documentation or as an example?So there already is this note in the documentation:\r\n![note](https://user-images.githubusercontent.com/24896311/106356898-1bf49e80-6303-11eb-8929-f695208c8653.PNG)\r\n@potipot do you think it is enough to just extend this note to also include multi-val/multi-test case?I was trying to find some information about this in the docs, but phrases \"multiple dataloaders\" or \"metrics for multiple dataloaders\" didn't get me what I was looking for. \r\nMaybe it would be beneficial to add your response with some extension like:\r\n\r\n> NOTE: if you want to separately collect metrics for multiple dataloaders you have to create seperate metrics for each validation dataloader  (similar to how you need seperate metrics for train/val/test).\r\nBy default pytorch-lightning will aggregate metrics across multiple dataloaders into one.\r\n```python\r\ndef __init__(self, ...)\r\n    ...\r\n    self.val_metrics = nn.ModuleList([pl.metrics.Accuracy() for _ in range(n_val_dataloaders)])\r\n\r\ndef validation_step(self, batch, batch_idx, dataset_idx):\r\n    ...\r\n    self.val_metrics[dataset_idx].update(preds, target)\r\n```\r\n\r\nI have not yet tested your example but this is just something that I was hoping to find in the documentation :).I think it is a great idea to add a second note. Would you be up for submitting a PR @potipot ?I'm still not sure though if aggregation is the default behavior with the new metrics API. I'm trying to verify this or maybe you can confirm?If you have a metric object for each individual dataloader, then the metric should accumulate separately.But if we just do\r\n```python\r\ndef __init__(self, ...)\r\n    ...\r\n    self.val_metrics = pl.metrics.Accuracy()\r\n\r\ndef validation_step(self, batch, batch_idx, dataset_idx):\r\n    ...\r\n    self.val_metrics.update(preds, target)\r\n```\r\n\r\nthey would be evaluated across all dataloaders and computed only at the end?> But if we just do\r\n> \r\n> ```python\r\n> def __init__(self, ...)\r\n>     ...\r\n>     self.val_metrics = pl.metrics.Accuracy()\r\n> \r\n> def validation_step(self, batch, batch_idx, dataset_idx):\r\n>     ...\r\n>     self.val_metrics.update(preds, target)\r\n> ```\r\n> \r\n> they would be evaluated across all dataloaders and computed only at the end?\r\n\r\nYes exactly :]",
    "meta": {
      "name": "How to accumulate metrics for multiple validation dataloaders"
    },
    "answer": "You would have to create seperate metrics per validation dataloader (similar to how you need seperate metrics for train/val/test). Something like this could maybe work for you\r\n``` python\r\ndef __init__(self, ...)\r\n    ...\r\n    self.val_metrics = nn.ModuleList([pl.metrics.Accuracy() for _ in range(n_val_dataloaders)])\r\n\r\ndef validation_step(self, batch, batch_idx, dataset_idx):\r\n    ...\r\n    self.val_metrics[dataset_idx].update(preds, target)\r\n```"
  },
  {
    "content": "Hi all,\r\n\r\nI'd like to pass through arguments in a yaml config file which are not declared with type hints in the classes receiving them, but are instead parsed out of **kwargs. Jsonargparse doesn't like this and complains that it needs type hints. Is there a simple way to do this? I'm aware I can define wrapper subclasses that declare the arguments in their `__init__` methods, but that seems like a hack.\r\n\r\nFor example, here's a snippet I'd like to use:\r\n```\r\nstrategy:                                                                       \r\n    class_path: pytorch_lightning.strategies.DDPStrategy                        \r\n    init_args:                                                                  \r\n        process_group_backend: gloo                                             \r\n        find_unused_parameters: false                                           \r\n```\r\n\r\nHere's what jsonargparse has to say about that:\r\n```\r\njsonargparse.util.ParserError: Parser key \"trainer.strategy\": Value \"Namespace(class_path='pytorch_lightning.strategies.DDPStrategy', init_args=Namespace(parallel_devices=None, cluster_\r\nenvironment=None, checkpoint_io=None, precision_plugin=None, ddp_comm_state=None, model_averaging_period=None, process_group_backend='gloo', find_unused_parameters=False))\" does not val\r\nidate against any of the types in typing.Union[str, pytorch_lightning.strategies.strategy.Strategy, NoneType]:\r\n  - Expected a <class 'str'> but got \"Namespace(class_path='pytorch_lightning.strategies.DDPStrategy', init_args=Namespace(parallel_devices=None, cluster_environment=None, checkpoint_io\r\n=None, precision_plugin=None, ddp_comm_state=None, model_averaging_period=None, process_group_backend='gloo', find_unused_parameters=False))\"\r\n  - Problem with given class_path \"pytorch_lightning.strategies.DDPStrategy\":\r\n    - 'Configuration check failed :: No action for destination key \"find_unused_parameters\" to check its value.'\r\n  - Expected a <class 'NoneType'> but got \"Namespace(class_path='pytorch_lightning.strategies.DDPStrategy', init_args=Namespace(parallel_devices=None, cluster_environment=None, checkpoi\r\nnt_io=None, precision_plugin=None, ddp_comm_state=None, model_averaging_period=None, process_group_backend='gloo', find_unused_parameters=False))\"\r\n```cc: @carmocca Hi! This is a known issue that is not implemented atm: you can see more info in #11574 and #11653",
    "meta": { "name": "Pass through **kwargs in yaml config file" },
    "answer": "Hi! This is a known issue that is not implemented atm: you can see more info in #11574 and #11653"
  },
  {
    "content": "How could someone shuffle the training dataloader (using [Datamodule](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.core.datamodule.html#datamodule)) on each epoch?I think it is as simple as setting the `shuffle` parameter to `True` when returning the `train_dataloader`, correct?\r\n\r\nAs shown in the code snippet below:\r\n```python\r\nclass MyDataModule(pl.LightningDataModule):\r\n\r\n    def __init__(self, params):\r\n        super().__init__()\r\n        self.params = params\r\n\r\n    def setup(self, stage=None):\r\n\r\n        # Assign train/val datasets for use in dataloaders\r\n        if stage == 'fit' or stage is None:\r\n            self.train_dataset = CodeSearchDataset(\r\n                data_path=self.params.dir + \"train.jsonl\")\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n            self.train_dataset,\r\n            batch_size=self.params.batch_size,\r\n            shuffle=True,\r\n            drop_last=True,\r\n            num_workers=self.params.num_workers\r\n        )\r\n```You can set `Trainer(reload_dataloaders_every_epoch=True)` and if you have also `shuffle=True` in your dataloader, it will do that by creating a new dataloader every epoch.\r\nThat's my understanding.It seems to be the case that the default behavior is data is shuffled only once at the beginning of the training. \r\nEvery epoch after that takes in the same shuffled data. If we set reload_dataloaders_every_n_epochs=1, we get shuffling every epoch. In the docs located [here](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#reload-dataloaders-every-n-epochs), (in the video) William mentions that by default behavior is to shuffle every epoch. Maybe a mistake?",
    "meta": { "name": "How to shuffle training data in every epoch?" },
    "answer": "You can set `Trainer(reload_dataloaders_every_epoch=True)` and if you have also `shuffle=True` in your dataloader, it will do that by creating a new dataloader every epoch.\r\nThat's my understanding."
  },
  {
    "content": "In vanilla pytorch, I save like this: \r\n```python\r\nif v_loss < best_val_loss:\r\n    print(\"Found better model, saving\")\r\n    model_save_path = f\"models/{sscat_id}/{attribute}/ts={time_stamp}/best.pth\"\r\n    best_val_loss = v_loss\r\n    torch.save(\r\n        {\r\n            \"epoch\": epoch,\r\n            \"model_state_dict\": model.state_dict(),\r\n            \"optim_state_dict\": optimizer.state_dict(),\r\n            \"report\": report,\r\n            \"label_encoder_dict\": label_encoder_dict,\r\n            \"inverse_label_encoder_dict\": {v: k for k, v in label_encoder_dict.items()},\r\n            \"weighted_f1\": weighted_f1,\r\n        },\r\n        model_save_path,\r\n```\r\n\r\n\r\nHow can I save extra keys like `report`, `label_encoder_dict` etc while using ModelCheckpoint callbacks? How do I save a variable inside the LightningModule instance? you can use [on_save_checkpoint](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#on-save-checkpoint) hook inside LightningModule.",
    "meta": { "name": "How to save additional variables while checkpointing?" },
    "answer": "you can use [on_save_checkpoint](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#on-save-checkpoint) hook inside LightningModule."
  },
  {
    "content": "I wasn't able to find this on github -> https://forums.pytorchlightning.ai/t/dose-pl-validate-and-train-at-the-same-time/1362/2\r\n\r\nI am logging step wise in training_step and validation_step\r\n\r\nbut i see this while training -> \r\n```\r\nEpoch 0:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 49/60 [00:59<00:13,  1.22s/it, loss=0.23, v_num=19, train_loss=0.0837] \r\nValidating: 0it [00:00, ?it/s]\r\nValidating:   0%|          | 0/11 [00:00<?, ?it/s]\r\nEpoch 0:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 51/60 [01:05<00:11,  1.28s/it, loss=0.23, v_num=19, train_loss=0.0837]\r\nValidating:  18%|\u2588\u258a        | 2/11 [00:05<00:20,  2.25s/it]\r\nEpoch 0:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 53/60 [01:05<00:08,  1.24s/it, loss=0.23, v_num=19, train_loss=0.0837]\r\nValidating:  36%|\u2588\u2588\u2588\u258b      | 4/11 [00:07<00:09,  1.38s/it]\r\nEpoch 0:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 55/60 [01:09<00:06,  1.26s/it, loss=0.23, v_num=19, train_loss=0.0837]\r\nValidating:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 6/11 [00:09<00:06,  1.25s/it]\r\nEpoch 0:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 57/60 [01:09<00:03,  1.23s/it, loss=0.23, v_num=19, train_loss=0.0837]\r\nValidating:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 8/11 [00:10<00:02,  1.11it/s]\r\nEpoch 0:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 59/60 [01:13<00:01,  1.24s/it, loss=0.23, v_num=19, train_loss=0.0837]\r\nValidating:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10/11 [00:13<00:01,  1.01s/it]\r\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60/60 [01:13<00:00,  1.23s/it, loss=0.23, v_num=19, train_loss=0.0837, val_loss=0.757]\r\n```\r\n\r\nHow can the model start validating before training has completed? by default, it validates after each epoch given that you can configured validation using `validation_step` and `val_dataloader`.\r\ndo you need to validate the model right after training is done?",
    "meta": { "name": "Does PL train and validate at the same time?" },
    "answer": "by default, it validates after each epoch given that you can configured validation using `validation_step` and `val_dataloader`.\r\ndo you need to validate the model right after training is done?"
  },
  {
    "content": "Hi all,\r\n\r\nI'm using Lightning to train a model which encounters large gradient updates early in training. The default init_scale of 2**16 causes the gradients to overflow to inf in certain layers, which leads to NaNs, which leads to various kinds of suboptimal behavior. But I'd still like to use FP16 for the larger batch sizes.\r\n\r\nWriting out the training loop by hand and passing GradScaler a smaller init_scale avoids this problem. Is there a way to pass this value through the Trainer class? The documentation doesn't mention a way to customize the scaler.I think I figured this out - you need to pass a plugin to the trainer class that implements mixed precision, and give the plugin your preferred scaler.\r\n\r\nYaml:\r\n```\r\ntrainer:\r\n    precision: 16\r\n    amp_backend: 'native'\r\n    amp_level: null\r\n\r\n    plugins:\r\n        - class_path: pytorch_lightning.plugins.precision.NativeMixedPrecisionPlugin\r\n          init_args:\r\n              precision: 16\r\n              device: 'cuda'\r\n              scaler:\r\n                  class_path: torch.cuda.amp.GradScaler\r\n                  init_args:\r\n                      # the default scale of 2**16 overflows early in training\r\n                      # and makes the gradient unstable\r\n                      init_scale: 256\r\n```",
    "meta": { "name": "Set GradScaler's init_scale" },
    "answer": "I think I figured this out - you need to pass a plugin to the trainer class that implements mixed precision, and give the plugin your preferred scaler.\r\n\r\nYaml:\r\n```\r\ntrainer:\r\n    precision: 16\r\n    amp_backend: 'native'\r\n    amp_level: null\r\n\r\n    plugins:\r\n        - class_path: pytorch_lightning.plugins.precision.NativeMixedPrecisionPlugin\r\n          init_args:\r\n              precision: 16\r\n              device: 'cuda'\r\n              scaler:\r\n                  class_path: torch.cuda.amp.GradScaler\r\n                  init_args:\r\n                      # the default scale of 2**16 overflows early in training\r\n                      # and makes the gradient unstable\r\n                      init_scale: 256\r\n```"
  },
  {
    "content": "Training a simple autoencoder as part of a larger project, and using it to get back up to speed on Lightning. Pytorch Lightning v 1.6.2, AMD Ryzen, 2 A6000s, Ubuntu 21.10, using DDP. I have 2 questions:\r\n\r\n\r\n1) During script startup, I warn the user if they are about to overwrite existing log files. When using DDP and both devices, pl asks this question twice, of course, once on each process. Where can I put this interaction to avoid doing it twice? I suppose I could put it in prepare_data() - but am hoping for something that feels more appropriate.\r\n\r\n2) Sometimes - but not always - calling trainer.test() immediately following training fails to find the checkpoint file. This happens with unchanged code and model - sometimes it works and sometimes it doesn't. Interestingly, when it fails, a message is displayed saying it is going to to use a file which **does** exists and is the best checkpoint, but it does not load it. Instead it fails twice, each time trying to load a different file name which **does not** exist. Guessing that each process is trying to run its own version of best checkpoint ... **Is there something I need to do/call before running test() to make sure this has all been resolved back to one process?** Or have I found a bug ...\r\n\r\nAny help appreciated.\r\n\r\nsethI resolved the first question. For those with a similar problem:\r\nThere is a set of rank_zero decorators which can be imported from utilities. The one I used is imported like so:\r\n```from   pytorch_lightning.utilities.rank_zero import rank_zero_only```\r\n\r\nPut user interaction into a function like so:\r\n```\r\n@rank_zero_only\r\ndef prelim(args):\r\n    # do as you please here - it will only run once from rank 0\r\n```\r\nCall it before the trainer.fit(), and it will only run once.\r\n\r\nSee [utilities docs](https://pytorch-lightning.readthedocs.io/en/stable/api_references.html#utilities-api) for details ...\r\n",
    "meta": { "name": "ddp help" },
    "answer": "I resolved the first question. For those with a similar problem:\r\nThere is a set of rank_zero decorators which can be imported from utilities. The one I used is imported like so:\r\n```from   pytorch_lightning.utilities.rank_zero import rank_zero_only```\r\n\r\nPut user interaction into a function like so:\r\n```\r\n@rank_zero_only\r\ndef prelim(args):\r\n    # do as you please here - it will only run once from rank 0\r\n```\r\nCall it before the trainer.fit(), and it will only run once.\r\n\r\nSee [utilities docs](https://pytorch-lightning.readthedocs.io/en/stable/api_references.html#utilities-api) for details ...\r\n"
  },
  {
    "content": "I am experimenting with model pruning in pytorch lightning. I noticed when pruning using the lottery ticket hypothesis (LTH), weights are not reset to the original initialization as proposed in the LTH paper and mentioned in the lightning docs. I reproduced the behaviour I faced in my own work using Tutorial 5: Transformers and Multi-Head Attention. Sharing this here to verify my analysis.\r\n\r\nCode I added:\r\n```python\r\nclass CheckPruningWeight(pl.Callback):\r\n  \r\n    def on_train_epoch_start(self, trainer, pl_module):\r\n        print(f\"\\nEPOCH {trainer.current_epoch} STARTING\")\r\n        for name in trainer.model.state_dict():\r\n          if 'input_net.1' in name:\r\n              print(name, trainer.model.state_dict()[name][:1])\r\n        print('\\n')\r\n\r\n    def on_train_epoch_end(self, trainer, pl_module):\r\n        print(f\"\\nEPOCH {trainer.current_epoch} ENDING\")\r\n        for name in trainer.model.state_dict():\r\n          if 'input_net.1' in name:\r\n              print(name, trainer.model.state_dict()[name][:1])\r\n```\r\n\r\nPruning callback:\r\n```python\r\n pruning_callback = pl.callbacks.ModelPruning(\r\n            pruning_fn=\"l1_unstructured\",\r\n            amount=0.2,\r\n            use_global_unstructured=True,\r\n            use_lottery_ticket_hypothesis=True,\r\n            verbose=1,\r\n            parameter_names=['weight'],\r\n            resample_parameters=False,\r\n            prune_on_train_epoch_end=False,\r\n        )\r\n```\r\nTrainer:\r\n```python\r\n trainer = pl.Trainer(\r\n        default_root_dir=root_dir,\r\n        callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\r\n                   pruning_callback,\r\n                   CheckPruningWeight()],\r\n        gpus=1 if str(device).startswith(\"cuda\") else 0,\r\n        max_epochs=5,\r\n        gradient_clip_val=5,\r\n        progress_bar_refresh_rate=1,\r\n    )\r\n```\r\n\r\n\r\nTraining Snippet:\r\n```\r\n \r\n  | Name                | Type               | Params\r\n-----------------------------------------------------------\r\n0 | input_net           | Sequential         | 352   \r\n1 | positional_encoding | PositionalEncoding | 0     \r\n2 | transformer         | TransformerEncoder | 8.5 K \r\n3 | output_net          | Sequential         | 1.4 K \r\n-----------------------------------------------------------\r\n10.3 K    Trainable params\r\n0         Non-trainable params\r\n10.3 K    Total params\r\n0.041     Total estimated model params size (MB)\r\nEpoch 4: 10%\r\n40/398 [00:00<00:08, 41.01it/s, loss=0.294, v_num=15]\r\n\r\nEPOCH 0 STARTING\r\ninput_net.1.weight tensor([[ 0.0505, -0.1712,  0.3061, -0.2354,  0.3003, -0.0694,  0.2055, -0.0367,\r\n         -0.1696,  0.2973]])\r\ninput_net.1.bias tensor([0.1139])\r\n\r\n\r\nApplied `L1Unstructured`. Pruned: 0/10346 (0.00%) -> 1990/10346 (19.23%)\r\n\r\nEPOCH 0 ENDING\r\ninput_net.1.bias tensor([0.1109])\r\ninput_net.1.weight_orig tensor([[ 0.0014, -0.0814,  0.2823, -0.3069,  0.2731, -0.1023,  0.2084, -0.0413,\r\n         -0.2384,  0.4107]])\r\ninput_net.1.weight_mask tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\r\n\r\nEPOCH 1 STARTING\r\ninput_net.1.bias tensor([0.1109])\r\ninput_net.1.weight_orig tensor([[ 0.0014, -0.0814,  0.2823, -0.3069,  0.2731, -0.1023,  0.2084, -0.0413,\r\n         -0.2384,  0.4107]])\r\ninput_net.1.weight_mask tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\r\n\r\n\r\nApplied `L1Unstructured`. Pruned: 1990/10346 (19.23%) -> 3582/10346 (34.62%)\r\n\r\nEPOCH 1 ENDING\r\ninput_net.1.bias tensor([0.1116])\r\ninput_net.1.weight_orig tensor([[ 0.0038, -0.0283,  0.1326, -0.1825,  0.2089, -0.0469,  0.0945, -0.0116,\r\n         -0.1252,  0.2888]])\r\ninput_net.1.weight_mask tensor([[0., 0., 1., 1., 1., 0., 1., 0., 1., 1.]])\r\n\r\nEPOCH 2 STARTING\r\ninput_net.1.bias tensor([0.1116])\r\ninput_net.1.weight_orig tensor([[ 0.0038, -0.0283,  0.1326, -0.1825,  0.2089, -0.0469,  0.0945, -0.0116,\r\n         -0.1252,  0.2888]])\r\ninput_net.1.weight_mask tensor([[0., 0., 1., 1., 1., 0., 1., 0., 1., 1.]])\r\n\r\n\r\nApplied `L1Unstructured`. Pruned: 3582/10346 (34.62%) -> 4856/10346 (46.94%)\r\n\r\nEPOCH 2 ENDING\r\ninput_net.1.bias tensor([0.1114])\r\ninput_net.1.weight_orig tensor([[ 0.0038, -0.0274,  0.1090, -0.1602,  0.2130, -0.0471,  0.0802, -0.0114,\r\n         -0.1096,  0.2959]])\r\ninput_net.1.weight_mask tensor([[0., 0., 1., 1., 1., 0., 0., 0., 1., 1.]])\r\n\r\nEPOCH 3 STARTING\r\ninput_net.1.bias tensor([0.1114])\r\ninput_net.1.weight_orig tensor([[ 0.0038, -0.0274,  0.1090, -0.1602,  0.2130, -0.0471,  0.0802, -0.0114,\r\n         -0.1096,  0.2959]])\r\ninput_net.1.weight_mask tensor([[0., 0., 1., 1., 1., 0., 0., 0., 1., 1.]])\r\n```\r\n\r\nThe training snippet shows the original weight tensor changing at the start of each epoch, changing values to that of the end of the previous epoch. By right, LTH should revert weights start of every epoch to the original initialization (start of epoch 0). Thoughts or correction is much appreciated.  \r\n\r\nAs I began implementing and comparing with my own custom pruning function, I came to the realization that weight_orig is the updated weight at the end of each epoch, not the original initialized weight. Printing the actual weight at the start of each epoch gives the original initialized weight. Had a different idea of what weight_orig meant, my bad! ",
    "meta": {
      "name": "Model Pruning (Lottery Ticket Hypothesis) Not Reinitializing Weights"
    },
    "answer": "As I began implementing and comparing with my own custom pruning function, I came to the realization that weight_orig is the updated weight at the end of each epoch, not the original initialized weight. Printing the actual weight at the start of each epoch gives the original initialized weight. Had a different idea of what weight_orig meant, my bad! "
  },
  {
    "content": "With the new experimental CLI, I've been trying to think of how to reconcile using the `add_model_specific_args` pattern with YAML configs; I've been digging through the docs and couldn't find the pieces I needed so hoping others could chime in. Basically I'd want to implement this once, and have backwards compatibility for both the `argparse` and new CLI routes, and I might be mistaken, but it feels that the two routes involve mutually exclusive patterns.\r\n\r\nLet's say we have an abstract `pl.LightningModule` that allows the user to choose an activation function:\r\n\r\n```python\r\nfrom ast import literal_eval\r\nfrom torch import nn\r\nimport pytorch_lightning as pl\r\n\r\nclass LitModule(pl.LightningModule):\r\n    def __init__(self, input_dim: int, output_dim: int, activation: str):\r\n        # cross your fingers it resolves, something like `nn.SilU`\r\n        act_class = literal_eval(activation)\r\n        # instantiate the activation function object\r\n        self.model = nn.Sequential(nn.Linear(input_dim, output_dim), act_class())\r\n```\r\n\r\nWith `argparse`, the way I would set this up would be to implement `add_model_specific_args`:\r\n\r\n```python\r\n...\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = parent_parser.add_argument_group(\"LitModule\")\r\n        parser.add_argument(\"--input_dim\", type=int)\r\n        parser.add_argument(\"--output_dim\", type=int)\r\n        parser.add_argument(\"--activation\", type=str, default=\"nn.SiLU\", help=\"Reference to activation function class in `torch.nn`.\")\r\n```\r\n\r\nAnd in my training script:\r\n\r\n```python\r\nfrom torch import nn\r\n\r\nparser = ArgumentParser()\r\nparser = LitModule.add_model_specific_args(parser)\r\n\r\nargs = parser.parse_args()\r\n\r\nmodel = LitModule(args)\r\n```\r\n\r\nWith the new CLI, the better option would be to use `nn.Module` for typing instead of `str` for the `activation` argument, i.e. redefining the `__init__` so we remove the possibility of running arbitrary code via `literal_eval` since it's taken care of by `jsonargparse`/`LightningCLI`:\r\n\r\n```python\r\nclass LitModule(pl.LightningModule):\r\n    def __init__(self, input_dim: int, output_dim: int, activation: nn.Module):\r\n        # don't need to use `literal_eval` anymore\r\n        self.model = nn.Sequential(nn.Linear(input_dim, output_dim), activation())\r\n```\r\n\r\n...and in my YAML config:\r\n\r\n```yaml\r\nmodel:\r\n   class_path: mymodule.LitModule\r\n   init_args:\r\n      input_dim: 8\r\n      output_dim: 2\r\n      activation: torch.nn.SiLU\r\n```\r\n\r\nI can't think of any way to implement this same pattern with `argparse`. While I prefer the CLI approach because of its potential for composability, it's experimental and sometimes you want more granular control as you get from a training script, i.e. putting the components together yourself, multiple optimizers, etc. which has a bit more flexibility. There might not be a one-size fits all, but I'd appreciate any feedback/discussion.If you truly want to have both, you could do `activation: Union[str, nn.Module]` and use the `literal_eval` only if an `str` is received. But even if you do this, there is little reason to keep using `add_model_specific_args` since with `LightningCLI` you could also give an `str` instead of the `class_path` and `init_args` pair.\r\n\r\nRegarding \"i.e. putting the components together yourself, multiple optimizers, etc. which has a bit more flexibility\" best if you give more details about what you want to do, and then see if it makes sense or not to use `LightningCLI`.",
    "meta": {
      "name": "Pattern for specifying modules in argument parser versus CLI"
    },
    "answer": "If you truly want to have both, you could do `activation: Union[str, nn.Module]` and use the `literal_eval` only if an `str` is received. But even if you do this, there is little reason to keep using `add_model_specific_args` since with `LightningCLI` you could also give an `str` instead of the `class_path` and `init_args` pair.\r\n\r\nRegarding \"i.e. putting the components together yourself, multiple optimizers, etc. which has a bit more flexibility\" best if you give more details about what you want to do, and then see if it makes sense or not to use `LightningCLI`."
  },
  {
    "content": "Hi, I am using an IterableDataset that generates infinite samples and I want it to change its behaviour as the training progresses. To do so, I need to access information about the current training step from inside the Dataset.\r\nI tried passing the trainer to the dataset as an argument, however, when using multiple workers the trainer is not updated.\r\nAnother idea was to define virtual epochs of N steps. To do so, I passed limit_train_batches = N as an argument to the Trainer. Also, when an epoch finishs, I need to update the dataset variables. To do it I set reload_dataloader_every_n_epochs = 1 and I wrote a train_dataloader method in my Lightning Module that returns a new dataloader with the updated variables. However, it seems that the train_dataloader method isn't called if I set the limit_train_batches variable. Any idea on how to solve this problem?\r\n\r\nThanks in advance.> However, it seems that the train_dataloader method isn't called if I set the limit_train_batches variable. Any idea on how to solve this problem?\r\n\r\nthis should not happen. Just tried a simple example using [BoringModel](https://colab.research.google.com/github/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report/bug_report_model.ipynb) and it is recalling the train dataloader hook correctly even when using `limit_train_batches`.I finally could solve it. The problem was that I passed the train and validation dataloaders as arguments to ```trainer.fit()```, so ```train_dataloader()``` wasn't called. I ended up implementing ```__len__()``` in the IterableDataset instead of using ```limit_train_batches```\r\n",
    "meta": {
      "name": "Reload train dataloader when using limit_train_batches"
    },
    "answer": "I finally could solve it. The problem was that I passed the train and validation dataloaders as arguments to ```trainer.fit()```, so ```train_dataloader()``` wasn't called. I ended up implementing ```__len__()``` in the IterableDataset instead of using ```limit_train_batches```\r\n"
  },
  {
    "content": "Task: Model Compression using [NNI](https://nni.readthedocs.io/). \r\n\r\nApproach: Loading a PyTorch Lightning trained model from a model checkpoint using `.load_checkpoint()`. \r\n\r\nProblem: After removing weights, my model class has reduced weights and has to be fine-tuned. As this is a compressed version of a trained model, I want to continue training with the optimizer state dict present in the checkpoint. If I try to fine-tune using \r\n```python\r\ntrainer = pl.Trainer(\r\n     gpus=self.gpus,\r\n     max_epochs=self.max_epochs,\r\n     callbacks=[checkpoint_callback],\r\n)\r\ntrainer.fit(self.model, datamodule, ckpt_path=ckpt_PATH)\r\n```\r\nThis gives an error as the model structure has changed (Conv2d filters have been reduced). Is there an approach that can be used that is easier?this is not possible with fit(..., ckpt_path=...) since this is partial resume and will load the model weights too. For you use-case you can do this maybe:\r\n\r\n```python\r\nclass OptimizerReload(Callback):\r\n    def __init__(self, ckpt_path):\r\n        self.ckpt_path = ckpt_path\r\n\r\n    def on_train_start(self, trainer, pl_module):\r\n        ckpt = torch.load(self.ckpt_path)\r\n        trainer.strategy.load_optimizer_state_dict(ckpt)\r\n```\r\n\r\nand, pass it to Trainer\r\n\r\n```python\r\ncb = OptimizerReload(ckpt_path)\r\ntrainer = Trainer(..., callbacks=[cb])\r\ntrainer.fit(model)\r\n```",
    "meta": {
      "name": "Using trainer.fit(model, datamodule, ckpt_path=path) on compressed model"
    },
    "answer": "this is not possible with fit(..., ckpt_path=...) since this is partial resume and will load the model weights too. For you use-case you can do this maybe:\r\n\r\n```python\r\nclass OptimizerReload(Callback):\r\n    def __init__(self, ckpt_path):\r\n        self.ckpt_path = ckpt_path\r\n\r\n    def on_train_start(self, trainer, pl_module):\r\n        ckpt = torch.load(self.ckpt_path)\r\n        trainer.strategy.load_optimizer_state_dict(ckpt)\r\n```\r\n\r\nand, pass it to Trainer\r\n\r\n```python\r\ncb = OptimizerReload(ckpt_path)\r\ntrainer = Trainer(..., callbacks=[cb])\r\ntrainer.fit(model)\r\n```"
  },
  {
    "content": "Hi folks,\r\n\r\nI can't figure out how to give a Strategy class (rather than a string) as an argument to the trainer in a CLI config file. Is doing so supported?\r\n\r\nDoing this works fine:\r\n```\r\ntrainer:\r\n    ...\r\n    strategy: 'deepspeed_stage_2_offload'\r\n    ...\r\n```\r\n\r\nBut this gives a jsonargparse error:\r\n```\r\ntrainer:\r\n    ...\r\n    strategy:\r\n        class_path: pytorch_lightning.strategies.DeepSpeedStrategy\r\n        init_args:\r\n            stage: 2\r\n            offload_optimizer: True\r\n            logging_batch_size_per_gpu: 16\r\n    ...\r\n```\r\n\r\nThe error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/media/sharon/wbrannon/github/clip-graph/bin/trainer.py\", line 7, in <module>\r\n    cli = cl.LightningCLI(\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 552, in __init__\r\n    self.parse_arguments(self.parser)\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 692, in parse_arguments\r\n    self.config = parser.parse_args()\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/utilities/cli.py\", line 268, in parse_args\r\n    return super().parse_args(*args, **kwargs)\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/deprecated.py\", line 112, in patched_parse\r\n    cfg = parse_method(*args, _skip_check=_skip_check, **kwargs)\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/core.py\", line 366, in parse_args\r\n    self.error(str(ex), ex)\r\n  File \"/media/sharon/wbrannon/miniconda3/envs/dl/lib/python3.9/site-packages/jsonargparse/core.py\", line 983, in error\r\n    raise ParserError(message) from ex\r\njsonargparse.util.ParserError: Parser key \"trainer.strategy\": Value \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, s\r\ntage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000\r\n.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\" does not validate against any of the types in typing.Union[str, pytorch_lightning.strategies.strategy.Strategy, NoneType]:\r\n  - Expected a <class 'str'> but got \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, stage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\"\r\n  - Problem with given class_path \"pytorch_lightning.strategies.DeepSpeedStrategy\":\r\n    - Configuration check failed :: Parser key \"params_buffer_size\": Expected a <class 'int'> but got \"100000000.0\"\r\n  - Expected a <class 'NoneType'> but got \"Namespace(class_path='pytorch_lightning.strategies.DeepSpeedStrategy', init_args=Namespace(zero_optimization=True, stage=2, remote_device='cpu', offload_optimizer=False, offload_parameters=False, offload_params_device='cpu', nvme_path='/local_nvme', params_buffer_count=5, params_buffer_size=100000000.0, max_in_cpu=1000000000.0, offload_optimizer_device='cpu', optimizer_buffer_count=4, block_size=1048576, queue_depth=8, single_submit=False, overlap_events=True, thread_count=1, pin_memory=False, sub_group_size=1000000000000.0, contiguous_gradients=True, overlap_comm=True, allgather_partitions=True, reduce_scatter=True, allgather_bucket_size=200000000.0, reduce_bucket_size=200000000.0, zero_allow_untested_optimizer=True, logging_batch_size_per_gpu='auto', config=None, logging_level=30, parallel_devices=None, cluster_environment=None, loss_scale=0.0, initial_scale_power=16, loss_scale_window=1000, hysteresis=2, min_loss_scale=1, partition_activations=False, cpu_checkpointing=False, contiguous_memory_optimization=False, synchronize_checkpoint_boundary=False, load_full_weights=False, precision_plugin=None, process_group_backend=None))\"\r\n```cc @carmocca It's a bug! Will be fixed with https://github.com/PyTorchLightning/pytorch-lightning/pull/12989 which should be included in next week's bugfix release\r\n\r\nThanks for the report!",
    "meta": { "name": "Specify Trainer strategy in CLI config?" },
    "answer": "It's a bug! Will be fixed with https://github.com/PyTorchLightning/pytorch-lightning/pull/12989 which should be included in next week's bugfix release\r\n\r\nThanks for the report!"
  },
  {
    "content": "I'm writing a yaml config file for a pytorch-lightning CLI script. The file specifies model, data, trainer arguments and misc parameters. The model arguments depend on the dataset -- specifically, the dimension of the model's first layer needs to match what's in the data.\r\n\r\nDo I need to hardcode this value or is there a way to tell PL to discover it dynamically? Here's an example:\r\n```\r\nseed_everything: 42\r\nckpt_path: /path/to/ckpt\r\n\r\nmodel:\r\n    class_path: src.mymodel.litclass\r\n    init_args:\r\n        n_classes: 1024\r\n        key: 'target'\r\n        lr: 0.0005\r\n        weight_decay: 0\r\n        model:\r\n            class_path: src.mymodel.baseclass\r\n            init_args:\r\n                n_layers: 3\r\n                n_heads: 4\r\n                d_input: ??????\r\n                d_hidden: 64\r\n                d_feedforward: 128\r\n                p_dropout: 0.5\r\n\r\ndata:\r\n    class_path: src.mydataset.mydatamodule\r\n    init_args:\r\n        data_dir: ../data\r\n```\r\n\r\nHow should I fill in d_input?I think you have to hard-code it here, but @carmocca can confirm.You can use argument linking: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#argument-linking\r\nAnd have any initial default value\r\n\r\nOr variable interpolation with omegaconf: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#variable-interpolation",
    "meta": {
      "name": "How to refer to dataset attributes in CLI config file?"
    },
    "answer": "You can use argument linking: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#argument-linking\r\nAnd have any initial default value\r\n\r\nOr variable interpolation with omegaconf: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#variable-interpolation"
  },
  {
    "content": "Python: 3.9.12\r\nPytorch: '1.11.0+cu102'\r\nPytorch-lightning: 1.6.1\r\n\r\nThe [docs for evaluation](https://pytorch-lightning.readthedocs.io/en/latest/common/evaluation_intermediate.html) say:\r\n> It is recommended to test with Trainer(devices=1) since distributed strategies such as DDP use [DistributedSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler) internally, which replicates some samples to make sure all devices have same batch size in case of uneven inputs. This is helpful to make sure benchmarking for research papers is done the right way.\r\n\r\nSimilarly I get a warning during runtime:\r\n```\r\nDuring `trainer.test()`, it is recommended to use `Trainer(devices=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\r\n```\r\n\r\nI would like to use DDP for training on 4 GPUs, does this mean I create an entirely new Trainer for just val/test?\r\n\r\nMy train Trainer:\r\n```python\r\ntrainer = pl.Trainer(\r\n            max_epochs=self.max_epochs,\r\n            logger=logger,\r\n            num_nodes=self.num_nodes,\r\n            # use 1 processes if on cpu\r\n            devices=self.num_gpus if self.num_gpus else 1,\r\n            accelerator=\"gpu\" if self.num_gpus else \"cpu\",\r\n            strategy=DDPPlugin(find_unused_parameters=False)\r\n            if self.num_gpus > 1\r\n            else None,\r\n            enable_checkpointing=False,\r\n            callbacks=callbacks,\r\n            profiler=\"simple\",  # or \"advanced\" which is more granular\r\n            fast_dev_run=self.fast_dev_run,  # For debugging\r\n        )\r\ntrainer.fit(model, datamodule)\r\n```\r\nIf I want to then run testing would I then do something like:\r\n```python\r\ntest_trainer= pl.Trainer(\r\n            max_epochs=self.max_epochs,\r\n            logger=logger,\r\n            num_nodes=self.num_nodes,\r\n            # use 1 device for test\r\n            devices=1,\r\n            accelerator=\"gpu\" if self.num_gpus else \"cpu\",\r\n            strategy=DDPPlugin(find_unused_parameters=False)\r\n            if self.num_gpus > 1\r\n            else None,\r\n            enable_checkpointing=False,\r\n            callbacks=callbacks,\r\n            profiler=\"simple\",  # or \"advanced\" which is more granular\r\n            fast_dev_run=self.fast_dev_run,  # For debugging\r\n        )\r\ntest_trainer.test(model, test_dataloader)\r\n```\r\n\r\nAm I understanding correctly? I don't think it's possible to set the number of devices dynamically since there's no setter defined for the `num_devices` property in the Trainer class. I'm not sure if it makes sense to make a separate trainer, so I feel a bit confused.yes, if you care about the accuracy of the test metrics and don't want to include extra samples, this is the only way. Not just a separate trainer, if using DDP, you should create a separate script all together else the whole script, along with the `trainer.test` call will be launched on each device during `trainer.fit` call.",
    "meta": { "name": "Separate Trainer for `train()` and `test()`?" },
    "answer": "yes, if you care about the accuracy of the test metrics and don't want to include extra samples, this is the only way. Not just a separate trainer, if using DDP, you should create a separate script all together else the whole script, along with the `trainer.test` call will be launched on each device during `trainer.fit` call."
  },
  {
    "content": "Am I right in calling the tune method as follows?\r\n\r\n`trainer.tune(model, train_dataloader, val_dataloader)\r\n`\r\n\r\nHere is the stacktrace of the error I am getting. \r\n\r\n```\r\ntrainer.tune(model, train_dataloader, val_dataloader)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in tune\r\n    self.tuner.tune(model, train_dataloader, val_dataloaders, datamodule)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\", line 63, in tune\r\n    self.lr_find(model, update_attr=True)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\", line 136, in lr_find\r\n    self.setup_trainer(model, train_dataloader, val_dataloaders, datamodule)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\", line 44, in setup_trainer\r\n    self.trainer.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 120, in setup_fit\r\n    self.trainer.config_validator.verify_loop_configurations(model)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py\", line 36, in verify_loop_configurations\r\n    self.__verify_train_loop_configuration(model)\r\n  File \"/opt/conda/envs/pl124/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py\", line 58, in __verify_train_loop_configuration\r\n    raise MisconfigurationException(\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: No `train_dataloader()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()`\r\n to be defined.\r\n```\r\n\r\n`trainer.fit(model, train_dataloader, val_dataloader)` works fine. I don't define the data loader as a method in my model, rather I pass one in to my `.fit` methods. This seems to work well for the `.fit` method, but not for `.tune`.\r\n\r\n\r\nFWIW, here is my model definition\r\n\r\n```python\r\nclass PretrainedResnet50FT(pl.LightningModule):\r\n    \r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument('--num_classes', type=int, default=2)\r\n        parser.add_argument('--lr', type=float, default=1e-3)\r\n        return parser\r\n\r\n    def __init__(self, hparams):\r\n        super().__init__()\r\n        self.hparams = hparams\r\n\r\n        image_modules = list(models.resnet50(pretrained=True, progress=False).children())[:-1]\r\n        self.resnet = nn.Sequential(*image_modules)\r\n        self.classifier = nn.Linear(2048, self.hparams.num_classes)\r\n\r\n    def forward(self, x):\r\n        out = self.resnet(x)\r\n        out = torch.flatten(out, 1)\r\n        out = self.classifier(out)\r\n        return out\r\n\r\n    def step(self, who, batch, batch_nb):    \r\n        x, task_labels, slide_id = batch\r\n        \r\n        #Define logits over the task and source embeddings\r\n        task_logits = self(x)\r\n\r\n        #Define loss values over the logits\r\n        loss = task_loss = F.cross_entropy(task_logits, task_labels, reduction = \"mean\")                \r\n                \r\n        #Train acc\r\n        task_preds = task_logits.argmax(-1)\r\n        task_acc = pl.metrics.functional.accuracy(task_preds, task_labels)\r\n        \r\n        #F1\r\n        task_f1 = pl.metrics.functional.f1(task_preds, task_labels, num_classes = self.hparams.num_classes)\r\n\r\n        self.log(who + '_loss', loss)\r\n        self.log(who + '_acc', task_acc)\r\n        self.log(who + '_f1', task_f1)\r\n\r\n        return loss\r\n\r\n    def training_step(self, batch, batch_nb):\r\n        # REQUIRED\r\n        loss = self.step('train', batch, batch_nb)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_nb):\r\n        loss = self.step('val', batch, batch_nb)\r\n        return loss\r\n        \r\n    def test_step(self, batch, batch_nb):\r\n        loss = self.step('test', batch, batch_nb)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\r\n```Arguments was not correctly passed from `tune` to `lr_find`. Was solved by this PR: https://github.com/PyTorchLightning/pytorch-lightning/pull/6784\r\nPlease upgrade to latest version of lightning :]Hi, I'm on 1.4.1 and have this issue. My code is similar. Am I doing something wrong?I'm still having this issue. Followed the example. Can anyone confirm/deny this is still a problem?",
    "meta": {
      "name": "trainer.tune causes \"No `train_dataloader()` method defined.\" error"
    },
    "answer": "Arguments was not correctly passed from `tune` to `lr_find`. Was solved by this PR: https://github.com/PyTorchLightning/pytorch-lightning/pull/6784\r\nPlease upgrade to latest version of lightning :]"
  },
  {
    "content": "When running my model I get the error message:`TypeError: validation_step() takes 2 positional arguments but 3 were given`\r\nHere is my code.\r\n```\r\ndef val_dataloader(self):\r\n        # expect to get train folder\r\n        dataset = load_train_data(self.val_data_path)\r\n        try:\r\n            num_workers = multiprocessing.cpu_count()\r\n        except:\r\n            num_workers = 1\r\n        dataloader = DataLoader(dataset, batch_size=1024, num_workers=num_workers, persistent_workers=True,\r\n                                collate_fn=dataset_collate_function, shuffle=True)\r\n\r\n        return dataloader\r\n```\r\n\r\n```\r\ndef validation_step(self, batch, batch_idx):\r\n        x_app = batch['feature'].float()\r\n        y_app = batch['app_label'].long()\r\n        y_tra, y_all, index = drop_na(batch)\r\n        y_hat_app, y_hat_tra, y_hat_all = self(x_app, index)\r\n\r\n        app_f1 = F1Score(num_classes=17)\r\n        tra_f1 = F1Score(num_classes=12)\r\n        all_f1 = F1Score(num_classes=6)\r\n        F1_app = app_f1(y_hat_app, y_app)\r\n        F1_tra = tra_f1(y_hat_tra, y_tra)\r\n        F1_all = all_f1(y_hat_all, y_all)\r\n        F1_score = (F1_app + F1_tra + F1_all) / 3.0\r\n        self.log('val_F1_app', F1_app, on_step=True, on_epoch=True)\r\n        self.log('val_F1_tra', F1_tra, on_step=True, on_epoch=True)\r\n        self.log('val_F1_all', F1_all, on_step=True, on_epoch=True)\r\n        self.log('val_F1_score', F1_score, on_step=True, on_epoch=True)\r\n\r\n        entropy_app = F.cross_entropy(y_hat_app, y_app)\r\n        entropy_tra = F.cross_entropy(y_hat_tra, y_tra)\r\n        entropy_all = F.cross_entropy(y_hat_all, y_all)\r\n        loss_weight = self.app_weight + 2 + 1\r\n        entropy = (self.app_weight * entropy_app + 2 *\r\n                   entropy_tra + 1*entropy_all) / loss_weight\r\n        self.log('val_app_loss', entropy_app, prog_bar=True,\r\n                 logger=True, on_step=True, on_epoch=True)\r\n        self.log('val_tra_loss', entropy_tra, prog_bar=True,\r\n                 logger=True, on_step=True, on_epoch=True)\r\n        self.log('val_all_loss', entropy_all, prog_bar=True,\r\n                 logger=True, on_step=True, on_epoch=True)\r\n        self.log('validation_loss', entropy, prog_bar=True,\r\n                 logger=True, on_step=True, on_epoch=True)\r\n\r\n        return {\"val_loss\": entropy, \"val_F1score\": F1_score}\r\n```\r\n\r\nAny advice on what happened?\r\nThank you very much!your code looks correct. Are you sure you are using a single validation dataloader?\r\nif yes, can you reproduce it using [BoringModel](https://colab.research.google.com/github/PytorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report/bug_report_model.ipynb)??",
    "meta": {
      "name": "TypeError: validation_step() takes 2 positional arguments but 3 were given"
    },
    "answer": "your code looks correct. Are you sure you are using a single validation dataloader?\r\nif yes, can you reproduce it using [BoringModel](https://colab.research.google.com/github/PytorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report/bug_report_model.ipynb)??"
  },
  {
    "content": "Hi, PL.\r\n\r\nIs there any way to skip validation for the first few epochs (ex. 10)?\r\nI searched for an hour, but the only thing I found is check_val_every_n_epoch.\r\n\r\nBut the thing I want is different from this.\r\nI want to check every epoch after a certain epoch.\r\n\r\nSince I'm a beginner, any comment would be greatly appreciated.\r\n   Hi @sangrockEG! You can utilise `self.current_epoch` in your LightningModule:\r\n\r\n```python\r\ndef validation_step(self, batch, batch_idx):\r\n    if self.current_epoch <= 9:\r\n        return\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#current-epoch",
    "meta": { "name": "Skip validation for the first few epochs" },
    "answer": "Hi @sangrockEG! You can utilise `self.current_epoch` in your LightningModule:\r\n\r\n```python\r\ndef validation_step(self, batch, batch_idx):\r\n    if self.current_epoch <= 9:\r\n        return\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#current-epoch"
  },
  {
    "content": "Hello,\r\nmy LightiningModule has an imputer which needs to be initialized from data (think about mean-imputation) before the training start. \r\n\r\n**Is there any ModelHook which allows me to perform a single pass on the entire dataset before the training start ?**   \r\n\r\nThanks in advance \r\n\r\n```\r\nclass Net(pl.LightningModule):\r\n\r\ndef __init__(self):\r\n      self.imputer = MyImputer()   # the imputer need to be initialized from the data\r\n      self.net = MyNet()\r\n\r\ndef training_step(self, bath, batch_idx):\r\n      x, y_hat = batch\r\n      x_imputed = self.imputer(x)\r\n      y = self.net(x_imputed)\r\n      loss = (y-y_hat).pow(2).sum()\r\n      return loss\r\n\r\n\r\ndef before_training(self, batch, batch_idx):\r\n     # How can I do this? Is there any ModelHook I can use/modify?\r\n     # I need to perform a full pass of the data to initialize the imputer before the training begins\r\n     self.imputer.initialize_from_data(batch)\r\n```\r\n\r\n\r\n\r\nyou can do\r\n```py\r\ndef on_train_start(self):\r\n    for batch in self.trainer.train_dataloader.loaders:\r\n        ...\r\n```",
    "meta": {
      "name": "How to loop over the entire dataset before training start"
    },
    "answer": "you can do\r\n```py\r\ndef on_train_start(self):\r\n    for batch in self.trainer.train_dataloader.loaders:\r\n        ...\r\n```"
  },
  {
    "content": "Hi,\r\n\r\nI'm training DDP with 4 GPUs but noticed that if I rerun the experiment the first epoch has the **exact same** but random order of the minibatches as the previous experiment.\r\n\r\nHow can I make it so that each time I run the experiment I get a different random order?\r\n\r\nI'm using PL version is 1.4.5 and pytorch 1.10.0.\r\n\r\nThank youSolved. Use `seed_everything(random_seed)`.",
    "meta": { "name": "How to get different random minibatch orders?" },
    "answer": "Solved. Use `seed_everything(random_seed)`."
  },
  {
    "content": "Hi guys,\r\n\r\nI'm attempting to get to input and output shapes of each of my layers using the `input_size` and `output_size` properties. However, attempting to access either of these returns the value `?`, rather than the size array. Could anyone shine some light on why this might be happening? Thanks!\r\n\r\n<details><summary>Code Snippet</summary>\r\n<p>\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport pytorch_lightning as pl\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pickle\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom scipy.io import wavfile\r\n\r\nclass CausalConv1d(torch.nn.Conv1d):\r\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\r\n        self.__padding = (kernel_size - 1) * dilation\r\n\r\n        super(CausalConv1d, self).__init__(\r\n            in_channels,\r\n            out_channels,\r\n            kernel_size=kernel_size,\r\n            stride=stride,\r\n            padding=self.__padding,\r\n            dilation=dilation,\r\n            groups=groups,\r\n            bias=bias,\r\n        )\r\n\r\n    def forward(self, input):\r\n        result = super(CausalConv1d, self).forward(input)\r\n        if self.__padding != 0:\r\n            return result[:, :, : -self.__padding]\r\n        return result\r\n\r\n\r\ndef _conv_stack(dilations, in_channels, out_channels, kernel_size):\r\n    \"\"\"\r\n    Create stack of dilated convolutional layers, outlined in WaveNet paper:\r\n    https://arxiv.org/pdf/1609.03499.pdf\r\n    \"\"\"\r\n    return nn.ModuleList(\r\n        [\r\n            CausalConv1d(\r\n                in_channels=in_channels,\r\n                out_channels=out_channels,\r\n                dilation=d,\r\n                kernel_size=kernel_size,\r\n            )\r\n            for i, d in enumerate(dilations)\r\n        ]\r\n    )\r\n\r\n\r\nclass WaveNet(nn.Module):\r\n    def __init__(self, num_channels, dilation_depth, num_repeat, kernel_size=2):\r\n        super(WaveNet, self).__init__()\r\n        dilations = [2 ** d for d in range(dilation_depth)] * num_repeat\r\n        internal_channels = int(num_channels * 2)\r\n        self.hidden = _conv_stack(dilations, num_channels, internal_channels, kernel_size)\r\n        self.residuals = _conv_stack(dilations, num_channels, num_channels, 1)\r\n        self.input_layer = CausalConv1d(\r\n            in_channels=1,\r\n            out_channels=num_channels,\r\n            kernel_size=1,\r\n        )\r\n\r\n        self.linear_mix = nn.Conv1d(\r\n            in_channels=num_channels * dilation_depth * num_repeat,\r\n            out_channels=1,\r\n            kernel_size=1,\r\n        )\r\n        self.num_channels = num_channels\r\n\r\n    def forward(self, x):\r\n        out = x\r\n        skips = []\r\n        out = self.input_layer(out)\r\n\r\n        for hidden, residual in zip(self.hidden, self.residuals):\r\n            x = out\r\n            out_hidden = hidden(x)\r\n\r\n            # gated activation\r\n            #   split (32,16,3) into two (16,16,3) for tanh and sigm calculations\r\n            out_hidden_split = torch.split(out_hidden, self.num_channels, dim=1)\r\n            out = torch.tanh(out_hidden_split[0]) * torch.sigmoid(out_hidden_split[1])\r\n\r\n            skips.append(out)\r\n\r\n            out = residual(out)\r\n            out = out + x[:, :, -out.size(2) :]\r\n\r\n        # modified \"postprocess\" step:\r\n        out = torch.cat([s[:, :, -out.size(2) :] for s in skips], dim=1)\r\n        out = self.linear_mix(out)\r\n        return out\r\n\r\n\r\ndef error_to_signal(y, y_pred):\r\n    \"\"\"\r\n    Error to signal ratio with pre-emphasis filter:\r\n    https://www.mdpi.com/2076-3417/10/3/766/htm\r\n    \"\"\"\r\n    y, y_pred = pre_emphasis_filter(y), pre_emphasis_filter(y_pred)\r\n    return (y - y_pred).pow(2).sum(dim=2) / (y.pow(2).sum(dim=2) + 1e-10)\r\n\r\n\r\ndef pre_emphasis_filter(x, coeff=0.95):\r\n    return torch.cat((x[:, :, 0:1], x[:, :, 1:] - coeff * x[:, :, :-1]), dim=2)\r\n\r\n\r\nclass SatNet(pl.LightningModule):\r\n    def __init__(self, hparams):\r\n        super(SatNet, self).__init__()\r\n        self.wavenet = WaveNet(\r\n            num_channels=hparams[\"num_channels\"],\r\n            dilation_depth=hparams[\"dilation_depth\"],\r\n            num_repeat=hparams[\"num_repeat\"],\r\n            kernel_size=hparams[\"kernel_size\"],\r\n        )\r\n        self.hparams.update(hparams)\r\n        self.save_hyperparameters()\r\n        self.test_ds = TensorDataset()\r\n\r\n    def prepare_data(self):\r\n\r\n        createTensorDataset = lambda x, y: TensorDataset(torch.from_numpy(x).unsqueeze(1), torch.from_numpy(y).unsqueeze(1))\r\n        \r\n        inRate, inData = wavfile.read(hparams[\"in_file\"])\r\n        outRate, outData = wavfile.read(hparams[\"out_file\"])\r\n        sampleTime = 0.1\r\n        sampleSize = int(inRate * sampleTime)\r\n        length = len(inData) - len(inData) % sampleSize\r\n\r\n        #Each row in this table represents the waveform samples seen in 0.1 seconds (4410 samples)\r\n        x = inData[:length].reshape((-1, sampleSize)).astype(np.float32)\r\n        y = outData[:length].reshape((-1, sampleSize)).astype(np.float32)\r\n\r\n        splitLocA = int(len(x) * 0.6)\r\n        splitLocB = int(len(x) * 0.8)\r\n\r\n        X_train, X_valid, X_test = np.split(x, [splitLocA, splitLocB])\r\n        y_train, y_valid, y_test = np.split(y, [splitLocA, splitLocB])\r\n\r\n        self.train_ds = createTensorDataset(X_train, y_train)\r\n        self.valid_ds = createTensorDataset(X_valid, y_valid)\r\n        self.test_ds = createTensorDataset(X_test, y_test)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.wavenet.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n            self.train_ds,\r\n            shuffle=True,\r\n            batch_size=self.hparams.batch_size,\r\n            num_workers=4,\r\n        )\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.valid_ds, batch_size=self.hparams.batch_size, num_workers=4)\r\n\r\n    def forward(self, x):\r\n        return self.wavenet(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_pred = self.forward(x)\r\n        loss = error_to_signal(y[:, :, -y_pred.size(2) :], y_pred).mean()\r\n        self.log(\"loss\", loss)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_pred = self.forward(x)\r\n        loss = error_to_signal(y[:, :, -y_pred.size(2) :], y_pred).mean()\r\n        return loss\r\n\r\n    def validation_epoch_end(self, outs):\r\n        lossArray = []\r\n        for tensor in outs:\r\n            lossArray.append(tensor.item())\r\n        avg_loss = (np.asarray(lossArray)).mean()\r\n        self.log(\"avg_val_loss\", avg_loss)\r\n\r\n# Training\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor = \"avg_val_loss\",\r\n    dirpath = \"models\\\\newPLModelTest\\\\\",\r\n    filename = \"sample-mnist-{epoch:02d}-{avg_val_loss:.4f}\",\r\n    save_top_k = 3,\r\n    mode = \"min\"\r\n)\r\n\r\nhparams = {\r\n    \"in_file\": \"data\\\\y_input_data.wav\",\r\n    \"out_file\": \"data\\\\x_input_data.wav\",\r\n    \"num_channels\": 12,\r\n    \"dilation_depth\": 10,\r\n    \"num_repeat\": 1,\r\n    \"kernel_size\": 3,\r\n    \"learning_rate\": 3e-3,\r\n    \"batch_size\": 64\r\n}\r\n\r\nsatnet = SatNet(hparams)\r\n \r\ntrainer = pl.Trainer(gpus = -1, max_epochs = 1, callbacks = [checkpoint_callback])\r\ntrainer.fit(satnet)\r\ntrainer.save_checkpoint(\"models\\\\newPLModelTest\\\\finalEpochSatnet.ckpt\")\r\n\r\n#Find layer in/out shapes\r\nfrom pytorch_lightning.utilities.model_summary import ModelSummary, LayerSummary\r\n\r\nmodel = SatNet.load_from_checkpoint(\"models\\\\newPLModelTest\\\\finalEpochSatnet.ckpt\")\r\nlayers = list(model.named_modules())\r\n\r\nlayer = layers[4][1]\r\nsummary = LayerSummary(layer)\r\nprint (summary.in_size) #RETURNS '?'\r\n\r\n```\r\n\r\n</p>\r\n</details>when you do this:\r\n```py\r\nmodel = SatNet.load_from_checkpoint(\"models\\\\newPLModelTest\\\\finalEpochSatnet.ckpt\")\r\n```\r\na new model is instantiated from scratch. Now since PyTorch modules are dynamic, they don't have input shape/output shape configured by default until a computation/forward pass is done through that layer when module is registered with `LayerSummary`. So you need to do a forward pass using a sample input to compute the required sizes. Check out the example linked in the docs.\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.model_summary.html#pytorch_lightning.utilities.model_summary.LayerSummary",
    "meta": { "name": "LayerSummary.input_size returns '?'" },
    "answer": "when you do this:\r\n```py\r\nmodel = SatNet.load_from_checkpoint(\"models\\\\newPLModelTest\\\\finalEpochSatnet.ckpt\")\r\n```\r\na new model is instantiated from scratch. Now since PyTorch modules are dynamic, they don't have input shape/output shape configured by default until a computation/forward pass is done through that layer when module is registered with `LayerSummary`. So you need to do a forward pass using a sample input to compute the required sizes. Check out the example linked in the docs.\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.utilities.model_summary.html#pytorch_lightning.utilities.model_summary.LayerSummary"
  },
  {
    "content": "Hi all,\r\n\r\nToday I tried to use DDP to accelerate my training. When not using it, I trained my model with batchsize=6 on 1 GPU. The iteration step per epoch is 6073 and the training time for the first epoch is 1h 29m. When using DDP with 1 node and 4 GPUs, the iteration step per epoch is still 6073 and the training time for the first epoch is 1h 9m. The batchsize was not changed. It seems that DDP is not working. I am using DDP as follows:\r\n`trainer = pl.Trainer(\r\n        accelerator=\"gpu\", devices=4, num_nodes=1, strategy=\"ddp\",\r\n        default_root_dir=models_save_path,`\r\nThe training log without DDP is:\r\n![image](https://user-images.githubusercontent.com/5463229/163566564-5af416b6-f9c5-43a3-8c83-377586f469c2.png)\r\nThe log with DDP is:\r\n![image](https://user-images.githubusercontent.com/5463229/163566586-79e26f6e-56cf-4321-b73b-aa7f8c0e32fa.png)\r\n\r\nAny suggestion?looks like in your case, DDP is not triggered for some reason since if you are not changing the batch_size and total batches in the progress bar should be reduced with DDP on 4 GPUs.\r\n\r\ndid you see any logs like this when you call `trainer.fit` ??\r\n```console\r\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\r\nInitializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\r\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\r\nInitializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\r\n```",
    "meta": { "name": "DDP is not accelerating my training" },
    "answer": "looks like in your case, DDP is not triggered for some reason since if you are not changing the batch_size and total batches in the progress bar should be reduced with DDP on 4 GPUs.\r\n\r\ndid you see any logs like this when you call `trainer.fit` ??\r\n```console\r\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\r\nInitializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\r\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\r\nInitializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\r\n```"
  },
  {
    "content": "```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-9-303af62577c3> in <module>\r\n     65           n=30, d_model=50,\r\n     66           stage1_med_dim=50, stage1_out_dim=100, stage2_out_dim=200,\r\n---> 67           epoch=10\r\n     68           )\r\n\r\n<ipython-input-9-303af62577c3> in train_mct(config, n, d_model, stage1_med_dim, stage1_out_dim, stage2_out_dim, epoch)\r\n     49     trainer = Trainer(**kwargs)\r\n     50     trainer.fit(model, train_dataloaders=train_dataloader,\r\n---> 51                 val_dataloaders=val_dataloader)\r\n     52 \r\n     53 \r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in fit(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\r\n    739             train_dataloaders = train_dataloader\r\n    740         self._call_and_handle_interrupt(\r\n--> 741             self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n    742         )\r\n    743 \r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _call_and_handle_interrupt(self, trainer_fn, *args, **kwargs)\r\n    683         \"\"\"\r\n    684         try:\r\n--> 685             return trainer_fn(*args, **kwargs)\r\n    686         # TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\r\n    687         except KeyboardInterrupt as exception:\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\r\n    775         # TODO: ckpt_path only in v1.7\r\n    776         ckpt_path = ckpt_path or self.resume_from_checkpoint\r\n--> 777         self._run(model, ckpt_path=ckpt_path)\r\n    778 \r\n    779         assert self.state.stopped\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _run(self, model, ckpt_path)\r\n   1197 \r\n   1198         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n-> 1199         self._dispatch()\r\n   1200 \r\n   1201         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _dispatch(self)\r\n   1277             self.training_type_plugin.start_predicting(self)\r\n   1278         else:\r\n-> 1279             self.training_type_plugin.start_training(self)\r\n   1280 \r\n   1281     def run_stage(self):\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py in start_training(self, trainer)\r\n    200     def start_training(self, trainer: \"pl.Trainer\") -> None:\r\n    201         # double dispatch to initiate the training loop\r\n--> 202         self._results = trainer.run_stage()\r\n    203 \r\n    204     def start_evaluating(self, trainer: \"pl.Trainer\") -> None:\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in run_stage(self)\r\n   1287         if self.predicting:\r\n   1288             return self._run_predict()\r\n-> 1289         return self._run_train()\r\n   1290 \r\n   1291     def _pre_training_routine(self):\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _run_train(self)\r\n   1309             self.progress_bar_callback.disable()\r\n   1310 \r\n-> 1311         self._run_sanity_check(self.lightning_module)\r\n   1312 \r\n   1313         # enable train mode\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _run_sanity_check(self, ref_model)\r\n   1373             # run eval step\r\n   1374             with torch.no_grad():\r\n-> 1375                 self._evaluation_loop.run()\r\n   1376 \r\n   1377             self.call_hook(\"on_sanity_check_end\")\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\loops\\base.py in run(self, *args, **kwargs)\r\n    143             try:\r\n    144                 self.on_advance_start(*args, **kwargs)\r\n--> 145                 self.advance(*args, **kwargs)\r\n    146                 self.on_advance_end()\r\n    147                 self.restarting = False\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py in advance(self, *args, **kwargs)\r\n    108         dl_max_batches = self._max_batches[dataloader_idx]\r\n    109 \r\n--> 110         dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\r\n    111 \r\n    112         # store batch level output per dataloader\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\loops\\base.py in run(self, *args, **kwargs)\r\n    138         self.reset()\r\n    139 \r\n--> 140         self.on_run_start(*args, **kwargs)\r\n    141 \r\n    142         while not self.done:\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py in on_run_start(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\r\n     84 \r\n     85         self._reload_dataloader_state_dict(data_fetcher)\r\n---> 86         self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\r\n     87 \r\n     88     def advance(\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py in _update_dataloader_iter(data_fetcher, batch_idx)\r\n    119     if not isinstance(data_fetcher, DataLoaderIterDataFetcher):\r\n    120         # restore iteration\r\n--> 121         dataloader_iter = enumerate(data_fetcher, batch_idx)\r\n    122     else:\r\n    123         dataloader_iter = iter(data_fetcher)\r\n\r\n~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py in __iter__(self)\r\n    195             raise MisconfigurationException(\"The iterate hasn't been provided. HINT: Did you call setup function ?.\")\r\n    196         self.reset()\r\n--> 197         self.dataloader_iter = iter(self.dataloader)\r\n    198         self._apply_patch()\r\n    199         self.prefetching(self.prefetch_batches)\r\n```\r\nAnd here is my code.\r\n```\r\ndef train_mct(config, epoch):\r\n    # prepare dir for model path\r\n\r\n    # seed everything\r\n    seed_everything(seed=9876, workers=True)\r\n\r\n    def train_dataloader(self):\r\n        # expect to get train folder\r\n        dataset = load_train_data(df)\r\n        dataloader = DataLoader(dataset, batch_size=32, num_workers=0, persistent_workers=True,\r\n                                collate_fn=dataset_collate_function, shuffle=True)\r\n\r\n        return dataloader\r\n\r\n    def val_dataloader(self):\r\n        # expect to get validation folder\r\n        dataset = load_train_data(df)\r\n        dataloader = DataLoader(dataset, num_workers=0, persistent_workers=True,\r\n                                collate_fn=dataset_collate_function, shuffle=False)\r\n\r\n        return dataloader\r\n\r\n    model = MCT(config=config).float()\r\n\r\n    kwargs = {\r\n        \"max_epochs\": epoch,\r\n        \"enable_progress_bar\": False,\r\n        \"val_check_interval\": 1.0,\r\n        \"callbacks\": [\r\n            EarlyStopping(\r\n                monitor='training_loss',\r\n                patience=20, mode='min',\r\n                check_on_train_epoch_end=True)\r\n        ]\r\n    }\r\n\r\n    trainer = Trainer(**kwargs)\r\n    trainer.fit(model, train_dataloaders=train_dataloader,\r\n                val_dataloaders=val_dataloader)\r\n\r\nconfig = {\r\n    \"n_layers_s1\": 3,\r\n    \"n_layers_s2\": 3,\r\n    \"n_heads\": 5,\r\n    \"d_ff\": 1024,\r\n    \"lr\": 0.01,\r\n    \"app_weight\": 4,\r\n    \"signal_length\": 1500,\r\n    \"res_s0\": True\r\n}\r\ntrain_mct(config, epoch=10)\r\n```\r\nAnd advice on what happpend.\r\nThank you!update\r\n```py\r\ntrainer.fit(model, train_dataloaders=train_dataloader,\r\n                val_dataloaders=val_dataloader)\r\n```\r\nto\r\n```py\r\ntrainer.fit(model, train_dataloaders=train_dataloader(),\r\n                val_dataloaders=val_dataloader())\r\n```\r\nsince `train_dataloader/val_dataloader` is a function in your case.",
    "meta": { "name": "TypeError: 'function' object is not iterable" },
    "answer": "update\r\n```py\r\ntrainer.fit(model, train_dataloaders=train_dataloader,\r\n                val_dataloaders=val_dataloader)\r\n```\r\nto\r\n```py\r\ntrainer.fit(model, train_dataloaders=train_dataloader(),\r\n                val_dataloaders=val_dataloader())\r\n```\r\nsince `train_dataloader/val_dataloader` is a function in your case."
  },
  {
    "content": "When I was training my model,  I got an error .\r\nTraceback is as follows:\r\n** \r\nFile \"/home/datasets/traffic_classification/ml/MCT.py\", line 355, in __init__\r\n self.fusion = Fusion_Block(n, d_model, self.n_layers_s1, self.n_layers_s2, self.n_heads, self.d_ff,\r\nFile \"/home/datasets/traffic_classification/ml/MCT.py\", line 313, in __init__\r\nself.c2trans = CNN_to_Trans(n, d_model)\r\nFile \"/usr/local/miniconda3/envs/deep_packet/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 92, in __init__\r\nsuper().__init__(*args, **kwargs)\r\nTypeError: __init__() takes 1 positional argument but 3 were given\r\n**\r\nHere is my code.\r\n```\r\nclass CNN_to_Trans(LightningModule):\r\n    def __int__(self, n, d_model):\r\n        super().__init__() \r\n        self.pooling = nn.AdaptiveAvgPool1d(n)\r\n        self.conv = nn.Conv1d(in_channels=n*d_model, out_channels=d_model, kernel_size=1,stride=1)\r\n        self.norm = nn.LayerNorm(d_model)\r\n        self.act_f = nn.ReLU()\r\n    def forward(self, x):\r\n        x = self.conv(x.transpose(1,2))\r\n        x = self.pooling(x).transpose(1,2)\r\n        x = self.norm(x)\r\n        x = self.act_f(x)\r\n        return x\r\n```\r\n\r\n```\r\nclass Fusion_Block(LightningModule):\r\n    def __init__(self, n, d_model, n_layers_s1, n_layers_s2, n_heads, d_ff, kernel_size, stage1_med_dim, stage1_out_dim, stage2_out_dim):\r\n        super(Fusion_Block, self).__init__()\r\n        ... ...\r\n        self.c2trans = CNN_to_Trans(n, d_model)\r\n        ... ...\r\n\r\n    def forward(...):\r\n        ... ...\r\n```\r\n\r\nThank you!@chriswangky  Hi, is it a typo? `__int__` --> `__init__`",
    "meta": {
      "name": "TypeError: __init__() takes 1 positional argument but 3 were given"
    },
    "answer": "@chriswangky  Hi, is it a typo? `__int__` --> `__init__`"
  },
  {
    "content": "I am currently building a big dataset and infrastructure using PyTorch Lightning. Unfortunately there is no precise information in the documentation.\r\n\r\nMy code can be seen as:\r\n```python\r\ndatamodule = DataModule(\r\n        transform=economy_average_vs_outcome,\r\n        download=False,\r\n        batch_size=2,\r\n        num_workers=4,\r\n    )\r\n# Preparing the data:\r\ndatamodule.prepare_data()\r\ndatamodule.setup()\r\nlogistic_regression = LogisticRegression(input_dim=2 * 39, num_classes=2)\r\nlogger = TensorBoardLogger(\"tb_logs\", name=\"Logistic Regression\")\r\ntrainer = pl.Trainer(\r\n        logger=logger,\r\n        accelerator=\"gpu\",\r\n        devices=1,\r\n        auto_select_gpus=True,\r\n        max_epochs=50,\r\n        log_every_n_steps=2,\r\n    )\r\n\r\n\r\n# Training the model:\r\ntrainer.fit(model=logistic_regression, datamodule=datamodule)\r\n```\r\n\r\nKeep in mind that this is sample code that is not final.\r\nIndependent of that there is no Tensorboard visualizations available.\r\n\r\n![image](https://user-images.githubusercontent.com/34846245/163412636-00dc7934-8cd6-49e6-8dc7-8a4dbca2257e.png)\r\n\r\n![image](https://user-images.githubusercontent.com/34846245/163412749-c3f96aa6-d2ed-4b6e-b2d0-1fe41eaefad4.png)\r\n\r\nThe code for ```LogisticRegression``` is imported from ```pl_bolts``` as follows:\r\n\r\n```\r\nfrom pl_bolts.models.regression import LogisticRegression\r\n```\r\n\r\nI should not be required to read the source code for the ```LogisticRegression``` to figure out what is the internal logging implementation.\r\n\r\nThe ```training_step``` is as follows coming from ```pl_bolts```:\r\n```python\r\n    def training_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Dict[str, Tensor]:\r\n        x, y = batch\r\n\r\n        # flatten any input\r\n        x = x.view(x.size(0), -1)\r\n\r\n        y_hat = self.linear(x)\r\n\r\n        # PyTorch cross_entropy function combines log_softmax and nll_loss in single function\r\n        loss = F.cross_entropy(y_hat, y, reduction=\"sum\")\r\n\r\n        # L1 regularizer\r\n        if self.hparams.l1_strength > 0:\r\n            l1_reg = self.linear.weight.abs().sum()\r\n            loss += self.hparams.l1_strength * l1_reg\r\n\r\n        # L2 regularizer\r\n        if self.hparams.l2_strength > 0:\r\n            l2_reg = self.linear.weight.pow(2).sum()\r\n            loss += self.hparams.l2_strength * l2_reg\r\n\r\n        loss /= x.size(0)\r\n\r\n        tensorboard_logs = {\"train_ce_loss\": loss}\r\n        progress_bar_metrics = tensorboard_logs\r\n        return {\"loss\": loss, \"log\": tensorboard_logs, \"progress_bar\": progress_bar_metrics}\r\n```the code in bolts is outdated and logging doesn't work like that anymore.\r\nyou need to do:\r\n```py\r\ndef training_step(self, batch, batch_idx):\r\n    ...\r\n    self.log(\"train_ce_loss\", loss, prog_bar=True)\r\n```\r\n\r\nmore info herE: https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html#logging-from-a-lightningmodule",
    "meta": {
      "name": "TensorboardLogger not displaying metrics for LogisticRegression"
    },
    "answer": "the code in bolts is outdated and logging doesn't work like that anymore.\r\nyou need to do:\r\n```py\r\ndef training_step(self, batch, batch_idx):\r\n    ...\r\n    self.log(\"train_ce_loss\", loss, prog_bar=True)\r\n```\r\n\r\nmore info herE: https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html#logging-from-a-lightningmodule"
  },
  {
    "content": "I am employing MULTI-GPU training using pytorch lightning. The below output displays the model:\r\n```\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n\u250f\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\r\n\u2503    \u2503 Name       \u2503 Type              \u2503 Params \u2503\r\n\u2521\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\r\n\u2502 0  \u2502 encoder    \u2502 Encoder           \u2502  2.0 M \u2502\r\n\u2502 1  \u2502 classifier \u2502 Sequential        \u2502  8.8 K \u2502\r\n\u2502 2  \u2502 criterion  \u2502 BCEWithLogitsLoss \u2502      0 \u2502\r\n\u2502 3  \u2502 train_acc  \u2502 Accuracy          \u2502      0 \u2502\r\n\u2502 4  \u2502 val_acc    \u2502 Accuracy          \u2502      0 \u2502\r\n\u2502 5  \u2502 train_auc  \u2502 AUROC             \u2502      0 \u2502\r\n\u2502 6  \u2502 val_auc    \u2502 AUROC             \u2502      0 \u2502\r\n\u2502 7  \u2502 train_f1   \u2502 F1Score           \u2502      0 \u2502\r\n\u2502 8  \u2502 val_f1     \u2502 F1Score           \u2502      0 \u2502\r\n\u2502 9  \u2502 train_mcc  \u2502 MatthewsCorrCoef  \u2502      0 \u2502\r\n\u2502 10 \u2502 val_mcc    \u2502 MatthewsCorrCoef  \u2502      0 \u2502\r\n\u2502 11 \u2502 train_sens \u2502 Recall            \u2502      0 \u2502\r\n\u2502 12 \u2502 val_sens   \u2502 Recall            \u2502      0 \u2502\r\n\u2502 13 \u2502 train_spec \u2502 Specificity       \u2502      0 \u2502\r\n\u2502 14 \u2502 val_spec   \u2502 Specificity       \u2502      0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nTrainable params: 2.0 M\r\nNon-trainable params: 0\r\n```\r\n\r\nI have set Encoder to be untrainable using the below code:\r\n\r\n```\r\nckpt = torch.load(chk_path)\r\nself.encoder.load_state_dict(ckpt['state_dict'])\r\nself.encoder.requires_grad = False\r\n```\r\n\r\nShouldn't ```trainable params``` be ```8.8 K``` rather than ```2.0 M``` ? \r\n\r\nMy optimizer is the following:\r\n```\r\noptimizer =  torch.optim.RMSprop(filter(lambda p: p.requires_grad, self.parameters()), lr =self.lr, weight_decay = self.weight_decay)\r\n```I don't think that is the right way to turn off the gradients:\r\nhttps://stackoverflow.com/questions/51748138/pytorch-how-to-set-requires-grad-false",
    "meta": { "name": "PyTorch Lightning (Trainable Params - Wrong)" },
    "answer": "I don't think that is the right way to turn off the gradients:\r\nhttps://stackoverflow.com/questions/51748138/pytorch-how-to-set-requires-grad-false"
  },
  {
    "content": "We use CacheDataset [MONAI CacheDataset](https://docs.monai.io/en/stable/data.html#monai.data.CacheDataset) to speed up data loading. However, when combining the lightning module's standard training code with DDP strategy and multi-GPU environment, the cached dataset is not working as expected:\r\n\r\nIf provided with a full length of data in the CacheDataset, the initial epoch takes forever to load because each GPU will try to read in and cache ALL data, which is unnecessary because in DDP each GPU will only use a portion of the data.\r\n\r\nA workaround is mentioned in here [MONAI issue](https://github.com/Project-MONAI/MONAI/issues/1589), which mentioning to partition data before feeding into the CacheDataset:\r\n[MONAI Tutorial](https://github.com/Project-MONAI/tutorials/blob/master/acceleration/distributed_training/unet_training_smartcache.py#L120)\r\n\r\nHowever, if I make the partitioning in the [setup()](https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html#setup) function, the trainer will train for total_data_length // num_gpus samples each epoch instead of total_ data_length.\r\n\r\nAnd if I put the CacheDataset with full data length in the prepare_data function, the subprocess's object can't access the dataset instance (saved in self.x, which is not recommended).\r\n\r\nSo what's the best practical way to handle this? My gut feeling is that I should use the partitioned dataset on each GPU, and let the loader use the full length of dataset instead of part of it. Any suggestions?hey @bill-yc-chen \r\n\r\nsince DDP executes scripts independently across devices, maybe try DDP_Spawn instead?\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/advanced/training_tricks.html#sharing-datasets-across-process-boundaries",
    "meta": { "name": "CacheDataset with DDP and Multi-GPUs" },
    "answer": "hey @bill-yc-chen \r\n\r\nsince DDP executes scripts independently across devices, maybe try DDP_Spawn instead?\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/advanced/training_tricks.html#sharing-datasets-across-process-boundaries"
  },
  {
    "content": "Hey,\r\n\r\n**Question**: How can I set a module/layer in my model-class to always be non-deterministic (irrespective of the `deterministic` flag in `pl.Trainer()`)?\r\n\r\n**Context**: I use `pl` to train a simple AutoEncoder that uses bilinear upscaling in the decoder part. For debugging, I use the `deterministic` flag of the `pl.Trainer()`. However, I receive the following error\r\n\r\n```\r\nRuntimeError: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.\r\n```\r\n\r\nUnfortunately, the error does not hint at how to set the module to be non-deterministic and neither does the documentation.\r\n\r\nCheers\r\ndsethzAny feedback on this issue?Hi @dsethz!\r\n\r\nThe error comes from PyTorch, but not from Lightning, and I think it's not (shouldn't be) feasible even in pure PyTorch because the flag is for reproducibility and if you allow randomness in certain layers, you can't reproduce the same result anymore.\r\n\r\nhttps://pytorch.org/docs/stable/notes/randomness.html",
    "meta": { "name": "How to flag certain modules as non-deterministic" },
    "answer": "Hi @dsethz!\r\n\r\nThe error comes from PyTorch, but not from Lightning, and I think it's not (shouldn't be) feasible even in pure PyTorch because the flag is for reproducibility and if you allow randomness in certain layers, you can't reproduce the same result anymore.\r\n\r\nhttps://pytorch.org/docs/stable/notes/randomness.html"
  },
  {
    "content": "In my custom `pl.LightningModule`, I try to specify two learning rate schedulers (`warming up` in the first epoch, and `multi-step` scheduler in the following epochs) for 1 `optimizer` as follows:\r\n\r\n```\r\n    def configure_optimizers(self):\r\n        args = self.args\r\n\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\r\n\r\n        warmup_iters = min(1000, self.num_batches - 1)\r\n        warmup_scheduler = {\r\n                    'scheduler': utils.warmup_lr_scheduler(optimizer, warmup_iters, 1./1000),\r\n                    'name': 'learning_rate',\r\n                    'interval':'step',\r\n                    'frequency': 1\r\n                }\r\n\r\n        lr_scheduler = {\r\n            'scheduler': torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.lr_steps, gamma=args.lr_gamma),\r\n            'name': 'learning_rate',\r\n            'interval':'epoch',\r\n            'frequency': 1\r\n        }\r\n        return [optimizer], [warmup_scheduler, lr_scheduler]\r\n```\r\nHowever, it seems like the learning rate in the `optimizer` only controlled by the first `warmup_scheduler`. \r\n\r\nSo how can I create 2 learning rate schedulers for 1 optimizer?\r\nMany thanks.The answer of my question would be involved and solve the question in https://github.com/PyTorchLightning/pytorch-lightning/discussions/6234.\r\nThus, I am still looking for the answer :)Dear @davidnvq,\r\n\r\nI don't believe there is a simple out-of-box solution in Lightning right now.\r\n\r\nHere are the 2 suggested approach:\r\n\r\n* Implement a callback which handles lr_scheduler stepping.\r\n* Fuse your 2 scheduler into 1.\r\nCould you maybe use a [ChainedScheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ChainedScheduler.html#chainedscheduler)?",
    "meta": { "name": "Create 2 learning rate schedulers for 1 optimizer" },
    "answer": "The answer of my question would be involved and solve the question in https://github.com/PyTorchLightning/pytorch-lightning/discussions/6234.\r\nThus, I am still looking for the answer :)"
  },
  {
    "content": "At the moment, I am calling `trainer.fit(model=model, datamodule=datamodule)` and the training works fine.\r\nHowever, what I want to do is return the final value of all the metrics that have been logged during the training.\r\n\r\nIn my case, the training and validation steps looks as follows:\r\n\r\n```\r\ndef validation_epoch_end(self, outputs: EPOCH_OUTPUT) -> None:\r\n        loss = torch.stack(outputs).mean()\r\n        self.log(\"val/loss\", loss)\r\n\r\ndef training_epoch_end(self, outputs: EPOCH_OUTPUT) -> None:\r\n        loss = torch.stack(outputs).mean()\r\n        self.log(\"train/loss\", loss)\r\n```\r\n\r\nBasically, at the end of the training, I would like to query the final values of `train/loss` and `val/loss` and basically every other metrics that have been logged.\r\n\r\n`trainer.callback_metrics` <- to access all the metrics\r\n`trainer.logged_metrics` <- to access only the logged metrics",
    "meta": {
      "name": "How can I get the final value of all the metrics that have been logged"
    },
    "answer": "`trainer.callback_metrics` <- to access all the metrics\r\n`trainer.logged_metrics` <- to access only the logged metrics"
  },
  {
    "content": "![image](https://user-images.githubusercontent.com/75936962/162600471-b3cb9237-f255-4d3c-828d-4150be132661.png)\r\n\r\n```python\r\nimport torch\r\nfrom torch import Tensor\r\nfrom pytorch_lightning import LightningModule\r\nclass Generator:\r\n    def __init__(self):\r\n        pass\r\n    def forward(self):\r\n        pass\r\n\r\nclass Discriminator:\r\n    def __init__(self):\r\n        pass\r\n    def forward(self):\r\n        pass\r\nclass SimpleGAN(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.G = Generator()\r\n        self.D = Discriminator()\r\n\r\n        # Important: This property activates manual optimization.\r\n        self.automatic_optimization = False\r\n\r\n    def sample_z(self, n) -> Tensor:\r\n        sample = self._Z.sample((n,))\r\n        return sample\r\n\r\n    def sample_G(self, n) -> Tensor:\r\n        z = self.sample_z(n)\r\n        return self.G(z)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # Implementation follows the PyTorch tutorial:\r\n        # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\r\n        g_opt, d_opt = self.optimizers()\r\n\r\n        X, _ = batch\r\n        batch_size = X.shape[0]\r\n\r\n        real_label = torch.ones((batch_size, 1), device=self.device)\r\n        fake_label = torch.zeros((batch_size, 1), device=self.device)\r\n\r\n        g_X = self.sample_G(batch_size)\r\n\r\n        ##########################\r\n        # Optimize Discriminator #\r\n        ##########################\r\n        d_x = self.D(X)\r\n        errD_real = self.criterion(d_x, real_label)\r\n\r\n        d_z = self.D(g_X.detach())\r\n        errD_fake = self.criterion(d_z, fake_label)\r\n\r\n        errD = errD_real + errD_fake\r\n\r\n        d_opt.zero_grad()\r\n        self.manual_backward(errD)\r\n        d_opt.step()\r\n\r\n        ######################\r\n        # Optimize Generator #\r\n        ######################\r\n        d_z = self.D(g_X)\r\n        errG = self.criterion(d_z, real_label)\r\n\r\n        g_opt.zero_grad()\r\n        self.manual_backward(errG)\r\n        g_opt.step()\r\n\r\n        self.log_dict({\"g_loss\": errG, \"d_loss\": errD}, prog_bar=True)\r\n\r\n    def configure_optimizers(self):\r\n        g_opt = torch.optim.Adam(self.G.parameters(), lr=1e-5)\r\n        d_opt = torch.optim.Adam(self.D.parameters(), lr=1e-5)\r\n        return g_opt, d_opt\r\nbatch=torch.randn(3,2)\r\nbatch_idx=torch.ones(3)\r\nSimpleGAN().training_step(batch,batch_idx)\r\n```Hi @Hou-jing!\r\n\r\n`pl.LightningModule` is supposed to be used with `pl.Trainer`. Would you mind trying it again with the trainer?When I upgraded the version to the latest version, I solved this problem\u3002\r\nHowever, my running speed has been greatly affected. I used to have an epoch every 8 minutes, but now it has been delayed for a long time, and the data can't be loaded. I don't know why\r\nAnd this is the code\r\n[https://colab.research.google.com/drive/1dCP7-1xK48-PohGc8-RKx3Ne2HWd4Jkq#scrollTo=frTD9xWvBEUT]\r\n\r\nI think \uff0cBecause the loss is calculated twice, the time is delayed",
    "meta": {
      "name": "When I use the official sample to carry out back propagation manually, I make mistakes. First, there is no optimizer, and second, there is no attribute in the image"
    },
    "answer": "When I upgraded the version to the latest version, I solved this problem\u3002\r\nHowever, my running speed has been greatly affected. I used to have an epoch every 8 minutes, but now it has been delayed for a long time, and the data can't be loaded. I don't know why\r\nAnd this is the code\r\n[https://colab.research.google.com/drive/1dCP7-1xK48-PohGc8-RKx3Ne2HWd4Jkq#scrollTo=frTD9xWvBEUT]\r\n\r\n"
  },
  {
    "content": "Hi,\r\nit would be great if you can help me unravel, what is a mystery to me.\r\n\r\n**Background**\r\nI have adapted a pretrained model for image regression. \r\n\r\n**Issue :**\r\nIf I finetune the model using the lightning trainer, the training loss stagnates at a value of ~10. However, in my pytorch training implementation training and validation loss become much less.\r\n\r\nCan you help me understand where my mistake is? Did I implement `.train_step` and `.forward` correctly?\r\n\r\nPLModule:\r\n```\r\nclass RGBYieldRegressor(LightningModule):\r\n    def __init__(self, optimizer:str = 'sgd', k:int = 0, lr:float = 0.001, momentum:float = 0.8, wd:float = 0.01, batch_size:int = 16, pretrained:bool = True):\r\n        super().__init__()\r\n\r\n        self.lr = lr\r\n        self.momentum = momentum\r\n        self.wd = wd\r\n        self.batch_size = batch_size\r\n        self.k = k\r\n\r\n        optimizers = {'adam': Adam, 'sgd': SGD}\r\n        self.optimizer = optimizers[optimizer]\r\n\r\n        self.criterion = nn.MSELoss(reduction='mean')\r\n\r\n        self.model_arch = model\r\n\r\n        num_target_classes = 1\r\n\r\n        self.model = models.resnet50(pretrained=pretrained)\r\n        num_filters = self.model.fc.in_features\r\n        self.model.fc = nn.Sequential(\r\n            nn.ReLU(),\r\n            nn.Linear(num_filters, num_target_classes))\r\n\r\n    def forward(self, x):\r\n        return torch.flatten(self.model(x))\r\n\r\n    def training_step(self, batch, batch_idx): # torch.autograd?\r\n        x, y = batch\r\n        y_hat = torch.flatten(self.model(x))\r\n        loss = self.criterion(y, y_hat)\r\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = torch.flatten(self.model(x))\r\n        loss = self.criterion(y, y_hat)\r\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = torch.flatten(self.model(x))\r\n        loss = self.criterion(y, y_hat)\r\n        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\r\n\r\n    def predicts_step(self, batch, batch_idx, dataloader_idx=0):\r\n        return self.model(batch).squeeze()\r\n\r\n    def configure_optimizers(self):\r\n        return self.optimizer(self.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.wd)\r\n\r\n```\r\n\r\nTrainer:\r\n```\r\ntrainer = Trainer(\r\n            max_epochs=50,  # general\r\n            num_sanity_val_steps=0,\r\n            devices=1,\r\n            accelerator=\"auto\",\r\n            callbacks=callbacks,\r\n            default_root_dir=this_output_dir,\r\n            weights_save_path=this_output_dir,\r\n            logger=logger,\r\n            num_processes=1,  \r\n        )\r\n        trainer.fit(lightningmodule, train_dataloaders=datamodule.train_dataloader(), val_dataloaders=datamodule.val_dataloader())\r\n```\r\n\r\nvs. pytorch training:\r\n```\r\nfor phase in ['train', 'val']:\r\n    if phase == 'train':\r\n        model.train()  # Set model to training mode\r\n    else:\r\n        model.eval()   # Set model to evaluate mode\r\n    running_loss = 0.0\r\n\r\n    # Iterate over data.\r\n    for inputs, labels in dataloaders[phase]:\r\n        inputs = inputs.to(device)\r\n        labels = labels.to(device)\r\n\r\n        # zero the parameter gradients\r\n        optimizer.zero_grad()\r\n\r\n        # forward\r\n        # track history if only in train\r\n        with torch.set_grad_enabled(phase == 'train'):\r\n            outputs = model(inputs)\r\n            loss = criterion(torch.flatten(outputs), labels.data)\r\n            if phase == 'train':\r\n                loss.backward()\r\n                optimizer.step()\r\n        # statistics\r\n        running_loss += loss.item() * inputs.size(0)\r\n\r\n    epoch_loss = running_loss / len(dataloaders[phase].dataset)\r\n    print('{} Loss: {:.4f}'.format(phase, epoch_loss))\r\n```\r\nupdate:\r\n```py\r\nloss = self.criterion(y, y_hat)\r\n```\r\nto\r\n```py\r\nloss = self.criterion(y_hat, y)\r\n```\r\neverywhere.\r\n\r\nalso:\r\n```py\r\ntrainer.fit(lightningmodule, train_dataloaders=datamodule.train_dataloader(), val_dataloaders=datamodule.val_dataloader())\r\n```\r\ncan be just\r\n```py\r\ntrainer.fit(lightningmodule, datamodule=datamodule)\r\n```Thanks for the swift reply!\r\nIndeed, I see this having an effect.\r\n\r\nBut I have difficulties understanding why. It is mean **squared** error, meaning that `self.criterion(y, y_hat) == self.criterion(y_hat, y)`, should it not?\r\nSo, why does it have an effect? Because of gradient computation?",
    "meta": {
      "name": "Trainer: loss stagnates, whereas custom train implementation continues converging ??"
    },
    "answer": "update:\r\n```py\r\nloss = self.criterion(y, y_hat)\r\n```\r\nto\r\n```py\r\nloss = self.criterion(y_hat, y)\r\n```\r\neverywhere.\r\n\r\nalso:\r\n```py\r\ntrainer.fit(lightningmodule, train_dataloaders=datamodule.train_dataloader(), val_dataloaders=datamodule.val_dataloader())\r\n```\r\ncan be just\r\n```py\r\ntrainer.fit(lightningmodule, datamodule=datamodule)\r\n```"
  },
  {
    "content": "When I trained on multi-GPU, the model weight file is corrupted. I guess the reason is that multi-gpus are saving the model weight at the same time. So how can I call the CheckpointCallback on a specific GPU?checkpoints are saved only on global rank 0 which means only one of the GPUs will save the checkpoint even in the case of multi-node training. The issue might be something else.",
    "meta": {
      "name": "model weight file corrupted when training on multi-gpus"
    },
    "answer": "checkpoints are saved only on global rank 0 which means only one of the GPUs will save the checkpoint even in the case of multi-node training. The issue might be something else."
  },
  {
    "content": "I call `save_hyperparameters()` in `__init__()`, and all hyper parameters sent to PL model are saved to checkpoint file. However, when i resume training from a checkpoint(call `trainer.fit(..., ckpt_path=checkpoint_file_path)`), the hyper parameters are not restored from checkpoint file and all of them keep initial values.hyperparameters are not restored by default because it allows users to update them if they want, using the checkpoint, while resuming.\r\n\r\nyou can do this:\r\n```py\r\nmodel = LitModel.load_from_checkpoint(checkpoint_file_path)\r\ntrainer.fit(model, ..., ckpt_path=checkpoint_file_path)\r\n```",
    "meta": { "name": "hyper parameters not restored while resuming training" },
    "answer": "hyperparameters are not restored by default because it allows users to update them if they want, using the checkpoint, while resuming.\r\n\r\nyou can do this:\r\n```py\r\nmodel = LitModel.load_from_checkpoint(checkpoint_file_path)\r\ntrainer.fit(model, ..., ckpt_path=checkpoint_file_path)\r\n```"
  },
  {
    "content": "Can I define multi training dataloaders in LightningDataModule? \r\n```\r\n    def train_dataloader(self):\r\n        lab_loader = torch.utils.data.DataLoader(\r\n                         self.train_subset_lab_train,\r\n                         batch_size=self.batch_size,\r\n                         shuffle=True,\r\n                         num_workers=self.num_workers,\r\n                         pin_memory=True,\r\n                         drop_last=True,\r\n                    )\r\n        unlab_loader = torch.utils.data.DataLoader(\r\n                         self.train_subset_unlab_train,\r\n                         batch_size=self.batch_size*self.uratio,\r\n                         shuffle=True,\r\n                         num_workers=self.num_workers,\r\n                         pin_memory=True,\r\n                         drop_last=True,\r\n                    )\r\n\r\n        return [lab_loader, unlab_loader]\r\n```yes@kleinzcy @tshu-w,\r\n\r\nPyTorch Lightning supports multiple data loaders but they will be sampled at the same time and return a batch composed of a batch sample for each dataloaders return for the train_dataloader method.\r\n\r\nAlternatively, if you want to make this sequential, you can implement a wrapper which will sample from one and the other dataloader.\r\n\r\n```py\r\nclass SequentialLoader(Iterator):\r\n\r\n    def __init__(self, *dataloaders):\r\n        self.dataloaders = dataloaders\r\n\r\n    def __len__(self):\r\n        return sum([len(dl) for dl in self.dataloaders])\r\n\r\n    def __iter__(self):\r\n        for dl in self.dataloaders:\r\n            dataloader_iter = iter(dl)\r\n            for batch in dataloader_iter:\r\n                yield batch\r\n```",
    "meta": {
      "name": "Do LightningDataModule support multi train dataloader like LightningModule?"
    },
    "answer": "@kleinzcy @tshu-w,\r\n\r\nPyTorch Lightning supports multiple data loaders but they will be sampled at the same time and return a batch composed of a batch sample for each dataloaders return for the train_dataloader method.\r\n\r\nAlternatively, if you want to make this sequential, you can implement a wrapper which will sample from one and the other dataloader.\r\n\r\n```py\r\nclass SequentialLoader(Iterator):\r\n\r\n    def __init__(self, *dataloaders):\r\n        self.dataloaders = dataloaders\r\n\r\n    def __len__(self):\r\n        return sum([len(dl) for dl in self.dataloaders])\r\n\r\n    def __iter__(self):\r\n        for dl in self.dataloaders:\r\n            dataloader_iter = iter(dl)\r\n            for batch in dataloader_iter:\r\n                yield batch\r\n```"
  },
  {
    "content": "I have a metric from `torchmetric` as follows:\r\n\r\n```python\r\nAccuracy(\r\n    num_classes=self.model.out_channels,\r\n     average='none',\r\n     ignore_index=self.ignore_index\r\n)\r\n```\r\nObviously I can not log this, however I don't want to set average to any aggregation. I want to log its mean in `training_step` but want to preserve the class wise metric to till end of epoch where I display it to terminal. I want the metric to reset at epoch end only, so can't call `compute()` in training step.\r\n\r\nHow to solve this?\r\n\r\nFrom [this comment](https://github.com/PyTorchLightning/pytorch-lightning/issues/4396#issuecomment-717571905) I had the idea that:\r\n> if the .compute()method is called the internal state is reset.\r\n\r\nHowever, this behavior has changed, now calling `compute()` does not reset the state of the metrics. See [PR #5409](https://github.com/PyTorchLightning/pytorch-lightning/pull/5409)\r\n",
    "meta": { "name": "Manually averaging metrics when logging" },
    "answer": "From [this comment](https://github.com/PyTorchLightning/pytorch-lightning/issues/4396#issuecomment-717571905) I had the idea that:\r\n> if the .compute()method is called the internal state is reset.\r\n\r\nHowever, this behavior has changed, now calling `compute()` does not reset the state of the metrics. See [PR #5409](https://github.com/PyTorchLightning/pytorch-lightning/pull/5409)\r\n"
  },
  {
    "content": "Hi,\r\nI want to use a pretrained ResNet or DenseNet with adjusted fc layer for image regression.\r\nAfter training, however, when I want to **load the finetuned DenseNet** for prediction I get a **RuntimeError**: Error(s) in loading state_dict.\r\nThis error does not occur for the ResNet.\r\nMy best guess is that this is somehow linked to checkpointing and that the value for `self.model_arch` might not be properly stored in the state_dict?\r\nBut I am not even sure if I need to tell the LightningModule what to save in the state_dict.\r\nAny ideas what might go wrong?\r\n\r\nBig Thanks already!\r\n\r\nLightningModule constructor:\r\n```\r\nclass ImageRegressor(LightningModule):\r\n    def __init__(self, optimizer:str = 'adam', k:int = 0, lr:float = 1e-3, batch_size:int = 16, pretrained:bool = True, tune_fc_only:bool = False, model: str = 'resnet50'):\r\n        super().__init__()\r\n        self.lr = lr\r\n        self.batch_size = batch_size\r\n        self.k = k\r\n        \r\n        optimizers = {'adam': Adam, 'sgd': SGD}\r\n        self.optimizer = optimizers[optimizer]\r\n        self.criterion = nn.MSELoss(reduction='mean')\r\n        self.model_arch = model\r\n        num_target_classes = 1\r\n\r\n        if self.model_arch == 'resnet50':\r\n            # init a pretrained resnet\r\n            self.model = models.resnet50(pretrained=pretrained)\r\n            num_filters = self.model.fc.in_features\r\n            self.model.fc = nn.Sequential(\r\n                nn.ReLU(),\r\n                nn.Linear(num_filters, num_target_classes))\r\n        elif self.model_arch == 'densenet':\r\n            print('setting up densenet')\r\n            self.model = models.densenet121(pretrained=pretrained)\r\n            num_filters = self.model.classifier.in_features\r\n            self.model.classifier = nn.Sequential(\r\n                nn.ReLU(),\r\n                nn.Linear(num_filters, num_target_classes))\r\n        if pretrained:\r\n            if tune_fc_only: # option to only tune the fully-connected layers\r\n                for child in list(self.model.children())[:-1]:\r\n                    for param in child.parameters():\r\n                        param.requires_grad = False\r\n```\r\n\r\nSaving is done automatically using this checkpoint:\r\n```\r\nModelCheckpoint(dirpath=this_output_dir,\r\n                                     filename='model_'+str(k)+'_{epoch}.pt',\r\n                                     monitor='val_loss')\r\n```\r\n\r\nloading checkpoint:\r\n```\r\nmodel = ImageRegressor(pretrained=True, tune_fc_only=True, model='densenet')\r\ntype(model).load_from_checkpoint(path)\r\n```\r\n\r\nshould be:\r\n\r\n```py\r\nmodel = ImageRegressor.load_from_checkpoint(path, pretrained=True, tune_fc_only=True, model='densenet')\r\n```\r\n\r\ncheck out the examples and description here: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#load-from-checkpoint\r\n\r\nslightly similar discussion: https://github.com/PyTorchLightning/pytorch-lightning/discussions/12399",
    "meta": {
      "name": "Loading model for prediction yields RuntimeError: Error(s) in loading state_dict"
    },
    "answer": "should be:\r\n\r\n```py\r\nmodel = ImageRegressor.load_from_checkpoint(path, pretrained=True, tune_fc_only=True, model='densenet')\r\n```\r\n\r\ncheck out the examples and description here: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#load-from-checkpoint\r\n\r\nslightly similar discussion: https://github.com/PyTorchLightning/pytorch-lightning/discussions/12399"
  },
  {
    "content": "Hello.\r\n\r\nI am currently trying to train a model using Deepspeed zero-2.\r\nIn Deepspeed official repository, can use bf16 when using zero-2, but isn't it currently supported in PL?if it's supported natively in deepspeed, PL should support it. Can you share some references from deepspeed?",
    "meta": { "name": "Doesn't it support bf16 when using Deepspeed?" },
    "answer": "if it's supported natively in deepspeed, PL should support it. Can you share some references from deepspeed?"
  },
  {
    "content": "Hi, is there a way in trainer to disable tf32 for ampere architecture? It's motivated by this discussion:https://discuss.pytorch.org/t/numerical-error-on-a100-gpus/148032/2\r\n\n\ncc @justusschock @kaushikb11 @awaelchli @borda @rohitgr7Hi @dnnspark! Simply setting the flags in your script doesn't work?\r\n```python\r\ntorch.backends.cuda.matmul.allow_tf32 = False\r\ntorch.backends.cudnn.allow_tf32 = False\r\n```Hi @akihironitta, Yup. that works. I wanted to check if there's a way (e.g. flag of `Trainer`) to do this.Hi, @dnnspark - Thanks for creating the issue and the question. From what I know, there is no such flag to do this currently with the `Trainer` class. Since this is specific to use-cases, I would prefer it to stay as a flag (like what @akihironitta mentioned). However, we are definitely open to feedback and suggestions (if you think that it should instead be an arg in the `Trainer` class).\r\n\r\nHope you're enjoying using PyTorch Lightning \u26a1 ! :)Just leaving my two cents here: When manually setting these, it might not work with `ddp_spawn`, since we don't rerun the whole script question here: If `torch is reimported in the new process, are those changed attributes retained? So In general, we need a mechanism to copy all the flags set to the main process to the spawned processes as well. Not saying, this should be a trainer flag though, this should happen under the hood. \r\n\r\nNot sure, how to properly do it either. For the beginning a hardcoded list of properties may be sufficient.@Borda just fyi: I think this should have been an issue. A lot easier to track there especially if it shows a possible limitation :)",
    "meta": { "name": "Option for disable tf32" },
    "answer": "Hi @dnnspark! Simply setting the flags in your script doesn't work?\r\n```python\r\ntorch.backends.cuda.matmul.allow_tf32 = False\r\ntorch.backends.cudnn.allow_tf32 = False\r\n```"
  },
  {
    "content": "This is the code\r\n\r\n```python\r\n    def test_step(self,batch,batch_idx):\r\n        image,label=batch\r\n        pred = self(image)\r\n        loss=self.criterion(pred.flatten(),label.float()) #calculate loss\r\n        acc=self.metrics(pred.flatten(),label)#calculate accuracy\r\n        pred=torch.sigmoid(pred)\r\n        return {'loss':loss,'acc':acc,'label':label,'pred':pred}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        loss=torch.stack([x[\"loss\"] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n        acc=torch.stack([x[\"acc\"] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n        label=torch.cat([x[\"label\"] for x in outputs]).detach().cpu().numpy().ravel()\r\n        pred=torch.cat([x[\"pred\"] for x in outputs]).detach().cpu().numpy().ravel()\r\n        pred=pred.astype(int)\r\n        print('torch acc',acc)\r\n        print(classification_report(label,pred))\r\n        print('sklearn',accuracy_score(label,pred))\r\n```\r\n\r\nThere is difference of 10-15% between accuracies obtained by torchmetrics and sklearnHi @talhaanwarch, could you keep the description as it was and share how you've fixed it? Someone in the community might have the same issue and this thread could be useful for them :)This is the solution\r\n```\r\ndef test_step(self,batch,batch_idx):\r\n        image,label=batch\r\n        pred = self(image)\r\n        \r\n        return {'label':label,'pred':pred}\r\n\r\ndef test_epoch_end(self, outputs):\r\n\r\n    label=torch.cat([x[\"label\"] for x in outputs])\r\n    pred=torch.cat([x[\"pred\"] for x in outputs])\r\n    acc=self.metrics(pred.flatten(),label)\r\n    pred=pred.detach().cpu().numpy().ravel()\r\n    label=label.detach().cpu().numpy().ravel()\r\n    \r\n    pred=np.where(pred>0.5,1,0).astype(int)\r\n    print('torch acc',acc)\r\n    print(classification_report(label,pred))\r\n    print('sklearn',accuracy_score(label,pred))\r\n\r\n\r\n```",
    "meta": { "name": "Torch accuracy and sklearn accuracy is v different" },
    "answer": "This is the solution\r\n```\r\ndef test_step(self,batch,batch_idx):\r\n        image,label=batch\r\n        pred = self(image)\r\n        \r\n        return {'label':label,'pred':pred}\r\n\r\ndef test_epoch_end(self, outputs):\r\n\r\n    label=torch.cat([x[\"label\"] for x in outputs])\r\n    pred=torch.cat([x[\"pred\"] for x in outputs])\r\n    acc=self.metrics(pred.flatten(),label)\r\n    pred=pred.detach().cpu().numpy().ravel()\r\n    label=label.detach().cpu().numpy().ravel()\r\n    \r\n    pred=np.where(pred>0.5,1,0).astype(int)\r\n    print('torch acc',acc)\r\n    print(classification_report(label,pred))\r\n    print('sklearn',accuracy_score(label,pred))\r\n\r\n\r\n```"
  },
  {
    "content": "Hi,\r\nI want to implement a BYOL-like model using Lightning CLI, and one of the key aspects is to use specific data augmentations. However, I don't manage to indicate to the cli that I want to use these specific data augmentations. Here is a snippet of my code:\r\n***main.py***\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.utilities.cli import LightningCLI, MODEL_REGISTRY\r\nfrom pl_bolts.datamodules import CIFAR10DataModule\r\nfrom pl_bolts.models.self_supervised.simclr import SimCLRTrainDataTransform, SimCLREvalDataTransform\r\n\r\n\r\nclass DummyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fc = nn.Linear(32*32*3, 10)\r\n        self.loss_fn = nn.MSELoss()\r\n\r\n    def shared_step(self, batch, batch_idx):\r\n        x, y = batch[0][:2]\r\n        z1 = self.fc(x.reshape(x.size(0), -1))\r\n        z2 = self.fc(y.reshape(y.size(0), -1))\r\n        return self.loss_fn(z1, z2)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self.shared_step(batch, batch_idx)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        return self.shared_step(batch, batch_idx)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters())\r\n\r\n\r\n# THE COMMENTED LINES BELOW RUN PERFECTLY\r\n# trainer = pl.Trainer()\r\n# model = DummyModel()\r\n# dm = CIFAR10DataModule()\r\n# dm.train_transforms = SimCLRTrainDataTransform(32)\r\n# dm.val_transforms = SimCLREvalDataTransform(32)\r\n# trainer.fit(model, dm)\r\n\r\ncli = LightningCLI(DummyModel, CIFAR10DataModule, run=False)\r\n\r\n# not instantiated!\r\nprint(cli.config_init.data.train_transforms)\r\n```\r\nand here is my ***config.yaml***:\r\n```yaml\r\ndata:\r\n  train_transforms:\r\n    class_path: pl_bolts.models.self_supervised.simclr.SimCLRTrainDataTransform\r\n    init_args:\r\n      input_height: 32\r\n  val_transforms:\r\n    class_path: pl_bolts.models.self_supervised.simclr.SimCLREvalDataTransform\r\n    init_args:\r\n      input_height: 32\r\n```\r\n\r\nTo run the code, I run the following command:\r\n```\r\npython main.py --config config.yaml\r\n```\r\nand here is the error I get:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/alain/code/misc/examples/main.py\", line 42, in <module>\r\n    cli.trainer.fit(cli.model, cli.datamodule)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 740, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\r\n    return self._run_train()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1311, in _run_train\r\n    self._run_sanity_check(self.lightning_module)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1375, in _run_sanity_check\r\n    self._evaluation_loop.run()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\r\n    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\", line 140, in run\r\n    self.on_run_start(*args, **kwargs)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 86, in on_run_start\r\n    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py\", line 121, in _update_dataloader_iter\r\n    dataloader_iter = enumerate(data_fetcher, batch_idx)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\", line 199, in __iter__\r\n    self.prefetching(self.prefetch_batches)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\", line 258, in prefetching\r\n    self._fetch_next_batch()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py\", line 300, in _fetch_next_batch\r\n    batch = next(self.dataloader_iter)\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\r\n    data = self._next_data()\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 363, in __getitem__\r\n    return self.dataset[self.indices[idx]]\r\n  File \"/home/alain/miniconda3/envs/ts/lib/python3.9/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\r\n    img = self.transform(img)\r\nTypeError: 'dict' object is not callable\r\n```\r\nWhat I understand from the error is that `LightningCLI` doesn't manage to interprete and instantiate the data augmentation model and understands it only as a dictionary with keys `class_path` and `init_args`. Does someone encounter a similar issue and/or knows how to solve it?\r\ncc @carmocca ",
    "meta": { "name": "Instantiate data augmentations through CLI" },
    "answer": "cc @carmocca "
  },
  {
    "content": "Hi, \r\nCould anyone advice me how to set up PyTorch lightning trainer to learn based on iterations instead of epochs? \r\n\r\nThank you! \r\nHi @mshooter , you can use the `min_steps` and `max_steps `arguments on the Trainer to do training based on iterations instead of epochs. https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#max-steps",
    "meta": { "name": "Training based on iterations" },
    "answer": "Hi @mshooter , you can use the `min_steps` and `max_steps `arguments on the Trainer to do training based on iterations instead of epochs. https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#max-steps"
  },
  {
    "content": "What is the best practice for performing evaluation on the dev and/or test set/s every X steps and not every epoch end?\r\n\r\nIn the [documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#) I saw the options of overriding `validation_epoch_end` and `test_epoch_end`, but could not find how to evaluate \"more frequently\".\r\n\r\nThanks!Hi @zorikg! You can configure it through the trainer flag `val_check_interval`. e.g.\r\n```python\r\n# check validation set 4 times during a training epoch\r\ntrainer = Trainer(val_check_interval=0.25)\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#val-check-interval",
    "meta": { "name": "How to evaluate every X steps?" },
    "answer": "Hi @zorikg! You can configure it through the trainer flag `val_check_interval`. e.g.\r\n```python\r\n# check validation set 4 times during a training epoch\r\ntrainer = Trainer(val_check_interval=0.25)\r\n```\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#val-check-interval"
  },
  {
    "content": "Hi!\r\nI need to create a callback that once every N training steps performs the forward pass over the PL module and do some calculations.\r\nMy first approach has been to simply create a callback and then define a new training_step() within that callback that does my needed calculations. The problem is that this is calculations are not being executed.\r\nUsing the debugger I see that the callback is correctly initialized and correctly passed to the trainer, but it is not entering in this newly defined training step. Here is a minimal example of what I need to do\r\n![image](https://user-images.githubusercontent.com/95293295/159908838-815cb65d-10a7-40d5-821c-31ae7a1d3346.png)\r\nDo you have some insights on what I am doing wrong?hey @malfonsoarquimea !\r\n\r\n`Callback.training_step` is not a hook so it won't be called automatically. For you use-case you can do something like:\r\n```py\r\nclass CustomCallback(Callback):\r\n    def __init__(..., every_n_train_steps):\r\n        self.every_n_train_steps = every_n_train_steps\r\n        ...\r\n    \r\n    def on_train_batch_end(self, trainer, pl_module, *args, **kwargs):\r\n        if trainer.global_step % self.every_n_train_steps == 0:\r\n            outputs = pl_module(self.input)\r\n```",
    "meta": { "name": "help defining new training_step() on a callback" },
    "answer": "hey @malfonsoarquimea !\r\n\r\n`Callback.training_step` is not a hook so it won't be called automatically. For you use-case you can do something like:\r\n```py\r\nclass CustomCallback(Callback):\r\n    def __init__(..., every_n_train_steps):\r\n        self.every_n_train_steps = every_n_train_steps\r\n        ...\r\n    \r\n    def on_train_batch_end(self, trainer, pl_module, *args, **kwargs):\r\n        if trainer.global_step % self.every_n_train_steps == 0:\r\n            outputs = pl_module(self.input)\r\n```"
  },
  {
    "content": "Greetings. I am getting NaN val loss `Cannot log infinite or NaN value to attribute training/val_loss`/ with cnnlstm network.I am thinking to use gradient clipping.\r\nBut the doc say gradient clipping should not be used with mixed precision.\r\n```\r\nIf using mixed precision, the gradient_clip_val does not need to be changed as the gradients are unscaled before applying the clipping function.\r\n```\r\nFurther, i am doing regression, i  dont know what value of gradient clipping should i use?\r\n\r\nFurther i checked trainer doc and find that\r\n```\r\ntrack_grad_norm\r\n(Union[int, float, str]) \u2013 -1 no tracking. Otherwise tracks that p-norm. May be set to \u2018inf\u2019 infinity-norm. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them.\r\n```\r\nCan you explain what this mean 'If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them'> But the doc say gradient clipping should not be used with mixed precision.\r\n\r\nYou totally can, that's saying that any scaling applied by 16bit precision training will be undone before clipping the gradients.\r\n\r\nWhich means you do not need to worry about changing the gradient clipping value with vs without `precision=16`\r\n\r\n>  i dont know what value of gradient clipping should i use?\r\n\r\nNobody does :P\r\nTry some experiments and find out!\r\n\r\n>  'If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them'\r\n\r\nSame thing as I explained above. It's just a technical detail, you do not need to worry about it\r\nI have been trying both mixed precision training and gradient clipping (norm value less than 0.5) together on a transformer model.  I have been getting nan after a certain point.  My batch sizes are only 2 so maybe it's too small?  Should I increase my gradient clipping value in that case?  It initially starts out fine but will go to nan loss after ~1000 iterations.",
    "meta": {
      "name": "Gradient Clipping with mix precision in case of NaN loss"
    },
    "answer": "> But the doc say gradient clipping should not be used with mixed precision.\r\n\r\nYou totally can, that's saying that any scaling applied by 16bit precision training will be undone before clipping the gradients.\r\n\r\nWhich means you do not need to worry about changing the gradient clipping value with vs without `precision=16`\r\n\r\n>  i dont know what value of gradient clipping should i use?\r\n\r\nNobody does :P\r\nTry some experiments and find out!\r\n\r\n>  'If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them'\r\n\r\nSame thing as I explained above. It's just a technical detail, you do not need to worry about it\r\n"
  },
  {
    "content": "My `deepspeed_zero2_config.json`:\r\n```json\r\n{\r\n    \"zero_optimization\": {\r\n        \"stage\": 2,\r\n        \"offload_optimizer\": {\r\n            \"device\": \"cpu\",\r\n            \"pin_memory\": true\r\n        },\r\n        \"allgather_partitions\": true,\r\n        \"allgather_bucket_size\": 2e8,\r\n        \"overlap_comm\": true,\r\n        \"reduce_scatter\": true,\r\n        \"reduce_bucket_size\": 2e8,\r\n        \"contiguous_gradients\": true\r\n    },\r\n    \"gradient_accumulation_steps\": \"auto\",\r\n    \"gradient_clipping\": \"auto\",\r\n    \"steps_per_print\": 2000,\r\n    \"train_batch_size\": \"auto\",\r\n    \"train_micro_batch_size_per_gpu\": \"auto\",\r\n    \"wall_clock_breakdown\": false\r\n}\r\n```\r\n\r\nI have some questions about how to configure DeepSpeed in Pytorch-Lightning:\r\n- I see that the [custom deepspeed config](https://pytorch-lightning.readthedocs.io/en/latest/advanced/model_parallel.html#custom-deepspeed-config) includes optimizer and scheduler. Should I add them in my config even I have configured in `Model.configure_optimizers`?\r\n  ```\r\n  You have not specified an optimizer or scheduler within the DeepSpeed config. Using `configure_optimizers` to define optimizer and scheduler.\r\n  ```\r\n- Should I add `fp16` config into deepspeed config json even I have passed `precision=\"bf16\"` in `pl.Trainer`?\r\n- Should I pass `logging_batch_size_per_gpu` to `pl.plugins.DeepSpeedPlugin` even I have configured batch_size in data loader?\r\n  ```\r\n  [2022-03-24 12:42:11,529] [WARNING] [deepspeed.py:630:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedPlugin(logging_batch_size_per_gpu=batch_size))`.\r\n  ```\r\n- It appears in log before training every time. Is that okey?\r\n  ```\r\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n  ninja: no work to do.\r\n  ```\r\n\r\nThanks a lot!\ud83d\ude0ayes, you don't need to set them inside config since this is done by Lightning already here if you set them in trainer and lightning module: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/strategies/deepspeed.py",
    "meta": {
      "name": "Should I configure FP16, optimizers, batch_size in DeepSpeed config of Pytorch-Lightning?"
    },
    "answer": "yes, you don't need to set them inside config since this is done by Lightning already here if you set them in trainer and lightning module: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/strategies/deepspeed.py"
  },
  {
    "content": "Hi everyone,\r\nIn my current setup, I would like to change the dataloader during a training epoch:\r\n\r\nThis is what I would like to achieve:\r\nstep 1.Train on dataset 1 for n batches\r\nstep 2.Train on dataset 2 for n batches\r\nstep 3.Go to step 1\r\n\r\nI found [this solution](https://forums.pytorchlightning.ai/t/how-to-switch-dataloaders-between-epochs-in-lightning/137/2) on the old forum but this only switches the dataset after each epoch.\r\n\r\nHere is my current attempt at switching it every n batches:\r\n```python\r\nclass SimpleModule(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model = ...\r\n        self.batch_size = ...\r\n        self.change_every_n_batch = 20\r\n    \r\n    def train_dataloader(self):\r\n        self.current_dataset = (self.global_step // self.change_every_n_batch) % 2\r\n        if self.current_dataset == 0:\r\n            dataset = Dataset1()\r\n        elif self.current_dataset == 1:\r\n            dataset = Dataset2()\r\n\r\n        dataloader = DataLoader(dataset, batch_size=self.batch_size)\r\n        return dataloader\r\n    \r\n    def on_train_batch_end(self, outputs, batch, batch_idx):\r\n        new_dataset = (self.global_step // self.change_every_n_batch) % 2\r\n        if new_dataset != self.current_dataset:\r\n            self.trainer.reset_train_dataloader(self)\r\n```\r\n`train_dataloader()` is called as expected every 20 batches by `on_train_batch_end()` but the returned dataloader does not seem to be used during the training loop.\r\n\r\nAny idea what could be going wrong? Or do you have a solution for what I want to achieve?\r\n\r\nThanks!hey @matprst !\r\n\r\nyou can set:\r\n- `limit_train_batches=n`. This will ensure that every training epoch will progress for only n batches\r\n- `reload_dataloaders_every_n_epochs=1`. this will ensure that train dataloader is reloaded after every epoch.\r\n\r\nand inside `train_dataloader`, flip the dataloader on each reload. something like:\r\n```py\r\ndef train_dataloader(self):\r\n    if self.some_flag:\r\n        dataset = Dataset1()\r\n    else:\r\n        dataset = Dataset2()\r\n\r\n    self.some_flag = not self.some_flag\r\n\r\n    return DataLoader(dataset, batch_size=self.batch_size)\r\n```Works like a charm, and much cleaner than what I thought! Thanks for the reply!\r\n\r\nI realise now that since I am using iterable datasets (they are large and don't fit into memory), the reloading restarts the iterable from the beginning rather than continuing where it stopped (or at least returning a random batch).\r\n\r\nThis is another problem with the dataset, so I will consider the question answered.",
    "meta": { "name": "How to switch dataloader every n training steps" },
    "answer": "hey @matprst !\r\n\r\nyou can set:\r\n- `limit_train_batches=n`. This will ensure that every training epoch will progress for only n batches\r\n- `reload_dataloaders_every_n_epochs=1`. this will ensure that train dataloader is reloaded after every epoch.\r\n\r\nand inside `train_dataloader`, flip the dataloader on each reload. something like:\r\n```py\r\ndef train_dataloader(self):\r\n    if self.some_flag:\r\n        dataset = Dataset1()\r\n    else:\r\n        dataset = Dataset2()\r\n\r\n    self.some_flag = not self.some_flag\r\n\r\n    return DataLoader(dataset, batch_size=self.batch_size)\r\n```"
  },
  {
    "content": "Hi everyone. I was recently running a lightning model and saved a checkpoint to store the intermediate results. When I try to open the checkpoint, I get an error that positional arguments (used to initialize the lightning module) are not present. This wouldn't be a big deal but one of the positional arguments is the encoder (used for BarlowTwins training). I was worried if I loaded the model checkpoint with an encoder initialized with starting weights, this would overwrite the weight parameters stored in the checkpoint. See the error log and a block of code below. Any suggestions on how I can appropriately load this stored model to resume training?\r\n\r\n      model_ckpt = BarlowTwins.load_from_checkpoint('/wynton/protected/home/ichs/dmandair/BRCAness/datasets/train/pcam/epoch=199-step=25599.ckpt')\r\n\r\n          Traceback (most recent call last):\r\n          File \"/wynton/protected/home/ichs/dmandair/BRCA/barlow.py\", line 435, in <module>\r\n            main(default_config)\r\n          File \"/wynton/protected/home/ichs/dmandair/BRCA/barlow.py\", line 427, in main\r\n            model_ckpt = BarlowTwins.load_from_checkpoint('/wynton/protected/home/ichs/dmandair/BRCAness/datasets/train/pcam/epoch=199-step=25599.ckpt')\r\n          File \"/wynton/protected/home/ichs/dmandair/anaconda3/envs/BRCA/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 156, in load_from_checkpoint\r\n            model = cls._load_model_state(checkpoint, strict=strict, **kwargs)\r\n          File \"/wynton/protected/home/ichs/dmandair/anaconda3/envs/BRCA/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\", line 198, in _load_model_state\r\n            model = cls(**_cls_kwargs)\r\n        TypeError: __init__() missing 5 required positional arguments: 'encoder', 'encoder_out_dim', 'num_training_samples', 'batch_size', and 'weight_decay'\r\n\r\n\r\noriginal model loaded with:\r\n\r\n        encoder = resnet18(zero_init_residual=True)\r\n    \r\n        model = BarlowTwins(\r\n            encoder=encoder,\r\n            encoder_out_dim=encoder_out_dim,\r\n            learning_rate = default_config['LR'],\r\n            weight_decay = default_config['WD'],\r\n            num_training_samples=262144,\r\n            batch_size=BATCH_SIZE,\r\n            z_dim=default_config['Z_DIM'],\r\n            lambda_coeff = default_config['LAMBDA'],\r\n            max_epochs=MAX_EPOCHS\r\n        )hey @dmandair !\r\n\r\ndid you call `self.save_hyperparameters()` inside your `LM.__init__`? else hyperparameters won't be saved inside the checkpoint and you might need to provide them again using `LMModel.load_from_checkpoint(..., encoder=encoder, encoder_out_dim=encoder_out_dim, ...)`.\r\n\r\nalso note that, if you are passing an `nn.Module` inside your LM and calling `self.save_hyperparameters()`, it will save that too inside your hparams, which is not a good thing considering that nn.Modules are saved inside checkpoint state_dict and might create issues for you. Ideally, you should ignore them using `self.save_hyperparameters(ignore=['encoder'])`. Check out this PR: https://github.com/PyTorchLightning/pytorch-lightning/pull/12068",
    "meta": { "name": "Error with loading model checkpoint" },
    "answer": "hey @dmandair !\r\n\r\ndid you call `self.save_hyperparameters()` inside your `LM.__init__`? else hyperparameters won't be saved inside the checkpoint and you might need to provide them again using `LMModel.load_from_checkpoint(..., encoder=encoder, encoder_out_dim=encoder_out_dim, ...)`.\r\n\r\nalso note that, if you are passing an `nn.Module` inside your LM and calling `self.save_hyperparameters()`, it will save that too inside your hparams, which is not a good thing considering that nn.Modules are saved inside checkpoint state_dict and might create issues for you. Ideally, you should ignore them using `self.save_hyperparameters(ignore=['encoder'])`. Check out this PR: https://github.com/PyTorchLightning/pytorch-lightning/pull/12068"
  },
  {
    "content": "I'm trying to gain some confidence in a model that seems to be training fine.\r\n\r\nAs a simple sanity check I'm trying to make sure I can load then test a checkpoint with the same input, expecting to be able to produce the same output each and every time (I'm using the same input and checkpoint each time so I expect the output to be the same).\r\n\r\nUnfortunately, I'm observing different output each time I reload the checkpoint.\r\n\r\nHere is the essence of what I'm doing:\r\n```\r\nfor n in range(2):\r\n        my_module = MyLightningModule.load_from_checkpoint(ckpt_path)\r\n\r\n        my_dataset = MyDataset()\r\n        batch = my_dataset.get_sanity_test_batch()  # confirmed to be the same batch every time\r\n\r\n        # this output is different every time (???)\r\n        output = my_module.model.generate(batch, max_length=some_length)\r\n```\r\n\r\n**It is also probably worth noting that the model trained/loaded by my_module is a hugginface T5 transformer ([_T5ForConditionalGeneration_](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5ForConditionalGeneration) )**\r\n\r\nPlease help me figure out how to ensure output is consistent after loading a trained checkpoint.Turns out that I was doing something a little different in the actual code than:\r\n`my_module = MyLightningModule.load_from_checkpoint(ckpt_path)`\r\n\r\nWhen I do this exactly, things work as expected :)",
    "meta": {
      "name": "Lighting Module Loaded From Checkpoint Generates Different Output Each Time"
    },
    "answer": "Turns out that I was doing something a little different in the actual code than:\r\n`my_module = MyLightningModule.load_from_checkpoint(ckpt_path)`\r\n\r\nWhen I do this exactly, things work as expected :)"
  },
  {
    "content": "Problem:\r\nI have a problem. when i train the model, the process would be blocking in validation step(or validation sanity check) but in the training step, it can work. And I debug in the pytorch-lightning , i found when loading data from validation dataloader it would be blocking. I am not sure what problem.\r\n\r\nEnvironment:\r\ndocker; the lastest pytorch-lighting;gpu a100\r\n\r\nlog:\r\nINFO Using validation DataLoader3DOffset with {}\r\nINFO Building Sampling Cache for Dataloder\r\nSampling Cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 1445.07it/s]\r\nINFO Using 5 num_processes and 2 num_cached_per_queue for augmentation.                                                                                                           | 0/2 [00:00<?, ?it/s]\r\nINFO VALIDATION KEYS:\r\n odict_keys(['case_0', 'case_7'])\r\nusing pin_memory on device 0\r\n\r\n---------------------------------------------------\r\n\r\nI test the validation step and it world jam \r\n\r\ncan you help me, thank you !!!!!> i found when loading data from validation dataloader it would jam.\r\n\r\nHi @clearknow, by \"jam\", you mean the process hangs? Would it be possible for you to provide your env details and script?sorry,  It is my fault. now i solve this problemsorry,  It is my fault. now i solve this problem",
    "meta": { "name": "the process would be blocking in Validation step" },
    "answer": "sorry,  It is my fault. now i solve this problem"
  },
  {
    "content": "I would appreciate some help understanding the `overfit_batches` argument for the trainer.\r\n\r\nWhen overfitting batches, it seems that the entire train dataloader is deepcopied ([code in v1.5.10](https://github.com/PyTorchLightning/pytorch-lightning/blob/9ebdc52ec631df92ca7cfc1ba852801dd36d3864/pytorch_lightning/trainer/data_loading.py#L465)). In my case, this immediately results in my machine running out of RAM because of the size of this dataset. I also have many validation dataloaders, so I believe this compounds the issue. \r\n\r\nWill this behavior of copying the entire dataloader be removed in a future release? I believe some other mechanism of duplication is necessary to avoid copying the data that is not included in the batches that are being overfitted. \r\n\r\nI was trying to read #10877 to understand how the behavior would change, but I am unsure.hey @jonathanking !\r\n\r\nthe deepcopy for train dataloader to the validation dataloader has been removed from master and will be available in the next release soon.",
    "meta": {
      "name": "overfit_batches duplicates entire train DataLoader causing out of memory"
    },
    "answer": "hey @jonathanking !\r\n\r\nthe deepcopy for train dataloader to the validation dataloader has been removed from master and will be available in the next release soon."
  },
  {
    "content": "So lets say i have a model and i'm using the newest CLI API to train it: The config uses sub modules and can look something like:\r\n\r\n```\r\n# config.yaml\r\nmodel:\r\n  class_path: pl_models_2.ModelPL\r\n  init_args:\r\n    margin: 0.3\r\n    basemodel:\r\n      class_path: pl_models_2.ModelBackbone\r\n      init_args:\r\n        base_model: resnet50\r\n        pooling: both\r\n\r\ndata:\r\n  batch_size: 32\r\n  image_size: 224\r\n  augmentation_strategy: medium2\r\n```\r\n\r\nNow i want  to run some scripts  or notebooks using the model i trained with this config and i would like to  instantiate the model using this config file.\r\nf/e. model = load_model(config.yaml)\r\n\r\nI was digging how this happens in LightningCLI and jsonargsparse but gave up after a while.. and started trying to hack this around with importlib, which works but feels like reinventing the wheel - i mean this logic has to be somewhere already :) i just cant find it.\r\n@mauvilsa ?If you want to use that exact config (not stripping out everything except the model) you can do the following:\r\n\r\n```python\r\nfrom jsonargparse import ArgumentParser\r\n\r\nparser = ArgumentParser()\r\nparser.add_argument('--model', type=ModelClass)\r\nparser.add_argument('--data', type=dict) # to ignore data\r\nconfig = parser.parse_path('config.yaml')\r\nconfig_init = parser.instantiate_classes(config)\r\n```\r\n\r\nThe instantiated model will be in `config_init.model`.",
    "meta": { "name": "LightningCLI  - instantiate model from config" },
    "answer": "If you want to use that exact config (not stripping out everything except the model) you can do the following:\r\n\r\n```python\r\nfrom jsonargparse import ArgumentParser\r\n\r\nparser = ArgumentParser()\r\nparser.add_argument('--model', type=ModelClass)\r\nparser.add_argument('--data', type=dict) # to ignore data\r\nconfig = parser.parse_path('config.yaml')\r\nconfig_init = parser.instantiate_classes(config)\r\n```\r\n\r\nThe instantiated model will be in `config_init.model`."
  },
  {
    "content": "When I create a Trainer and run Trainer.fit() I am now getting the following error:\r\n\r\n```\r\nraise TypeError(\"cannot assign '{}' as child module '{}' \"\r\nTypeError: cannot assign 'int' as child module 'precision' (torch.nn.Module or None expected)\r\n```\r\n\r\nThis is a new error and this code was just working earlier. Do yall know what could be causing this issue?hey @cwoolfo1 !\r\n\r\ncan you share the complete stack trace?Do you have an attribute precision defined in your lightning module? If so, this is an improper override of the lightning module which is leading to this error: https://github.com/PyTorchLightning/pytorch-lightning/blob/49a4a36ad45b937dd0124ecfb08eb7400dbf3950/pytorch_lightning/core/lightning.py#L102-L103\r\n\r\n",
    "meta": {
      "name": "New error in Trainer started appearing recently in a previously running code."
    },
    "answer": "Do you have an attribute precision defined in your lightning module? If so, this is an improper override of the lightning module which is leading to this error: https://github.com/PyTorchLightning/pytorch-lightning/blob/49a4a36ad45b937dd0124ecfb08eb7400dbf3950/pytorch_lightning/core/lightning.py#L102-L103\r\n\r\n"
  },
  {
    "content": "My training code (after a lot of setup) looks like this:\r\n\r\n```python\r\n\r\n# Create a trainer\r\ntrainer = pl.Trainer.from_argparse_args(argparse.Namespace(**dict_args),\r\n                                        callbacks=my_callbacks)\r\n# Look for largest batch size and optimal LR\r\ntuner = Tuner(trainer)\r\ntuner.scale_batch_size(\r\n    model,\r\n    train_dataloaders=data_module.get_descending_size_train_dataloader(),\r\n    init_val=1)\r\ntuner.lr_find(model, data_module.train_dataloader())\r\n\r\n# Train the model\r\ntrainer.fit(model, data_module)\r\ntrainer.test(model, data_module)\r\n\r\n\r\n```\r\n\r\nI thought making a data module was the best practice. Am I not allowed to pass a dataloader to both `trainer.fit` and `tuner.scale_batch_size`? It's not completely clear from the documentation. Is my _only_ option to re-incorporate my data module into my Lightning Module? It'd be great to have both!passing train_dataloader directly to batch_size scaling call will give you an exception error. But passing datamodule is allowed. The reason is that after each batch size scale iteration, we need to reinitialize the dataloader using the scaled value for batch_size param and if you pass in the dataloader itself, it won't be possible to reinitialize the dataloader as of now. Maybe we can add support for it in the future.",
    "meta": {
      "name": "If tuner.scale_batch_size() accepts a train_dataloader, why can't this be used independently of trainer.fit(model, datamodule)?"
    },
    "answer": "passing train_dataloader directly to batch_size scaling call will give you an exception error. But passing datamodule is allowed. The reason is that after each batch size scale iteration, we need to reinitialize the dataloader using the scaled value for batch_size param and if you pass in the dataloader itself, it won't be possible to reinitialize the dataloader as of now. Maybe we can add support for it in the future."
  },
  {
    "content": "I have used the below code\r\n1. Dataclass below\r\n![carbon (2)](https://user-images.githubusercontent.com/56019599/149141526-b9944c92-fa36-43c7-a93b-2dc78f0ec643.png)\r\n\r\n2. Model Class below\r\n![carbon (3)](https://user-images.githubusercontent.com/56019599/149141568-dce360de-68bf-4f27-b2eb-5f2fc4195bba.png)\r\n\r\n3. Trainnig below\r\n![carbon (1)](https://user-images.githubusercontent.com/56019599/149140420-1cc753ac-f9df-4f80-b8c2-2b944f43d1a2.png)\r\n\r\nI have trained the model for 20 epochs, After that trainer. test( ) didn't run. I Got the Following error lines :\r\n\r\n![Screenshot from 2022-01-12 17-47-57](https://user-images.githubusercontent.com/56019599/149139274-35e32fb8-72c6-41f6-a7f8-0ff2a0bfb53a.png)\r\n![Screenshot from 2022-01-12 17-48-21](https://user-images.githubusercontent.com/56019599/149139306-848c9b49-3aa1-49a3-b937-f3433fa785a5.png)\r\n\r\nI have also tried the way suggested in Doc [here](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#test) But still, the error exists. This happening mostly in Google collab and in AwsInstance. In Local, It works sometimes and sometimes not. \r\nPlease let me know what am I missing here.\r\nhey @purnasai-soulpageit !\r\n\r\nyou need to pass in the datamodule to `trainer.test`.\r\n```\r\ntrainer.test(datamodule=data_module)\r\n```\r\nsince lightning no longer patches the data_modules passed during `.fit` you need to pass it there too.Hi @rohitgr7 I tried passing all including datamodule, ckpt_path earlier, but then for some reason, they threw me an error.  I just checked again passing just datamodule and it worked.\r\nThanks",
    "meta": { "name": "trainer.test( ) not working" },
    "answer": "hey @purnasai-soulpageit !\r\n\r\nyou need to pass in the datamodule to `trainer.test`.\r\n```\r\ntrainer.test(datamodule=data_module)\r\n```\r\nsince lightning no longer patches the data_modules passed during `.fit` you need to pass it there too."
  },
  {
    "content": "I have an issue with a weighted mse function that I instantiate in the setup, with a buffer as parameter. Something like this:\r\n\r\n```python\r\n@torch.jit.script\r\ndef weighted_mse_func(weights, y, y_hat):\r\n    # weighted regression loss\r\n    reg_loss = torch.dot(weights,torch.mean(F.mse_loss(y_hat, y, reduction='none'), dim=0))\r\n    return reg_loss\r\n\r\ndef weighted_mse(weights):\r\n    def func(y, y_hat):\r\n        return weighted_mse_func(weights, y, y_hat)\r\n    return func\r\n\r\n\r\nclass model(pl.LightningModule):\r\n    def __init__(self, weights):\r\n        weights = torch.tensor(weights.copy(), dtype=self.dtype, device=self.device)\r\n        self.register_buffer(\"weights\", weights)\r\n    \r\n    def setup(self, stage):\r\n        super().setup(stage)\r\n        self.loss = weighted_mse(self.weights)\r\n```\r\n\r\nWhen initializing training on the GPU I get an error because `self.weights` is on CPU and not in GPU, if after the error I check the device of the buffer it's on GPU. So if I re-run the trainer, it works fine, also works fine if I call model.cuda() before training. What is going on? Why is the buffer not in GPU on the setup where it fails, but it is afterward? Is something asynchronous going on here?try:\r\n```py\r\nclass model(pl.LightningModule):\r\n    def __init__(self, weights):\r\n        super().__init__()\r\n        self.register_buffer(\"weights\", torch.tensor(weights.copy(), dtype=self.dtype))\r\n    \r\n    def on_fit_start(self):\r\n        self.loss = weighted_mse(self.weights)\r\n```It seems that pytorch doesn't move buffers parameters in-place (like it is done for parameters), this results in references to buffers being useless if they are moved from one device to another. This issue is discussed in pytorch/pytorch#43815.",
    "meta": { "name": "When are buffers moved to gpu?" },
    "answer": "It seems that pytorch doesn't move buffers parameters in-place (like it is done for parameters), this results in references to buffers being useless if they are moved from one device to another. This issue is discussed in pytorch/pytorch#43815."
  },
  {
    "content": "I notice that `step` in PyTorch Lighting can mean `batch` or `optimizer.step()`.  So what does `step` mean in `max_steps` ? @rohitgr7\r\n\r\nThe following code maybe helpful:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/5da065e287b44e2c1fe4f7951003813ed45365c9/pytorch_lightning/loops/epoch/training_epoch_loop.py#L258-L260\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/5da065e287b44e2c1fe4f7951003813ed45365c9/pytorch_lightning/loops/epoch/training_epoch_loop.py#L323-L331\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/5da065e287b44e2c1fe4f7951003813ed45365c9/pytorch_lightning/loops/epoch/training_epoch_loop.py#L90-L93\r\n\r\nit means global_step. So if you have accumulation factors = 2 and max_steps = 10, then the total training batches that will be covered here will be 20 instead of 10. `global_step` indicate total optimization steps.",
    "meta": { "name": "what does `step` mean in `max_steps` ?" },
    "answer": "it means global_step. So if you have accumulation factors = 2 and max_steps = 10, then the total training batches that will be covered here will be 20 instead of 10. `global_step` indicate total optimization steps."
  },
  {
    "content": "I am trying to build docs locally. I followed the steps [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md#documentation) but when I go to `http://docs/build/html/index.html` I get: \"This site can\u2019t be reached\"\r\n\r\nHere is the end of the output for when I run `make html`:\r\n```\r\ncopying images... [ 97%] _static/images/mnist_imgs/restart_runtime.png\r\ncopying images... [ 98%] _static/images/mnist_imgs/tpu_start.png\r\ncopying images... [ 99%] _static/images/mnist_imgs/tpu_fast.png\r\ncopying images... [100%] _static/images/general/PTL101_youtube_thumbnail.jpg\r\n\r\ncopying static files... done\r\ncopying extra files... done\r\ndumping search index in English (code: en)... done\r\ndumping object inventory... done\r\nbuild succeeded, 1 warning.\r\n\r\nThe HTML pages are in build/html.\r\nThe name of the builder is: htmlCopying sphinx_paramlinks stylesheet... done\r\n```\r\n\r\nIt looks all fine to me... Hey @daniellepintz I think the URL should be something different. (the protocol shouldn't be http.)\r\n\r\nIn my env, the project root dir is:\r\n```\r\n/Users/nitta/work/github.com/PyTorchLightning/pytorch-lightning/\r\n```\r\nand the URL is:\r\n```\r\nfile:///Users/nitta/work/github.com/PyTorchLightning/pytorch-lightning/docs/build/html/index.html\r\n```",
    "meta": { "name": "Unable to build docs locally" },
    "answer": "Hey @daniellepintz I think the URL should be something different. (the protocol shouldn't be http.)\r\n\r\nIn my env, the project root dir is:\r\n```\r\n/Users/nitta/work/github.com/PyTorchLightning/pytorch-lightning/\r\n```\r\nand the URL is:\r\n```\r\nfile:///Users/nitta/work/github.com/PyTorchLightning/pytorch-lightning/docs/build/html/index.html\r\n```"
  },
  {
    "content": "Just received qty 2 of A6000 and these are not compatible \r\nwith my existing docker file (lack of sm_86 support)\r\n```\r\nFROM pytorch/pytorch:1.6.0-cuda10.1-cudnn7-runtime\r\nRUN pip install pytorch-lightning==1.0.7\r\n```\r\n\r\nSo upgraded my docker to \r\n```\r\nFROM pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime\r\nRUN pip install pytorch-lightning==1.5.10\r\n```\r\n\r\nI also made changed to my code the for the lightning braking change from \r\n```\r\ntrainer = pl.Trainer( gpus=[0,1],  \r\n        distributed_backend='ddp', , \r\n        ....\r\n```\r\n\r\nto \r\n\r\n```\r\ntrainer = pl.Trainer( gpus=[0,1],  \r\n        strategy='ddp', \r\n        ....\r\n```\r\n\r\nWhen I try to train it just stops.  So set env  NCCL_DEBUG=WARN \r\n [as per suggestion]( https://github.com/PyTorchLightning/pytorch-lightning/issues/9641)\r\nto get the following output:\r\n\r\n```\r\ninitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\r\ninitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\r\n----------------------------------------------------------------------------------------------------\r\ndistributed_backend=nccl\r\nAll distributed processes registered. Starting with 2 processes\r\n----------------------------------------------------------------------------------------------------\r\n60b476048acc:22:22 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nNCCL version 2.10.3+cuda11.1\r\n60b476048acc:120:120 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\n\r\n```\r\n\r\nSame happens when I try \r\n```\r\nFROM pytorch/pytorch:1.8.1-cuda11.1-cudnn8-runtime \r\nFROM pytorch/pytorch:1.9.1-cuda11.1-cudnn8-runtime \r\nFROM pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime\r\n```\r\n\r\nMy old setup was 2xRTX Titan with nvlink while the new setup is 2xA6000 without a nvlink. [nvidia doc](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/nccl1.html) says that PCI is used but unclear if I need to do something to use this.\r\n\r\n[Distributed communication docs](https://pytorch.org/docs/stable/distributed.html) say \"NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA)\" .\r\n\r\nI suspect I am missing something about the breaking changes from pl 1.0 to 1.5. Would appreciate hints as to what to look for.\r\nIs NCCL something used in pl 1.0 or is this new to pl 1.5?\r\nDoes NCCL need to be installed?\r\nI reduced the delta to only \r\n```\r\nFROM pytorch/pytorch:1.6.0-cuda10.1-cudnn7-runtime\r\nTO pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime \r\n```\r\nstaying on \r\n`pytorch-lightning==1.0.7`\r\nand still get the error\r\n`misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]`\r\nSo something must be no longer included in the docker builds of pytorch.\r\nDuplicate of #12235.",
    "meta": { "name": "NCCL WARN Failed to open libibverbs.so[.1]" },
    "answer": "Duplicate of #12235."
  },
  {
    "content": "Hi\r\nI first trained the model on kaggle on celeba:\r\nHere's the link to the notebook: https://www.kaggle.com/yashrathikaggle/resnet-gender-detection-with-98-16-accuracy/notebook\r\n\r\nWhile training the trained automatically checkpointed the best val_acc and train_loss. I downloaded the checkpoint on colab and tried to load the model to see the outputs. And it seems the model outputs are random. It doesn't looks like I was able to get the model.\r\n\r\nLink to colab: https://colab.research.google.com/drive/1E9eg3BkBQCsyPjmy1TZ3oR-kK7Ti4JXt#scrollTo=waqBK-6WaxrX\r\n\r\nPlease help if I am doing something wrong here.hey @yashrathi-git !\r\n\r\nyour code looks correct.\r\nWhy do you think that outputs are random?\r\nyou can also validate your model after loading using:\r\n```py\r\nmodel = LitModel.load_from_checkpoint(...)\r\ntrainer = Trainer(...)\r\ntrainer.validate(model)  # or trainer.test(model)\r\n```\r\nif the above gives you the desired metrics then the model is loaded correctly.\r\n\r\none more thing, you might need to call `model.eval()` after loading the weights from the checkpoint.",
    "meta": {
      "name": "Pytorch-lightning is not able to load the model checkpoint"
    },
    "answer": "hey @yashrathi-git !\r\n\r\nyour code looks correct.\r\nWhy do you think that outputs are random?\r\nyou can also validate your model after loading using:\r\n```py\r\nmodel = LitModel.load_from_checkpoint(...)\r\ntrainer = Trainer(...)\r\ntrainer.validate(model)  # or trainer.test(model)\r\n```\r\nif the above gives you the desired metrics then the model is loaded correctly.\r\n\r\none more thing, you might need to call `model.eval()` after loading the weights from the checkpoint."
  },
  {
    "content": "I may be misunderstanding something about the trainer argument `fast_dev_run`. When I provide `fast_dev_run=1` and I add a print statement in my LightningModule's `test_step` function, the print statement does not appear. In addition, I can see a progress bar for my training set and validation set, but no progress bar appears for the test set.\r\n\r\nIs `fast_dev_run` actually running n batches of my training set? I have passed in a DataModule to `trainer.fit()` that includes a test_dataloader.`fit` only runs training & validation, not testing.\r\n\r\n`trainer.test` runs the test_step",
    "meta": {
      "name": "fast_dev_run does not execute pl.LightningModule.test_step()"
    },
    "answer": "`fit` only runs training & validation, not testing.\r\n\r\n`trainer.test` runs the test_step"
  },
  {
    "content": "In the docstring it says the save_dir is None, but then why does it return a path? Should we change either the docstring, or the implementation here?\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/9d8faecdb2b873b52b95f2772f4bf48068a0af9a/pytorch_lightning/loggers/neptune.py#L516-L524Any insights on this @awaelchli @edward-io @Borda @rohitgr7?It would be best to ask the neptune team directly @kamil-kaczmarek @Raalsky @Blaizzy\r\nNeptune doesn't log anything to a local directory so save_dir there doesn't make much sense, so I guess at the time of implementation, the best other option was to point at the \".neptune\" folder. Hi @daniellepintz\r\n\r\nPrince Canuma here, a Data Scientist at Neptune.ai\r\n\r\nI will let the engineering team know about this,\r\n\r\nBy default, Neptune will create a '.neptune' folder inside the current working directory. In the case of https://github.com/PyTorchLightning/pytorch-lightning/pull/6867 it changes the model checkpoint path to be '.neptune' folder in case the user doesn't define his own path using ModelCheckpointCallback() for example.\r\nCheck this commit: https://github.com/PyTorchLightning/pytorch-lightning/commit/5ac80ece048c16f19e9c05b1873a08fe0a37be90\r\n\r\n> But still, a bit confused because I thought Neptune doesn't save anything locally.\r\n\r\nNeptune uses the '.neptune' folder to store metadata temporarily. For example, you track a run in offline mode or there is a network connectivity issue in which case neptune also automatically switches to offline mode and save the data to disk. Later you can synchronize the locally stored metadata with the servers using the neptune sync CLI command.\r\n\r\nDocs:\r\n\r\nhttps://docs.neptune.ai/api-reference/command-line-interface#neptune\r\nhttps://docs.neptune.ai/api-reference/command-line-interface#neptune-sync",
    "meta": { "name": "Confusion about NeptuneLogger.save_dir implementation" },
    "answer": "Hi @daniellepintz\r\n\r\nPrince Canuma here, a Data Scientist at Neptune.ai\r\n\r\nI will let the engineering team know about this,\r\n\r\nBy default, Neptune will create a '.neptune' folder inside the current working directory. In the case of https://github.com/PyTorchLightning/pytorch-lightning/pull/6867 it changes the model checkpoint path to be '.neptune' folder in case the user doesn't define his own path using ModelCheckpointCallback() for example.\r\nCheck this commit: https://github.com/PyTorchLightning/pytorch-lightning/commit/5ac80ece048c16f19e9c05b1873a08fe0a37be90\r\n\r\n> But still, a bit confused because I thought Neptune doesn't save anything locally.\r\n\r\nNeptune uses the '.neptune' folder to store metadata temporarily. For example, you track a run in offline mode or there is a network connectivity issue in which case neptune also automatically switches to offline mode and save the data to disk. Later you can synchronize the locally stored metadata with the servers using the neptune sync CLI command.\r\n\r\nDocs:\r\n\r\nhttps://docs.neptune.ai/api-reference/command-line-interface#neptune\r\nhttps://docs.neptune.ai/api-reference/command-line-interface#neptune-sync"
  },
  {
    "content": "I am getting the below error when running `trainer.fit`:\r\n`AttributeError: 'Trainer' object has no attribute 'run_evaluation'`\r\n\r\nFull traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"sdr_main.py\", line 81, in <module>\r\n    main()\r\n  File \"sdr_main.py\", line 28, in main\r\n    main_train(model_class_pointer, hyperparams,parser)\r\n  File \"sdr_main.py\", line 73, in main_train\r\n    trainer.fit(model)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 741, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1289, in run_stage\r\n    return self._run_train()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1319, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 140, in run\r\n    self.on_run_start(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in on_run_start\r\n    self.trainer.call_hook(\"on_train_start\")\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1495, in call_hook\r\n    callback_fx(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py\", line 138, in on_train_start\r\n    callback.on_train_start(self, self.lightning_module)\r\n  File \"/content/SDR/utils/pytorch_lightning_utils/callbacks.py\", line 10, in on_train_start\r\n    return trainer.run_evaluation()\r\nAttributeError: 'Trainer' object has no attribute 'run_evaluation'\r\n```\r\n\r\nMy `Trainer` object:\r\n```\r\ntrainer = pytorch_lightning.Trainer(\r\n    num_sanity_val_steps=2,\r\n    gradient_clip_val=hparams.max_grad_norm,\r\n    callbacks=[RunValidationOnStart()],\r\n    checkpoint_callback=ModelCheckpoint(\r\n        save_top_k=3,\r\n        save_last=True,\r\n        mode=\"min\" if \"acc\" not in hparams.metric_to_track else \"max\",\r\n        monitor=hparams.metric_to_track,\r\n        dirpath=model.hparams.hparams_dir,\r\n        filename=\"{epoch}\",\r\n        verbose=True,\r\n    ),\r\n    logger=logger,\r\n    max_epochs=hparams.max_epochs,\r\n    gpus=hparams.gpus,\r\n    strategy=\"dp\",\r\n    limit_val_batches=hparams.limit_val_batches,\r\n    limit_train_batches=hparams.limit_train_batches,\r\n    limit_test_batches=hparams.limit_test_batches,\r\n    check_val_every_n_epoch=hparams.check_val_every_n_epoch,\r\n    profiler=SimpleProfiler(),\r\n    accumulate_grad_batches=hparams.accumulate_grad_batches,\r\n    reload_dataloaders_every_epoch=True,\r\n    resume_from_checkpoint=hparams.resume_from_checkpoint,\r\n)\r\n```\r\n\r\nAny idea on how to fix this? My pytorch-lightning version is 1.5.10```\r\n  File \"/content/SDR/utils/pytorch_lightning_utils/callbacks.py\", line 10, in on_train_start\r\n    return trainer.run_evaluation()\r\nAttributeError: 'Trainer' object has no attribute 'run_evaluation'\r\n```\r\n\r\n@hassiahk What are you trying to evaluate there? Could I see what your callback looks like?hey !\r\n\r\nthis was removed in the previous release.\r\n\r\nYou can try:\r\n```py\r\ntrainer.validating = True\r\ntrainer.reset_val_dataloader()\r\ntrainer.val_loop.run()\r\ntrainer.training = True\r\n```",
    "meta": {
      "name": "AttributeError: 'Trainer' object has no attribute 'run_evaluation'"
    },
    "answer": "hey !\r\n\r\nthis was removed in the previous release.\r\n\r\nYou can try:\r\n```py\r\ntrainer.validating = True\r\ntrainer.reset_val_dataloader()\r\ntrainer.val_loop.run()\r\ntrainer.training = True\r\n```"
  },
  {
    "content": "Hey, I was wondering if there was a correct way to evaluate and log multi-class metrics at the end of an epoch, such that the metrics could be evaluated across multiple GPUs using ddp? \r\n\r\nTypically, if I was looking at a image classification problem I could use the following:\r\n\r\n```\r\nself.train_average_accuracy = torchmetrics.Accuracy(num_classes=self.num_classes)\r\n\r\nx, y = batch\r\noutputs = self(x)  # Evaluate logits.\r\nsigmoid_outputs = torch.sigmoid(outputs)  # Produce sigmoid outputs for multi-label metrics.\r\nloss = self.compute_loss(outputs, y)\r\naverage_acc = self.train_average_accuracy(sigmoid_outputs, y)\r\n```\r\n\r\nI could log the either with single GPU or multi-GPU accuracy using:\r\n\r\n```\r\nself.log(\r\n\"train_average_acc\",\r\naverage_acc,\r\non_step=False,\r\non_epoch=True,\r\nprog_bar=True,\r\n sync_dist=self.multi_gpu)\r\n```\r\n\r\nHowever, if I try modifying the accuracy metric to use multi-label accuracy using: \r\n```\r\ntorchmetrics.Accuracy(threshold=0.5, average=None, num_classes=self.num_classes)\r\n```\r\n\r\nI find that the metric is logged as nan, which is I'm guessing due to the tensor which contains the class wide accuracies not updating between batches. I've also tried to get around this I ended up passing `y` and `y_pred` into the `def validation_epoch_end(self, outputs)` and evaluating the metric using concatenated batches. However, I still find nan values logged. Is there any other way of logging the multi-label metrics other than, say iterating over the tensor containing the class level metrics?As an update, I managed to develop a work around by iterating over the tensor that contained multi-label metrics that are evaluated on a class level. I.e, this function coulds do it\r\n\r\n```\r\n    for idx, metric_value in enumerate(class_metrics):  # Iterate over tensor elements.\r\n        if mode == \"debugging\":\r\n            print(f\"{phase}_{metric}_{class_labels[idx]}: {metric_value}\")\r\n\r\n        else:  # Note we cannot have forward slashes within class_labels when logging.\r\n            self.log(\r\n                f\"{phase}_{metric}_{class_labels[idx]}\",\r\n                metric_value,\r\n                on_step=False,\r\n                on_epoch=True,\r\n                sync_dist=self.multi_gpu,\r\n            )\r\n```\r\n\r\nWhere the metrics are evaluated on a class wise level using.\r\n\r\n```average_acc = self.train_average_accuracy(sigmoid_outputs, y)```\r\n\r\nand the logging function can be placed within the training or validation step using\r\n\r\n```log_class_metric(self, class_metrics=class_acc, metric=\"acc\", phase=\"train\")```",
    "meta": { "name": "Logging Multi-Label Metrics" },
    "answer": "As an update, I managed to develop a work around by iterating over the tensor that contained multi-label metrics that are evaluated on a class level. I.e, this function coulds do it\r\n\r\n```\r\n    for idx, metric_value in enumerate(class_metrics):  # Iterate over tensor elements.\r\n        if mode == \"debugging\":\r\n            print(f\"{phase}_{metric}_{class_labels[idx]}: {metric_value}\")\r\n\r\n        else:  # Note we cannot have forward slashes within class_labels when logging.\r\n            self.log(\r\n                f\"{phase}_{metric}_{class_labels[idx]}\",\r\n                metric_value,\r\n                on_step=False,\r\n                on_epoch=True,\r\n                sync_dist=self.multi_gpu,\r\n            )\r\n```\r\n\r\nWhere the metrics are evaluated on a class wise level using.\r\n\r\n```average_acc = self.train_average_accuracy(sigmoid_outputs, y)```\r\n\r\nand the logging function can be placed within the training or validation step using\r\n\r\n```log_class_metric(self, class_metrics=class_acc, metric=\"acc\", phase=\"train\")```"
  },
  {
    "content": "I want to resume training from a checkpoint, but I want to use a different learning rate, How to achieve that? I don't  really care about the training states and don't mind start a fresh training as long as the weights are proprely restored.\r\n\r\nRight now I'm using ``resume_from_checkpoint=ckpt_file`` when creating the trainer, this automatically would give the old learning rate. \r\n\r\nI also tried remove ``resume_from_checkpoint=ckpt_file``, and do \r\n```\r\nnet_learner.load_from_checkpoint(cfg.ckpt_path, cfg=cfg)\r\ntrainer.fit(net_learner, train_data_loader, val_data_loader)\r\n```\r\nbut it seems the weights are erased, and the trainer starts from random weights.\r\n\r\nAny help will be most appreciated, thanks so much!I have the same question. More generally it would be useful to be able to change certain model settings when resuming training, while keeping all other settings the same, or at least be able as you said to restore the model weights and start a new training session with them.I opened this as an [issue](https://github.com/PyTorchLightning/pytorch-lightning/issues/12118). However (as you'll see in the discussion there), it turns out that in my case there was no problem - the `.load_from_checkpoint()` method works as expected. I probably just made a different mistake which caused my loss to (immediately) blow up after resuming training, which I interpreted as arising from the issue that you described of the weights being overwritten with a new initialization. I shouldn't have jumped to that conclusion so quickly as I didn't actually verify that the weights were different in my case. I just tried it again and it works fine now.\r\n\r\nIn your case, it looks like you're using the wrong syntax, which I hadn't spotted but another user did - please refer to the link to see how it should be used. This should solve the problem for you.",
    "meta": {
      "name": "How to continue training with a different learning rate"
    },
    "answer": "I opened this as an [issue](https://github.com/PyTorchLightning/pytorch-lightning/issues/12118). However (as you'll see in the discussion there), it turns out that in my case there was no problem - the `.load_from_checkpoint()` method works as expected. I probably just made a different mistake which caused my loss to (immediately) blow up after resuming training, which I interpreted as arising from the issue that you described of the weights being overwritten with a new initialization. I shouldn't have jumped to that conclusion so quickly as I didn't actually verify that the weights were different in my case. I just tried it again and it works fine now.\r\n\r\nIn your case, it looks like you're using the wrong syntax, which I hadn't spotted but another user did - please refer to the link to see how it should be used. This should solve the problem for you."
  },
  {
    "content": "Hey,\r\n\r\nI set up an AutoEncoder for image reconstruction using the `LightningModule` (i.e. `my_model`). At inference, I currently run `pl.Trainer.test()` to obtain my test metrics, `pl.Trainer.predict()` to obtain my image predictions, and then in a 3rd loop save each prediction obtained from the `pl.Trainer.predict()`-step. This is awfully complicated and ideally I would like to directly save predictions in `test_step()`. I have the following `test_step()`.\r\n\r\n```python\r\ndef test_step(self, batch, batch_idx):\r\n    batch_hat = self(batch)\r\n    loss_test = nn.functional.mse_loss(batch_hat, batch)\r\n    self.log(\"loss_test\", loss_test, on_step=True, on_epoch=True, sync_dist=True)\r\n    \r\n    # I want to add something like this (NOT FUNCTIONAL)\r\n    filepath = trainer.datamodule.dataset_test.my_file_names[batch_idx]\r\n    my_save_function(batch_hat, filepath)\r\n\r\n    return loss_test\r\n```\r\n\r\nand I call `test_step()` with the conventional\r\n\r\n```python\r\ntrainer.test(my_model, my_datamodule)\r\n```\r\n, where `my_datamodule` is `LightningDataModule` and `trainer` is a `pl.Trainer()`.\r\n\r\n\r\n**Question**: How can I access the name of my samples in `batch` within `test_step()` that are stored in `my_datamodule`? \r\n**Goal**: I want to re-use the file names assigned to my input images for the predictions in `test_step()`.\r\n\r\nBest,\r\ndsethzhey @dsethz !\r\n\r\nyou can return the filenames from your `Dataset.__getitem__` which will be available inside the batch and you can access them inside `test_step` easily.",
    "meta": { "name": "Save predictions in `test_step`" },
    "answer": "hey @dsethz !\r\n\r\nyou can return the filenames from your `Dataset.__getitem__` which will be available inside the batch and you can access them inside `test_step` easily."
  },
  {
    "content": "      6 t = pl.Trainer(gpus=[0])\r\n----> 7 lr_finder = t.lr_find(module)\r\n\r\nWhen I try to train a model, I got an error like;\r\n\r\n**\"AttributeError: 'Trainer' object has no attribute 'lr_find' \"**\r\n\r\nHow can I fix this?\r\nThanks,hey @ozgugoksu \r\n\r\nits `trainer.tuner.lr_find`.",
    "meta": {
      "name": "AttributeError: 'Trainer' object has no attribute 'lr_find'"
    },
    "answer": "hey @ozgugoksu \r\n\r\nits `trainer.tuner.lr_find`."
  },
  {
    "content": "Hi, I was recently reading this example from NVIDIA DALI: \r\nhttps://github.com/NVIDIA/DALI/blob/629c57592b9b4e91b8213e6c77c1af179f7dd079/docs/examples/frameworks/pytorch/pytorch-lightning.ipynb\r\n\r\nI wanted to split the model and datamodule apart. In that case, how can I get `local_rank`, `global_rank` and `world_size` for datamodule's setup?hey @austinmw !\r\n\r\nyou can access them using `self.trainer.local_rank`, `self.trainer.global_rank` & `self.trainer.world_size`.",
    "meta": { "name": "self.local_rank in LightningDataModule" },
    "answer": "hey @austinmw !\r\n\r\nyou can access them using `self.trainer.local_rank`, `self.trainer.global_rank` & `self.trainer.world_size`."
  },
  {
    "content": "```python\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\r\n        print(self.trainer.max_steps)\r\n        lr_scheduler = {\r\n            'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer,\r\n                                                             max_lr=self.lr,\r\n                                                             total_steps=self.trainer.max_steps,\r\n                                                             anneal_strategy='linear',\r\n                                                             cycle_momentum=False,\r\n                                                             pct_start=0.1),\r\n            'interval': 'step',\r\n            'frequency': 1\r\n        }\r\n        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler, \"monitor\": 'val_acc'}\r\n```\r\n\r\n```\r\nraise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\r\nValueError: Expected positive integer total_steps, but got -1\r\n```ok i know the answer,",
    "meta": {
      "name": "ValueError: Expected positive integer total_steps, but got -1"
    },
    "answer": "ok i know the answer,"
  },
  {
    "content": "How to load a model saved in PyTorch Lightning in Vanilla PyTorch?check this out: https://pytorch-lightning.readthedocs.io/en/latest/common/production_inference.html#id1",
    "meta": { "name": "Loading Lightning model in PyTorch" },
    "answer": "check this out: https://pytorch-lightning.readthedocs.io/en/latest/common/production_inference.html#id1"
  },
  {
    "content": "### BUG\r\nI finished installing pytorch_lightnin by pip but can not import pytorch_lightning.\r\n\r\n### Environment\r\n\r\n- CUDA: 11.2\r\n- GPU: v100 * 8\r\n- Packges:\r\n-OpenCC==1.1.0\r\n-pytorch-lightning==1.1.2\r\n-six==1.14.0\r\n-tensorboard==2.4.0\r\n-tensorboard-plugin-wit==1.7.0\r\n-threadpoolctl==2.1.0\r\n-tokenizers==0.9.4\r\n-torch==1.10.2+cu113\r\n-transformers==4.1.1\r\n-yacs\r\n-lxml\r\n\r\n- Error message\r\n![image](https://user-images.githubusercontent.com/47704881/154795220-cab9b91e-73df-4c4e-b446-53e17125568a.png)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n@Rafael8830 Could you try reinstalling it? If the issue still persists, could you provide more info about your environment?",
    "meta": {
      "name": "import pytorch_lightning fails with ModuleNotFoundError: No module named 'tensorboard'"
    },
    "answer": "@Rafael8830 Could you try reinstalling it? If the issue still persists, could you provide more info about your environment?"
  },
  {
    "content": "My `ModelCheckpoint` callback:\r\n```py\r\nckpt_callback = pl.callbacks.ModelCheckpoint(\r\n    filename=\"T5-{epoch}-{step}-{val_loss:.2f}-{val_ppl:.2f}\",\r\n    monitor=\"val_loss\",\r\n    save_top_k=-1,\r\n    every_n_train_steps=100\r\n)\r\n```\r\n\r\nQuestions:\r\n1. I got the checkpoints starting with **199** step:\r\n    <img width=\"406\" alt=\"image\" src=\"https://user-images.githubusercontent.com/42370681/154623035-6eac0ab8-ecbd-466d-aa61-3cce8ff09c7f.png\">\r\n    Why does **NOT** `pl` save the checkpoint in `99` step?\r\n2. Why is step not an integer multiple of 100? Because counting starts at 0? It is kind of weird.hey @ShaneTian !\r\n\r\n1. maybe no validation loop is triggered after 99 steps, possibly because total training steps per epoch > 100?\r\n2. yes, it is zero-indexed.\r\n",
    "meta": { "name": "About `ModelCheckpoint` starting point" },
    "answer": "hey @ShaneTian !\r\n\r\n1. maybe no validation loop is triggered after 99 steps, possibly because total training steps per epoch > 100?\r\n2. yes, it is zero-indexed.\r\n"
  },
  {
    "content": "Hi all, is it okay to feed the optimizer that's been initialized outside this code to `pl.LightningModule`?\r\n```\r\ndef Model(pl.LightningModule):\r\n    def __init__(optimizer):\r\n        self.optimizer = optimizer\r\n    def configure_optimizers(self) -> Any:                                                                                      \r\n        optimizer = self.optimizer          # like this                                                                                    \r\n        scheduler = {                                                                                                               \r\n            'scheduler': LambdaLR(optimizer, self.lr_lambda),                                                                       \r\n            'interval': 'step',                                                                                                     \r\n            'frequency': 1,   \r\n        }                                                                                                                                                                                                                   \r\n    return [optimizer], [scheduler]\r\n```\r\nThanks!yes, I think you can, but not a good practice, we recommend.\r\n\r\njust curious, why are you feeding it like that?\r\n@rohitgr7 \r\nI know it's been a while but the problem still exists, as it turns out that the optimizer must be created before initializing the quantizer.  \r\n\r\n1. From what I understand, the `configure_optimizers` is called after `__init__`, so I'm not sure how to initialize the optimizer in `configure_optimizers`, before initializing the quantizer.\r\n2. There are only 2 cases when I need the quantizer. \r\n    a. save the quantized model (Either when valid loss is low, or simply save it at every epoch)\r\n       `torch.save(quantizer.get_quantized_state(), \"some_file.th\")`\r\n    b. add `quantizer.model_size` to training loss\r\n\r\n```py\r\nclass LitModel(LightningModule):\r\n    def __init__():\r\n     \r\n    def training_step():\r\n        loss = loss_function(predict, answer)\r\n        loss += quantizer.model_size()               # quantizer is needed here!\r\n        return loss \r\n\r\n   def configure_optimizers():\r\n```\r\nDo you have any thoughts on the code structure? \r\n\r\nThank you and hope to get your feedback!",
    "meta": {
      "name": "Is it okay to feed optimizer to `configure_optimizers`"
    },
    "answer": "yes, I think you can, but not a good practice, we recommend.\r\n\r\njust curious, why are you feeding it like that?\r\n"
  },
  {
    "content": "With a basic Datamodule like:\r\n```\r\nclass MyDM(pl.lightningDataModule):\r\n    def __init__(self,):\r\n        <init some stuff>\r\n\r\n    def setup(self, stage:typing.Optional[str] = None):\r\n        .... <sort out dataset etc>\r\n\r\n   def train_dataloader(self):\r\n         ....\r\n   etc etc\r\n\r\nmodel = MyModel()\r\ndata = MyDM()\r\ntrainer pl.Trainer(reload_dataloaders_every_n_epochs=5)\r\ntrainer.fit(model, data)\r\n```\r\nDoes the flag reload_dataloaders_every_n_epochs=N cause data.setup() to be called every reload. \r\nI expect my dataset to be constantly changing and am currently assuming that I can define anything I don't expect to be changing in __init__ and anything I will need to change every N epochs in setup. A quick look at the pl.trainer source code (specifically the reset_train_dataloader method) doesn't immediately elucidate this for me.no, it doesn't call setup at every reload, just the corresponding `_dataloader` hook. You can define the datasets in setup and access them inside dataloader_hooks or can initialize the corresponding dataset inside dataloader_hook as well.",
    "meta": { "name": "Clarification on reload_dataloaders_every_epoch" },
    "answer": "no, it doesn't call setup at every reload, just the corresponding `_dataloader` hook. You can define the datasets in setup and access them inside dataloader_hooks or can initialize the corresponding dataset inside dataloader_hook as well."
  },
  {
    "content": "I implemented pytorch lightning-based learning as follows.\r\n\r\n------------------------------------------\r\n\r\ndm = build_datamodule(config)\r\nmodel = build_model(config)\r\n\r\ntrainer = Trainer(\r\n...\r\naccelerator=\"ddp\",\r\n...\r\n)\r\n\r\ntrainer.fit(model, dm)\r\n\r\n------------------------------------------\r\n\r\nIn this situation, in order to set different model parameters for each gpu process, distributed.get_rank() must be called at the stage of model building.\r\nHowever, the trainer.fit function doesn't seem to be able to implement this because it requires an already built model.\r\nI wonder if there is any other way to do this.\r\nHello, you are right that this currently isn't supported. I am working on adding this feature as part of this issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/11922\n\nCould you confirm that the issue and proposed solution meet your needs?",
    "meta": {
      "name": "How to call torch.distributed.get_rank() in model building phase"
    },
    "answer": "Hello, you are right that this currently isn't supported. I am working on adding this feature as part of this issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/11922\n\nCould you confirm that the issue and proposed solution meet your needs?"
  },
  {
    "content": "I am trying to import the BoringModel in this colab https://colab.research.google.com/drive/1wrPzif6zddJvdDgMnYsa05172Nv3WzWk?usp=sharing but am getting the error `ModuleNotFoundError: No module named 'tests.helpers'`\r\n\r\nAny ideas how to fix this?with pip install it installs just the core-package (`pytorch_lightning`) and not any other directories. Since tests in this env will be just another package, not specific to pytorch_lightning.",
    "meta": {
      "name": "Can't import `tests.helpers.boring_model.BoringModelBoringModel` in colab"
    },
    "answer": "with pip install it installs just the core-package (`pytorch_lightning`) and not any other directories. Since tests in this env will be just another package, not specific to pytorch_lightning."
  },
  {
    "content": "I was wondering if anyone has observed odd performance when training multi-GPU models? I\u2019ve developed a script which trains a toy dataset (in this case the cats and dogs model), using a ResNet or EfficientNet. The script works fine locally on the GPU. However, when I move the script to the cloud and train using multiple GPUs strange things start to happen. The script trains fine on the cloud using 1 GPU, albeit slow as I was testing using a M60. However, if I run the same script on 4x K80 with ddp I find that the training process is around ~15% slower (which I\u2019m guessing is the difference between K80 and M60).\r\n\r\n![2022-02-13 11_23_24-Window](https://user-images.githubusercontent.com/98221950/153755313-7fee130e-315f-4e5d-96ac-578960bdd5a2.png)\r\n\r\nI checked GPU usage and the GPUs are all being used. However, model performance seems slower/worse than using just one GPU. Any ideas why this could be?\r\n\r\n@deepbakes Could this be because of `benchmark=True` ?",
    "meta": { "name": "Odd Performance Using Multi-GPU + Azure" },
    "answer": "@deepbakes Could this be because of `benchmark=True` ?"
  },
  {
    "content": " Hi everyone.\r\n\r\nI am trying to use 4 gpus in a single node to train my model with DDP strategy. But everytime I run **trainer.fit**, the whole bunch of codes are executed 4 times repeatedly, and it requires 4 times of CPU memory compared to a single GPU case.\r\n\r\nI am not sure whether it is intended behavior or not. I ran the following sample code. It trains MNIST data on 4 gpus.\r\n\r\n    import warnings\r\n    warnings.filterwarnings(\"ignore\")\r\n    \r\n    import os\r\n    import torch\r\n    from pytorch_lightning import LightningModule, Trainer\r\n    from torch import nn\r\n    from torch.nn import functional as F\r\n    from torch.utils.data import DataLoader#, random_split\r\n    from torchvision import transforms\r\n    from torchvision.datasets import MNIST\r\n\r\n    PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\r\n\r\n    class MNISTModel(LightningModule):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.l1 = torch.nn.Linear(28 * 28, 10)\r\n\r\n        def forward(self, x):\r\n            return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n        def training_step(self, batch, batch_nb):\r\n            x, y = batch\r\n            loss = F.cross_entropy(self(x), y)\r\n            return loss\r\n\r\n        def configure_optimizers(self):\r\n            return torch.optim.Adam(self.parameters(), lr=0.02)\r\n\r\n\r\n    if __name__ == '__main__':\r\n        print('Hello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\r\n        mnist_model = MNISTModel()\r\n\r\n        train_ds = MNIST(PATH_DATASETS, train=True, download=True, transform=transforms.ToTensor())\r\n        train_loader = DataLoader(train_ds, batch_size=256)\r\n\r\n        trainer = Trainer(gpus=4, strategy='ddp', max_epochs=1, replace_sampler_ddp=True, num_nodes=1)\r\n        trainer.fit(mnist_model, train_loader)\r\n\r\n\r\nAnd I got the following output:\r\n\r\n    Hello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    GPU available: True, used: True\r\n    TPU available: False, using: 0 TPU cores\r\n    IPU available: False, using: 0 IPUs\r\n    Hello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\r\n    Hello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    Hello world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n    initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\r\n    initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\r\n    initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\r\n    ----------------------------------------------------------------------------------------------------\r\n    distributed_backend=nccl\r\n    All distributed processes registered. Starting with 4 processes\r\n    ----------------------------------------------------------------------------------------------------\r\n\r\nThe training is done well, but the thing is that 'Hello world!' is printed four times. My problem here is that train data is loaded four times also and it takes four times of CPU memory. I am not sure whether it is the intended behavior or am I doing something wrong?\r\n\r\nHow do you deal with DDP if train data is too large to be copied by multiple (=gpu num) times?\r\nhey @earendil25!\r\n\r\nthis is how DDP works exactly. To populate data across devices, DistributedSampler is added to avoid data duplication on each device and the model is wrapped around DistributedDataParallel to sync gradients. The command is launched on each device individually. Alternatively, you can also try DDP_Spawn, which creates spawn processes and won't execute the whole script on each device.",
    "meta": { "name": "trainer.fit(strategy='ddp') executes code repeatedly" },
    "answer": "hey @earendil25!\r\n\r\nthis is how DDP works exactly. To populate data across devices, DistributedSampler is added to avoid data duplication on each device and the model is wrapped around DistributedDataParallel to sync gradients. The command is launched on each device individually. Alternatively, you can also try DDP_Spawn, which creates spawn processes and won't execute the whole script on each device."
  },
  {
    "content": "During `fit()` I want to log multiple scalars to a _single_ chart using the wandb logger. I am trying to achieve this with this example code:\r\n\r\n`self.log('val/top-k', {'1': 0.6, '5': 0.8, '10': 0.9})`\r\n\r\nOn wandb this creates _three_ charts with the headings `val/top-k.1` and `(...).5` and `(...).10` resp.\r\n\r\nAm I doing this wrong or do I need to call the wandb logger directly (which I try to avoid to keep the code logger agnostic).\r\n\r\nThanks!hey @hogru!\r\n\r\ncan you share some references to how wandb plots multiple charts? I don't think there is a direct API. Personally, I used to log them individually and combine them in a single chart afterward. But if there is an option to do that via its API, then we can look more and think about adding support for it if it's not already there.Hello @rohitgr7,\r\n\r\nthank you for the quick response! You are right. I am new to both Pytorch Lightning and W&B and based on TensorBoard experience and looking at the possibility to log a dict I was assuming that there \"must\" be a way to do this. Now that I have seen that W&B allows to customize the graphs in their UI (and more importantly it keeps the setup across runs which is nice) I can do that within the W&B Web UI.\r\n\r\nThanks again!",
    "meta": { "name": "Logging multiple scalars to a single wandb chart" },
    "answer": "hey @hogru!\r\n\r\ncan you share some references to how wandb plots multiple charts? I don't think there is a direct API. Personally, I used to log them individually and combine them in a single chart afterward. But if there is an option to do that via its API, then we can look more and think about adding support for it if it's not already there."
  },
  {
    "content": "Hi,\r\n\r\nAs part of my _validation_step_ in a AE I am trying to return an object containing input and reconstructed output.\r\n\r\n_validation_epoch_end_ does not get the accumulated outputs.\r\n\r\nHere's the code:\r\n```\r\ndef validation_step(self, batch, batch_idx):\r\n        x, x_recon, _, _, _ = self.forward(batch)\r\n        outputs = {\r\n            'x': x,\r\n            'x_recon': x_recon\r\n        }\r\n        return outputs\r\n```\r\n\r\nthese are the shapes:\r\n`x.shape:  torch.Size([2, 1, 257, 63])`\r\n`x_recon.shape:  torch.Size([2, 1, 257, 63])`\r\n\r\nand this is what _validation_step_outputs_ looks like:\r\n`[{'x': tensor(0.0033, device='cuda:0'), 'x_recon': tensor(-0.0102, device='cuda:0')}]`\r\n\r\nAny idea why?\r\n\r\nThanks\r\n\r\n\r\nhey @mcomunita!\r\n\r\nthis shouldn't happen. Quick ques: are you using DP?",
    "meta": { "name": "Validation step: error when trying to return object" },
    "answer": "hey @mcomunita!\r\n\r\nthis shouldn't happen. Quick ques: are you using DP?"
  },
  {
    "content": "Hi,\r\n\r\nI am trying to optimise the hyperparameters of my network using raytune. My implementation is pretty much based on this: \r\nhttps://docs.ray.io/en/master/tune/tutorials/tune-pytorch-lightning.html#selecting-a-scheduler\r\n\r\nWhen I train my network using pre-set hyperparameters, it works smoothly. The problems come from the callback, so when I add the following line:\r\n`TuneReportCallback({\"loss\":\"val_loss\"}, on=\"validation_end\")`\r\n\r\nI get the following error:\r\n![Screenshot 2022-02-15 at 17 34 23](https://user-images.githubusercontent.com/57765003/154106370-c343c64c-b255-4bb7-86a3-9fdf2293492a.png)\r\n\r\nAnyone knows how to solve this??\r\nI don't think the problem is with my code as I haven't done anything different compared to the tutorial!\r\n\r\nMy code can be found here:\r\nhttps://github.com/annalauralerede/anomaly-detection/blob/main/lstmae_pl_opt.pyI think it dues to this : https://github.com/ray-project/ray/issues/20741\r\n\r\nAs of ray[tune]==1.10.0, either downgrade your pytorch-lightning to 1.4.\r\nOr upgrade your raytune to be compatible with pytorch-lightning 1.5+ ( the fix has been merged in this commit https://github.com/ray-project/ray/pull/20562).\r\n`$ sudo pip install ray==1.11.0rc0`\r\n\r\n",
    "meta": {
      "name": "AttributeError: 'Trainer' object has no attribute 'running_sanity_check\""
    },
    "answer": "I think it dues to this : https://github.com/ray-project/ray/issues/20741\r\n\r\nAs of ray[tune]==1.10.0, either downgrade your pytorch-lightning to 1.4.\r\nOr upgrade your raytune to be compatible with pytorch-lightning 1.5+ ( the fix has been merged in this commit https://github.com/ray-project/ray/pull/20562).\r\n`$ sudo pip install ray==1.11.0rc0`\r\n\r\n"
  },
  {
    "content": "Hi all, I tried to train a model with pl. And I just ran the `trainer.fit()` as below:\r\n```python\r\ntrainer.fit(model, train_dataloaders=model.train_dataloader(),\r\n                val_dataloaders=model.val_dataloader())\r\n```\r\nAnd I found that `model.training == False`, when it gets into `forward()`...\r\nIs there any solution or does anybody know the potential reason for this?\r\nOr does anyone know where can I find the source code for `training_step()` so that I can check and debug `forward()` in `fit()`?\r\n\r\nThank you very much.does it print `self.training = False` for all the training steps? maybe you might have checked it during the initial steps where val sanity check happens.",
    "meta": {
      "name": "When doing `fit()`, `self.training` in `forward()` keeps turning into False?"
    },
    "answer": "does it print `self.training = False` for all the training steps? maybe you might have checked it during the initial steps where val sanity check happens."
  },
  {
    "content": "Am using the following callback,\r\n```python\r\n checkpoint_callback = ModelCheckpoint(monitor='val/loss',\r\n                                        mode='min',\r\n                                        save_last=True,\r\n                                        filename=cfg.CALLBACKS.FILENAME,\r\n                                        auto_insert_metric_name=cfg.CALLBACKS.AUTO_INSERT_METRIC_NAME,\r\n                                        dirpath=LOGGER_DIR)\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/37276766/153702380-cb8af624-4df0-4165-bcda-f27385bbb0aa.png)\r\n\r\nI am not sure what is going wrong. Am using `F.cross_entropy` for loss.- Lowest loss (say CE) will not always give you the highest accuracy. (https://kharshit.github.io/blog/2018/12/07/loss-vs-accuracy)\r\n- So, always use the accuracy (or desired metric at hand) to select the best model instead of loss.\r\n\r\n(This is probably common knowledge but this was news for me. Putting it out there so someone else doesn't bang their head for two days!)",
    "meta": { "name": "lowest val/loss ckpt != highest val/Accuracy" },
    "answer": "- Lowest loss (say CE) will not always give you the highest accuracy. (https://kharshit.github.io/blog/2018/12/07/loss-vs-accuracy)\r\n- So, always use the accuracy (or desired metric at hand) to select the best model instead of loss.\r\n\r\n(This is probably common knowledge but this was news for me. Putting it out there so someone else doesn't bang their head for two days!)"
  },
  {
    "content": "My code is not printing `print('train acc loss',acc,loss)` in `trained_epoch_end` but its printing `print('val acc loss',acc,loss)` in `validation_epoch_end` \r\n```\r\nclass Model(LightningModule):\r\n  def __init__(self):\r\n    super(Model,self).__init__()\r\n    self.model=ResneT(21)\r\n    self.lr=1e-3\r\n    self.bs=128\r\n    self.worker=6\r\n    self.acc=torchmetrics.Accuracy()\r\n    self.creterion=nn.BCEWithLogitsLoss()\r\n    self.scheduler='lambda'\r\n  def forward(self,x):\r\n    x=self.model(x)\r\n    return x\r\n\r\n  def configure_optimizers(self):\r\n    opt=torch.optim.AdamW(params=self.parameters(),lr=self.lr )\r\n    return opt\r\n\r\n  def train_dataloader(self):\r\n    dataset=DataReader(train_df)\r\n    dataloader=DataLoader(dataset,batch_size=self.bs,num_workers=self.worker,shuffle=True,\r\n                         pin_memory=True,collate_fn=collate_fn)\r\n    return dataloader\r\n\r\n  def training_step(self,batch,batch_idx):\r\n    signal,label=batch\r\n    out=self(signal.float())\r\n    loss=self.creterion(out.flatten(),label.float().flatten())\r\n    acc=self.acc(out.flatten(),label.long().flatten())\r\n    return {'loss':loss,'acc':acc}\r\n\r\n  def trained_epoch_end(self,outputs):\r\n    acc=torch.stack([x['acc'] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n    loss=torch.stack([x['loss'] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n    self.log('train acc',acc)\r\n    self.log('train loss',loss)\r\n    print('train acc loss',acc,loss)\r\n\r\n  def val_dataloader(self):\r\n    dataset=DataReader(val_df)\r\n    dataloader=DataLoader(dataset,batch_size=self.bs,num_workers=self.worker,shuffle=False,\r\n                          pin_memory=True,\r\n                         collate_fn=collate_fn)\r\n    return dataloader\r\n\r\n  def validation_step(self,batch,batch_idx):\r\n    signal,label=batch\r\n    out=self(signal.float())\r\n    loss=self.creterion(out.flatten(),label.float().flatten())\r\n    acc=self.acc(out.flatten(),label.long().flatten())\r\n    return {'loss':loss,'acc':acc}\r\n\r\n  def validation_epoch_end(self,outputs):\r\n    acc=torch.stack([x['acc'] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n    loss=torch.stack([x['loss'] for x in outputs]).mean().detach().cpu().numpy().round(2)\r\n    print('val acc loss',self.current_epoch,acc,loss)\r\n    self.log('val acc',acc)\r\n    self.log('val loss',loss)\r\n```You need to implement `training_epoch_end`, not `trained_epoch_end`: https://github.com/PyTorchLightning/pytorch-lightning/blob/1515ef90ee2724bcba46e1434eb4b4f9719ebdd7/pytorch_lightning/core/lightning.py#L689",
    "meta": { "name": "Code not printing values in trained_epoch_end" },
    "answer": "You need to implement `training_epoch_end`, not `trained_epoch_end`: https://github.com/PyTorchLightning/pytorch-lightning/blob/1515ef90ee2724bcba46e1434eb4b4f9719ebdd7/pytorch_lightning/core/lightning.py#L689"
  },
  {
    "content": "Hello,\r\nI am trying to build a model which uses multiple optimisers. When I try to train the model I get the error `validation_step() missing 1 required positional argument: 'optimizer_idx'`. I have reproduced this error on the BoringModel used for bug reports:\r\n\r\n```\r\n#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            loss = self(batch).sum()\r\n            self.log(\"train_loss\", loss)\r\n        if optimizer_idx == 1:\r\n            loss = self(batch).sum()\r\n            self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            loss = self(batch).sum()\r\n            self.log(\"valid_loss\", loss)\r\n        if optimizer_idx == 1:\r\n            loss = self(batch).sum()\r\n            self.log(\"valid_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def test_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            loss = self(batch).sum()\r\n            self.log(\"test_loss\", loss)\r\n        if optimizer_idx == 1:\r\n            loss = self(batch).sum()\r\n            self.log(\"test_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        opt_a = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        opt_b = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        return [opt_a, opt_b], []\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        limit_test_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nWhat am I missing?\r\n\r\nThanks for the help!Validation doesn't require optimizers. Try removing the \n`optimizer_idx` argument from your `validation_step` method definition ",
    "meta": { "name": "Using Multiple Optimisers gives Index Error?" },
    "answer": "Validation doesn't require optimizers. Try removing the \n`optimizer_idx` argument from your `validation_step` method definition "
  },
  {
    "content": "Hello,\r\n\r\nI am trying to remove some layers from `DistributedDataParallel` to prevent them being synchronized between devices.\r\n\r\nI spent last 6 hours googling, and I have found out, that there's a attribute `_ddp_params_and_buffers_to_ignore` which can be set to module that is passed to `DistributedDataParallel` constructor. I've implemented custom strategy plugin to `Trainer`, I have checked that the parameters are then passed to a `parameters_to_ignore` attribute of the `DistributedDataParallel` but somehow if I check gradients, of the layer, they are always the same.\r\n\r\nIs there some simpler way to remove some layer / module from being synchronized between more devices in DDP strategy?\r\n\r\nThank you in advance for any help!hey @Honzys !\r\n\r\ncan you confirm whether the list of the param names you have set to be ignored are same as that of then your model is initialized in LightningModule:\r\n\r\n\r\n```py\r\nclass LitModel(LightningModule):\r\n    def __init__(self, ...):\r\n        super().__init__()\r\n        self.model = ...\r\n        self.named_parameters() # extract key names from this and check whether the list of keys are exactly same\r\n```\r\nfor reference, you can check out the test here where this feature was integrated: https://github.com/pytorch/pytorch/pull/44826/filesOkay, It was my mistake, I deeply apologize for wasting your time there. The layer indeeds gets removed from the DistributedDataParallel (or rather not even getting there).\r\n\r\nBut I've found another error when trying to set the `_ddp_params_and_buffers_to_ignore` inside the `LightningModule`, so I've created issue here - https://github.com/PyTorchLightning/pytorch-lightning/issues/11844 .\r\n\r\nThank you anyway!",
    "meta": { "name": "Remove parameters from autograd backward hook" },
    "answer": "Okay, It was my mistake, I deeply apologize for wasting your time there. The layer indeeds gets removed from the DistributedDataParallel (or rather not even getting there).\r\n\r\nBut I've found another error when trying to set the `_ddp_params_and_buffers_to_ignore` inside the `LightningModule`, so I've created issue here - https://github.com/PyTorchLightning/pytorch-lightning/issues/11844 .\r\n\r\nThank you anyway!"
  },
  {
    "content": "The documentation advises the usage of '_type_as_' when initializing new tensors in multi-gpu settings:\r\n\r\n> [https://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html#init-tensors-using-type-as-and-register-buffer](url)\r\nWhen you need to create a new tensor, use type_as. This will make your code scale to any arbitrary number of GPUs or TPUs with Lightning.\r\n\r\nThe example shows a case where a new tensor is initialized inside a LightningModule forward function:\r\n\r\n> def forward(self, x):\r\n         z = torch.Tensor(2, 3)\r\n         z = z.type_as(x)\r\n\r\nPresumably _x_ is a tensor that has already been initialized on the target gpu.\r\n\r\nMy question is what to do in the case where we want to initialize a new tensor on the target gpu, and we **do not** have access to a tensor that has already been initialized on the target gpu? \r\n\r\nFor example, how does one properly initialize a new tensor when it is created inside a Dataset constructor that is instantiated during LightningDataModule _setup()_?\r\n\r\n> class SomeDataModule(LightningDataModule):\r\n...\r\n    def setup(self, stage: Optional[str] = None):\r\n        if stage in (None, \"fit\"):\r\n            dataset = SomeDataset()\r\n...\r\n\r\nwhere:\r\n\r\n> class SomeDataset(Dataset):\r\n    def __init__(self):\r\n        self.some_tensor = torch.Tensor(2,3)\r\n\r\nWill using _type_as_ on the new tensor initialize the data on the target gpu?\r\n\r\n> self.some_tensor = self.some_tensor.type_as(self.some_tensor)\r\n\r\nOr is a different approach necessary? (e.g. [register_buffer()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer))\r\n\r\n\r\nif it's part of the dataset, it's already moved to the target device when a batch is created while iterating over the dataset.",
    "meta": { "name": "Multi-GPU Tensor Initialization Question" },
    "answer": "if it's part of the dataset, it's already moved to the target device when a batch is created while iterating over the dataset."
  },
  {
    "content": "Thanks to **all the contributors of PyTorch Lightning** for a fantastic product! \r\n\r\nI want to save a checkpoint, hparams & tfevents after training finishes. I have written this callback:\r\n\r\n```python\r\nclass AfterTrainCheckpoint(pl.Callback):\r\n    \"\"\"\r\n    Callback for saving the checkpoint weights, hparams and tf.events after training finishes\r\n    \"\"\"\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\r\n        print(f\"Saving final checkpoint...\")\r\n        # As we advance one step at end of training, we use `global_step - 1`\r\n        final_checkpoint_name = f\"final_models/final_step_{trainer.global_step - 1}.ckpt\"\r\n        final_hparams_name = f\"final_models/final_step_{trainer.global_step - 1}.yaml\"\r\n\r\n        trainer.save_checkpoint(final_checkpoint_name)\r\n        save_hparams_to_yaml(config_yaml=final_hparams_name, hparams=trainer.model.hparams)\r\n```\r\n\r\n1. Is this the best 'Lightning' way to achieve this?\r\n2. How can I save the final `events.out.tfevents` file to a new directory?\r\n3. Should I be setting `save_last=True`? seen in `ModelCheckpoint` in [the docs](https://pytorch-lightning.readthedocs.io/en/stable/_modules/pytorch_lightning/callbacks/model_checkpoint.html#ModelCheckpoint). I am slightly confused about \"monitor metrics logged during training/validation steps or end of epochs are not guaranteed to be available at this stage.\"hey @dispoth !!\r\n\r\n1. I'd say use `on_fit_end` instead, since the last checkpoint in the model checkpoint is saved in this hook, so it won't guarantee to have that ckpt when your callback calls it.\r\n2. you can copy the log files directly? the are available inside `trainer.log_dir`.\r\n3. yes, they will be available during both `on_train_end` and `on_fit_end`.",
    "meta": {
      "name": "Saving checkpoint, hparams & tfevents after training to separate folder"
    },
    "answer": "hey @dispoth !!\r\n\r\n1. I'd say use `on_fit_end` instead, since the last checkpoint in the model checkpoint is saved in this hook, so it won't guarantee to have that ckpt when your callback calls it.\r\n2. you can copy the log files directly? the are available inside `trainer.log_dir`.\r\n3. yes, they will be available during both `on_train_end` and `on_fit_end`."
  },
  {
    "content": "I want to enable dropout during `.predict` and tried implementing the following:\r\n\r\n```\r\nmodel.eval() \r\nfor m in model.modules():\r\n    if m.__class__.__name__.startswith('Dropout'):\r\n        m.train()\r\n                \r\n...\r\n\r\ntrainer.predict(\r\n    model, dataloaders=data_loader, return_predictions=True\r\n)\r\n```\r\n\r\nIt seems like `.predict` is overriding this because I get identical predictions with different seeds. \r\n\r\nCan someone explain how to accomplish this, or point me to the relevant docs? (Couldn't find them & tried looking for while) \r\n\r\nThank you!hey @35ajstern !\r\n\r\nyou can enable this inside `predict_step` itself. Check this out: https://github.com/PyTorchLightning/pytorch-lightning/blob/f35e2210e240b443fd4dafed8fe2e30ee7d579ea/docs/source/common/production_inference.rst#prediction-api\r\n\r\nthis is part of a PR, will be available in the docs once merged.",
    "meta": { "name": "Enabling dropout during trainer.predict" },
    "answer": "hey @35ajstern !\r\n\r\nyou can enable this inside `predict_step` itself. Check this out: https://github.com/PyTorchLightning/pytorch-lightning/blob/f35e2210e240b443fd4dafed8fe2e30ee7d579ea/docs/source/common/production_inference.rst#prediction-api\r\n\r\nthis is part of a PR, will be available in the docs once merged."
  },
  {
    "content": "I would like to create a hook that automatically uploads checkpoints to the cloud (e.g., AWS, Azure) when they're created. I tried using `on_save_checkpoint` roughly like this:\r\n\r\n```\r\ndef on_save_checkpoint(self, trainer: pl.Trainer, pl_module: pl.LightningModule, checkpoint: Dict[str, Any]) -> dict:\r\n    checkpoint_bytes = io.BytesIO()\r\n    torch.save(checkpoint, checkpoint_bytes)\r\n    # Upload the BytesIO somewhere...\r\n```\r\n\r\nHowever, states for optimizers, schedulers, AMP, etc. are added after `on_save_checkpoint` hooks are called. Is there an elegant way to create a hook that receives the **fully formed checkpoint state**?hey @dcharatan !\r\n\r\nI'd rather suggest using the [remote filesystems](https://pytorch-lightning.readthedocs.io/en/latest/common/remote_fs.html#remote-filesystems). You can also specify the remote path inside `ModelCheckpoint`.\r\n\r\nor use [CheckpointIO](https://pytorch-lightning.readthedocs.io/en/latest/common/checkpointing.html#custom-checkpoint-io-plugin) plugin.",
    "meta": { "name": "Hook for Fully Formed Checkpoints" },
    "answer": "hey @dcharatan !\r\n\r\nI'd rather suggest using the [remote filesystems](https://pytorch-lightning.readthedocs.io/en/latest/common/remote_fs.html#remote-filesystems). You can also specify the remote path inside `ModelCheckpoint`.\r\n\r\nor use [CheckpointIO](https://pytorch-lightning.readthedocs.io/en/latest/common/checkpointing.html#custom-checkpoint-io-plugin) plugin."
  },
  {
    "content": "I want to create a hook that uploads checkpoints to cloud storage (e.g. AWS, Azure). I tried using the `on_save_checkpoint` hook as follows:\r\n\r\n```\r\ndef on_save_checkpoint(self, trainer: pl.Trainer, pl_module: pl.LightningModule, checkpoint: Dict[str, Any]) -> dict:\r\n    checkpoint_bytes = io.BytesIO()\r\n    torch.save(checkpoint, checkpoint_bytes)\r\n    # Upload the BytesIO...\r\n```\r\n\r\nHowever, states for optimizers, learning rate schedulers, etc. are added to the checkpoint dict after `on_save_checkpoint` is called. Is there an elegant way to create a hook that receives **fully formed checkpoints**?\r\n\r\nedit: sorry, this is a duplicate -- GitHub was giving me errors when I postedSee #11704.",
    "meta": { "name": "Hook for Fully Formed Checkpoints" },
    "answer": "See #11704."
  },
  {
    "content": "Hello,\r\n\r\nI\u00b4m running my model in a cluster with multiples GPUs (2). My problem is that I would like to access all the datapoints in the batch (predictions and labels). Because I\u00b4m using more than 2 GPUs, my batch in divided between those two devices for parallelisation purposes, which means than when I access the data in the batch in eval/training, I\u00b4m getting just half the batch.\r\n\r\nHow could I obtain the complete batch and the predictions of the model that are divided among different devices/GPUs? @rohitgr7 suggested using self.all_gather, but after trying it on my LightningModule\u2019s forward method, I get just half the batch, that is, just the data stored in one of the two GPUs being used.\r\n\r\nThanks!\r\n\r\nPD:  may it be possible to access this info through \"validation_epoch_end\", \"test_epoch_end\", etc?if you are going to use the complete batch on the single GPU, then why use DDP?\r\n\r\nif you need predictions on a single device, you can rather gather all the predictions using `all_gather`.",
    "meta": { "name": "Get batch\u2019s datapoints across all GPUs" },
    "answer": "if you are going to use the complete batch on the single GPU, then why use DDP?\r\n\r\nif you need predictions on a single device, you can rather gather all the predictions using `all_gather`."
  },
  {
    "content": "How does gradient accumulation interact with DeepSpeed learning rate scheduling (e.g. the per-step warm-up scheduler)? Is the learning rate updated after every iteration, or only after the model weights are ultimately updated?it considers the accumulation before doing lr_scheduler_step:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/86b177ebe5427725b35fde1a8808a7b59b8a277a/pytorch_lightning/loops/epoch/training_epoch_loop.py#L387-L390",
    "meta": { "name": "Gradient accumulation + DeepSpeed LR scheduler" },
    "answer": "it considers the accumulation before doing lr_scheduler_step:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/86b177ebe5427725b35fde1a8808a7b59b8a277a/pytorch_lightning/loops/epoch/training_epoch_loop.py#L387-L390"
  },
  {
    "content": "Hi,\r\n\r\nI see the doc describing the functions of LightningDataModule.\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/datamodules.html#Defining-The-MNISTDataModule\r\n\r\nHere is my thinking. If some variable, e.g., a transform, can be defined in __init__ function, and later shared across different GPUs. Theoretically, if we load data in __init__, the data should also be able to transfer to different GPUs similarly. In the case of a single machine with multiple GPUs, the data will be copied multiple times in the memory. In the case of multiple machines, the data will broadcast through the network from the main node to other nodes. Broadcasting large data through networks may have efficiency issue, which is why we had better load data in the setup function.\r\n\r\nPlease let me know whether my analysis is correct or not. Basically, I am not clear about how the variables, e.g., i.e. self.something, defined in __init__ are shared across multiple GPUs/machines. Thanks!\r\n\r\n@zhiqiangdon,\r\n\r\nThe data don't get broadcasted. They are just manipulated (downloaded, loaded, processed, etc...)\r\n\r\nLoading isn't any issue, but downloading or preprocessing in a distributed setting is.\r\n\r\nImagine the following scenario with 2 processes.\r\n\r\nA -> download the data at time t1 and B start to download the data at time t2. Both process will override each other and the data will corrupted. Same thing with preprocessing.\r\n\r\nIf you don't do any data manipulation, it is entirely fine loading the data in init.\r\n\r\nHowever, if you do, use the `prepare_data` method called only on a single process (rank 0) and anything else you want within the setup method.Thanks @tchaton,\r\n\r\n1. Based on your description, I guess `prepare_data` has some lock mechanism in the distributed setting. That is, if one process starts running `prepare_data`, it will block others from running `prepare_data` to avoid duplication. Right?\r\n2. If using multiple machines, I guess using `prepare_data` may become tricky. There are two cases. First, if multiple machines share the same disk, only one process among all the machines should run `prepare_data`. Otherwise, the data overriding and corruption may happen. Second, if each machine has an independent disk, then each machine should have one process running `prepare_data`. Is this the underhood logic of how `prepare_data` works?\r\n3. Do you mean each process runs `__init__` function independently in a similar way as `setup`?\r\n4. If `__init__` and `setup` functions both run distributedly, is one reason splitting them due to the `prepare_data`? The running order is `__init__` -> `prepare_data` -> `setup`. If there is no need to download and preprocess data, then we can move code from `setup` to `__init__`. Right?\r\n@zhiqiangdon \r\n1. lightning just runs `prepare_data` on the main process before the distributed process actually starts so there is no blocking happening behind the scenes.\r\n2. To tackle this issue we have [prepare_data_per_node](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#prepare-data-per-node). A node is just a machine. If they share the disk then `prepare_data_per_node` should be set to False.\r\n3. User runs the `__init__` function when they initialize the DataModule, lightning just send to across devices.Thanks @rohitgr7,\r\n\r\nJust to clarify. If I use `self.data = load_data(data_path)` in `__init__`, will the data be loaded only by the rank 0 process? If so, will the self.data be sent to across devices later by lightning? ",
    "meta": {
      "name": "What if I load data in __init__ function of LightningDataModule"
    },
    "answer": "@zhiqiangdon \r\n1. lightning just runs `prepare_data` on the main process before the distributed process actually starts so there is no blocking happening behind the scenes.\r\n2. To tackle this issue we have [prepare_data_per_node](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#prepare-data-per-node). A node is just a machine. If they share the disk then `prepare_data_per_node` should be set to False.\r\n3. User runs the `__init__` function when they initialize the DataModule, lightning just send to across devices."
  },
  {
    "content": "Hey!\r\n\r\nI'm trying to use `LightningArgumentParser.link_arguments` to link an argument from the Datamodule to the init_args of the LR scheduler with:\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n```\r\nand I get the following error:\r\n```python\r\nValueError: No action for key \"lr_scheduler.init_args.num_training_steps\".\r\n```\r\n\r\nI was wondering if such thing is possible, or is linking to `init_args` is exclusive to the model and data classes?\r\n\r\n<details>\r\n  <summary><b>Code to reproduce</b></summary>\r\n\r\n  `trainer.py`\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch.nn\r\nfrom pytorch_lightning.utilities.cli import LR_SCHEDULER_REGISTRY\r\nfrom pytorch_lightning.utilities.cli import LightningCLI\r\nfrom torch.optim import Optimizer\r\nfrom torch.optim.lr_scheduler import LambdaLR\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\n\r\nclass MyLightningCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n\r\n\r\n@LR_SCHEDULER_REGISTRY\r\nclass WarmupLR(LambdaLR):\r\n    def __init__(\r\n        self,\r\n        optimizer: Optimizer,\r\n        warmup_proportion: float,\r\n        num_training_steps: int,\r\n        last_epoch=-1,\r\n    ) -> None:\r\n        self.num_training_steps = num_training_steps\r\n        self.num_warmup_steps = round(num_training_steps * warmup_proportion)\r\n        super().__init__(optimizer, lr_lambda=self.lr_lambda, last_epoch=last_epoch)\r\n\r\n    def lr_lambda(self, current_step: int) -> float:\r\n        if current_step < self.num_warmup_steps:\r\n            return float(current_step) / float(max(1, self.num_warmup_steps))\r\n        return max(\r\n            0.0,\r\n            float(self.num_training_steps - current_step)\r\n            / float(max(1, self.num_training_steps - self.num_warmup_steps)),\r\n        )\r\n\r\n\r\nclass DataModule(pl.LightningDataModule):\r\n    def __init__(self, name):\r\n        super().__init__()\r\n        self.length = len(name)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(Dataset())\r\n\r\n    def get_num_training_steps(self) -> int:\r\n        return self.length\r\n\r\n\r\nclass LitModel(pl.LightningModule):\r\n    def __init__(self, num_labels):\r\n        super().__init__()\r\n        self.num_labels = num_labels\r\n        self.nn = torch.nn.Linear(num_labels, num_labels)\r\n\r\n    def training_step(self, *args, **kwargs):\r\n        return\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    cli = MyLightningCLI(\r\n        model_class=LitModel,\r\n        datamodule_class=DataModule,\r\n    )\r\n```\r\n\r\n`config.yaml`\r\n```yaml\r\ndata:\r\n  name: blablabla\r\n\r\nmodel:\r\n  num_labels: 5\r\n\r\noptimizer:\r\n  class_path: torch.optim.Adam\r\n  init_args:\r\n    lr: 0.01\r\n\r\nlr_scheduler:\r\n  warmup_proportion: 0.1\r\n\r\ntrainer:\r\n  max_epochs: 2\r\n```\r\n</details>cc: @carmocca You need to add an empty `configure_optimizers` method to your model as there's a bug that disallows leaving it unimplemented. It will be fixed with #11672 \r\n\r\nThe error appears because the `lr_scheduler` arguments have not been added yet. You can see the order here:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/86b177ebe5427725b35fde1a8808a7b59b8a277a/pytorch_lightning/utilities/cli.py#L603-L609\r\n\r\nSo you have two options:\r\n\r\n1. Delay the linking until we've automatically added the lr scheduler classes\r\n\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    @staticmethod\r\n    def link_optimizers_and_lr_schedulers(parser):\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n        LightningCLI.link_optimizers_and_lr_schedulers(parser)\r\n```\r\n\r\n2. Manually add the classes yourself at this hook:\r\n\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        # Manually add the lr scheduler classes\r\n        parser.add_lr_scheduler_args(LR_SCHEDULER_REGISTRY.classes)\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n```\r\n\r\nAlso, the config for the scheduler should be:\r\n\r\n```yaml\r\nlr_scheduler:\r\n  class_path: __main__.WarmupLR\r\n  init_args:\r\n    warmup_proportion: 0.01\r\n    num_training_steps: 1\r\n```",
    "meta": {
      "name": "Link arguments from Datamodule into init_args of lr_scheduler"
    },
    "answer": "You need to add an empty `configure_optimizers` method to your model as there's a bug that disallows leaving it unimplemented. It will be fixed with #11672 \r\n\r\nThe error appears because the `lr_scheduler` arguments have not been added yet. You can see the order here:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/86b177ebe5427725b35fde1a8808a7b59b8a277a/pytorch_lightning/utilities/cli.py#L603-L609\r\n\r\nSo you have two options:\r\n\r\n1. Delay the linking until we've automatically added the lr scheduler classes\r\n\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    @staticmethod\r\n    def link_optimizers_and_lr_schedulers(parser):\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n        LightningCLI.link_optimizers_and_lr_schedulers(parser)\r\n```\r\n\r\n2. Manually add the classes yourself at this hook:\r\n\r\n```python\r\nclass MyLightningCLI(LightningCLI):\r\n    def add_arguments_to_parser(self, parser):\r\n        # Manually add the lr scheduler classes\r\n        parser.add_lr_scheduler_args(LR_SCHEDULER_REGISTRY.classes)\r\n        # Set lr_scheduler's num_training_steps from datamodule class\r\n        parser.link_arguments(\r\n            \"data\",\r\n            \"lr_scheduler.init_args.num_training_steps\",\r\n            compute_fn=lambda dm: dm.get_num_training_steps(),\r\n            apply_on=\"instantiate\",\r\n        )\r\n```\r\n\r\nAlso, the config for the scheduler should be:\r\n\r\n```yaml\r\nlr_scheduler:\r\n  class_path: __main__.WarmupLR\r\n  init_args:\r\n    warmup_proportion: 0.01\r\n    num_training_steps: 1\r\n```"
  },
  {
    "content": "Hello, I have some question about the self.log function and batch_size during the trainer.\r\nIf I have two GPUs, and I want to train my model with batch_size 16 per GPU and I use DDP, so what's the number of batch_size in Datamodule and what's the number of batch_size in self.log, If I want to calculate my metrics correctly?hey @exiawsh \r\n\r\nit should stay as batch_size for a single device only. With DDP if you set `batch_size=7`, then each device gets the batch of `batch_size=7`, and effective batch_size increases with the number of devices. Now if you want to log by accumulating metrics across devices, you need to set `sync_dist=True`. Check out the section here: https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging",
    "meta": { "name": "Question about the log system" },
    "answer": "hey @exiawsh \r\n\r\nit should stay as batch_size for a single device only. With DDP if you set `batch_size=7`, then each device gets the batch of `batch_size=7`, and effective batch_size increases with the number of devices. Now if you want to log by accumulating metrics across devices, you need to set `sync_dist=True`. Check out the section here: https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging"
  },
  {
    "content": "When trying to disable `find_unused_parameters` in the trainer by doing the following, \r\n\r\n`strategy=DDPStrategy(find_unused_parameters=False)`\r\n\r\nAm being thrown an import error for `from pytorch_lightning.strategies import DDPStrategy`\r\n\r\nError: \r\n\r\n`No module named 'pytorch_lightning.strategies'``pytorch_lightning.strategies` will be available in v1.6 release and is only available in master at the moment.\r\n\r\nFor now, you can use:\r\n```\r\nfrom pytorch_lightning.plugins import DDPPlugin\r\n\r\ntrainer = pl.Trainer(\r\n    ...,\r\n    strategy=DDPPlugin(find_unused_parameters=False),\r\n)\r\n```\r\n\r\nSee the stable version of docs (not latest) here: https://pytorch-lightning.readthedocs.io/en/stable/guides/speed.html?highlight=find_unused_parameters#when-using-ddp-plugins-set-find-unused-parameters-false",
    "meta": { "name": "Disabling find_unused_parameters" },
    "answer": "`pytorch_lightning.strategies` will be available in v1.6 release and is only available in master at the moment.\r\n\r\nFor now, you can use:\r\n```\r\nfrom pytorch_lightning.plugins import DDPPlugin\r\n\r\ntrainer = pl.Trainer(\r\n    ...,\r\n    strategy=DDPPlugin(find_unused_parameters=False),\r\n)\r\n```\r\n\r\nSee the stable version of docs (not latest) here: https://pytorch-lightning.readthedocs.io/en/stable/guides/speed.html?highlight=find_unused_parameters#when-using-ddp-plugins-set-find-unused-parameters-false"
  },
  {
    "content": "Hi,\r\n\r\nWhen training a WGAN we update the discriminator several times for each update of the generator (a typical choice is 5 to 1).\r\n\r\nIn PL we control this by setting the \"frequency\" parameter within the _configure_optimizers_ function:\r\n`return (\r\n        {'optimizer': dis_opt, 'frequency': 5},\r\n        {'optimizer': gen_opt, 'frequency': 1}\r\n    )\r\n`\r\n\r\nNow, if the number of batches for each epoch is not divisible by the sum of frequencies (6 in this case), the generator will end up being trained less than the discriminator.\r\n\r\nIf, for example, there are 11 batches in our dataset, it will result in the discriminator being updated 10 times and the generator only 1 for each epoch because the optimizers' order is reset at the beginning of each epoch.\r\n\r\nIs there a workaround for this? The most useful solution would be to be able to save the number of updates across epochs.\r\n\r\nThanks for any suggestion.Solved using `limit_train_batches` parameter in Trainer.",
    "meta": {
      "name": "WGAN - discriminator and generator updates inconsistency"
    },
    "answer": "Solved using `limit_train_batches` parameter in Trainer."
  },
  {
    "content": "Hello, I'm trying to validate my model on multiple subsets of the initial validation set to compare performance. Reading [this page](https://pytorch-lightning.readthedocs.io/en/stable/guides/data.html) I got the idea that returning a list contaning the multiple Dataloaders would be enough. My val_dataloader method became the following:\r\n\r\n![image](https://user-images.githubusercontent.com/73995923/146572624-438bc306-f049-4cba-972a-64162b98294e.png)\r\n\r\nBut this isn't working properly. I get the following error: _\"TypeError: validation_step() takes 3 positional arguments but 4 were given\"_\r\n(it worked properly when I only used 1 validation Dataloader).\r\nWhat am I doing wrong? Can someone help me with this? Or just point me to some more documentation on this.\r\n\r\nThanks in advance!Could you share what your validation step implementation is?you must be missing the additional `dataloader_idx` required in the `validation_step` for multiple dataloaders\r\ndocs: https://pytorch-lightning.readthedocs.io/en/latest/guides/data.html#multiple-validation-test-predict-dataloaders",
    "meta": { "name": "Multiple Validation Sets" },
    "answer": "you must be missing the additional `dataloader_idx` required in the `validation_step` for multiple dataloaders\r\ndocs: https://pytorch-lightning.readthedocs.io/en/latest/guides/data.html#multiple-validation-test-predict-dataloaders"
  },
  {
    "content": "Hello, is there a way to call validation for every N global steps? For example, we could have vall_check_interval greater than the number of batches in the training dataset.Unfortunately, not at the moment.\r\n\r\nThere's an issue for it here https://github.com/PyTorchLightning/pytorch-lightning/issues/8135",
    "meta": { "name": "val_check_interval every N global steps?" },
    "answer": "Unfortunately, not at the moment.\r\n\r\nThere's an issue for it here https://github.com/PyTorchLightning/pytorch-lightning/issues/8135"
  },
  {
    "content": "I have created an issue on this. Moderators, please delete this discussionWill be discussed in #11420.",
    "meta": {
      "name": "Unable to load pretrained weight into custom model in Pytorch Lightning"
    },
    "answer": "Will be discussed in #11420."
  },
  {
    "content": "Hi.\r\n\r\nI'm trying to come up with ways to get my validation loss shown in the progress bar. My model is defined like this:\r\n\r\n```\r\nclass DummyNet(pl.LightningModule):\r\n    def __init__(self, batch_size):\r\n        super().__init__()\r\n        self.batch_size = batch_size\r\n        self.fc = nn.Sequential(\r\n            nn.Dropout(0.5),\r\n            nn.Linear(512, 2)\r\n        )\r\n        # loss\r\n        self.loss_fn = nn.CrossEntropyLoss()\r\n        # metrics\r\n        metrics = torchmetrics.MetricCollection(\r\n            {\r\n                \"accuracy\": torchmetrics.Accuracy(),\r\n                \"precision\": torchmetrics.Precision(),\r\n                \"recall\": torchmetrics.Recall(),\r\n                \"auc\": torchmetrics.AUC(reorder=True),\r\n            }, \r\n        )\r\n        self.f1 = nn.ModuleDict({\r\n            \"train_f1\": torchmetrics.F1(),\r\n            \"val_f1\": torchmetrics.F1(),\r\n            \"test_f1\": torchmetrics.F1(),\r\n        })\r\n        self.metrics = nn.ModuleDict({\r\n            f\"{k}_metrics\": metrics.clone(prefix=k) for k in \"train val test\".split()\r\n        })\r\n    def forward(self, x):\r\n        x = self.fc(x)\r\n        return x\r\n    \r\n    def loop_step(self, batch, stage):\r\n        x, targets = batch[\"windows\"], batch[\"diagnosis\"]\r\n        logits = self(x)\r\n        loss = self.loss_fn(logits, targets)\r\n        preds = logits.argmax(-1)\r\n        # computing metrics\r\n        f1_str = f\"{stage}_f1\"\r\n        metric_str = f\"{stage}_metrics\"\r\n        self.f1[f1_str](preds, targets)\r\n        self.metrics[metric_str](preds, targets)\r\n        # logging metrics\r\n        on_step = False if stage != \"train\" else True\r\n        self.log(f1_str, self.f1[f1_str], on_step=on_step, on_epoch=True)\r\n        self.log_dict(self.metrics[metric_str], on_step=False, on_epoch=True)   \r\n        self.log(f\"{stage}_loss\", loss, on_step=on_step, on_epoch=True)\r\n        return loss\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        return self.loop_step(batch, \"train\")\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        return self.loop_step(batch, \"val\")\r\n\r\n    def testing_step(self, batch, batch_idx):\r\n        return self.loop_step(batch, \"test\")\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=0.01)\r\n```\r\n\r\nBut as of now none of my metrics nor my validation loss comes up in the progress bar. Is it because I'm returning `loss` in the dictionary and not \"{stage}_loss\"?\r\n\r\nThank you.\r\nHi @FeryET, I believe the below should work as documented in https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging.\r\n```python\r\nself.log(..., prog_bar=True)\r\n```\r\n",
    "meta": { "name": "How to show the validation loss in progress bar?" },
    "answer": "Hi @FeryET, I believe the below should work as documented in https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging.\r\n```python\r\nself.log(..., prog_bar=True)\r\n```\r\n"
  },
  {
    "content": "I am working with [pytorchvideo](https://github.com/facebookresearch/pytorchvideo) with pytorch lightning but it say ```UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.```\r\n```\r\nfrom pytorchvideo.models.resnet import create_resnet\r\nclass OurModel(LightningModule):\r\n    def __init__(self):\r\n        super(OurModel,self).__init__()\r\n        self.model =  create_resnet(\r\n                      input_channel=3, # RGB input from Kinetics\r\n                      model_depth=50, # For the tutorial let's just use a 50 layer network\r\n                      model_num_class=1, # Kinetics has 400 classes so we need out final head to align\r\n                      norm=nn.BatchNorm3d,\r\n                      activation=nn.ReLU)\r\n\r\n    def forward(self,x):\r\n        return self.model(x)\r\n\r\n  \r\n    def val_dataloader(self):\r\n        val_dataset=LabeledVideoDataset(labeled_video_paths=\\\r\n                    list(zip(val_df.vidpath,val_df.pulse)),\r\n                   clip_sampler=make_clip_sampler(\"uniform\", 2),\\\r\n                    transform=train_transform,  decode_audio=False)\r\n        \r\n        val_loader=DataLoader(val_dataset,shuffle=False,\r\n                   batch_size=self.batch_size,\r\n                   num_workers=self.numworker,\r\n                   pin_memory=True)\r\n        return val_loader\r\n    \r\n    def validation_step(self,batch,batch_idx):\r\n        out = self(batch[\"video\"]).flatten()\r\n        loss=self.criterion(out,batch[\"label\"].float())\r\n        mae=self.metric(out,batch[\"label\"].float())\r\n        return {'loss':loss,'mae':mae.detach()}\r\n\r\n```\r\nA you can see, shuffle is False. but it keep me giving warning that\r\n```\r\nUserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\r\n```\r\nSorry, i am not sure whether i had to ask it at pytorchvideo or hereHi @talhaanwarch, you're asking it in the right place!\r\n\r\nIt's a warning from Lightning, and as I looked at the definition of `make_clip_sampler` of pytorchvideo, I believe it's the same reason as https://github.com/PyTorchLightning/pytorch-lightning/discussions/10771. You can simply ignore it with some filter like below if you need.\r\n```python\r\nimport warnings\r\nwarnings.simplefilter('ignore', category=UserWarning, message=\"Your `val_dataloader` has `shuffle=True`.*\")\r\n```",
    "meta": { "name": "val_dataloader` has `shuffle=True` though its false" },
    "answer": "Hi @talhaanwarch, you're asking it in the right place!\r\n\r\nIt's a warning from Lightning, and as I looked at the definition of `make_clip_sampler` of pytorchvideo, I believe it's the same reason as https://github.com/PyTorchLightning/pytorch-lightning/discussions/10771. You can simply ignore it with some filter like below if you need.\r\n```python\r\nimport warnings\r\nwarnings.simplefilter('ignore', category=UserWarning, message=\"Your `val_dataloader` has `shuffle=True`.*\")\r\n```"
  },
  {
    "content": "I'm confused about two API:\r\n- `Module.load_from_checkpoint`\r\n- `trainer.resume_from_checkpoint`[resume_from_checkpoint](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#resume-from-checkpoint) is used to resume the training using the checkpointed state_dicts. It will reload model's state_dict, optmizer's and schedulers's state_dicts, training state as well in a general case.\r\nuse-case: to restart the training\r\n\r\n[load_from_checkpoint](https://pytorch-lightning.readthedocs.io/en/stable/common/weights_loading.html#checkpoint-loading) just reloads the model's state_dict and return the model with the loaded weights.\r\nuse-case: for quick evaluation/prediction.",
    "meta": {
      "name": "what's the difference between `load_from_checkpoint ` and `resume_from_checkpoint`"
    },
    "answer": "[resume_from_checkpoint](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#resume-from-checkpoint) is used to resume the training using the checkpointed state_dicts. It will reload model's state_dict, optmizer's and schedulers's state_dicts, training state as well in a general case.\r\nuse-case: to restart the training\r\n\r\n[load_from_checkpoint](https://pytorch-lightning.readthedocs.io/en/stable/common/weights_loading.html#checkpoint-loading) just reloads the model's state_dict and return the model with the loaded weights.\r\nuse-case: for quick evaluation/prediction."
  },
  {
    "content": "My question is like title. Thank you!checkout the example here: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-with-dataparallel",
    "meta": {
      "name": "ddp: how to combine multi-gpus outputs like \"training_step_end\" which is only used in dp/ddp2?"
    },
    "answer": "checkout the example here: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-with-dataparallel"
  },
  {
    "content": "```python\r\nclass Mynet(pl.LightningModule)\r\n    ... ...\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x)\r\n        loss = F.nll_loss(logits, y)\r\n        accu = ((torch.argmax(logits, dim=1) == y).sum()/x.shape[0]).item()   \r\n        return {'loss':loss, 'accuracy':accu}\r\n```\r\n\r\ni define training_step like this, the progress bar show loss,  but not show accu. \r\ni want to know why?\r\n\r\npytorch_lightning.__version__ = 1.5.7You need to log it: \r\n\r\n\r\n```py\r\nself.log(\"accuracy\", accu, prog_bar=True)\r\nreturn {'loss':loss}\r\n```\r\n\r\nDocs: https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#logging-from-a-lightningmodule\r\n",
    "meta": { "name": "Accuracy doesn't show up in progress bar" },
    "answer": "You need to log it: \r\n\r\n\r\n```py\r\nself.log(\"accuracy\", accu, prog_bar=True)\r\nreturn {'loss':loss}\r\n```\r\n\r\nDocs: https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#logging-from-a-lightningmodule\r\n"
  },
  {
    "content": "According to the manual_backward() documentation, it takes care of scaling when using mixed precision. In that case, is it correct to assume one can simply and safely use loss.backward() during manual optimization if not using mixed precision?hey @MGheini \r\n\r\nIt's not just precision but a common hook to support all other strategies like deepspeed/ddp and certain hooks like `on_after_backward` are called too. So `manual_backward` is suggested to make sure no-code change is required for eg in case any of the strategies is updated by the user.",
    "meta": {
      "name": "self.manual_backward() vs. loss.backward() when optimizing manually"
    },
    "answer": "hey @MGheini \r\n\r\nIt's not just precision but a common hook to support all other strategies like deepspeed/ddp and certain hooks like `on_after_backward` are called too. So `manual_backward` is suggested to make sure no-code change is required for eg in case any of the strategies is updated by the user."
  },
  {
    "content": "Hello,\r\n\r\nI had an error where the training_step() was not run properly. I just found out the cause was because of the optimizer_step(). My training step immediately runs after I commented out optimizer_step().\r\n\r\nSome other users also experienced the same error as described here: https://stackoverflow.com/questions/66756245/training-step-not-executing-in-pytorch-lightning\r\n\r\nMy question is: Now that training_step() is running, but my train_loss is explosive because of the lack of a learning rate scheduler, henceforth, what can I implement to re-enable back my learning rate scheduler?\r\n\r\nHere's my chunk of code:\r\n```\r\n    def configure_optimizers(self):\r\n        \r\n        \"\"\"\r\n        AdamW Optimizer lr=0.0006\r\n\r\n        \"\"\"        \r\n        optimizer = optim.AdamW(self.parameters(),\r\n                               lr=self.lr,\r\n                               weight_decay=0.01 # Default\r\n                               )\r\n        \r\n        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n            optimizer,\r\n            mode='min',\r\n            factor=0.1,\r\n            patience=2,\r\n            min_lr=1e-6,\r\n            verbose=True\r\n        )\r\n\r\n        return optimizer\r\n    \r\n\r\n        \r\n    #def optimizer_step(self, epoch=None,\r\n    #                   batch_idx=None,\r\n    #                   optimizer=None,\r\n    #                   optimizer_idx=None,\r\n    #                   optimizer_closure=None,\r\n    #                   on_tpu=None,\r\n    #                   using_native_amp=None,\r\n    #                   using_lbfgs=None,\r\n    #                   second_order_closure=None):              \r\n    #    \r\n    #    if batch_idx == 0: # to call the scheduler after each validation\r\n    #        \r\n    #        self.scheduler.step(self.epoch_val_loss)\r\n    #        \r\n    #        print(f'metric: {self.epoch_val_loss}, \\\r\n    #              best: {self.scheduler.best}, \\\r\n    #                  num_bad_epochs: {self.scheduler.num_bad_epochs}') # for debugging\r\n    #    \r\n    #    optimizer.step()\r\n    #    \r\n    #    optimizer.zero_grad()\r\n```\r\n\r\nThank you!\r\nhey @tsuijenk \r\n\r\n`optimizer _closure` must be passed in `optimizer.step()` since it runs training_step and backward call. You can check the docstrings and examples here: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#optimizer-step",
    "meta": {
      "name": "PyTorch Lightning Optimizer_Step() prevents training_step() from running"
    },
    "answer": "hey @tsuijenk \r\n\r\n`optimizer _closure` must be passed in `optimizer.step()` since it runs training_step and backward call. You can check the docstrings and examples here: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#optimizer-step"
  },
  {
    "content": "The documentation about the `on_before_optimizer_step` hook mentions that:\r\n\r\n> The hook is only called if gradients do not need to be accumulated.\r\n\r\nHowever, in theory, combining both seems possible to me. \r\nI took a look at PR #8048 which is the implementation of this hook but it didn't seem clear there.hey @NathanGodey !\r\n\r\nit doesn't mean that it won't be called if `accumulate_grad_batches > 1`, but only be called when accumulation is done and it's time to update the gradients with `optimizer.step`.",
    "meta": {
      "name": "Why is on_before_optimizer_step incompatible with accumulate_grad_batches ?"
    },
    "answer": "hey @NathanGodey !\r\n\r\nit doesn't mean that it won't be called if `accumulate_grad_batches > 1`, but only be called when accumulation is done and it's time to update the gradients with `optimizer.step`."
  },
  {
    "content": "Hi!\r\n\r\nI\u2019m working on a LSTM to predict price changes. The data has to be transformed (standardized) when training/validering and later inverse-transformed when predicting in production.\r\n\r\nI\u2019m using the LightningModule as well as the LightingDataModule, but I\u2019m not sure where to apply the StandardScaler\u2019s transform and more specifically; where to save the scaler-parameters and where to apply the inverse-transform on the predictions. And ideeas?\r\n\r\n// RAssuming that you are using pytorch TensorDataset, you can apply transform inside `setup` and inverse transform inside `predict_step` itself.",
    "meta": { "name": "Where to transform and inverse-transform" },
    "answer": "Assuming that you are using pytorch TensorDataset, you can apply transform inside `setup` and inverse transform inside `predict_step` itself."
  },
  {
    "content": "Hi,\r\nI've currently refactored a part of code to use Pytorch Lightning instead of a regular pytorch script. I find a 35% speed up on a toy dataset, where the model had 20.9 K parameters.\r\n\r\nHowever, when trying a bigger version of the model (9.1 M params), the initialization of the model takes a lot more time (even before lightning prints the number of params). I've managed to nail it down to the `self.save_hyperparameters()` function in init, since the initialization is instantaneous without calling `self.save_hyperparameters()`, but takes more than a full minute when calling it.\r\n\r\nRemoving `self.save_hyperparameters()` causes an issue in my code since I am calling  `model.load_from_checkpoint()` afterward. Would you have any thoughts on how I can speed up the code? The \"regular\" pytorch model manages to initialize, train and save the model faster than the Lightning one due to this slow down ...\r\n\r\nThanks!What objects are you passing in? `self.save_hyperparameters()` is inspecting the signature, would be good to know which parameter is causing this.Thanks for answering! I'm simply calling `self.save_hyperparameters()` as part of the `__init__` function (code below). `model` is the main network model, which is a feedforward nn implemented with nn.module as base class. It contains an embedder and one input layer, 2 hidden layers and an output layer (in this case, the sizes of the layers are [6,000, 1008, 1008, 168]).\r\n\r\n```\r\nclass EstimatorLightningModule(pl.LightningModule):\r\n    def __init__(\r\n        self,\r\n        model: EstimatorNetwork,\r\n        loss_scaling: Callable = None,\r\n        lr: float = 1e-5,\r\n        weight_decay: float = 1e-10,\r\n    ) -> None:\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.model = model\r\n        self.loss_scaling = loss_scaling\r\n        self.lr = lr\r\n        self.weight_decay = weight_decay\r\n```Awesome thanks I\u2019ll try this out! Just to make sure, this will still allow me to checkout the best model afterward? I\u2019m unsure what you meant by the \u201csignature\u201d of the model. Thanks!\n\n> On Jan 1, 2022, at 10:33 AM, Adrian W\u00e4lchli ***@***.***> wrote:\n> \n> \ufeff\n> You have to set self.save_hyperparameters(ignore=\"model\"), since you are passing in a full model as a \"hyperparameter\", you don't want to save that!\n> \n> \u2014\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n> You are receiving this because you authored the thread.\nYou have to set` self.save_hyperparameters(ignore=\"model\")`, since you are passing in a full model as a \"hyperparameter\", you don't want to save that!",
    "meta": {
      "name": "`save_hyperparameters()` is very slow when there are many hyperparameters, any speed up?"
    },
    "answer": "You have to set` self.save_hyperparameters(ignore=\"model\")`, since you are passing in a full model as a \"hyperparameter\", you don't want to save that!"
  },
  {
    "content": "I'm trying to incorporate the [pytorch_ema](https://github.com/fadel/pytorch_ema) library into the PL training loop. I found one topic relating to using pytorch_ema in lightning in [this discussion thread](https://forums.pytorchlightning.ai/t/adopting-exponential-moving-average-ema-for-pl-pipeline/488), but how would this work if i want to save a model checkpoint based on the EMA weights? for example if i want to save the model weights using just pytorch, i could do something like\r\n\r\n```\r\n# using accuracy as an example\r\nif current_val_acc >= best_val_acc:\r\n    with ema.average_parameters():\r\n        torch.save(model.state_dict(), saved_model_pth)\r\n```\r\n\r\nso that i save the smoothed weights, but restore the original weights to the model so it doesn't affect training\r\n\r\none workaround i can think of is to create my own model saving logic in the `validation_epoch_end` instead of relying on the `ModelCheckpoint` callback, but that seems to be a bit hacky. are there any potentially better solutions?you can replace the model state_dict inside the checkpoint\r\n\r\n```py\r\nclass LitModel(LightningModule):\r\n    ...\r\n    \r\n    def on_save_checkpoint(self, checkpoint):\r\n        with ema.average_parameters():\r\n            checkpoint['state_dict'] = self.state_dict()\r\n```",
    "meta": { "name": "using EMA with model checkpoints" },
    "answer": "you can replace the model state_dict inside the checkpoint\r\n\r\n```py\r\nclass LitModel(LightningModule):\r\n    ...\r\n    \r\n    def on_save_checkpoint(self, checkpoint):\r\n        with ema.average_parameters():\r\n            checkpoint['state_dict'] = self.state_dict()\r\n```"
  },
  {
    "content": "i use `self.training` param to judge what data return.\r\n\r\n`return x if self.training else (torch.cat(z, 1), x)`\r\n\r\nbut when i load my model, i use debug mode find that the self.training is save True.\r\n```Python\r\nself.model = CustomModel.load_from_checkpoint(model_path)\r\nself.model.training = False\r\n```\r\n\r\ni use above code change model.training status, but its not work\r\n\r\nthis is my inference full code:\r\n\r\n```Python\r\nclass CustomModelInference:\r\n    def __init__(\r\n            self,\r\n            model_path: str,\r\n            conf_thres: float = 0.25,\r\n            iou_thres: float = 0.45,\r\n            max_det: int = 1000,\r\n            device: str = 'cuda:0',\r\n            need_classes: list | None = None\r\n    ):\r\n        self.conf_thres = conf_thres\r\n        self.iou_thres = iou_thres\r\n        self.max_det = max_det\r\n        self.device = device\r\n        self.need_classes = need_classes\r\n\r\n        self.model = CustomModel.load_from_checkpoint(model_path)\r\n        self.model.training = False\r\n        self.model.to(device)\r\n        self.stride = int(self.model.stride.max())\r\n        self.names = self.model.names\r\n        self.imgsz = self.model.imgsz\r\n\r\n    @torch.no_grad()\r\n    def infer(self, img: np.ndarray):\r\n        imgsz = check_img_size(self.imgsz, s=self.stride)\r\n        cudnn.benchmark = True\r\n        img = letterbox(img, imgsz, stride=self.stride, auto=True)[0]\r\n        # img = np.stack(img, 0)\r\n        if len(img.shape) == 3:\r\n            img = img[None]\r\n        img = img[..., ::-1].transpose((0, 3, 1, 2))\r\n        img = np.ascontiguousarray(img)\r\n        img = torch.from_numpy(img).to(self.device)\r\n        img = img.float()\r\n        img = img / 255.0\r\n\r\n        out, train_out = self.model(img)\r\n```fine, i know the answer, i have to set model.eval()......training mode is the default in pytorch, so this seems to be correct. \r\n\r\n```py\r\nimport torch \r\nclass MyModel(torch.nn.Module):\r\n    pass\r\n\r\nmodel = MyModel()\r\nmodel.training\r\nOut[14]: True\r\n```",
    "meta": { "name": "model inference but self.training is save true" },
    "answer": "fine, i know the answer, i have to set model.eval()......"
  },
  {
    "content": "Hi! I am playing around with pytorch-lightning. \r\n\r\n**Problem**\r\nI tried to use 2 gpus and manually merge training loss described in [Lightning in 2 steps](https://pytorch-lightning.readthedocs.io/en/latest/starter/new-project.html#data-flow).\r\nBut when I call `training_step_end()`, it just gives me only one gpu's loss, not all gpus loss. \r\n![image](https://user-images.githubusercontent.com/31476895/134098524-698ec172-0aa6-46aa-a3a2-2e916850f60c.png)\r\n\r\n**Question**\r\nDo I have to reduce loss myself in `training_step_end()`?\r\n\r\n**My code**\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch import optim\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms, datasets\r\nimport torch.nn.functional as F\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass BaseImageClassificationSystem(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.backbone = nn.Sequential(nn.Conv2d(1, 64, 3), nn.AdaptiveAvgPool2d((1, 1)))\r\n        self.fc = nn.Linear(64, 10)\r\n\r\n    def forward(self, x):\r\n        return self.fc(torch.flatten(self.backbone(x), 1))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.fc(torch.flatten(self.backbone(x), 1))\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('train/loss', loss)\r\n        return loss\r\n\r\n    def training_step_end(self, losses):\r\n        print(losses)\r\n        return (losses[0] + losses[1]) / 2\r\n\r\n    def configure_optimizers(self):\r\n        return optim.SGD(self.parameters(), lr=0.01)\r\n\r\n\r\ntrain_dl = DataLoader(datasets.MNIST(root='./', train=True, transform=transforms.ToTensor(), download=True),\r\n                      batch_size=128)\r\nmodel = BaseImageClassificationSystem()\r\ntrainer = pl.Trainer(num_processes=8, gpus='1, 2', accelerator='ddp', max_epochs=100)\r\ntrainer.fit(model, train_dl)\r\n```\r\n\r\n**Output**\r\n```Bash\r\ntensor(2.3002, device='cuda:2', grad_fn=<NllLossBackward>)\r\ntensor(2.2930, device='cuda:1', grad_fn=<NllLossBackward>)\r\n```\r\nIt's mentioned in the doc that this configuration works only for DP or DDP2, but in your code, you are using DDP so there will only be 1 loss item since gradient sync happens within DDP so each device has its own loss and backward call and won't require manual reduction of loss across devices.",
    "meta": { "name": "Confusion in training_step_end() API" },
    "answer": "It's mentioned in the doc that this configuration works only for DP or DDP2, but in your code, you are using DDP so there will only be 1 loss item since gradient sync happens within DDP so each device has its own loss and backward call and won't require manual reduction of loss across devices."
  },
  {
    "content": "Hi, I am trying to make my code invariant to the choice of strategies by being able to compute the global batch size which depends on the strategy. For example, for DDP it is `N * batch_size` with N being the number of processes.\r\n\r\nThe use case I can think of is using the global batch size to initialize the optimizer.\r\n\r\n```python\r\ntrainer(num_nodes=1, gpus=2, strategy='ddp') # pass the strategy ddp for example\r\n\r\nclass MyLightningModule(pl.LightningModule):\r\n    @property\r\n    def global_batch_size(self) -> int:\r\n        if self.trainer.strategy is None:\r\n            return self.trainer.datamodule.train.loader.batch_size\r\n        elif self.trainer.strategy is DDPStrategy:\r\n            return self.trainer.num_nodes * self.trainer.gpus *\\       # There might be a better way to compute the\r\n                      self.trainer.datamodule.train.loader.batch_size  # number of processes using the strategy\r\n        ...\r\n\r\n    def configure_optimizers(self) -> Dict[Any, Any]:\r\n        optimizer, scheduler = hydra.utils.instantiate(\r\n            self.hparams.optimizer, model=self, batch_size=self.global_batch_size, _recursive_=False)\r\n\r\n        return {\r\n            'optimizer': optimizer,\r\n            'lr_scheduler': scheduler\r\n        }\r\n```\r\n\r\nTo do so, I would like to retrieve inside my Lightning module the strategy used by my trainer. I tried to find in the trainer code how to access the strategy and I found the property:\r\n\r\n```python\r\n# in pytorch_lightning.trainer.trainer.py\r\nclass Trainer(...):\r\n    ...\r\n    @property\r\n    def strategy(self) -> Strategy:\r\n        return self._accelerator_connector.strategy\r\n```\r\n\r\nHowever `self.trainer.strategy` in `configure_optimizers` raises `AttributeError: 'Trainer' object has no attribute 'strategy'`.\r\n\r\nWeirdly, `self.trainer._accelerator_connector.strategy` works and returns the passed strategy in the trainer. Yet, if I understood correctly the `_accelerator_connector` should resolve the strategy `'ddp'` to `DDPStrategy` in its initialization but it returns `'ddp'`:\r\n \r\n```python\r\n# in pytorch_lightning.trainer.connectors.accelerator_connector.py\r\n\r\nclass AcceleratorConnector(...):\r\n    def __init__(...):\r\n        ...\r\n        self.strategy = self.final_strategy()\r\n     ...\r\n```\r\n\r\nIs it possible to access the strategy used for training?\r\n\r\n\r\n\r\njust out of curiosity, what sort of scheduler/optimizer are you initializing using the global_batch_size?",
    "meta": { "name": "How to access the strategy of the trainer" },
    "answer": "just out of curiosity, what sort of scheduler/optimizer are you initializing using the global_batch_size?"
  },
  {
    "content": "When I print `trainer.logged_metrics`, I got\r\n```\r\n{\r\n    'train_f1': tensor(1.),\r\n    'train_prc': tensor(1.),\r\n    'train_rec': tensor(1.),\r\n    'train_loss': tensor(0.0152),\r\n    'valid_f1': 0.861257791519165,\r\n    'valid_prc': 0.9134419560432434,\r\n    'valid_rec': 0.8147138953208923,\r\n    'valid_loss': 0.16973154246807098\r\n}\r\n```\r\nWhy trainer.logged_metrics is mixed of tensor and non tensor value.would you mind sharing the log calls in your training_step and validation_step hooks?",
    "meta": {
      "name": "Why trainer.logged_metrics is mixed of tensor and non tensor value."
    },
    "answer": "would you mind sharing the log calls in your training_step and validation_step hooks?"
  },
  {
    "content": "`LightningModule` can be coupled with various callbacks? I wonder if it is possible `LightningLite` can also reuse those callbacks, such as `wandb`, `checkpoint`?I don't think they are because the hooks in callbacks are just some code injections that are supposed to run at a certain point where lightning defines the training loops/validation loops but in `LightningLite` user defines the training loop, so it won't be possible. Although in the case of wandb for eg you can use their native API to log the stuff.",
    "meta": { "name": "Does LightningLite still support various callbacks?" },
    "answer": "I don't think they are because the hooks in callbacks are just some code injections that are supposed to run at a certain point where lightning defines the training loops/validation loops but in `LightningLite` user defines the training loop, so it won't be possible. Although in the case of wandb for eg you can use their native API to log the stuff."
  },
  {
    "content": "When I update the following repository to 1.5.2, I get the following error. Is there an implementation guide somewhere?\r\n\r\nKeiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning https://github.com/Keiku/PyTorch-Lightning-CIFAR10\r\n\r\n```\r\n(PyTorch-Lightning-CIFAR10) \u22ca> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main \u2a2f\r\npipenv run python train.py +experiments=train_exp01 hydra.run.dir=outputs/train_exp01\r\n\r\nGlobal seed set to 0\r\nGPU available: True, used: False\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\n/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1579: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\r\n  rank_zero_warn(\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 68, in main\r\n    trainer.fit(model,\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 737, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 682, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 772, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1141, in _run\r\n    self.accelerator.setup(self)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu.py\", line 35, in setup\r\n    return super().setup(trainer)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 93, in setup\r\n    self.setup_optimizers(trainer)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 351, in setup_optimizers\r\n    optimizers, lr_schedulers, optimizer_frequencies = self.training_type_plugin.init_optimizers(\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 245, in init_optimizers\r\n    return trainer.init_optimizers(model)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/optimizers.py\", line 35, in init_optimizers\r\n    optim_conf = self.call_hook(\"configure_optimizers\", pl_module=pl_module)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1496, in call_hook\r\n    output = model_fx(*args, **kwargs)\r\n  File \"/mnt/nfs/kuroyanagi/clones/PyTorch-Lightning-CIFAR10/model.py\", line 73, in configure_optimizers\r\n    total_steps = cfg.train.num_epochs * len(self.train_dataloader())\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/core/hooks.py\", line 477, in train_dataloader\r\n    raise NotImplementedError(\"`train_dataloader` must be implemented to be used with the Lightning Trainer\")\r\nNotImplementedError: `train_dataloader` must be implemented to be used with the Lightning Trainer\r\n\r\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\r\n(PyTorch-Lightning-CIFAR10) \u22ca> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main \u2a2f \r\n```for this problem, tracking issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/10430I have released a version corresponding to v1.5.2. Please check it out.\r\n\r\nUpgrade to PyTorch Lightning 1.5.2 by Keiku \u00b7 Pull Request #1 \u00b7 Keiku/PyTorch-Lightning-CIFAR10 https://github.com/Keiku/PyTorch-Lightning-CIFAR10/pull/1",
    "meta": {
      "name": "After updating to 1.5.2, NotImplementedError: `train_dataloader`"
    },
    "answer": "for this problem, tracking issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/10430"
  },
  {
    "content": "Hi I'm newbie for pytorch-lightning.\r\nPlease teach me about this topic.\r\n\r\nI want to process 100,000 records. So I set `max_step` = 100,000.\r\nAnd to speed up learning, I also set `strategy = ddp` and use 4 GPUs with single node.\r\nBut when I observe behavior of pytorch-lightning, it seems process 400,000 records. \r\nbut step number is still 100,000 steps.\r\n\r\nIs there any recognition that the number of steps specified when using multiple GPUs needs to be divided by the number of GPUs with the expected number of steps?\r\n(on above example, should I set `max_step` to 25,000? )\r\n\r\n(my pytorch-lightinng version is 1.5.4)\r\n\r\nhey @kash203 \r\n\r\nwith DDP if batch_size is 32 with 4 GPUs then the effective batch size actually 32*4. See the [docs here](https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html#batch-size).\r\n\r\nnow in your case, there are 100_000 samples. Considering batch_size as 1, a single training step with 4 GPUs means 4 calls to the dataloader at 4 samples are covered in a single step. Thus your max_steps here should be 25_000 as you stated.\r\n\r\nIn case you want to process all the samples only once, you can just set max_epoch=1. `max_steps` is generally used for sequential learning tasks where data is iteratively created.",
    "meta": {
      "name": "How should the number of steps be set against to processed data when using ddp and multi GPU"
    },
    "answer": "hey @kash203 \r\n\r\nwith DDP if batch_size is 32 with 4 GPUs then the effective batch size actually 32*4. See the [docs here](https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html#batch-size).\r\n\r\nnow in your case, there are 100_000 samples. Considering batch_size as 1, a single training step with 4 GPUs means 4 calls to the dataloader at 4 samples are covered in a single step. Thus your max_steps here should be 25_000 as you stated.\r\n\r\nIn case you want to process all the samples only once, you can just set max_epoch=1. `max_steps` is generally used for sequential learning tasks where data is iteratively created."
  },
  {
    "content": "I would like to use [Adafactor](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.Adafactor) as my optimizer with LightningCLI. I've tried the method described in the [documentation](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_cli.html#optimizers-and-learning-rate-schedulers) for custom optimizers but it didn't work. Can anybody tell me how they would train a model with this optimizer using LightningCLI?Hey @goncalomcorreia,\r\n\r\nDid you add the optimizer to the Optimizer registry?\r\n\r\nHere is where we pre-register all the optimizers: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/utilities/cli.py#L83\r\nHi! I got it to work in the meantime. I added this to the main file where I call CLI:\r\n\r\n```\r\nimport transformers\r\nfrom pytorch_lightning.utilities.cli import OPTIMIZER_REGISTRY\r\n\r\n@OPTIMIZER_REGISTRY\r\nclass Adafactor(transformers.Adafactor):\r\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(*args, **kwargs)\r\n```\r\n\r\nThe main issue was in the config file---apparently one needs to write:\r\n\r\n```\r\noptimizer:\r\n  class_path: __main__.Adafactor\r\n```\r\n\r\ninstead of:\r\n\r\n\r\n```\r\noptimizer:\r\n  class_path: Adafactor\r\n```\r\n\r\nDoing the former, I got it to work.\r\n\r\nBy the way, is there a way to have the optimizer register in a separate file than the one that calls CLI?",
    "meta": {
      "name": "How can one use an external optimizer with LightningCLI?"
    },
    "answer": "Hi! I got it to work in the meantime. I added this to the main file where I call CLI:\r\n\r\n```\r\nimport transformers\r\nfrom pytorch_lightning.utilities.cli import OPTIMIZER_REGISTRY\r\n\r\n@OPTIMIZER_REGISTRY\r\nclass Adafactor(transformers.Adafactor):\r\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(*args, **kwargs)\r\n```\r\n\r\nThe main issue was in the config file---apparently one needs to write:\r\n\r\n```\r\noptimizer:\r\n  class_path: __main__.Adafactor\r\n```\r\n\r\ninstead of:\r\n\r\n\r\n```\r\noptimizer:\r\n  class_path: Adafactor\r\n```\r\n\r\nDoing the former, I got it to work.\r\n\r\nBy the way, is there a way to have the optimizer register in a separate file than the one that calls CLI?"
  },
  {
    "content": "In [TorchGeo](https://github.com/microsoft/torchgeo), we use PyTorch Lightning to organize reproducible benchmarks for geospatial datasets. Currently, we have a set of LightningDataModules for each dataset and a much smaller number of LightningModules for each task (semantic segmentation, classification, regression, etc.). Each Dataset defines its own `plot()` method that describes how to plot images and masks.\r\n\r\nDuring training/validation steps, we would like to plot a few examples to see how training is progressing. However, the LightningModule doesn't seem to know anything about the LightningDataModule/DataLoader/Dataset. Because of this, if we want to perform dataset-specific plotting during training or validation steps, we're forced to create a separate LightningModule for each dataset, increasing code duplication and defeating the whole purpose of PyTorch Lightning ([example](https://github.com/microsoft/torchgeo/blob/main/torchgeo/trainers/chesapeake.py)).\r\n\r\nIs there an easy way for a LightningModule to tell which DataModule/DataLoader/Dataset is being used and call its `dataset.plot()` method?\r\n\r\n@calebrob6 @isaaccorley\r\n\r\n@tchaton this is slightly related to #10469 but different enough that I wanted to start a separate discussion about it.@adamjstewart There is a reference to datamodule via trainer from LightningModule, but would that solve your issue?\r\n```python\r\nself.trainer.datamodule\r\n```\r\nThis dependence sounds like the data isn't as separable from the model/loop. \n\nRelying on self.trainer.datamodule is not foolproof.\n\nSomeone could use your lightning module but pass the data loaders directly to the trainer.fit function. In this case, there is no datamodule provided, and the module could fail unless it checks against this ",
    "meta": {
      "name": "How to access `LightningDataModule` in `LightningModule`"
    },
    "answer": "@adamjstewart There is a reference to datamodule via trainer from LightningModule, but would that solve your issue?\r\n```python\r\nself.trainer.datamodule\r\n```\r\n"
  },
  {
    "content": "Hi there,\r\n\r\nI am using the ModelCheckpoint callback to save my model every n epochs but I cannot find a way to prevent PL from overwriting/deleting the previous checkpoint.\r\n\r\nIdeally, I would like to keep the default naming convention {epoch}-{step} but without losing previous checkpoints.\r\n\r\nThanksyou can create a custom ModelCheckpoint instance with `save_top_k=-1` and pass it in inside Trainer callbacks.\r\n\r\n```py\r\nckpt_callback = ModelCheckpoint(save_top_k=-1, ...)\r\ntrainer = Trainer(callbacks=[ckpt_callback], ...)\r\n```",
    "meta": { "name": "Save checkpoints without overwrite" },
    "answer": "you can create a custom ModelCheckpoint instance with `save_top_k=-1` and pass it in inside Trainer callbacks.\r\n\r\n```py\r\nckpt_callback = ModelCheckpoint(save_top_k=-1, ...)\r\ntrainer = Trainer(callbacks=[ckpt_callback], ...)\r\n```"
  },
  {
    "content": "Hello everyone, I'm currently implementing a Wasserstain type of GAN using Gradient Penalty. I want to save the checkpoints monitoring the negative critic loss, which starts from low values, increases to higher values in the first epochs and then decreases reaching almost 0. A plot of this loss can be seen in the paper: https://arxiv.org/pdf/1704.00028.pdf\r\n\r\nThe problem is that if I use **ModelCheckpoint** and set the monitor parameter to negative critic_loss and mode = 'min', it basically saves the first epoch only. However I don't want to consider the training start epochs, when the negative loss increase, but only the epochs when the loss decrease.\r\n\r\nI'm currently using multi-gpu training\r\n\r\nHow can I implement this? Should I override the function on_train_epoch_end and save there the checkpoints, after checking the above criteria? Or should I use a lightning Callback? If so how can I acces to the monitored values?\r\nThanks in advance\r\nThanks to @tchaton on the slack community I solved the issue overriding the `ModelCheckpoint` class.\r\nIn the `on_train_epoch_end` I've added a new check that follow the above conditions, as such:\r\n\r\n```python\r\nclass WGANModelCheckpoint(ModelCheckpoint):\r\n    def __init__(self,\r\n                 dirpath: Optional[Union[str, Path]] = None,\r\n                 filename: Optional[str] = None,\r\n                 monitor: Optional[str] = None,\r\n                 verbose: bool = False,\r\n                 save_last: Optional[bool] = None,\r\n                 save_top_k: int = 1,\r\n                 save_weights_only: bool = False,\r\n                 mode: str = \"min\",\r\n                 auto_insert_metric_name: bool = True,\r\n                 every_n_train_steps: Optional[int] = None,\r\n                 train_time_interval: Optional[timedelta] = None,\r\n                 every_n_epochs: Optional[int] = None,\r\n                 save_on_train_epoch_end: Optional[bool] = None,\r\n                 period: Optional[int] = None,\r\n                 every_n_val_epochs: Optional[int] = None):\r\n        super().__init__(\r\n            dirpath=dirpath,\r\n            filename=filename,\r\n            monitor=monitor,\r\n            verbose=verbose,\r\n            save_last=save_last,\r\n            save_top_k=save_top_k,\r\n            save_weights_only=save_weights_only,\r\n            mode=mode,\r\n            auto_insert_metric_name=auto_insert_metric_name,\r\n            every_n_train_steps=every_n_train_steps,\r\n            train_time_interval=train_time_interval,\r\n            every_n_epochs=every_n_epochs,\r\n            save_on_train_epoch_end=save_on_train_epoch_end,\r\n            period=period,\r\n            every_n_val_epochs=every_n_val_epochs)\r\n        self.is_monitoring_on = False\r\n\r\n    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", unused: Optional = None) -> None:\r\n        \"\"\"Save a checkpoint at the end of the training epoch.\"\"\"\r\n        # as we advance one step at end of training, we use `global_step - 1` to avoid saving duplicates\r\n        trainer.fit_loop.global_step -= 1\r\n        if (\r\n            not self._should_skip_saving_checkpoint(trainer)\r\n            and self._save_on_train_epoch_end\r\n            and self._every_n_epochs > 0\r\n            and (trainer.current_epoch + 1) % self._every_n_epochs == 0\r\n            and (self.is_monitoring_on or self.monitor_can_start(trainer, pl_module))\r\n        ):\r\n            self.save_checkpoint(trainer)\r\n        trainer.fit_loop.global_step += 1\r\n\r\n    def monitor_can_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> bool:\r\n        \"\"\"Let start monitoring only after the loss curve start increasing\"\"\"\r\n       monitor_candidates = self._monitor_candidates(trainer, trainer.current_epoch, trainer.global_step - 1)\r\n        current = monitor_candidates.get(self.monitor)\r\n\r\n        # Check if the critic loss is increasing (the network is starting to\r\n        # train)\r\n        if trainer.current_epoch > 0 and pl_module.previous_metric < current:\r\n            self.is_monitoring_on = True\r\n\r\n        pl_module.previous_metric = current.detach().clone()\r\n\r\n        return self.is_monitoring_on\r\n```\r\n\r\nThe function `monitor_can_start()` does the trick.",
    "meta": { "name": "Save checkpoint with specific monitor criteria" },
    "answer": "Thanks to @tchaton on the slack community I solved the issue overriding the `ModelCheckpoint` class.\r\nIn the `on_train_epoch_end` I've added a new check that follow the above conditions, as such:\r\n\r\n```python\r\nclass WGANModelCheckpoint(ModelCheckpoint):\r\n    def __init__(self,\r\n                 dirpath: Optional[Union[str, Path]] = None,\r\n                 filename: Optional[str] = None,\r\n                 monitor: Optional[str] = None,\r\n                 verbose: bool = False,\r\n                 save_last: Optional[bool] = None,\r\n                 save_top_k: int = 1,\r\n                 save_weights_only: bool = False,\r\n                 mode: str = \"min\",\r\n                 auto_insert_metric_name: bool = True,\r\n                 every_n_train_steps: Optional[int] = None,\r\n                 train_time_interval: Optional[timedelta] = None,\r\n                 every_n_epochs: Optional[int] = None,\r\n                 save_on_train_epoch_end: Optional[bool] = None,\r\n                 period: Optional[int] = None,\r\n                 every_n_val_epochs: Optional[int] = None):\r\n        super().__init__(\r\n            dirpath=dirpath,\r\n            filename=filename,\r\n            monitor=monitor,\r\n            verbose=verbose,\r\n            save_last=save_last,\r\n            save_top_k=save_top_k,\r\n            save_weights_only=save_weights_only,\r\n            mode=mode,\r\n            auto_insert_metric_name=auto_insert_metric_name,\r\n            every_n_train_steps=every_n_train_steps,\r\n            train_time_interval=train_time_interval,\r\n            every_n_epochs=every_n_epochs,\r\n            save_on_train_epoch_end=save_on_train_epoch_end,\r\n            period=period,\r\n            every_n_val_epochs=every_n_val_epochs)\r\n        self.is_monitoring_on = False\r\n\r\n    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", unused: Optional = None) -> None:\r\n        \"\"\"Save a checkpoint at the end of the training epoch.\"\"\"\r\n        # as we advance one step at end of training, we use `global_step - 1` to avoid saving duplicates\r\n        trainer.fit_loop.global_step -= 1\r\n        if (\r\n            not self._should_skip_saving_checkpoint(trainer)\r\n            and self._save_on_train_epoch_end\r\n            and self._every_n_epochs > 0\r\n            and (trainer.current_epoch + 1) % self._every_n_epochs == 0\r\n            and (self.is_monitoring_on or self.monitor_can_start(trainer, pl_module))\r\n        ):\r\n            self.save_checkpoint(trainer)\r\n        trainer.fit_loop.global_step += 1\r\n\r\n    def monitor_can_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> bool:\r\n        \"\"\"Let start monitoring only after the loss curve start increasing\"\"\"\r\n       monitor_candidates = self._monitor_candidates(trainer, trainer.current_epoch, trainer.global_step - 1)\r\n        current = monitor_candidates.get(self.monitor)\r\n\r\n        # Check if the critic loss is increasing (the network is starting to\r\n        # train)\r\n        if trainer.current_epoch > 0 and pl_module.previous_metric < current:\r\n            self.is_monitoring_on = True\r\n\r\n        pl_module.previous_metric = current.detach().clone()\r\n\r\n        return self.is_monitoring_on\r\n```\r\n\r\nThe function `monitor_can_start()` does the trick."
  },
  {
    "content": "Has anyone else run into this error:\r\n`ValueError('signal only works in main thread')`\r\n\r\nI'm running a hyper parameter sweep using Weights and Biases's framework.\r\n\r\nRunning on a GPU on Google Colab which causes all launched runs to fail. Running it locally (Mac OS) prompts 'signal only works in main thread' to be printed to stdout (which also happens on Colab) but it doesn't crash.\r\n\r\nAny ideas? It seems people using [Ray with PL](https://github.com/PyTorchLightning/pytorch-lightning/issues/3651) have come across this. The hacky solution presented there (`os.environ['SLURM_JOB_NAME'] = 'bash'`) doesn't work in my case (neither on Mac OS or Colab).Hi @maxwass \r\n\r\nDoes [this notebook](https://colab.research.google.com/drive/1Tr9dYlwBKk6-LgLKGO8KYZULnguVA992?usp=sharing#scrollTo=ceYIx-8ezA5y) work if you run it locally?\r\n\r\nI have [another example](https://github.com/borisdayma/lightning-kitti) but it may be outdated with latest PL versions (haven't tested it recently).Update: My code  successfully runs on an aws server, and my local Mac  printing 'signal only works in main thread', but does not fail. On Colab, it fails with `Run snk3j2ue errored: ValueError('signal only works in main thread')`\r\nNow running on Colab and no longer getting the printed `'signal only works in main thread'` nor the `ValueError('signal only works in main thread')` : I had to port my sweep over to a yaml file and run it with the wandb login/sweep/agent calls from the CLI, not the python API.@max0x7ba @borisdayma The issue has been with #10610 A regression was introduced in version 1.5.5 and the problem returned.\r\nThis [PR](https://github.com/PyTorchLightning/pytorch-lightning/pull/10610/files#diff-455634aa5542cb67cb1ec373b47aa36cea4a924164006add72afec20a14818c7) solved the problem and was released in version 1.5.3. It also works in version 1.5.4, but in version 1.5.5 the problem returned.",
    "meta": { "name": "ValueError('signal only works in main thread')" },
    "answer": "@max0x7ba @borisdayma The issue has been with #10610 "
  },
  {
    "content": "Hi, I'm trying to save a model trained using deepspeed stage 2 using this code:\r\n```\r\ntrainer = pl.Trainer(\r\n    gpus=4,\r\n    plugins=DeepSpeedPlugin(\r\n              stage=3,\r\n              cpu_offload=True,\r\n              partition_activations=True,),\r\n    precision=16,\r\n    accelerator=\"ddp\",\r\n    )\r\ntrainer.fit(model, train_dataloader)\r\n```\r\n\r\nWith stage 2 it worked if I added this code:\r\n\r\n```\r\ntrainer = pl.Trainer(gpus=0,max_epochs=0,)\r\ntrainer.fit(model, train_dataloader)\r\npickle.dump(model,open(\"model.p\",\"wb\")\r\n```\r\n\r\nBut using stage=3 I get this error:\r\n\r\nTraceback (most recent call last):\r\n  File \"t5-11b-regression.py\", line 227, in <module>\r\n    torch.save(model,fileName)\r\n  File \"/home/ec2-user/.local/lib/python3.7/site-packages/torch/serialization.py\", line 379, in save\r\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\r\n  File \"/home/ec2-user/.local/lib/python3.7/site-packages/torch/serialization.py\", line 484, in _save\r\n    pickler.dump(obj)\r\nAttributeError: Can't pickle local object 'FP16_DeepSpeedZeroOptimizer_Stage3._register_hooks_recursively.<locals>._post_forward_module_hook'\r\n\r\nI also tried saving using torch.save, but got same error. I also tried both pytorch-lightning version 1.3.8 and 1.4.1\r\n\r\ncc: @SeanNaren Hey @ViktorThink!\r\n\r\nThanks for bringing this up, I think we can make this clearer in the documentation for next time.\r\n\r\nTo save I recommend you using `trainer.save_checkpoint('model.pt')` once your model has been trained. This is because DeepSpeed requires special care that is handled via the pytorch Trainer, so in your above example:\r\n\r\n```python\r\ntrainer = pl.Trainer(\r\n    gpus=4,\r\n    plugins=DeepSpeedPlugin(\r\n              stage=3,\r\n              cpu_offload=True,\r\n              partition_activations=True,),\r\n    precision=16,\r\n    accelerator=\"ddp\",\r\n    )\r\ntrainer.fit(model, train_dataloader)\r\ntrainer.save_checkpoint('model.pt')\r\n```\r\n\r\nNote when using DeepSpeed we save a directory not a single file. More information can be read in the documentation here: https://pytorch-lightning.readthedocs.io/en/latest/advanced/advanced_gpu.html#deepspeedSame error with pytorch-lightning 1.4.9 and 1.5.0rc1 on python 3.7 and 3.8 (DeepSpeedPlugin version is 0.5.4)\r\n\r\nafter evaluation phase, checkpoint callback tries to save shared model to disk, but torch can't pickle `deepspeed`'s hooks as functions are not pickleble. Is there any way to fix this? (I am also not sure why `_save_checkpoint` is called in `save_non_zero_checkpoint` context in `deepspeed`, maybe this is the problem with miss configuration?)\r\n\r\nwith Pytorch-lightning 1.4.9 I can use `save_full_weights` to overcome this. \r\nThis is my stack trace:\r\n```raceback (most recent call last):                                                                                                                                                               \r\n  File \"main.py\", line 457, in <module>                                                                                                                                                         \r\n    trainer.fit(model, datamodule=dpr_datamodule)                                                                                                                                               \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 635, in fit                                      \r\n    self._call_and_handle_interrupt(self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule)                                                                                      \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 587, in _call_and_handle_interrupt               \r\n    return trainer_fn(*args, **kwargs)                                                                                                                                                          \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 665, in _fit_impl                                \r\n    self._run(model)                                                                                                                                                                            \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1100, in _run                                    \r\n    self._dispatch()                                                                                                                                                                            \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1179, in _dispatch                               \r\n    self.training_type_plugin.start_training(self)                                                                                                                                              \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 183, in start_training\r\n    self._results = trainer.run_stage()                                                                                                                                                         \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1189, in run_stage                               \r\n    return self._run_train()                                                                                                                                                                    \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1228, in _run_train                              \r\n    self.fit_loop.run()                                                                                                                                                                         \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 145, in run                                           \r\n    self.advance(*args, **kwargs)                                                                                                                                                               \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py\", line 211, in advance                                   \r\n    self.epoch_loop.run(data_fetcher)                                                                                                                                                           \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 146, in run                                           \r\n    self.on_advance_end()                                                                                                                                                                       \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 233, in on_advance_end           \r\n    self._run_validation()                                                                                                                                                                      \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 330, in _run_validation          \r\n    self.val_loop.run()                                                                                                                                                                         \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 151, in run                                           \r\n    output = self.on_run_end()                                                                                                                                                                  \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 137, in on_run_end              \r\n    self._on_evaluation_end()                                                                                                                                                                   \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 199, in _on_evaluation_end      \r\n    self.trainer.call_hook(\"on_validation_end\", *args, **kwargs)                                                                                                                                \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1392, in call_hook                               \r\n    callback_fx(*args, **kwargs)                                                                                                                                                                \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_hook.py\", line 221, in on_validation_end                  \r\n    callback.on_validation_end(self, self.lightning_module)                                                                                                                                     \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 327, in on_validation_end             \r\n    self.save_checkpoint(trainer)                                                                                                                                                               \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 384, in save_checkpoint               \r\n    self._save_none_monitor_checkpoint(trainer, monitor_candidates)                                                                                                                             \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 681, in _save_none_monitor_checkpoint \r\n    trainer.save_checkpoint(filepath, self.save_weights_only)                                                                                                                                   \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1809, in save_checkpoint                         \r\n    self.checkpoint_connector.save_checkpoint(filepath, weights_only)                                                                                                                           \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 465, in save_checkpoint  \r\n    self.trainer.training_type_plugin.save_checkpoint(_checkpoint, filepath)                                                                                                                    \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/deepspeed.py\", line 714, in save_checkpoint          \r\n    self.deepspeed_engine.save_checkpoint(filepath, client_state=checkpoint)                                                                                                                    \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/deepspeed/runtime/engine.py\", line 2231, in save_checkpoint                                  \r\n    self._save_checkpoint(save_dir, tag, client_state=client_state)                                                                                                                             \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/deepspeed/runtime/engine.py\", line 2408, in _save_checkpoint                                 \r\n    torch.save(state, save_path)                                                                                                                                                                \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/torch/serialization.py\", line 379, in save                                                   \r\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol)                                                                                                                                  \r\n  File \"/net/ascratch/people/plgdmytro/anaconda3/envs/deepspeed_env_37/lib/python3.7/site-packages/torch/serialization.py\", line 484, in _save                                                  \r\n    pickler.dump(obj)                                                                                                                                                                           \r\nAttributeError: Can't pickle local object 'FP16_DeepSpeedZeroOptimizer_Stage3._register_hooks_recursively.<locals>._post_forward_module_hook'```After some debugging with a user, I've come up with a final script to show how you can use the `convert_zero_checkpoint_to_fp32_state_dict` to generate a single file that can be loaded using pickle, or lightning.\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\r\nfrom pytorch_lightning.utilities.deepspeed import convert_zero_checkpoint_to_fp32_state_dict\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        limit_test_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n        strategy=DeepSpeedPlugin(stage=2),\r\n        precision=16,\r\n        gpus=2,\r\n        callbacks=ModelCheckpoint(dirpath='checkpoints', save_last=True)\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n\r\n    # once saved via the model checkpoint callback,\r\n    # it saves a folder containing the deepspeed checkpoint rather than a single file\r\n    checkpoint_path = \"checkpoints/last.ckpt/\"\r\n\r\n    if trainer.is_global_zero:\r\n        single_ckpt_path = \"single_model.pt\"\r\n\r\n        # magically converts the folder into a single lightning loadable pytorch file (for ZeRO 1,2 and 3)\r\n        convert_zero_checkpoint_to_fp32_state_dict(checkpoint_path, single_ckpt_path)\r\n        loaded_parameters = BoringModel.load_from_checkpoint(single_ckpt_path).parameters()\r\n\r\n        model = model.cpu()\r\n        # Assert model parameters are identical after loading\r\n        for orig_param, saved_model_param in zip(model.parameters(), loaded_parameters):\r\n            if model.dtype == torch.half:\r\n                # moved model to float32 for comparison with single fp32 saved weights\r\n                saved_model_param = saved_model_param.half()\r\n            assert torch.equal(orig_param, saved_model_param)\r\n```\r\n\r\nThe above where we use the Trainer as an engine still works, but now you'd need to pass the checkpoint path like so `trainer.predict(ckpt_path=..., ...)`",
    "meta": { "name": "How save deepspeed stage 3 model with pickle or torch" },
    "answer": "After some debugging with a user, I've come up with a final script to show how you can use the `convert_zero_checkpoint_to_fp32_state_dict` to generate a single file that can be loaded using pickle, or lightning.\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\r\nfrom pytorch_lightning.utilities.deepspeed import convert_zero_checkpoint_to_fp32_state_dict\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        limit_test_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n        strategy=DeepSpeedPlugin(stage=2),\r\n        precision=16,\r\n        gpus=2,\r\n        callbacks=ModelCheckpoint(dirpath='checkpoints', save_last=True)\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n\r\n    # once saved via the model checkpoint callback,\r\n    # it saves a folder containing the deepspeed checkpoint rather than a single file\r\n    checkpoint_path = \"checkpoints/last.ckpt/\"\r\n\r\n    if trainer.is_global_zero:\r\n        single_ckpt_path = \"single_model.pt\"\r\n\r\n        # magically converts the folder into a single lightning loadable pytorch file (for ZeRO 1,2 and 3)\r\n        convert_zero_checkpoint_to_fp32_state_dict(checkpoint_path, single_ckpt_path)\r\n        loaded_parameters = BoringModel.load_from_checkpoint(single_ckpt_path).parameters()\r\n\r\n        model = model.cpu()\r\n        # Assert model parameters are identical after loading\r\n        for orig_param, saved_model_param in zip(model.parameters(), loaded_parameters):\r\n            if model.dtype == torch.half:\r\n                # moved model to float32 for comparison with single fp32 saved weights\r\n                saved_model_param = saved_model_param.half()\r\n            assert torch.equal(orig_param, saved_model_param)\r\n```\r\n\r\nThe above where we use the Trainer as an engine still works, but now you'd need to pass the checkpoint path like so `trainer.predict(ckpt_path=..., ...)`"
  },
  {
    "content": "I call `trainer.fit(model=model_lit, datamodule=datamodule)`\r\n\r\nI have set parameters:\r\n\r\n```\r\nlimit_val_batches: 0\r\nlimit_test_batches: 0\r\n```\r\n\r\n`datamodule.val_dataloader()` returns **None**\r\n\r\nBut it is still trying to perform validation... \r\nIs it a problem of lightning, or am I doing something wrong? :)\r\n\r\n![image](https://user-images.githubusercontent.com/27057946/145991475-291a9199-22f8-43ce-a195-127d992ab09e.png)\r\n\r\n\r\nWell. The reason why validation was called is `overfit_batches=1` parameter.it's now turned off by default on master:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/2faaf35b91a00aff397609a875a66c87f8ed6390/pytorch_lightning/trainer/trainer.py#L636-L640",
    "meta": {
      "name": "Can I turn off Validation step when overfit_batches=X?"
    },
    "answer": "it's now turned off by default on master:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/2faaf35b91a00aff397609a875a66c87f8ed6390/pytorch_lightning/trainer/trainer.py#L636-L640"
  },
  {
    "content": "AttributeError: module 'pytorch_lightning' has no attribute 'metrics'\r\n\r\nwhat is this...? T.TPytorch Lightning Metrics were moved to a separate package/library/repo, torchmetrics, starting from Lightning 1.5",
    "meta": {
      "name": "AttributeError: module 'pytorch_lightning' has no attribute 'metrics'"
    },
    "answer": "Pytorch Lightning Metrics were moved to a separate package/library/repo, torchmetrics, starting from Lightning 1.5"
  },
  {
    "content": "I would like to save the top-10 checkpionts along training. By checking documentations, setting `save_top_k`, `monitor` and `mode` options in [`ModelCheckpoint`](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html?highlight=ModelCheckpoint) jointly seem to do the job.\r\n\r\nBut I am not sure what are the parameters available for the this callback to monitor. Are they logged values saved during `training_step()` or `validation_step()` through `self.log(\"loss\", XYZ)`?\r\n\r\nThank you in advance!yes, that's correct.",
    "meta": {
      "name": "Accessing available values to monitor when saving checkpoints"
    },
    "answer": "yes, that's correct."
  },
  {
    "content": "I've stumbled upon the problem of not being able to use `accumulate_grad_batches` argument in the Trainer as I was doing manual optimization in my LightningModule to use adversarial loss functions.\r\n\r\nHowever, I think it would be possible to implement something that would \"store\" calls to the `step` method for the module's optimizers and actually apply them once every `accumulate_grad_batches` iterations. I've seen several related issues about similar behavours when overriding `optimizer_step` or close to my use case (#5054, #5108). The proposed fixes always leave some manual get-arounds in the final code.\r\n\r\n**My question: is there a reason for such incompatibility of `accumulate_grad_batches` with manual optimization ?**\r\n\r\nOne reason might be the need to `step` different optimizers at different paces (one every batch, another every n batches ...) but this seems to be an extreme use case.gradient accumulation revolves around `loss.backward` and `optimizer.step` calls. Since both of them are not controlled by PL during manual optimization and is up to the user to call it, it can't really be integrated natively for this case.Hey @NathanGodey,\r\n\r\nmanual optimization was built to provide full control optimization control to the user while abstracting distributed training and precision.\r\n\r\nThere is no way Lightning can automate properly accumulate grad batches for all the possible use cases and therefore isn't supported.\r\n\r\nHowever, you can easily implement it by not calling zero_grad, step every n batches.",
    "meta": {
      "name": "Why accumulate_grad_batches cannot be used with manual optimization?"
    },
    "answer": "Hey @NathanGodey,\r\n\r\nmanual optimization was built to provide full control optimization control to the user while abstracting distributed training and precision.\r\n\r\nThere is no way Lightning can automate properly accumulate grad batches for all the possible use cases and therefore isn't supported.\r\n\r\nHowever, you can easily implement it by not calling zero_grad, step every n batches."
  },
  {
    "content": "Hallo, \r\nI am trying to train a BERT model with the squad dataset. \r\nSince the model has a maximum input length, examples with longer contexts must be split into several samples. \r\nIn the validation_epoch_end() and test_epoch_end() methods I want to compute the squad metric (https://github.com/PyTorchLightning/metrics/blob/master/torchmetrics/text/squad.py) that needs the predicted answers in text format. \r\nBut for each sample the model predicts only the start and end token. \r\nTo generate the answers in text format I need the dataset from the val/test_dataloader as well as the original val/test_dataset (with text instead of input_ids, offset_mappings etc.)\r\nSo I was wondering how to access the original dataset in those methods. \r\nOf course I can use self.trainer.datamodule.original_val_dataset but then I need to ensure that the trainer uses a datamodule and not only dataloaders. What is the best practice for this case? \r\nAnd how do I use this model in production? Can I compute the text answers in the LightningModule and return them when predict() is called ?\r\nOr should predict() only return the start and end tokens and the text answer is computed outside of the LightningModule? \r\nThank you in advance!I'd suggest accessing the dataloader/dataset isn't a reliable solution, but rather you can return the original text and offests in Dataset.getitem or you can store the offests and text in the state variables for easy access.\r\n\r\nHere are some examples I worked with on kaggle:\r\nFor the first one where you return the offests/text: https://www.kaggle.com/rohitgr/roberta-with-pytorch-lightning-train-test-lb-0-710\r\nThis one uses an old version of lightning, but should still be relevant I guess.\r\n\r\nHere I store them in my datamodule: https://www.kaggle.com/rohitgr/chaii-q-a-with-pytorch-lightining\r\nthis one is recent so should be working.",
    "meta": { "name": "How to compute a squad metric?" },
    "answer": "I'd suggest accessing the dataloader/dataset isn't a reliable solution, but rather you can return the original text and offests in Dataset.getitem or you can store the offests and text in the state variables for easy access.\r\n\r\nHere are some examples I worked with on kaggle:\r\nFor the first one where you return the offests/text: https://www.kaggle.com/rohitgr/roberta-with-pytorch-lightning-train-test-lb-0-710\r\nThis one uses an old version of lightning, but should still be relevant I guess.\r\n\r\nHere I store them in my datamodule: https://www.kaggle.com/rohitgr/chaii-q-a-with-pytorch-lightining\r\nthis one is recent so should be working."
  },
  {
    "content": "Hey,\r\n\r\nI've been using both, PyTorch Lightning/CLI and jsonargparse for quite a while. Yet, I haven't found a simple method to instantiate a specific DataModule whose parameters are set in a config.yaml that was used during training. I have 2 workarounds which are both unsatisfying:\r\n\r\n## Workaround 1 - Reuse CLI\r\n\r\nDefine an instantiate-only CLI and use `.datamodule`\r\n```\r\nclass InstantiateOnlyLightningCLI(LightningCLI): # probably unnecessary in current RC: run=False\r\n    def fit(self) -> None:\r\n        return None\r\n\r\ncli = InstantiateOnlyLightningCLI(\r\n        wave.WaveNet,\r\n        pl.LightningDataModule,\r\n        subclass_mode_model=False,\r\n        subclass_mode_data=True,\r\n    )\r\ncli.datamodule\r\n```\r\nDownsides\r\n - trainer and model are instantiated although not used\r\n - `--help` is populated with many unneeded parameters\r\n\r\n## Workaround 2 - Load Yaml directly\r\nWhen the actual datamodule is known then\r\n```\r\nwith open(config, \"r\") as f:\r\n  plcfg = yaml.safe_load(f.read())\r\n  datamodule = SpecificDataModule(**plcfg[\"data\"][\"init_args\"])\r\n```\r\nDownsides\r\n - No parameter validation\r\n - No class arguments (i.e transformation classes) supported\r\n\r\nDoes anyone have a better solution?@mauvilsa This is similar to https://github.com/PyTorchLightning/pytorch-lightning/discussions/10363. You can use jsonargparse directly to create a parser and instantiate. You can do the following:\r\n\r\n```python\r\nfrom jsonargparse import ArgumentParser\r\n\r\nparser = ArgumentParser()\r\nparser.add_argument('--model', type=dict) # to ignore model\r\nparser.add_argument('--data', type=pl.LightningDataModule)\r\nconfig = parser.parse_path('config.yaml')\r\nconfig_init = parser.instantiate_classes(config)\r\n```\r\n\r\nThe instantiated data module will be in `config_init.data`. In the pytorch-lightning source code the add of arguments is done slightly different but this argparse style should be more familiar to more people. Just for reference in lightning for subclass mode it is https://github.com/PyTorchLightning/pytorch-lightning/blob/a7aed2af7a0de344c4a8eac32f9a86a36a3eaeec/pytorch_lightning/utilities/cli.py#L164@mauvilsa thanks for the reply. In the end, to load from a PyTorch-Lightning config you need to\r\n\r\n```\r\nparser = jsonargparse.ArgumentParser()\r\nparser.add_argument(\"--model\", type=dict)  # to ignore model\r\nparser.add_argument(\"--trainer\", type=dict)  # to ignore trainer\r\nparser.add_argument(\"--data\", type=datasets.MNISTDataModule)\r\nparser.add_argument(\"--config\", action=jsonargparse.ActionConfigFile)\r\nparser.add_argument(\"--seed_everything\", type=Any) # ignore\r\n\r\nconfig = parser.parse_args()\r\nconfig_init = parser.instantiate_classes(config)\r\n```\r\n\r\nFor now :) I believe jsonargparse does not support `parse_known_args` so that we could specifying all potential additional fields that PL adds to the config?\r\n",
    "meta": {
      "name": "Best practices: CLI and Loading DataModule from config.yaml"
    },
    "answer": "This is similar to https://github.com/PyTorchLightning/pytorch-lightning/discussions/10363. You can use jsonargparse directly to create a parser and instantiate. You can do the following:\r\n\r\n```python\r\nfrom jsonargparse import ArgumentParser\r\n\r\nparser = ArgumentParser()\r\nparser.add_argument('--model', type=dict) # to ignore model\r\nparser.add_argument('--data', type=pl.LightningDataModule)\r\nconfig = parser.parse_path('config.yaml')\r\nconfig_init = parser.instantiate_classes(config)\r\n```\r\n\r\nThe instantiated data module will be in `config_init.data`. In the pytorch-lightning source code the add of arguments is done slightly different but this argparse style should be more familiar to more people. Just for reference in lightning for subclass mode it is https://github.com/PyTorchLightning/pytorch-lightning/blob/a7aed2af7a0de344c4a8eac32f9a86a36a3eaeec/pytorch_lightning/utilities/cli.py#L164"
  },
  {
    "content": "hi all,\r\n\r\nMy model validation code (see below) appears to leak memory which leads to a rapid increase in GPU memory usage and, eventually, to an OOM error right before the validation loop is about to complete (about 90% done or so). CUDA memory usage hovers around 8-9GB during training, then increases rapidly to ca. 15+GB during validation, hitting the memory limit of my GPU card. \r\n\r\nWhat am I doing wrong here?\r\n\r\n```\r\nclass Lightning_WGAN_GP(pl.LightningModule):\r\n    \"\"\"Conditional Wasserstein GAN with gradient penalty.\"\"\"\r\n \r\n    # (...)\r\n\r\n    def _get_noise(self, X: torch.Tensor) -> torch.Tensor:\r\n        bs, _, h, w = X.shape\r\n        return torch.randn(bs, 1, h, w).type_as(X)\r\n\r\n    def validation_step(self, batch: Tuple[Dict, ...], batch_idx: int) -> Dict:\r\n        del batch_idx  # not used\r\n        X, X_hr, real = batch[0][\"X_lr\"], batch[0][\"X_hr\"], batch[1][\"y\"]\r\n\r\n        with torch.no_grad():\r\n            noise = self._get_noise(X)\r\n            fake = self.gen(noise, X, X_hr)  # calling the generator\r\n            loss_gen_val = F.l1_loss(fake, real)   # generator loss\r\n            disc_real = self.disc(X, real, X_hr).reshape(-1)  # calling the discriminator\r\n            disc_fake = self.disc(X, fake, X_hr).reshape(-1)\r\n            loss_disc_val = -torch.mean(disc_real) + torch.mean(disc_fake)  # discriminator loss\r\n            \r\n        self.log(\"gen_val_loss\", loss_gen_val, on_epoch=True, on_step=False, prog_bar=True, logger=True)\r\n        self.log(\"disc_val_loss\", loss_disc_val, on_epoch=True, on_step=False, prog_bar=True, logger=True)\r\n\r\n        return {\"gen_val\": loss_gen_val, \"disc_val\": loss_disc_val, \"batch\": batch}\r\n```\r\n\r\n\r\n![Screenshot 2021-12-06 at 19 57 18](https://user-images.githubusercontent.com/47196359/144905777-aa2799bb-fd7a-402f-a239-2c30c38a1fda.png)\r\n\r\n```\r\nRuntimeError: CUDA out of memory. Tried to allocate 266.00 MiB (GPU 0; 16.00 GiB total capacity; 12.84 GiB already allocated; 96.55 MiB free; 13.52 GiB reserved in total by PyTorch)\r\n```\r\n\r\nDecreasing (or increasing) the validation batch size doesn't make the problem go away. Any thoughts?\r\n\r\n```\r\n$ conda list | grep pytorch\r\npytorch                   1.9.1           cuda102py38ha031fbe_3    conda-forge\r\npytorch-gpu               1.9.1           cuda102py38hf05f184_3    conda-forge\r\npytorch-lightning         1.5.3              pyhd8ed1ab_0    conda-forge\r\n```\r\n\r\n**Later edit**: Skipping the validation loop, i.e.,\r\n```\r\ngan_trainer.fit(gan_model, train_dataloaders=dl_train)\r\n``` \r\ngets rid of the OOM error (the trainer makes it past the 1st epoch). \r\n\r\nAlso, I am running in mixed precision (although i suspect precision doesn't have much to do with this issue?)\r\n\r\nThank you!\r\nDear @mishooax,\r\n\r\nYou are returning the batch from the validation_step, which would be stored. As it is currently on the GPU, after X batches, you would get a OOM.\r\n\r\nUnless you need the batch on epoch end, I would recommend to not return anything from the validation_step.",
    "meta": { "name": "CUDA OOM during validation of first epoch" },
    "answer": "Dear @mishooax,\r\n\r\nYou are returning the batch from the validation_step, which would be stored. As it is currently on the GPU, after X batches, you would get a OOM.\r\n\r\nUnless you need the batch on epoch end, I would recommend to not return anything from the validation_step."
  },
  {
    "content": "If I select X workers in my dataloader, and I train my model using ddp with Y GPUs, is the effective amount of workers running in the machine X*Y or X?Yes! The script will launch Y times in a separate process. Each process will then create their own dataloader with as many workers as given. So X*Y workers in total accessing your filesystem.",
    "meta": { "name": "How many effective workers does my code use?" },
    "answer": "Yes! The script will launch Y times in a separate process. Each process will then create their own dataloader with as many workers as given. So X*Y workers in total accessing your filesystem."
  },
  {
    "content": "I am trying to get my losses in Tensorboard, but I am quite confused.\r\nI simply return a dict after `training_step` and `validation_step`, containing `loss` and `log`. However, the only thing that is showing up in Tensorboard of that run is the 'hp_metric' thing... Nothing on the scalars of the losses... Both during and after training.\r\nI got it working with manual logging (self.log(...)), however, that should not be necessary right? And is more complicated when I want for example training and validation loss in one plot. I am working on a Super Resolution GAN. This is my trainer:\r\n\r\n```\r\nclass LitTrainer(pl.LightningModule):\r\n    def __init__(self,\r\n                 netG,\r\n                 netD,\r\n                 lr: float = 0.0002,\r\n                 b1: float = 0.5,\r\n                 b2: float = 0.999,\r\n                 **kwargs\r\n                 ):\r\n        super().__init__()\r\n        self.save_hyperparameters(ignore=[\"netG\", \"netD\"])\r\n\r\n        self.netG = netG\r\n        self.netD = netD\r\n\r\n        self.criterion_GAN = GANLoss(\"vanilla\")\r\n        self.criterion_edge = edge_loss1\r\n        self.criterion_pixel = torch.nn.L1Loss()\r\n\r\n    def forward(self, inputs):\r\n        return self.netG(inputs)\r\n\r\n    def prepare_batch(self, batch):\r\n        return batch[\"LR\"][tio.DATA].squeeze(4), batch[\"HR\"][tio.DATA].squeeze(4)\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        imgs_lr, imgs_hr = self.prepare_batch(batch)\r\n\r\n        # train generator\r\n        if optimizer_idx == 0:\r\n            self.gen_hr = self(imgs_lr)\r\n\r\n            loss_adv = self.criterion_GAN(self.netD(self.gen_hr), True)\r\n            loss_edge = self.criterion_edge(self.gen_hr, imgs_hr)\r\n            loss_pixel = self.criterion_pixel(self.gen_hr, imgs_hr)\r\n            g_loss = loss_adv + loss_edge + loss_pixel\r\n\r\n            # self.log(\"loss/G train\", g_loss, on_step=True, on_epoch=True)\r\n            tensorboard_logs = {\"loss_g\": {\"train\": g_loss}}\r\n            return {\"loss\": g_loss, \"log\": tensorboard_logs}\r\n\r\n        # train discriminator\r\n        if optimizer_idx == 1:\r\n\r\n            # for real image\r\n            pred_real = self.netD(imgs_hr)\r\n            real_loss = self.criterion_GAN(pred_real, True)\r\n            # for fake image\r\n            pred_fake = self.netD(self.gen_hr.detach())\r\n            fake_loss = self.criterion_GAN(pred_fake, False)\r\n\r\n            d_loss = (real_loss + fake_loss) / 2\r\n            tensorboard_logs = {\"loss_d\": {\"train\": d_loss}}\r\n\r\n            # self.log(\"loss/D train\", d_loss, on_step=True)\r\n            return {\"loss\": d_loss, \"log\": tensorboard_logs}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        with torch.no_grad():\r\n            imgs_lr, imgs_hr = self.prepare_batch(batch)\r\n            gen_hr = self(imgs_lr)\r\n            loss_adv = self.criterion_GAN(self.netD(gen_hr), True)\r\n            loss_edge = self.criterion_edge(gen_hr, imgs_hr)\r\n            loss_pixel = self.criterion_pixel(gen_hr, imgs_hr)\r\n            g_loss = loss_adv + loss_edge + loss_pixel\r\n\r\n            # for real image\r\n            pred_real = self.netD(imgs_hr)\r\n            real_loss = self.criterion_GAN(pred_real, True)\r\n            # for fake image\r\n            pred_fake = self.netD(self.gen_hr.detach())\r\n            fake_loss = self.criterion_GAN(pred_fake, False)\r\n\r\n            d_loss = (real_loss + fake_loss) / 2\r\n            tensorboard_logs = {\"loss_g\": {\"val\": g_loss},\r\n                                \"loss_d\": {\"val\": d_loss},\r\n                                }\r\n\r\n            # self.log(\"loss/G val\", g_loss, on_step=True)\r\n            # self.log(\"loss/D val\", d_loss, on_step=True)\r\n\r\n            return {\"log\": tensorboard_logs}\r\n\r\n    def configure_optimizers(self):\r\n        lr = self.hparams.lr\r\n        b1 = self.hparams.b1\r\n        b2 = self.hparams.b2\r\n        opt_g = torch.optim.Adam(self.netG.parameters(), lr=lr, betas=(b1, b2))\r\n        opt_d = torch.optim.Adam(self.netD.parameters(), lr=lr, betas=(b1, b2))\r\n        return [opt_g, opt_d], []\r\n```\r\n\r\nAnd this is how I start training:\r\n\r\n```\r\nlogger = TensorBoardLogger(\"log\", name=\"test\")\r\n\r\nmodel = LitTrainer(netG=generator, netD=discriminator)\r\ntrainer = pl.Trainer(gpus=1, max_epochs=1, logger=logger, log_every_n_steps=10)\r\ntrainer.fit(model, train_dataloaders=training_loader, val_dataloaders=val_loader)\r\n```Are you using the latest Lightning version? If yes, this won't work. The new way for logging is through `self.log` instead of returning it from the step methods. See the docs [here](https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging).",
    "meta": { "name": "Logging tensorboard not showing loss" },
    "answer": "Are you using the latest Lightning version? If yes, this won't work. The new way for logging is through `self.log` instead of returning it from the step methods. See the docs [here](https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging)."
  },
  {
    "content": "When I try to set `Trainer` like this:\r\n\r\n```python\r\ntrainer = Trainer(\r\n    num_sanity_val_steps=0, \r\n    logger=cfg.load_loggers,\r\n    callbacks=cfg.callbacks,\r\n    max_epochs= cfg.General.epochs,\r\n    gpus=cfg.General.gpus,\r\n    amp_level=cfg.General.amp_level, # O2\r\n    precision=cfg.General.precision,  \r\n    accumulate_grad_batches=cfg.General.grad_acc,\r\n    deterministic=True,\r\n    check_val_every_n_epoch=1,\r\n)\r\n```\r\n\r\nIt will throw an error:\r\n\r\n`pytorch_lightning.utilities.exceptions.MisconfigurationException: You have asked for amp_level='O2' but it's only supported with amp_backend='apex'`\r\n\r\nHey @RuixiangZhao,\r\n\r\nThere are currently 2 precision backends. AMP and APEX. level are supported only with apex and you need to provide Trainer(amp_backend='apex') to activate it as native is the default.\r\n\r\nI am curious, what was missing from the `MisconfigurationException` to make this clearer? ",
    "meta": { "name": "Trainer(amp_level='O2')" },
    "answer": "Hey @RuixiangZhao,\r\n\r\nThere are currently 2 precision backends. AMP and APEX. level are supported only with apex and you need to provide Trainer(amp_backend='apex') to activate it as native is the default.\r\n\r\nI am curious, what was missing from the `MisconfigurationException` to make this clearer? "
  },
  {
    "content": "I was wondering if there's a way to use apex.parallel.DistributedDataParallel instead of pytorch native DistributedDataParallel. (I am trying to reproduce a paper that used Apex DDP and apex mixed precision and i am getting lower results using pytorch native one)Here is a quick draft of what you could try:\r\n\r\n```py\r\nfrom pytorch_lightning.plugins.training_type import DDPPlugin\r\nfrom apex.parallel import DistributedDataParallel\r\nclass ApexDDPPlugin(DDPPlugin):\r\n\r\n    def _setup_model(self, model: Module):\r\n        return  DistributedDataParallel(module=model, device_ids=self.determine_ddp_device_ids(), **self._ddp_kwargs)\r\n\r\n    @property\r\n    def lightning_module(self):\r\n        return self.module.module\r\n```\r\nI'm not sure if apex DistributedDataParallel supports device ids (it seems not??), you may need to remove it.\r\n\r\nUse it in the trainer:\r\n\r\n```py\r\ntrainer = Trainer(gpus=2, strategy=ApexDDPPlugin(), precision=...)\r\ntrainer.fit(model)\r\n```",
    "meta": {
      "name": "how to use Apex DistributedDataParallel with Lightining?"
    },
    "answer": "Here is a quick draft of what you could try:\r\n\r\n```py\r\nfrom pytorch_lightning.plugins.training_type import DDPPlugin\r\nfrom apex.parallel import DistributedDataParallel\r\nclass ApexDDPPlugin(DDPPlugin):\r\n\r\n    def _setup_model(self, model: Module):\r\n        return  DistributedDataParallel(module=model, device_ids=self.determine_ddp_device_ids(), **self._ddp_kwargs)\r\n\r\n    @property\r\n    def lightning_module(self):\r\n        return self.module.module\r\n```\r\nI'm not sure if apex DistributedDataParallel supports device ids (it seems not??), you may need to remove it.\r\n\r\nUse it in the trainer:\r\n\r\n```py\r\ntrainer = Trainer(gpus=2, strategy=ApexDDPPlugin(), precision=...)\r\ntrainer.fit(model)\r\n```"
  },
  {
    "content": "Hi\r\n\r\nI am running `pyTorch 1.10` and `pytorch-lightning 1.5.4`. I want to downgrade pytorch-lightning to `0.7.1` because the code I am testing uses this version and It looks to me a lot of breaking changes have happened since then. \r\n\r\nHow can I do that please? Would a `pip uninstall pytorch-lightning` and then `pip install pytorch-lightning==0.7.1` suffice or there is something else I need to take care of?\r\n\r\nDoes anyone know also if `0.7.1` is going to be compatible with  `pyTorch 1.10` ?Hello\r\nUnfortunately I must predict that your chances will be very low that 0.7.1 will run with pytorch 1.10. I would not expect it. \r\n\r\nBut in any case, if the code was written in that version, the best is to use the pytorch version that was used in that project. Otherwise you may struggle to reproduce the results entirely. If you use conda or virtualenv, you can create different environments isolated from each other, for example, one for the old PL+pytorch project and one with the latest packages. \r\n",
    "meta": { "name": "Uninstalling pytorch-lightning" },
    "answer": "Hello\r\nUnfortunately I must predict that your chances will be very low that 0.7.1 will run with pytorch 1.10. I would not expect it. \r\n\r\nBut in any case, if the code was written in that version, the best is to use the pytorch version that was used in that project. Otherwise you may struggle to reproduce the results entirely. If you use conda or virtualenv, you can create different environments isolated from each other, for example, one for the old PL+pytorch project and one with the latest packages. \r\n"
  },
  {
    "content": "I am learning deep RL, and as an exercise I looked through and tried to implement this example of DQN in pytorch lightning:\r\n\r\n[DQN example](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/reinforce-learning-DQN.html)\r\n\r\nI believe that I have found a bug and I am not sure how to flag it or how to upload a fix myself. This is my first time trying to contribute to an open source project and any advice will be greatly appreciated.\r\n\r\nOne line 106 of the section for the DQN Lightning Module, the expression for epsilon (which decays over the first eps_last_frame steps) is incorrect. The code is currently:\r\n`\r\nepsilon = max(\r\n    self.hparams.eps_end,\r\n    self.hparams.eps_start - self.global_step + 1 / self.hparams.eps_last_frame,\r\n)`\r\n\r\nI believe this is incorrect for the following reason: 0 <= epsilon <= 1, so self.global_step immediately overcomes the other two terms. There is no decay, only a single timestep at `eps_start + 1/eps_last_frame`, and then timesteps at `eps_end`. This also seems like an error because with the default values,  `eps_start + 1/eps_last_frame > 1`. \r\n\r\nThe intended behavior is for the epsilon to decay linearly from `eps_start` to `eps_end` over the first `eps_last_frame` frames. I believe the second argument to max() should be:\r\n\r\n`self.hparams.eps_start - (self.global_step / self.hparams.eps_last_frame) * (self.hparams.eps_start - self.hparams.eps_end)`\r\n\r\nWho do I contact, or is there a way I can upload this fix myself?Hello! This fix would be done in the Lightning Tutorials repo here: https://github.com/PyTorchLightning/lightning-tutorials\r\n\r\n1. Fork the repo (button on top right)\r\n2. Clone the repo from your fork\r\n2. Go to the DQN example source code/notebook\r\n3. Edit with your proposed fix\r\n4. commit the changes and push them to your fork\r\n5. Then go to the https://github.com/PyTorchLightning/lightning-tutorials/pulls and open a Pull Request. \r\n\r\nLet me know if that helps and thanks for checking out the tutorials and the help!",
    "meta": {
      "name": "Found mistake in pytorch-lightning DQN example. How do I upload a fix?"
    },
    "answer": "Hello! This fix would be done in the Lightning Tutorials repo here: https://github.com/PyTorchLightning/lightning-tutorials\r\n\r\n1. Fork the repo (button on top right)\r\n2. Clone the repo from your fork\r\n2. Go to the DQN example source code/notebook\r\n3. Edit with your proposed fix\r\n4. commit the changes and push them to your fork\r\n5. Then go to the https://github.com/PyTorchLightning/lightning-tutorials/pulls and open a Pull Request. \r\n\r\nLet me know if that helps and thanks for checking out the tutorials and the help!"
  },
  {
    "content": "These hooks are called when trainer initialization begins and ends, before the model has been set, essentially allowing the user to modify the Trainer constructor.  Should we be giving the user this much control over Trainer constructor? Are there scenarios where this is needed? Or can we deprecate these hooks?\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/338f3cf63686935355c749920b2f298f3d18a26f/pytorch_lightning/trainer/callback_hook.py#L55-L63\r\n\r\ncc @ananthsub @carmocca @tchaton @awaelchli do you know how these hooks are used? Have you seen any examples of these being used by the community? These hooks go way way back, but I can't think of when they'd be needed given the user \"owns\" the Trainer initialization. It's also unclear when `on_init_start` actually happens: does that mean callbacks should be the first thing initialized? \r\n\r\nit seems a lot more straightforward to write this:\r\n```python\r\ntrainer = Trainer(...)\r\nrun_all_my_fancy_logic_now(trainer)\r\n\r\n# use the trainer here\r\n```\r\n\r\nLet's discuss in https://github.com/PyTorchLightning/pytorch-lightning/issues/10894",
    "meta": {
      "name": "[RFC] Thoughts on `on_init_start` and `on_init_end` hooks"
    },
    "answer": "@carmocca @tchaton @awaelchli do you know how these hooks are used? Have you seen any examples of these being used by the community? These hooks go way way back, but I can't think of when they'd be needed given the user \"owns\" the Trainer initialization. It's also unclear when `on_init_start` actually happens: does that mean callbacks should be the first thing initialized? \r\n\r\nit seems a lot more straightforward to write this:\r\n```python\r\ntrainer = Trainer(...)\r\nrun_all_my_fancy_logic_now(trainer)\r\n\r\n# use the trainer here\r\n```\r\n\r\nLet's discuss in https://github.com/PyTorchLightning/pytorch-lightning/issues/10894"
  },
  {
    "content": "I am trying to get multi-gpu training working, on single gpu it is al working fine. However, when I increase the number of GPUs I get a pickling error, and I don't know what to do about it. For the dataloader I am using the patch-based approach from TorchIO, which creates a Queue, maybe that is the cause? Does anyone has experience with TorchIO Queue and Lightning multi-gpu maybe? Or is something else going on?\r\n\r\nThe error i am getting is as follows:\r\n\r\n```\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3,4]\r\n\r\nTraceback (most recent call last):\r\n  File \"/filepath/SRGAN-patch_tio.py\", line 94, in <module>\r\n    main()\r\n  File \"/filepath/SRGAN-patch_tio.py\", line 91, in main\r\n    trainer.fit(model, train_dataloaders=training_loader)\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 737, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 682, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 772, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1195, in _run\r\n    self._dispatch()\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1274, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 173, in start_training\r\n    self.spawn(self.new_process, trainer, self.mp_queue, return_result=False)\r\n  File \"/home/name/.local/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 201, in spawn\r\n    mp.spawn(self._wrapped_function, args=(function, args, kwargs, return_queue), nprocs=self.num_processes)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 179, in start_processes\r\n    process.start()\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n  File \"/mnt/beta/name/miniconda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 547, in __getstate__\r\n    raise NotImplementedError(\"{} cannot be pickled\", self.__class__.__name__)\r\nNotImplementedError: ('{} cannot be pickled', '_SingleProcessDataLoaderIter')\r\n```How are launching your training? Can you try to set `strategy='ddp'` instead of the default 'ddp_spawn' for multiple GPUs. For me that works and in opposite to ddp_spawn it does not pickle your dataloader.",
    "meta": { "name": "Dataloader pickle torchio" },
    "answer": "How are launching your training? Can you try to set `strategy='ddp'` instead of the default 'ddp_spawn' for multiple GPUs. For me that works and in opposite to ddp_spawn it does not pickle your dataloader."
  },
  {
    "content": "Hello! \r\n\r\nI hope to **calculate the  gradient of the loss on the model input** every batch when training. The calculated gradients are then processed by some other functions and added into the loss function. The way I do this  in pytorch is like this, with the `torch.autograd.grad` function:\r\n\r\n```python\r\nmodel_input.requires_grad = True\r\nmodel.zero_grad()\r\nmodel.eval()\r\ngrads = torch.autograd.grad(loss, model_input, grad_outputs=None, only_inputs=True, retain_graph=False)[0]\r\nmodel.train()\r\n```\r\n\r\nCan I add these codes directly into the `training_step` function in pytorch-lightning? \r\n\r\nMy concern is: \r\n1. Will this gradient calculation (`torch.autograd.grad` function)  influence the accuracy of model training since I add it in the `training_step` function? \r\n2. Do I need to set `model.zero_grad()`,   `model.eval()`  and `model.train()`  when calculating the gradients on the input?\r\n\r\nThank you very much!\r\n\r\n> 1. Will this gradient calculation (`torch.autograd.grad` function)  influence the accuracy of model training since I add it in the `training_step` function?\r\n\r\nYes, I believe. I don't think it'll work with `retain_graph=False` there because backward pass uses the graph to compute gradients wrt weights using the `loss` returned from `training_step`.\r\n\r\n> 2. Do I need to set `model.zero_grad()`,   `model.eval()`  and `model.train()`  when calculating the gradients on the input?\r\n\r\nYes, partially. I think you need `zero_grad()` after `grads = torch.autograd.grad(...)` to avoid accumulating gradients wrt weights, but I'm not sure why you need `eval()` and `train()`.",
    "meta": {
      "name": "Can I use \"torch.autograd.grad\" in \"training_step\" funtion?"
    },
    "answer": "> 1. Will this gradient calculation (`torch.autograd.grad` function)  influence the accuracy of model training since I add it in the `training_step` function?\r\n\r\nYes, I believe. I don't think it'll work with `retain_graph=False` there because backward pass uses the graph to compute gradients wrt weights using the `loss` returned from `training_step`.\r\n\r\n> 2. Do I need to set `model.zero_grad()`,   `model.eval()`  and `model.train()`  when calculating the gradients on the input?\r\n\r\nYes, partially. I think you need `zero_grad()` after `grads = torch.autograd.grad(...)` to avoid accumulating gradients wrt weights, but I'm not sure why you need `eval()` and `train()`."
  },
  {
    "content": "I am fine-tuning hugging face transformer models, essentially exactly as shown in the following example found in the pytorch lightning docs:\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html\r\n\r\nWhere we instantiate the LightningModule doing something like this:\r\n\r\n```python\r\nclass GLUETransformer(LightningModule):\r\n\r\n    def __init__(self, ... ):\r\n        super().__init__()\r\n        self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\r\n        self.model = AutoModelForSequenceClassification.from_pretrained(\r\n            model_name_or_path, config=self.config\r\n        )\r\n```\r\n\r\nBut I have been confused about how I should be saving and loading checkpoints. \r\n\r\nWhen saving checkpoints, should I be using \r\n`mymodel.model.save_pretrained(\"model_save_dir\")`, \r\nand reloading from this checkpoint using \r\n`AutoModelForSequenceClassification.from_pretrained(\"model_save_dir\")`,\r\n\r\nor saving with \r\n`trainer.save_checkpoint(\"model_save_dir/checkpoint.ckpt\")`,\r\nand reloading with \r\n`GLUETransformer.load_from_checkpoint(\"model_save_dir/checkpoint.ckpt\")`?Dear @brijow,\r\n\r\nYou should be using the second approach. An even better one would be to rely on `ModelCheckpoint` to save the checkpoints and provide `Trainer(resume_from_checkpoint=...)` for reloading all the states.\r\n\r\nBest,\r\nT.C@brijow Is there a way to unpack HF PL checkpoints into constituents (e.g. pytorch_model.bin, config.json, tokenizer.json etc.) usually found on the HF hub hosted models. Most importantly, how would I extract just the pytorch_model.bin from the PL checkpoint?",
    "meta": {
      "name": "Saving and loading HF transformer model fine tuned with PL?"
    },
    "answer": "Dear @brijow,\r\n\r\nYou should be using the second approach. An even better one would be to rely on `ModelCheckpoint` to save the checkpoints and provide `Trainer(resume_from_checkpoint=...)` for reloading all the states.\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "Hi, I'm new to PyTorch Lightning, used it for the first time and kind of liked it. However, I am facing this one problem, Implemented a classification task for which I trained the model with Huggingface pretrained model as base and classification head on top. The model is training successfully and giving decent validation losses. The problem is, I'm not quite able to figure out the inferencing part. \r\n\r\ncan anyone please point out what is it that I'm doing wrong? It's probably something very basic. \r\n\r\nI'll add the classes of the lightning modules and the Data Modules below.\r\n```\r\n# |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n# | Define the Pytorch Lightning Module Classifier Class     |\r\n# |__________________________________________________________|\r\n\r\nclass ABSASentimentClassifier(pl.LightningModule):\r\n\r\n  def __init__(self, learning_rate = setup['lr'], weights=None, **kwargs):\r\n    super().__init__()\r\n\r\n    self.save_hyperparameters('learning_rate', 'max_epochs')\r\n    self.model = ABSAModel_Bert()\r\n    self.weights = weights\r\n    self.preds = []\r\n  \r\n  def training_step(self, batch, batch_nb):\r\n\r\n    # Forward\r\n    y_hat = self.model(batch)\r\n\r\n    # if self.weights:\r\n    #   self.weights = torch.tensor(class_weights,dtype=torch.float) \r\n    \r\n    # Loss\r\n    loss_fct = torch.nn.CrossEntropyLoss()\r\n    \r\n    loss = loss_fct(y_hat.view(-1, self.model.num_labels), batch['label'].view(-1))\r\n\r\n    # Logs\r\n    self.log_dict({'training_loss':loss}, prog_bar=True)\r\n\r\n    return loss\r\n\r\n  \r\n  def validation_step(self, batch, batch_nb):\r\n    \r\n    # Forward\r\n    y_hat = self.model(batch)\r\n        \r\n    # Loss\r\n    loss_fct = torch.nn.CrossEntropyLoss()\r\n    loss = loss_fct(y_hat.view(-1, self.model.num_labels), batch['label'].view(-1))\r\n\r\n    # Acc\r\n    a, y_hat = torch.max(y_hat, dim=1)\r\n    val_acc = accuracy_score(y_hat.cpu(), batch['label'].cpu())\r\n    val_acc = torch.tensor(val_acc)\r\n    \r\n    # Logs\r\n    self.log_dict({'val_loss':loss,'val_acc':val_acc}, prog_bar=True)\r\n    \r\n    return loss\r\n\r\n  \r\n  def test_step(self, batch, batch_nb):\r\n    self.model.eval()\r\n    \r\n    # Forward\r\n    yhat = self.model(batch)\r\n      \r\n    # Loss\r\n    # loss_fct = torch.nn.CrossEntropyLoss()\r\n    # loss = loss_fct(y_hat.view(-1, self.model.num_labels), batch['label'].view(-1))\r\n    \r\n    # a, y_hat = torch.max(y_hat, dim=1)\r\n    # test_acc = accuracy_score(y_hat.cpu(), batch['label'].cpu())\r\n    \r\n    # Logs\r\n    # self.log_dict({'test_loss':loss,'test_acc':test_acc}, prog_bar=True)\r\n    self.preds = self.preds.extend(yhat.cpu().detach().numpy().tolist())\r\n    return \r\n\r\n  \r\n  def predict_dataloader(self, batch, batch_idx: int , dataloader_idx: int = None):\r\n\r\n    return self.model(batch)\r\n\r\n\r\n  '''\r\n  |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n  | Training Setup  |\r\n  |_________________|\r\n  '''\r\n  def configure_optimizers(self):\r\n    '''\r\n    |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    |   REQUIRED                                                            |\r\n    |   can return multiple optimizers and learning_rate schedulers         |\r\n    |   (LBFGS it is automatically supported, no need for closure function) |\r\n    |_______________________________________________________________________|\r\n    '''\r\n    optimizer = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.hparams.learning_rate, eps=1e-08)\r\n    scheduler = {   \r\n      'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-5, \r\n                                                       steps_per_epoch=len(self.trainer.datamodule.train_dataloader()),\r\n                                                       epochs=self.hparams.max_epochs),\r\n                 \r\n      'interval': 'step'  # called after each training step\r\n    } \r\n    \r\n    return [optimizer], [scheduler]\r\n\r\n  @staticmethod\r\n  def add_model_specific_args(parent_parser, root_dir):  # pragma: no-cover\r\n    \"\"\"\r\n    |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    | Define parameters that only apply to this model     |\r\n    |_____________________________________________________|\r\n    \"\"\"\r\n    parser = ArgumentParser(parents=[parent_parser])\r\n\r\n    # data\r\n    parser.add_argument('--data_root', default=os.path.join(root_dir, 'train_val_data'), type=str)\r\n\r\n    # training params (opt)\r\n    parser.add_argument('--learning_rate', default=setup['lr'], type=float, help = \"type (default: %(default)f)\")\r\n    return parser\r\n```\r\n\r\nalso the dataset class is :\r\n```\r\nclass ABSADataset(Dataset):\r\n  def __init__(self, df, tokenizer, max_len=setup['max_sen_length']):\r\n    self.texts = df['text']\r\n    self.aspects = df['aspect']\r\n    if 'label' in df.columns:\r\n      # print('****Labels Present****')\r\n      self.targets = df['label']\r\n\r\n    else:\r\n      self.targets = None\r\n\r\n    self.tokenizer = tokenizer\r\n    self.max_len = max_len\r\n\r\n  def __len__(self):\r\n    return len(self.aspects)\r\n\r\n  def __getitem__(self, idx):\r\n\r\n    # convert indexes, tensor->list\r\n    if torch.is_tensor(idx):\r\n      idx = idx.tolist()\r\n    \r\n    # define the aspect and text item\r\n    text = (str(self.texts[idx]))\r\n    aspect = str(self.aspects[idx])\r\n\r\n    # define the label\r\n    target = self.targets[idx]\r\n\r\n    # pair the aspect and text for pair-encoding\r\n    pairs = [text, aspect]\r\n    \r\n    '''\r\n    # |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    # | For Debugging            |\r\n    # |__________________________|\r\n    # print(f' text: {text}')\r\n    # print(f' aspect: {aspect}')\r\n    # print(type(text))\r\n    # print(type(aspect))\r\n    '''\r\n    \r\n    # encode the feature pair\r\n    encoded = self.tokenizer.encode_plus(pairs,\r\n                                    add_special_tokens=True,\r\n                                    padding='max_length', \r\n                                    max_length=setup['max_sen_length'], \r\n                                    return_attention_mask=True,\r\n                                    return_tensors='pt',\r\n                                    truncation=True)\r\n    '''\r\n    # |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    # | For Debugging            |\r\n    # |__________________________|\r\n    # for ids in encoded['input_ids']:\r\n    #   print('*'*20)\r\n    #   print(f'{self.tokenizer.decode(ids)} of length = {len(self.tokenizer.decode(ids).split(\" \"))}')\r\n    #   print(f'is encoded as : \\n{ids} \\nwith length = {len(ids)}')\r\n    #   print('*'*20)\r\n    '''\r\n    \r\n    return {\r\n        'label' : target,\r\n        'input_ids' : encoded['input_ids'],\r\n        'attention_mask' : encoded['attention_mask'] \r\n    }\r\n```\r\n\r\nMy goal is to be able to generate predictions for data without any labels present, using the trained model (saved as checkpoint (.ckpt))\r\n\r\nThis is what I did:\r\n```\r\ntestset = ABSATest_Dataset(test, tokenizer=transformer_tokenizer)\r\n\r\ntestLoader = DataLoader(testset, batch_size=setup['test_batch_size'])\r\n\r\ntrainer.predict(model_infer, testLoader)\r\n```\r\nWhere model_infer is :\r\n```\r\nmodel_infer = ABSASentimentClassifier.load_from_checkpoint(PATH_TO_CKPT_FILE)\r\n```\r\n\r\nand got :\r\n```\r\n---------------------------------------------------------------------------\r\nMisconfigurationException                 Traceback (most recent call last)\r\n<ipython-input-44-c724efd019b7> in <module>()\r\n----> 1 trainer.predict(model_infer, testLoader)\r\n\r\n5 frames\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in predict(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\r\n    987         \"\"\"\r\n    988         return self._call_and_handle_interrupt(\r\n--> 989             self._predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\r\n    990         )\r\n    991 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _call_and_handle_interrupt(self, trainer_fn, *args, **kwargs)\r\n    680         \"\"\"\r\n    681         try:\r\n--> 682             return trainer_fn(*args, **kwargs)\r\n    683         # TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\r\n    684         except KeyboardInterrupt as exception:\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _predict_impl(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\r\n   1030         )\r\n   1031 \r\n-> 1032         results = self._run(model, ckpt_path=self.predicted_ckpt_path)\r\n   1033 \r\n   1034         assert self.state.stopped\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _run(self, model, ckpt_path)\r\n   1115             parsing.clean_namespace(model.hparams)\r\n   1116 \r\n-> 1117         verify_loop_configurations(self, model)\r\n   1118 \r\n   1119         # attach model log function to callback\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py in verify_loop_configurations(trainer, model)\r\n     38         __verify_eval_loop_configuration(trainer, model, \"test\")\r\n     39     elif trainer.state.fn == TrainerFn.PREDICTING:\r\n---> 40         __verify_eval_loop_configuration(trainer, model, \"predict\")\r\n     41 \r\n     42     __verify_dp_batch_transfer_support(trainer, model)\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py in __verify_eval_loop_configuration(trainer, model, stage)\r\n    187             raise MisconfigurationException(\"`predict_step` cannot be None to run `Trainer.predict`\")\r\n    188         elif not has_step and not is_overridden(\"forward\", model):\r\n--> 189             raise MisconfigurationException(\"`Trainer.predict` requires `forward` method to run.\")\r\n    190     else:\r\n    191         # -----------------------------------\r\n\r\nMisconfigurationException: `Trainer.predict` requires `forward` method to run.\r\n```\r\n\r\nALso, I haven't defined a forward function in the lightning module because it is present in the model class:\r\n```\r\nclass ABSAModel_Bert(torch.nn.Module):\r\n\r\n  def __init__(self, num_labels=setup['num_labels'], config = setup, **kwargs):\r\n    super(ABSAModel_Bert, self).__init__()\r\n    \r\n    self.num_labels = num_labels\r\n    self.bert = transformers.AutoModel.from_pretrained(config['model_name'])\r\n    self.bert_config = transformers.AutoConfig.from_pretrained(config['model_name'])\r\n\r\n    self.pre_classifier = torch.nn.Linear(self.bert_config.hidden_size, self.bert_config.hidden_size)\r\n\r\n    self.classifier = torch.nn.Linear(self.bert_config.hidden_size, self.num_labels)\r\n\r\n    self.dropout = torch.nn.Dropout(self.bert_config.hidden_dropout_prob)\r\n    # print(f'Using Dropout = {self.bert.config.seq_classif_dropout}')\r\n\r\n    self.relu = torch.nn.ReLU()\r\n\r\n    '''\r\n    |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    | freeze the layers of Bert for training if needed so that |   \r\n    | the embeddings of all layers of Bert are not changed     |\r\n    |__________________________________________________________|\r\n    '''\r\n    # for param in self.bert.parameters():\r\n    #   param.requires_grad = False\r\n\r\n  \r\n  def forward(self, batch):\r\n\r\n  #   print((batch['input_ids'].squeeze(1)).shape)\r\n  #   print(\"*\"*10)\r\n  #   print(batch['input_ids'])\r\n  #   print(\"*\"*10)\r\n    \r\n    outputs = self.bert(input_ids=batch['input_ids'].squeeze(1), \r\n                        attention_mask=batch['attention_mask'])\r\n    \r\n    # output from last hidden layer\r\n    hidden_state = outputs[0]  # (batch_size, seq_len, dim)\r\n\r\n    '''\r\n    |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    | *output of [CLS] token                                   |\r\n    |                                                          |\r\n    | [CLS] token contains the pooled embeddings of the entire | \r\n    | Sequence, these are used for the classification.         |\r\n    |__________________________________________________________|\r\n    '''\r\n    pooled_output = hidden_state[:, 0] # (batch_size, dim)\r\n\r\n    '''\r\n    |\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af|\r\n    | sending the [CLS] token embeddings through Linear, ReLU  |\r\n    | and Dropout layers                                       |\r\n    |__________________________________________________________|\r\n    '''\r\n    pooled_output = self.pre_classifier(pooled_output)  # (batch_size, dim)\r\n    pooled_output = self.relu(pooled_output)  # (batch_size, dim)\r\n    pooled_output = self.dropout(pooled_output)  # (batch_size, dim)\r\n    logits = self.classifier(pooled_output)  # (batch_size, num_labels)\r\n\r\n    return logits\r\n\r\n  def get_outputs(self, input_ids, attention_mask):\r\n    outputs = self.bert(input_ids=input_ids, \\\r\n                        attention_mask=attention_mask)\r\n    \r\n```since your `model` is an instance of your `LightningModule` it cannot rely on `model.forward` to generate the predictions because `predict_step` by default calls `LightningModule.predict`.\r\nyou need to either override predict_step\r\n```py\r\ndef predict_step(...):\r\n    return self.model(...)\r\n```\r\nor define forward method in your lightning module\r\n```py\r\ndef forward(...):\r\n    return self.model(...)\r\n```",
    "meta": {
      "name": "Not able to Generate Predictions with Trainer.predict()"
    },
    "answer": "since your `model` is an instance of your `LightningModule` it cannot rely on `model.forward` to generate the predictions because `predict_step` by default calls `LightningModule.predict`.\r\nyou need to either override predict_step\r\n```py\r\ndef predict_step(...):\r\n    return self.model(...)\r\n```\r\nor define forward method in your lightning module\r\n```py\r\ndef forward(...):\r\n    return self.model(...)\r\n```"
  },
  {
    "content": "Hi!\r\nI have been working with PyTorch lightning for a year or so, but I am still confused (probably because I am not a software developer and my development skills are not really sharp). \r\nDo any of you know of any guide or reference on how callbacks and hooks interact with each other? Some of my particular questions are:\r\n\r\n- While training, where (or in which order) each callback (train_step, train_epoch_start...) is called?\r\n- How exactly do the hooks work?\r\nI am aware that maybe this is a really basic question, but any advice on how to better understand the workflow would be great!\r\nThanks in advance, you rock guys!hi!\r\n\r\n- There is a gif here representing the call order for some of the hooks. We are planning to add more info regarding this inside the docs. https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html\r\n- You can consider hooks as just normal methods of objects that are called at certain points within the code. For eg when training starts it calls `on_train_start`, when training epoch starts it calls `on_train_epoch_start` etc... The hook name can also help you determine where it's called.\r\n\r\nFor a complete cycle, there can be just one lightning module, so hooks related to the lightning module will be called only once whenever they are required, but in the case of callbacks, it can be many so it sequentially makes the calls to the same hook at that time for each callback. You can read about callbacks and hooks here: https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html",
    "meta": {
      "name": "Any guide on how the callbacks and hooks workflow works?"
    },
    "answer": "hi!\r\n\r\n- There is a gif here representing the call order for some of the hooks. We are planning to add more info regarding this inside the docs. https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html\r\n- You can consider hooks as just normal methods of objects that are called at certain points within the code. For eg when training starts it calls `on_train_start`, when training epoch starts it calls `on_train_epoch_start` etc... The hook name can also help you determine where it's called.\r\n\r\nFor a complete cycle, there can be just one lightning module, so hooks related to the lightning module will be called only once whenever they are required, but in the case of callbacks, it can be many so it sequentially makes the calls to the same hook at that time for each callback. You can read about callbacks and hooks here: https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html"
  },
  {
    "content": "Hi!\r\nI am currently working on a project where I would like to checkpoint my model in separated pieces. \r\nMy model has a backbone composed by:\r\n\r\n- a backbone, which is also composed by 3 modules\r\n- several heads, each one being a module\r\nI would like to save one ckpt with the backbone and one ckpt per head. I understand that I should create a custom callback inheriting from ModelCheckpoint and then modifying on_save_checkpoint, I am not really aware of how to do it.\r\non_save_checkpoint is defined as: \r\n\r\n![image](https://user-images.githubusercontent.com/95293295/144058369-9930d133-734a-46cc-9413-9c663a4d6ac9.png)\r\n \r\nAnother solution would be to modify my lightning module to load the ckpt when the training ends as a dict and then save each subpart as a ckpt using torch.save(), but I understand that this solution is much less elegant.\r\n\r\nAny suggestions? Thanks in advance!\r\nI'd suggest using [checkpoint_io](https://pytorch-lightning.readthedocs.io/en/latest/advanced/checkpoint_io.html) plugin for your use-case.",
    "meta": { "name": "checkpoint every module in a different ckpt file" },
    "answer": "I'd suggest using [checkpoint_io](https://pytorch-lightning.readthedocs.io/en/latest/advanced/checkpoint_io.html) plugin for your use-case."
  },
  {
    "content": "As shown in the screenshot,RichProgressBar is hard to read in light theme.  I don't have much time to report a bug, so I open a discussion temporary.\r\n<img width=\"1917\" alt=\"Screen Shot 2021-11-30 at 10 47 40\" src=\"https://user-images.githubusercontent.com/13161779/143977084-5182f702-6f41-4d39-85b3-1b202f5db309.png\">\r\ncc @kaushikb11 @SeanNaren Thanks @tshu-w fr reporting the issue! I will look into if it's possible to detect theme in interactive environments.\r\n\r\nTill then you could customize the `description` color by customizing `RichProgressTheme`\r\n\r\n```python\r\ntrainer = Trainer(..., callbacks=RichProgressBar(theme=RichProgressBarTheme(description=\"black\")))\r\n```\r\n\r\nDo check [RichProgressTheme](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/callbacks/progress/rich_progress.py#L166) out to customize styles for various components.",
    "meta": { "name": "RichProgressBar is hard to read in light theme" },
    "answer": "Thanks @tshu-w fr reporting the issue! I will look into if it's possible to detect theme in interactive environments.\r\n\r\nTill then you could customize the `description` color by customizing `RichProgressTheme`\r\n\r\n```python\r\ntrainer = Trainer(..., callbacks=RichProgressBar(theme=RichProgressBarTheme(description=\"black\")))\r\n```\r\n\r\nDo check [RichProgressTheme](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/callbacks/progress/rich_progress.py#L166) out to customize styles for various components."
  },
  {
    "content": "I'll explain: Let's say that I have two nn.modules inside my main LightningModule, but one of them is frozen, i.e. doesn't learn during the training but is only used for inferencing during training (requires_grad is False in this module) and I would like to avoid saving the state_dictionray of this static (frozen) module to the checkpoint file.\r\n\r\nIn plain PyTorch I'd probably filter manually the state_dictionray fields of the frozen module before the saving.  \r\nIs there a simple way to do that with pytorch-lightning? Or to raise some flag inside the modules which say to the LightningModule not to save all the parameters inside this frozen module?\r\n#\r\nA simple toy example for clarification.\r\nIn this example, I'd like to avoid saving the parameters of self.frozen_nn_module. \r\nAll parameters in self.frozen_nn_module don't require grads. \r\n\r\n```\r\nclass LightMod(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        #some non frozen module  \r\n        self.non_frozen_nn_module = non_frozen_nn_module\r\n        #some frozen(static) nn.Module\r\n        self.frozen_nn_module= frozen_nn_module\r\n\r\n    def forward(self, x):\r\n    Some code....\r\n```\r\n\r\n\r\n\r\nyou have to do that here too.\r\nwithin lightning you can override `on_save_checkpoint` hook of LightningModule.\r\n```py\r\ndef on_save_checkpoint(self, checkpoint):\r\n    checkpoint['state_dict'] <- remove/pop keys from here\r\n``` ",
    "meta": {
      "name": "Is there a way to save only part of the Lightning sub-modules to the checkpoint file?"
    },
    "answer": "you have to do that here too.\r\nwithin lightning you can override `on_save_checkpoint` hook of LightningModule.\r\n```py\r\ndef on_save_checkpoint(self, checkpoint):\r\n    checkpoint['state_dict'] <- remove/pop keys from here\r\n``` "
  },
  {
    "content": "I wander If I misunderstanding doc. Trainer test with ckpt_path doesn't load ckpt. model.load_from_checkpoint works.\r\n```\r\nIn [20]: model = MMTSMatcher(model_name=\"bert-base-uncased\")\r\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\r\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n\r\nIn [21]: trainer.test(model, data, ckpt_path=\"tests/mmtsmatcher_wdcdatamodule/shoes_small_false_32_0808-182020/checkpoints/epoch=05-valid_f1=81.22%.ckpt\")\r\nThe following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\r\n/data/wts/.local/share/anaconda3/envs/multimodalER/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\r\n  rank_zero_deprecation(\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\r\nTesting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69/69 [01:27<00:00,  1.04s/it]--------------------------------------------------------------------------------\r\nDATALOADER:0 TEST RESULTS\r\n{'test_f1': 0.0,\r\n 'test_loss': 0.6186538934707642,\r\n 'test_prc': 0.0,\r\n 'test_rec': 0.0}\r\n--------------------------------------------------------------------------------\r\nTesting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69/69 [01:27<00:00,  1.27s/it]\r\n/data/wts/.local/share/anaconda3/envs/multimodalER/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\r\n  rank_zero_deprecation(\r\nOut[21]:\r\n[{'test_f1': 0.0,\r\n  'test_prc': 0.0,\r\n  'test_rec': 0.0,\r\n  'test_loss': 0.6186538934707642}]\r\n\r\nIn [22]: model = MMTSMatcher.load_from_checkpoint(checkpoint_path=\"tests/mmtsmatcher_wdcdatamodule/shoes_small_false_32_0808-182020/checkpoints/epoch=05-valid_f1=81.22%.ckpt\", hparams_file=\"tests/mmtsmatcher_wdcd\r\n    ...: atamodule/shoes_small_false_32_0808-182020/hparams.yaml\")\r\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\r\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n\r\nIn [23]: trainer.test(model, data)\r\nThe following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: EarlyStopping, ModelCheckpoint\r\n/data/wts/.local/share/anaconda3/envs/multimodalER/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\r\n  rank_zero_deprecation(\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\r\nTesting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69/69 [01:27<00:00,  1.21it/s]--------------------------------------------------------------------------------\r\nDATALOADER:0 TEST RESULTS\r\n{'test_f1': 0.670258641242981,\r\n 'test_loss': 0.284279465675354,\r\n 'test_prc': 0.5298126339912415,\r\n 'test_rec': 0.9120234847068787}\r\n--------------------------------------------------------------------------------\r\nTesting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69/69 [01:27<00:00,  1.27s/it]\r\n/data/wts/.local/share/anaconda3/envs/multimodalER/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\r\n  rank_zero_deprecation(\r\nOut[23]:\r\n[{'test_f1': 0.670258641242981,\r\n  'test_prc': 0.5298126339912415,\r\n  'test_rec': 0.9120234847068787,\r\n  'test_loss': 0.284279465675354}]\r\n```Dear @tshu-w,\r\n\r\nMind opening an issue with a reproducible script using the BoringModel.\r\n\r\nBest,\r\nT.C ",
    "meta": { "name": "Trainer test with ckpt_path not work as expected" },
    "answer": "Dear @tshu-w,\r\n\r\nMind opening an issue with a reproducible script using the BoringModel.\r\n\r\nBest,\r\nT.C "
  },
  {
    "content": "On https://pytorch-lightning.readthedocs.io/en/latest/starter/converting.html, it says that \".test() loads the best checkpoint automatically\". Is that also the case for .predict()? yes, by default it does load the best checkpoint if you don't provide the model, you can set it too if you want!\r\n```py\r\ntrainer.predict(ckpt_path='best')\r\n```",
    "meta": { "name": "Does .predict() also use the best weights?" },
    "answer": "yes, by default it does load the best checkpoint if you don't provide the model, you can set it too if you want!\r\n```py\r\ntrainer.predict(ckpt_path='best')\r\n```"
  },
  {
    "content": "It is strange that my `val_dataloader`'s `shuffle` is set to `False`, but I still get this warning. Any ideas on how to solve this?I know what the bug is, the sampler used for val_dataloader is not `SequentialSampler` but it does not do any shuffling.",
    "meta": {
      "name": "UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders."
    },
    "answer": "I know what the bug is, the sampler used for val_dataloader is not `SequentialSampler` but it does not do any shuffling."
  },
  {
    "content": "I want to test performance of my model on a few different feature sets and visualize losses of different features on the same plot in tensorboard. \r\nMy current work flow is, \r\n\r\n```\r\nclass LightningWrapper(pl.LightningModule):\r\n    def__init__(self, features):\r\n        self.features = features \r\n\r\n    def training_step(self, batch, batch_idx):\r\n        ....\r\n        .....\r\n        self.logger.experiment.add_scalars(\"version_0\", { f\"{self.features}\" : loss})\r\n\r\ntrainer = pl.Trainer()\r\nfor feature_set in potential_feature_sets:\r\n    model = LightningWrapper(feature_set)\r\n    trainer.fit(model)\r\n```\r\n\r\nBut this is creating different versions inside tensorboard logging. Does each call to Lightning module create a new version ? Is this because the loggers are not shared across feature sets in my current design ? How do I share logger across different feature sets or different models ?\r\n\r\n\r\n\r\nFound a rather simple solution that worked for me. Put different features/different models under different directories and fire up tensorboard in root directory. \r\n\r\n```\r\nclass LightningWrapper(pl.LightningModule):\r\n    def__init__(self, features):\r\n        self.features = features \r\n\r\n    def training_step(self, batch, batch_idx):\r\n        ....\r\n        .....\r\n        self.logger.experiment.add_scalar(\"loss\", loss)\r\n\r\n\r\nfor feature_set in potential_feature_sets:\r\n    logger = pl.loggers.TensorBoardLogger(f\"lightning_logs/{num_features}\", version=\"0\")\r\n    trainer = pl.Trainer(logger=logger)\r\n    model = LightningWrapper(feature_set)\r\n    trainer.fit(model)\r\n```\r\n",
    "meta": { "name": "How to do model comparison with pytorch lightning" },
    "answer": "Found a rather simple solution that worked for me. Put different features/different models under different directories and fire up tensorboard in root directory. \r\n\r\n```\r\nclass LightningWrapper(pl.LightningModule):\r\n    def__init__(self, features):\r\n        self.features = features \r\n\r\n    def training_step(self, batch, batch_idx):\r\n        ....\r\n        .....\r\n        self.logger.experiment.add_scalar(\"loss\", loss)\r\n\r\n\r\nfor feature_set in potential_feature_sets:\r\n    logger = pl.loggers.TensorBoardLogger(f\"lightning_logs/{num_features}\", version=\"0\")\r\n    trainer = pl.Trainer(logger=logger)\r\n    model = LightningWrapper(feature_set)\r\n    trainer.fit(model)\r\n```\r\n"
  },
  {
    "content": "In case of pytorch-lightning 1.5.2, omission of argument of `trainer.test()` does not work.\r\n\r\n```python\r\n        trainer.fit(model, datamodule)\r\n        trainer.test()\r\n```\r\n\r\nIf it is below, it worked fine. Is this the expected behavior?\r\n\r\n```python\r\n        trainer.fit(model, datamodule)\r\n        trainer.test(model, datamodule.test_dataloader())\r\n```\r\n\r\nI'm currently refactoring the following:\r\n\r\nKeiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning https://github.com/Keiku/PyTorch-Lightning-CIFAR10yes, you need to pass the datamodule. I think this was the case after v1.5 and not just v1.5.2.",
    "meta": {
      "name": "After changing to pytorch-lightning 1.5.2, omitting the argument of trainer.test() does not work."
    },
    "answer": "yes, you need to pass the datamodule. I think this was the case after v1.5 and not just v1.5.2."
  },
  {
    "content": "I am trying to port my very old pytorch code to lightning and in my training loop, I have something as follows:\r\n\r\n```\r\nbatch_order = np.arange(data.x_train.shape[0])\r\nbatch_index = np.random.choice(batch_order, batch_size, p=seq_sample_probs).tolist()\r\nbatch = torch.tensor(data.x_train[batch_index], dtype=torch_dtype, device=torch_device, requires_grad=False)\r\n\r\n# then call the forward on this batch\r\nmodel.encoder.forward(batch)\r\n```\r\n\r\nI was wondering how I can incorporate this batch index selection in the lightning code. In my code, I have the usual:\r\n\r\n```\r\ndef train_dataloader(self):\r\n        return DataLoader(self.train_dataset, batch_size=self.batch_size)\r\n```\r\n\r\nBut I do not know where I can inject my sampling code inside all this.I think that's the job of the PyTorch sampler.",
    "meta": { "name": "Doing the sampling of batch indexes inside lightning" },
    "answer": "I think that's the job of the PyTorch sampler."
  },
  {
    "content": "Is the grad scaler included in the `model.state_dict()` after steup by `model, optimizer = self.setup(model, optimizer)`?\r\n\r\nIf not, how can I save and load the state of the grad scaler for resume?Hey @hiyyg.\r\n\r\nYou can access the scale through the precision plugin as follows:\r\n\r\n```py\r\nself._precision_plugin.scaler\r\n```\r\n\r\nI believe you could get the state and reload it manually.\r\n\r\nBest,\r\nT.C\r\n",
    "meta": {
      "name": "How does LightningLite handle the grad scaler state dict of torch.amp?"
    },
    "answer": "Hey @hiyyg.\r\n\r\nYou can access the scale through the precision plugin as follows:\r\n\r\n```py\r\nself._precision_plugin.scaler\r\n```\r\n\r\nI believe you could get the state and reload it manually.\r\n\r\nBest,\r\nT.C\r\n"
  },
  {
    "content": "I wrote following code:\r\n```python\r\n    def configure_optimizers(self):\r\n        ......\r\n        return [\r\n            {\r\n                'optimizer': optimizer,\r\n                'lr_scheduler': {\r\n                    'scheduler': scheduler,\r\n                    'interval': 'step',\r\n                    'frequency': 1\r\n                }\r\n            }\r\n```\r\nI choose `step` as the `interval`. Actually, **I don't understand what `step` means**!!!\r\n\r\nIn my opinion, `step` may mean a batch? But when I set Trainer parameter: ` accumulate_grad_batches=5`, will `lr_scheduler` still execute after one batch or it only execute after every ` accumulate_grad_batches` batches? If the answer is the later, so the `step` means the call of `optimizer.step()`?\r\n\r\n(I know `accumulate_grad_batches` can affect `optimizer`, but I don't know whether it can affect `lr_scheduler`)yes, `step` means optimization step and `accumulate_grad_batches` will be taken under consideration while calling the lr_scheduler.\r\nRef code:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/8ea39d2c8f68cc33273c3431a310a262e2240cf9/pytorch_lightning/loops/epoch/training_epoch_loop.py#L434-L437",
    "meta": {
      "name": "What is the relationship beween accumulate_grad_batches and lr_scheduler?"
    },
    "answer": "yes, `step` means optimization step and `accumulate_grad_batches` will be taken under consideration while calling the lr_scheduler.\r\nRef code:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/8ea39d2c8f68cc33273c3431a310a262e2240cf9/pytorch_lightning/loops/epoch/training_epoch_loop.py#L434-L437"
  },
  {
    "content": "I am currently try this colab notebook https://colab.research.google.com/github/PytorchLightning/lightning-tutorials/blob/publication/.notebooks/lightning_examples/mnist-tpu-training.ipynb#scrollTo=2772a2e1 provided by PL teams to get some experience with TPU training. But when I try to execute the third cell, there is some Import error with the _XLAC module. \r\n\r\n<img width=\"937\" alt=\"Screen Shot 2021-10-17 at 01 44 00\" src=\"https://user-images.githubusercontent.com/37470762/137598792-4478d28d-485a-45da-9186-947387a5394a.png\">\r\nhi,\r\nthe problem seems to be with torch_xla,\r\n\r\ntry\r\n\r\n`!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl`\r\n\r\n`!pip install pytorch-lightning`\r\n\r\nif you have cuda 10.2\r\n`!pip3 install torch torchvision torchaudio`\r\n\r\nif you have cuda 11.1\r\n`!pip3 install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html`\r\n\r\nand you can add an issue to `https://github.com/pytorch/xla`\r\n\r\nwhich is the library which seems to have the problem can you try these.\r\n\r\nWith best regards,\r\nRanuga@tungts1101 Hi, please have a look at #8315.Hi @tungts1101! this error is raised when the PyTorch and PyTorch xla are not of the same versions.\r\nYou could verify using `pip list | grep torch` and could install the latest versions for both!",
    "meta": { "name": "Example on training with TPU does not run at all" },
    "answer": "Hi @tungts1101! this error is raised when the PyTorch and PyTorch xla are not of the same versions.\r\nYou could verify using `pip list | grep torch` and could install the latest versions for both!"
  },
  {
    "content": "When the train.fit() starts to train my model, it starts validating at 95% of training of current epoch instead of waiting until 100%.\r\n\r\nWhy does it happen? It makes Val-loss/acc/miou not accurate anymore\u2026\r\n![79A18250-AF4C-4F48-BB41-EB930C6ACA7F](https://user-images.githubusercontent.com/75472853/142167568-d02d1ae6-5886-4d9e-bd06-7bc37e3a3e28.jpeg)\r\nthe master progress bar that shows you the progress of the corresponding epoch consists of both train and val steps. So it completes the training within the first 95% of the total steps and the last 5% is completed for validation.",
    "meta": {
      "name": "Trainer.fit validating before finishing current training epoch"
    },
    "answer": "the master progress bar that shows you the progress of the corresponding epoch consists of both train and val steps. So it completes the training within the first 95% of the total steps and the last 5% is completed for validation."
  },
  {
    "content": "I want to better understand the setup and prepare_data methods in multi gpu scenariu in context of NLP and text processing.\r\n\r\nI have prepared the DataModule which process json line file with pairs of sentence for translation task. The file contains 10M lines.\r\nIn `prepare_data()` I open the data file, read it to memory, do some basic filtering (remove to long sentences and do some sorting based on length in order to group similar length sentences together) then I write it to another file (filtered_data.json).\r\nNext in the `setup()` method I read filtered_data.json and split it to train and valid. \r\nI can perform split deterministically so train and valid splits will always have the same elements or I can split randomly then each GPU process will have a different train and valid sets.\r\n \r\nWhen using it in multi-gpu (2 GPUs) each process will have its own copy of the train and valid set (am I right?). Which approach is better in context data utilization, random or deterministically?\r\nI do not fully understand how distributed DataLoader handles these two approaches? Could someone explain it in detail?\r\n\r\nIf data are loaded deterministically then all GPU processes, especially forward and backward pass will return the same values (for gpu 1 and 2), it is efficient? How gradients are merged and how network weight updates will be performed.\r\nOr maybe the second (random split) approach is better because gradients computed on different samples and merged from 2 gpus will result in a better estimation of the true gradient.\r\n> I have prepared the DataModule which process json line file with pairs of sentence for translation task. The file contains 10M lines.\r\n> In prepare_data I open the data file, read it to memory, do some basic filtering (remove to long sentences and do some sorting based on length in order to group similar length sentences together) then I write it to another file (filtered_data.json).\r\n\r\nDo all of that either offline in a different script, or do it in the `prepare_data` hook. \r\n\r\n> Next in the setup method I read filtered_data.json and split it to train and valid.\r\n\r\nSounds good. Each GPU/node will run the same, so you will have the same train and val split in all of them (initially). Don't split the data differently for each GPU, that part will be done by the DistributedSampler [1].\r\n\r\n> I do not fully understand how distributed DataLoader handles these two approaches? Could someone explain it in detail?\r\n\r\nLightning takes your DataLoader and adds a DistributedSampler. The DS knows on which GPU it is and will sample only a portion of your data on one GPU and another portion on the other GPU. Each GPU sees a different split of train and a different split of val.\r\n\r\n> If data are loaded deterministically then all GPU processes, especially forward and backward pass will return the same values (for gpu 1 and 2), it is efficient? How gradients are merged and how network weight updates will be performed.\r\n\r\nAs explained above, the dataloader on each GPU will return different samples on each GPU automatically. Each GPU will have the same network weights, uses different data to compute gradients, then gradients are averaged so each GPU gets the same update and starts with the same weights for the next forward/backward [2].\r\n\r\n> Or maybe the second (random split) approach is better because gradients computed on different samples and merged from 2 gpus will result in a better estimation of the true gradient.\r\n\r\nYes, again this is automatically done for you.\r\n\r\nReferences:\r\n[1] [DistributeSampler](https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler)\r\n[2] [Distributed Training in PyTorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\r\n[3] [Multi-GPU training in PyTorch](https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html)\r\n",
    "meta": {
      "name": "Better undestanding how data is loded in datamodule setup method for multi GPU setting in NLP"
    },
    "answer": "> I have prepared the DataModule which process json line file with pairs of sentence for translation task. The file contains 10M lines.\r\n> In prepare_data I open the data file, read it to memory, do some basic filtering (remove to long sentences and do some sorting based on length in order to group similar length sentences together) then I write it to another file (filtered_data.json).\r\n\r\nDo all of that either offline in a different script, or do it in the `prepare_data` hook. \r\n\r\n> Next in the setup method I read filtered_data.json and split it to train and valid.\r\n\r\nSounds good. Each GPU/node will run the same, so you will have the same train and val split in all of them (initially). Don't split the data differently for each GPU, that part will be done by the DistributedSampler [1].\r\n\r\n> I do not fully understand how distributed DataLoader handles these two approaches? Could someone explain it in detail?\r\n\r\nLightning takes your DataLoader and adds a DistributedSampler. The DS knows on which GPU it is and will sample only a portion of your data on one GPU and another portion on the other GPU. Each GPU sees a different split of train and a different split of val.\r\n\r\n> If data are loaded deterministically then all GPU processes, especially forward and backward pass will return the same values (for gpu 1 and 2), it is efficient? How gradients are merged and how network weight updates will be performed.\r\n\r\nAs explained above, the dataloader on each GPU will return different samples on each GPU automatically. Each GPU will have the same network weights, uses different data to compute gradients, then gradients are averaged so each GPU gets the same update and starts with the same weights for the next forward/backward [2].\r\n\r\n> Or maybe the second (random split) approach is better because gradients computed on different samples and merged from 2 gpus will result in a better estimation of the true gradient.\r\n\r\nYes, again this is automatically done for you.\r\n\r\nReferences:\r\n[1] [DistributeSampler](https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler)\r\n[2] [Distributed Training in PyTorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\r\n[3] [Multi-GPU training in PyTorch](https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html)\r\n"
  },
  {
    "content": "Why do these two test codes result in different test results(both average acc and average loss)?\r\n\r\n1. \r\n```python\r\ndef test_step(self, batch, batch_idx):\r\n    input_ids, labels = batch\r\n    outs = self(input_ids)\r\n    loss = self.loss_fn(outs, labels)\r\n    acc = self.acc_fn(outs, labels)\r\n    self.log_dict({'test_loss': loss, 'test_acc': acc}, on_step=False, on_epoch=True, logger=False)\r\n    return loss, acc\r\n```\r\n\r\n2. \r\n```python\r\ndef test_step(self, batch, batch_idx):\r\n    input_ids, labels = batch\r\n    outs = self(input_ids)\r\n    loss = self.loss_fn(outs, labels)\r\n    acc = self.acc_fn(outs, labels)\r\n    return loss, acc\r\n\r\ndef test_epoch_end(self, step_outputs):\r\n    avg_loss = torch.stack([x[0] for x in step_outputs]).mean()\r\n    avg_acc = torch.stack([x[1] for x in step_outputs]).mean()\r\n    self.log_dict({'test_loss': avg_loss, 'test_acc': avg_acc}, logger=False)\r\n```\r\n\r\nI only use one GPU for testing.It seems that it's because the last batch may have a different number of samples.just in case anyone else sees this discussion, adding more context to @Bowen-n answer. Within lightning we use a weighted average to accumulate the results at the epoch end where weights are the batch_size for each batch inside `test_step`.",
    "meta": {
      "name": "Test results different between logging in test_step and logging in test_epoch_end"
    },
    "answer": "just in case anyone else sees this discussion, adding more context to @Bowen-n answer. Within lightning we use a weighted average to accumulate the results at the epoch end where weights are the batch_size for each batch inside `test_step`."
  },
  {
    "content": "Hi all, do you know how to save the best model? Since pytorchlighting 's earlystop callback will monitor val_loss and if val_loss stop decreasing,\u00a0it will stop training automaticlly. In this case, the checkpoint of the final model would be the final epoch (the val_loss starts to increase). Can I save epoch 5 or 6 (before val_loss increasing) as the best model?\r\n![CC51157F-0EBE-4D35-B343-5C85F17A4EC2](https://user-images.githubusercontent.com/75472853/140654686-02726372-16fd-40b4-9d7d-ee323ae500ba.jpeg)\r\nyou can specify\r\n```py\r\nModelCheckpoint(monitor='val_loss', save_top_k=1, ...)\r\n```\r\nthis will save the best model.",
    "meta": { "name": "Early stop saving best checkpoints" },
    "answer": "you can specify\r\n```py\r\nModelCheckpoint(monitor='val_loss', save_top_k=1, ...)\r\n```\r\nthis will save the best model."
  },
  {
    "content": "I am training 5-fold CV with PyTorch Lightning in a for loop. I am also logging all the results to wandb. I want wanbd to reinitalize the run after each fold, but it seems to continue with the same run and it logs all the results to the same run. I also tried passing kwargs in the WandbLogger as mentioned in the docs [here](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html#pytorch_lightning.loggers.WandbLogger.params.**kwargs), with no luck.\r\nHere's a pseudo code of it:\r\n\r\n```python\r\ndef run(fold):\r\n    kwargs = {\r\n        \"reinit\": True,\r\n        \"group\": f\"{CFG['exp_name']}\"\r\n    }\r\n    wandb_logger = WandbLogger(project='<name>', \r\n                        entity='<entity>', \r\n                        config = CFG,\r\n                        name=f\"fold_{fold}\",\r\n                        **kwargs\r\n            )\r\n    trainer = Trainer(\r\n        precision=16,\r\n        gpus=1,\r\n        fast_dev_run=False,\r\n        callbacks = [checkpoint_callback],\r\n        logger=wandb_logger,\r\n        progress_bar_refresh_rate=1,\r\n        max_epochs=2,\r\n        log_every_n_steps=1\r\n\r\n    )\r\n\r\n    trainer.fit(\r\n        lit_model,\r\n        data_module\r\n    )\r\n\r\nif __name__ == \"__main__\":\r\n    for fold in range(5):\r\n        run(fold)\r\n```@Gladiator07 you could try call `wandb.finish()` at the end of every run. This should close the wandb process. A new one will be started when you call the next run",
    "meta": { "name": "How to reinit wanbd in a for loop with PL Trainer" },
    "answer": "@Gladiator07 you could try call `wandb.finish()` at the end of every run. This should close the wandb process. A new one will be started when you call the next run"
  },
  {
    "content": "I converted some Pytorch code to Lightning. The dataset is loaded lazily by the train & eval dataloaders.\r\n\r\nHowever, when moving the code to Lightning, I noticed a huge slowdown. After digging around, I noticed that there was a ~10 seconds delay between each epoch. For comparison, on my vanilla Pytorch, an epoch takes ~4s.\r\n\r\nI first thought it was a data loading problem, but during the 10s delay, no data is loaded (at least that's what my `print` tell me).\r\n\r\nI think the issue is related to the number of workers, because setting `n_workers=0` solves the problem (but is slower in the end, since only one worker is not enough). I know starting workers is slow, however I have `persistent_workers=True` and this does not happen in normal Pytorch. My data loaders also have `pin_memory=True` (removing pin_memory does not solve the problem).\r\n\r\nSince this is company code, I cannot disclose the before/after, but I'll try to \"anonymize\" some code if necessary. Here is the lightning module:\r\n```py\r\nclass RawModule(pl.LightningModule):\r\n    def __init__(self):\r\n        super(RawModule, self).__init__()\r\n\r\n        self.encoder1 = nn.Sequential(...)\r\n        self.encoder2 = nn.Sequential(...)\r\n\r\n    def forward(self, data1, data2):\r\n        result1 = self.encoder1(data1)\r\n        result2 = self.encoder2(data2)\r\n\r\n        result1 = result1 .view(result1 .size(0), -1)\r\n        result2 = result2 .view(result2 .size(0), -1)\r\n\r\n        result1 = F.normalize(result1 , p=2, dim=1)\r\n        result2 = F.normalize(result2 , p=2, dim=1)\r\n\r\n\r\n        return result1, result2\r\n\r\n    \r\n    def calculate_loss(self, batch):\r\n        x, r, y = batch\r\n        a, v = self.forward(r, x)\r\n\r\n        d = nn.functional.cosine_similarity(a, v)\r\n        loss = logloss(d.unsqueeze(1), y)\r\n\r\n        return loss\r\n\r\n\r\nclass Module(RawModule):\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self.calculate_loss(batch)\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self.calculate_loss(batch)\r\n        self.log(\"validation_loss\", loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\r\n        return optimizer\r\n\r\n\r\nif __name__ == '__main__':\r\n    # stuff...\r\n\r\n    train_loader = data_utils.DataLoader(\r\n        train_dataset, batch_size=256, shuffle=True,\r\n        num_workers=5, persistent_workers=True,\r\n        pin_memory=True,\r\n    )\r\n\r\n    val_loader = data_utils.DataLoader(\r\n        test_dataset, batch_size=256,\r\n        num_workers=2, persistent_workers=True,\r\n        pin_memory=True,\r\n    )\r\n\r\n    # Model\r\n    load_from_pytorch = True\r\n\r\n    if checkpoint_path is None:\r\n        model = Module()\r\n\r\n        if load_from_pytorch:\r\n            if not checkpoint_path:\r\n                raise ValueError(\"Please provide a checkpoint path\")\r\n            model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\r\n    else:\r\n        model = Module.load_from_checkpoint(checkpoint_path)\r\n\r\n\r\n    trainer = pl.Trainer(\r\n        gpus=1,\r\n        max_epochs=5,\r\n        check_val_every_n_epoch=10,\r\n        log_every_n_steps=5,\r\n    )\r\n    trainer.fit(model, train_loader, val_loader)\r\n```\r\n\r\nHere is the result of `profiler=\"simple\"`: \r\n```\r\nAction                                  |  Mean duration (s)    |Num calls              |  Total time (s)       |  Percentage %         |\r\n----------------------------------------------------------------------------------------------------------------------------------------\r\nTotal                                   |  -                    |_                      |  48.813               |  100 %                |\r\n----------------------------------------------------------------------------------------------------------------------------------------\r\nrun_training_epoch                      |  27.922               |1                      |  27.922               |  57.202               |\r\nfetch_next_sanity_check_batch           |  4.4013               |3                      |  13.204               |  27.05                |\r\nget_sanity_check_batch                  |  4.4013               |3                      |  13.204               |  27.05                |\r\nfetch_next_train_batch                  |  1.2734               |10                     |  12.734               |  26.087               |\r\nget_train_batch                         |  1.2734               |10                     |  12.734               |  26.087               |\r\nrun_training_batch                      |  0.47733              |9                      |  4.296                |  8.8009               |\r\noptimizer_step_with_closure_0           |  0.40089              |9                      |  3.608                |  7.3915               |\r\nvalidation_step                         |  0.664                |2                      |  1.328                |  2.7206               |\r\nevaluation_step_and_end                 |  0.664                |2                      |  1.328                |  2.7206               |\r\ntraining_step_and_backward              |  0.12644              |9                      |  1.138                |  2.3313               |\r\nbackward                                |  0.096889             |9                      |  0.872                |  1.7864               |\r\ntraining_step                           |  0.029556             |9                      |  0.266                |  0.54494              |\r\nmodel_forward                           |  0.029556             |9                      |  0.266                |  0.54494              |\r\non_train_start                          |  0.016                |1                      |  0.016                |  0.032778             |\r\n```\r\n\r\nHere is the result of `profiler=\"advanced\"`: [https://pastebin.com/q3C5P826](https://pastebin.com/q3C5P826).\r\n\r\nFinally, here is a video demonstrating the problem. I'm printing each piece of data loading, to prove it's not the issue.\r\nhttps://user-images.githubusercontent.com/30944236/140587623-ae184fa3-370a-42be-8593-200026d11ba4.mp4\r\n\r\nRandom informations:\r\n* I'm on Windows 10\r\n* CPU: AMD Ryzen 5 5600X 6 Core\r\n* GPU: Nvidia RTX 3070\r\n* Pytorch version: 1.10.0\r\n* Pytorch Lightning version: 1.5.0\r\n\r\nAny idea on how to find the source of the problem?Fixed in the 1.5.1 release. See the #10389 issue, or the release itself.",
    "meta": {
      "name": "Lightning is very slow - Performance divided by ~4 compared to Pytorch. 10s wait between epochs."
    },
    "answer": "Fixed in the 1.5.1 release. See the #10389 issue, or the release itself."
  },
  {
    "content": "Hi! I'm working on a SLURM cluster with preemption, so I'm really excited to see the support of Fault-tolerant Training in 1.5.0. However, when I upgrade package and try `PL_FAULT_TOLERANT_TRAINING=1 python train.py xxx` in the cluster, it doesn't seem to work.\r\n\r\nI look into the code of `Trainer`, it seems that the code responsible for fault-tolerant is [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/45f6a3b1758f88af7fd776915539800cbc0137a9/pytorch_lightning/trainer/trainer.py#L668). I assume preemption is a `BaseException` so the code will go to [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/45f6a3b1758f88af7fd776915539800cbc0137a9/pytorch_lightning/trainer/trainer.py#L693) and finally [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/45f6a3b1758f88af7fd776915539800cbc0137a9/pytorch_lightning/trainer/trainer.py#L1550) so that we save a checkpoint?\r\n\r\nHowever, when set some print in the code, when I use ctrl+C to interrupt code, it indeed goes to [this](https://github.com/PyTorchLightning/pytorch-lightning/blob/45f6a3b1758f88af7fd776915539800cbc0137a9/pytorch_lightning/trainer/trainer.py#L681) `KeyBoardInterrupt`. But if I use `scontrol requeue` to simulate a preemption, the code didn't got to `BaseException`. And that's why it didn't save a checkpoint for Fault-tolerant Training.\r\n\r\nIs there anything wrong with my code? I assume interruptions like `scancel` `requeue` are considered in this case. Can anyone help me? Thank you in advance!\r\n\r\nEDIT: I've looked in the code a little bit more, it seems that when I do `scancel` or `scontrol requeue`, the code directly exit, without throwing an exception, and that's why it didn't go to the `except  _on_exception` section. Is this expected behavior? Or is there anyway to solve it?\r\n\r\nI think that's related to the signal that SLURM sent to my program, and I already see a `SignalConnector` dealing with SLURM in pytorch-lightning [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/45f6a3b1758f88af7fd776915539800cbc0137a9/pytorch_lightning/trainer/connectors/signal_connector.py#L40). I also see [this answer](https://stackoverflow.com/questions/65326039/how-to-handle-job-cancelation-in-slurm) about the signal of SLURM. Maybe I should set it in the sbatch script? Any suggestions?\r\nSolved. That's indeed because in my SLURM cluster, there is no time interval between signal sending and program killing, so PyTorch-Lightning just don't have time to do checkpointing",
    "meta": { "name": "Questions about Fault-tolerant Training" },
    "answer": "Solved. That's indeed because in my SLURM cluster, there is no time interval between signal sending and program killing, so PyTorch-Lightning just don't have time to do checkpointing"
  },
  {
    "content": "In my experiment, I have a loss function, which is not defined by an expression. But I have a formulation of the gradient (formally a subgradient) so I have to pass the gradient manually. In pytorch, I implement it in the following way and it is working fine.\r\n\r\n```python\r\nself.optimizer.zero_grad()\r\ny_hat = self.model(x_train)\r\ngrad =  compute_grad(y_hat, y)\r\ny_hat.backward(gradient=grad)\r\nself.optimizer.step()\r\n```\r\nWould the following be a correct implementation in lightning?\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n        opt = self.optimizers()\r\n        x,y = batch\r\n        y_hat =  self(x)\r\n        grad =  compute_grad(y_hat, y)\r\n        opt.zero_grad()\r\n        y_hat.backward(gradient= grad)\r\n        opt.step()\r\n```@JayMan91 Haven't tried, but you could probably try manual optimization.\r\n\r\n```python\r\ndef __init__(...):\r\n    ...\r\n    self.automatic_optimization = False  # use manual optimization\r\n    ...\r\n\r\ndef training_step(...):\r\n    ...\r\n    self.manual_backward(y_hat, gradient=grad)\r\n    ...\r\n```\r\n\r\nmanual optimization: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization\r\n`LightningModule.manual_backward`: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#manual-backward\r\n",
    "meta": { "name": "How to pass gradients to `backward()`" },
    "answer": "@JayMan91 Haven't tried, but you could probably try manual optimization.\r\n\r\n```python\r\ndef __init__(...):\r\n    ...\r\n    self.automatic_optimization = False  # use manual optimization\r\n    ...\r\n\r\ndef training_step(...):\r\n    ...\r\n    self.manual_backward(y_hat, gradient=grad)\r\n    ...\r\n```\r\n\r\nmanual optimization: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization\r\n`LightningModule.manual_backward`: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#manual-backward\r\n"
  },
  {
    "content": "## \u2753 Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI am training MNIST with below code. 1 GPU training is ok.\r\nBut it shows slow start of new epoch when num_workers is a large number and the number of gpus > 2.\r\nEven dataloading itself is slower than with 1gpu.\r\n#### Code\r\n\r\nimport torch\r\nfrom torch import nn\r\nimport pytorch_lightning as pl \r\nfrom torchvision import datasets, transforms\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchvision.datasets import MNIST \r\nfrom torch.nn import functional as F\r\nimport torch.distributed as dist\r\nimport os, sys\r\n\r\nclass LightningMNISTClassifier(pl.LightningModule):\r\n    def __init__(self):\r\n        super(LightningMNISTClassifier, self).__init__()\r\n        self.layer_1 = nn.Linear(28*28, 128)\r\n        self.layer_2 = nn.Linear(128, 256)\r\n        self.layer_3 = nn.Linear(256, 10)\r\n    \r\n    def forward(self, x):\r\n        batch_size, channels, width, height = x.size()\r\n        x = x.view(batch_size, -1)\r\n        x = self.layer_1(x)\r\n        x = torch.relu(x)\r\n        x = self.layer_2(x)\r\n        x = torch.relu(x)\r\n        x = self.layer_3(x)\r\n        x = torch.log_softmax(x, dim=-1)\r\n        return x\r\n\r\n    def prepare_data(self):\r\n        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307, ), (0.3081,))])\r\n        mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\r\n        self.mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\r\n        self.mnist_train, self.mnist_val = random_split(mnist_train, [55000,5000])\r\n    \r\n    def train_dataloader(self):\r\n        data_loader2 = DataLoader(self.mnist_train, batch_size=64, num_workers=7, shuffle=True) \r\n        # data_loader2 = DataLoader(self.mnist_train, batch_size=64, shuffle=True) \r\n        return data_loader2\r\n    def val_dataloader(self):\r\n        return DataLoader(self.mnist_val, batch_size=64)\r\n\r\n    # def test_dataloader(self):\r\n    #     return DataLoader(self.mnist_test, batch_size=64)\r\n    \r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n    \r\n    def cross_entropy_loss(self, logits, labels):\r\n        return F.nll_loss(logits, labels)\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        logs = {'train_loss': loss}\r\n        return {\"loss\": loss, \"log\": logs}\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"val_loss\": loss}\r\n    \r\n    def validation_epoch_end(self, outputs):\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        tensorboard_logs = {\"val_loss\": avg_loss}\r\n        return {\"avg_val_loss\": avg_loss, 'log':tensorboard_logs}\r\n\r\nif __name__ == '__main__':\r\n    model = LightningMNISTClassifier()\r\n\r\n    trainer = pl.Trainer(gpus=4, distributed_backend='ddp')\r\n\r\n    trainer.fit(model)\r\n#### What have you tried?\r\nHorovod backend does not show slow start of new epoch.\r\n\r\n#### What's your environment?\r\n\r\n - OS: ubuntu 18.04\r\n - Packaging pip\r\n - Version pytorch 1.5.0, 0.7.6\r\nI experience similar things - when running with ddp, then it seems the higher num_workers the longer it takes before getting data to the GPUs.\r\nIt seems stuck at the dataloaders which might be killed and reinitialized the whole time by multiprocessing?\r\n@mpaepper Your comment may be right. \r\nI did the same thing using torch 1.4.0.\r\nBelow log is about new start of epoch 2. \r\nDeprecationWarnings were shown at the beginning of  the first epoch ngpu*num_workers times.\r\nAt the second epoch the same warnings are shown again. \r\nMoreover, the each log itself is very slow.\r\n\r\n------------------------------------------------------------------\r\nEpoch 2:   0%|                                                                                              | 0/470 [00:00<?, ?it/s, loss=0.180, v_num=113]/opt/conda/lib/python3.7/site-packages/torchvision/io/video.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n/opt/conda/lib/python3.7/site-packages/torchvision/io/video.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n/opt/conda/lib/python3.7/site-packages/torchvision/io/video.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n/opt/conda/lib/python3.7/site-packages/torchvision/io/video.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n/opt/conda/lib/python3.7/site-packages/torchvision/io/video.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n/opt/conda/lib/python3.7/site-packages/torchvision/io/video.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n/opt/conda/lib/python3.7/site-packages/torchvision/io/video.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n/opt/conda/lib/python3.7/site-packages/torchvision/io/video.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n/opt/conda/lib/python3.7/site-packages/torchvision/io/video.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n  import imp\r\n/opt/conda/lib/python3.7/site-packages/torchvision/io/video.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n...I found that the slow deprecationwarnings shown above  are due to the torchvision library. I changed to a simple dataset and the slow start disappeared until now. The isssue is not due to pl.",
    "meta": {
      "name": "slow new epoch start with setting ddp, num_workers, gpus"
    },
    "answer": "I found that the slow deprecationwarnings shown above  are due to the torchvision library. I changed to a simple dataset and the slow start disappeared until now. "
  },
  {
    "content": "I want to use ModelCheckpoint to save mode while training, however, nothing has been saved. The following is my code. I don't know what leads to this problem, any suggestions?\r\n \r\n![image](https://user-images.githubusercontent.com/22525811/140004044-fbba057f-6932-450e-97ac-1889c59d1d29.png)\r\nI solve the problem by add `self.log('val_loss', val_loss)` in function `validation_epoch_end`.",
    "meta": { "name": "ModelCheckpoint save nothing" },
    "answer": "I solve the problem by add `self.log('val_loss', val_loss)` in function `validation_epoch_end`."
  },
  {
    "content": "I would like to save one single inference example per validation stage.  To this end I came up with:\r\n```\r\nclass Model(pl.LightningModule):\r\n\r\n    ...\r\n\r\n    def validation_epoch_end(self, validation_step_outputs):\r\n        # self.trainer.log_dir is not set during fast_dev_test\r\n        if self.trainer.log_dir is not None:\r\n            x, y = (d.unsqueeze(0) for d in self.trainer.datamodule.valid_set.dataset_pair())\r\n            y_hat = self.model(x)\r\n            x, y, y_hat = (d.squeeze().cpu().numpy() for d in (x, y, y_hat))\r\n\r\n            save_dir = Path(self.trainer.log_dir) / 'wavs'\r\n            save_dir.mkdir(exist_ok=True)\r\n\r\n            soundfile.write(save_dir/f'{self.global_step:06}_input.wav',\r\n                    x, 44100, subtype='PCM_24')\r\n            soundfile.write(save_dir/f'{self.global_step:06}_output.wav',\r\n                    y_hat, 44100, subtype='PCM_24')\r\n            soundfile.write(save_dir/f'{self.global_step:06}_target.wav',\r\n                    y, 44100, subtype='PCM_24')\r\n```\r\n\r\nThis works OK when running on a single device (CPU) but when running on GPUs on a slurm cluster I get:\r\n\r\n```\r\nRuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\r\n```\r\nI guess because the data has not been sent to the same device as the model.\r\n\r\nThis seems like a problem that must have been solved before.  Can anyone suggest a typical pytorch-lightning way of doing this?\r\n\r\nThanks for your input\r\nLeohere:\r\n```py\r\nx, y = (d.unsqueeze(0) for d in self.trainer.datamodule.valid_set.dataset_pair())\r\n```\r\nI think this is something not part of the dataloader, so PL won't move to the device automatically.\r\nyou can do:\r\n```py\r\nx = x.to(self.device)\r\ny_hat = self.model(x)\r\n...\r\n```",
    "meta": {
      "name": "Saving one single inference example per validation stage"
    },
    "answer": "here:\r\n```py\r\nx, y = (d.unsqueeze(0) for d in self.trainer.datamodule.valid_set.dataset_pair())\r\n```\r\nI think this is something not part of the dataloader, so PL won't move to the device automatically.\r\nyou can do:\r\n```py\r\nx = x.to(self.device)\r\ny_hat = self.model(x)\r\n...\r\n```"
  },
  {
    "content": "### Throws this Exception. IDK what to do. Can anyone help me? \r\n\r\n**Code:**\r\n\r\n```\r\nimport enum\r\nimport os\r\nimport torch\r\nimport torchmetrics\r\n\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.callbacks import EarlyStopping\r\n\r\n\r\nfrom torch.utils.data import Dataset, DataLoader\r\nimport pytorch_lightning as pl\r\n\r\nfrom dataclasses import dataclass, asdict\r\nimport pytorch_lightning_spells as pls\r\nimport typer\r\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\r\n\r\nfrom Cord19Dataset import Cord19Dataset, DATASET_DIR, Parts\r\nfrom t2t import BaseConfig, T5BaseModel, masked_cross_entropy_loss\r\nfrom pathlib import Path\r\nimport pandas as pd\r\n\r\n\r\nclass Corpus(enum.Enum):\r\n    CORD19 = \"cord19\"\r\n\r\nCACHE_DIR = \r\n\r\n@dataclass\r\nclass Config(BaseConfig):\r\n    dataset: Corpus = Corpus.CORD19\r\n\r\nfrom pathlib import Path\r\nimport pandas as pd\r\n\r\n\r\n\r\n\r\nclass T5Model(T5BaseModel):\r\n    def __init__(self, config: Config, **kwargs):\r\n        model = T5ForConditionalGeneration.from_pretrained(config.base_t5_model)\r\n        tokenizer = T5Tokenizer.from_pretrained(config.base_t5_model)\r\n        super().__init__(config, model, tokenizer)\r\n        self.config = config\r\n        # log the config values\r\n        self.save_hyperparameters(asdict(config))\r\n        self.train_dataset = Cord19Dataset(Parts.TRAIN)\r\n        print(\"Train dataset: \", len(self.train_dataset))\r\n        self.valid_dataset = Cord19Dataset(Parts.VALID)\r\n        print(\"Valid dataset: \", len(self.valid_dataset))\r\n        \r\nprint(Cord19Dataset(Parts.TRAIN))\r\n\r\ndef main(\r\n        t5_model: str = \"t5-small\", lr: float = 1e-4,  # 3^4\r\n        epochs: int = 5, fp16: bool = False,\r\n        #dataset: Corpus = Corpus.CORD19, batch_size: int = 16,\r\n        dataset: Corpus = Corpus.CORD19, batch_size: int = 8,\r\n        max_len: int = 64, grad_accu: int = 1,\r\n        num_gpus: int = 1\r\n      \r\n):\r\n    pl.seed_everything(int(os.environ.get(\"SEED\", 738)))\r\n    config = Config(\r\n        base_t5_model=t5_model,\r\n        learning_rate=lr,\r\n        epochs=epochs,\r\n        dataset=dataset,\r\n        max_len=max_len,\r\n        grad_accu=grad_accu,\r\n        batch_size=batch_size,\r\n        fp16=fp16,\r\n        weight_decay=0,\r\n        num_gpus=num_gpus,\r\n        loss_fn=masked_cross_entropy_loss\r\n    )\r\n\r\n    pl_module = T5Model(config)\r\n\r\n    callbacks = [\r\n        pl.callbacks.ModelCheckpoint(\r\n            dirpath=str(DATASET_DIR / \"model_checkpoints\"),\r\n            monitor='val_loss',\r\n            mode=\"min\",\r\n            filename='{step:06d}-{val_loss:.4f}',\r\n            save_top_k=1,\r\n            save_last=False\r\n        ),\r\n        pl.callbacks.LearningRateMonitor(logging_interval='step'),\r\n    ]\r\n    trainer = pl.Trainer(\r\n        accelerator='dp' if num_gpus > 1 else None,\r\n        # amp_backend=\"apex\", amp_level='O2',\r\n        precision=16 if config.fp16 else 32,\r\n        gpus=config.num_gpus,\r\n        val_check_interval=0.25,\r\n        gradient_clip_val=10,\r\n        max_epochs=epochs,\r\n        # max_steps=steps,\r\n        callbacks=callbacks,\r\n        accumulate_grad_batches=grad_accu,\r\n        # auto_scale_batch_size='power' if batch_size is None else None,\r\n        logger=[\r\n            pl.loggers.TensorBoardLogger(str(DATASET_DIR / \"tb_logs\"), name=\"\"),\r\n            pls.loggers.ScreenLogger(),\r\n            # pl.loggers.WandbLogger(project=\"t5-paraphrase\")\r\n        ],\r\n        log_every_n_steps=100\r\n    )\r\n\r\n    trainer.fit(pl_module)\r\n\r\n#     pl_module.model.save_pretrained(CACHE_DIR / f\"{config.base_t5_model}_last\")\r\n#     pl_module.tokenizer.save_pretrained(CACHE_DIR / f\"{config.base_t5_model}_last\")\r\n#     print(\"Last model saved\")\r\n\r\n    assert isinstance(callbacks[0], pl.callbacks.ModelCheckpoint)\r\n    print(callbacks[0].best_model_path)\r\n    pl_module = T5Model.load_from_checkpoint(\r\n        callbacks[0].best_model_path,\r\n        config=config\r\n    )\r\n    pl_module.model.save_pretrained(DATASET_DIR / f\"{config.base_t5_model}_best\")\r\n    pl_module.tokenizer.save_pretrained(DATASET_DIR / f\"{config.base_t5_model}_best\")\r\n    print(\"Best model saved\")\r\n    pl_module.model.save_pretrained(path+t5+\"model/\")\r\n    pl_module.tokenizer.save_pretrained(path+t5+\"tokenizer/\")\r\n\r\n```\r\n\r\nThis gives me: \r\n\r\n```\r\nGlobal seed set to 738\r\nSome weights of the model checkpoint at t5-small were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\r\n- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\r\n  stream(template_mgs % msg_args)\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00<00:00, 175.62it/s]\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 24.88it/s]\r\nLoading train dataset into memory...\r\nUsed File: ['dataset_1.jbl', 'dataset_2.jbl', 'dataset_3.jbl', 'dataset_4.jbl', 'dataset_5.jbl', 'dataset_6.jbl']\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/[...]Pretraining-T5-PyTorch-Lightning/model_checkpoints exists and is not empty.\r\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\r\n\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\r\nValid dataset:  124\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\r\n  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\r\n\r\n  | Name  | Type                       | Params\r\n-----------------------------------------------------\r\n0 | model | T5ForConditionalGeneration | 60.5 M\r\n-----------------------------------------------------\r\n60.5 M    Trainable params\r\n0         Non-trainable params\r\n60.5 M    Total params\r\n242.026   Total estimated model params size (MB)\r\nSteps per epochs: 46\r\nValidation sanity check: 0%\r\n0/2 [00:00<?, ?it/s]\r\n---------------------------------------------------------------------------\r\nMisconfigurationException                 Traceback (most recent call last)\r\n<ipython-input-10-b1e689bd5891> in <module>\r\n      2 # if __name__ == \"__main__\":\r\n      3 #     typer.run(main)\r\n----> 4 main()\r\n\r\n<ipython-input-9-576858e8663d> in main(t5_model, lr, epochs, fp16, dataset, batch_size, max_len, grad_accu, num_gpus)\r\n     56     )\r\n     57 \r\n---> 58     trainer.fit(pl_module)\r\n     59 \r\n     60 #     pl_module.model.save_pretrained(CACHE_DIR / f\"{config.base_t5_model}_last\")\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader)\r\n    551         self.checkpoint_connector.resume_start()\r\n    552 \r\n--> 553         self._run(model)\r\n    554 \r\n    555         assert self.state.stopped\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\r\n    916 \r\n    917         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n--> 918         self._dispatch()\r\n    919 \r\n    920         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _dispatch(self)\r\n    984             self.accelerator.start_predicting(self)\r\n    985         else:\r\n--> 986             self.accelerator.start_training(self)\r\n    987 \r\n    988     def run_stage(self):\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n     90 \r\n     91     def start_training(self, trainer: \"pl.Trainer\") -> None:\r\n---> 92         self.training_type_plugin.start_training(trainer)\r\n     93 \r\n     94     def start_evaluating(self, trainer: \"pl.Trainer\") -> None:\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\r\n    159     def start_training(self, trainer: \"pl.Trainer\") -> None:\r\n    160         # double dispatch to initiate the training loop\r\n--> 161         self._results = trainer.run_stage()\r\n    162 \r\n    163     def start_evaluating(self, trainer: \"pl.Trainer\") -> None:\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\r\n    994         if self.predicting:\r\n    995             return self._run_predict()\r\n--> 996         return self._run_train()\r\n    997 \r\n    998     def _pre_training_routine(self):\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run_train(self)\r\n   1029             self.progress_bar_callback.disable()\r\n   1030 \r\n-> 1031         self._run_sanity_check(self.lightning_module)\r\n   1032 \r\n   1033         # enable train mode\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run_sanity_check(self, ref_model)\r\n   1113             # run eval step\r\n   1114             with torch.no_grad():\r\n-> 1115                 self._evaluation_loop.run()\r\n   1116 \r\n   1117             self.on_sanity_check_end()\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/base.py in run(self, *args, **kwargs)\r\n    109             try:\r\n    110                 self.on_advance_start(*args, **kwargs)\r\n--> 111                 self.advance(*args, **kwargs)\r\n    112                 self.on_advance_end()\r\n    113                 self.iteration_count += 1\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py in advance(self, *args, **kwargs)\r\n    109 \r\n    110         dl_outputs = self.epoch_loop.run(\r\n--> 111             dataloader_iter, self.current_dataloader_idx, dl_max_batches, self.num_dataloaders\r\n    112         )\r\n    113 \r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/base.py in run(self, *args, **kwargs)\r\n    109             try:\r\n    110                 self.on_advance_start(*args, **kwargs)\r\n--> 111                 self.advance(*args, **kwargs)\r\n    112                 self.on_advance_end()\r\n    113                 self.iteration_count += 1\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py in advance(self, dataloader_iter, dataloader_idx, dl_max_batches, num_dataloaders)\r\n    109         with self.trainer.profiler.profile(\"evaluation_step_and_end\"):\r\n    110             output = self.evaluation_step(batch, batch_idx, dataloader_idx)\r\n--> 111             output = self.evaluation_step_end(output)\r\n    112 \r\n    113         self.batch_progress.increment_processed()\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py in evaluation_step_end(self, *args, **kwargs)\r\n    159         \"\"\"Calls the `{validation/test}_step_end` hook\"\"\"\r\n    160         hook_name = \"test_step_end\" if self.trainer.testing else \"validation_step_end\"\r\n--> 161         output = self.trainer.call_hook(hook_name, *args, **kwargs)\r\n    162         return output\r\n    163 \r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in call_hook(self, hook_name, *args, **kwargs)\r\n   1222             if is_overridden(hook_name, model_ref):\r\n   1223                 hook_fx = getattr(model_ref, hook_name)\r\n-> 1224                 output = hook_fx(*args, **kwargs)\r\n   1225 \r\n   1226             # call the accelerator hook\r\n\r\n/home/[...]Pretraining-T5-PyTorch-Lightning/t2t/__init__.py in validation_step_end(self, outputs)\r\n    227                 outputs['target']['ids'].view(-1).cpu()\r\n    228             )\r\n--> 229             self.log(\"val_\" + name, metric)\r\n    230 \r\n    231     def _should_log(self, flag):\r\n\r\n/opt/anaconda/envs/venv/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py in log(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, tbptt_reduce_fx, tbptt_pad_token, enable_graph, sync_dist, sync_dist_op, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\r\n    432                 if not self._metric_attributes:\r\n    433                     raise MisconfigurationException(\r\n--> 434                         \"Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\"\r\n    435                         \" You can fix this by setting an attribute for the metric in your `LightningModule`.\"\r\n    436                     )\r\n\r\nMisconfigurationException: Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged. You can fix this by setting an attribute for the metric in your `LightningModule`.\r\n```\r\n\r\nhey @JaquJaqu \r\ncan you reformat your code for better readability and also add the exact error with traceback?\r\nok well i got it. Seems like I accidentally updated the module...",
    "meta": {
      "name": "MisconfigurationException: Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged. You can fix this by setting an attribute for the metric in your `LightningModule`."
    },
    "answer": "ok well i got it. Seems like I accidentally updated the module..."
  },
  {
    "content": "When I load the saved Lightning model and test on test data, I saw different epochs are running starting from 0, suggesting the model is **being trained once again**. Please suggest how to get the test score on the test data. \r\nLet me illustrate in detail. I have a  Lightning model like the following\r\n```\r\nclass  MyLightningModule(pl.LightningModule):\r\n    def __init__(self,loss_fn, lr=1e-1):\r\n        super().__init__()\r\n        self.loss_fn = loss_fn\r\n        self.lr = lr\r\n        self.save_hyperparameters(\"lr\")\r\n    \r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = parent_parser.add_argument_group(\"SemanticPO\")\r\n        parser.add_argument(\"--lr\", type=float, default=0.1)\r\n        return parent_parser\r\n    def forward(self,x):\r\n        return self.layer1(x)   \r\n    def training_step(self, batch, batch_idx):\r\n        x,y,sol = batch\r\n        y_hat =  self(x)\r\n        loss = self.loss_fn(y,y_hat)\r\n        self.log(\"train_loss\",loss, prog_bar=True, on_step=True, on_epoch=True )\r\n        return loss\r\n    def validation_step(self, batch, batch_idx):\r\n        x,y,sol = batch\r\n        y_hat =  self(x)\r\n        val_loss=  self.loss_fn(y,y_hatl)\r\n        self.log(\"val_loss\", val_loss, prog_bar=True, on_step=True, on_epoch=True, )\r\n        return val_loss\r\n    def test_step(self, batch, batch_idx):\r\n        # Here we just reuse the validation_step for testing\r\n        return self.validation_step(batch, batch_idx)\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\r\n        return optimizer \r\n```\r\nI am using three separate data for training. testing and validation.\r\n```\r\ntrain_dl = DataLoader(train_df, batch_size= 64)\r\nvalid_dl = data_utils.DataLoader(valid_df, batch_size= 64)\r\ntest_dl = data_utils.DataLoader(test_df, batch_size= 64)\r\n```\r\nFirst, I train and validate in the following manner\r\n```\r\nmodel = MyLightningModule(loss_fn= Myloss)\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor=\"val_loss\",\r\n    dirpath=\"ckpt_dir/\",\r\n    filename=\"mymodel-{epoch:02d}-{val_loss:.2f}\",\r\n    save_top_k=2, mode=\"min\",\r\n)\r\ntrainer = pl.Trainer(max_epochs= 25, callbacks=[checkpoint_callback])\r\ntrainer.fit(model, train_dl, valid_dl)\r\n```\r\nThe training is complete and the checkpoints are saved as expected.\r\nThen I want to restore the saved model and test it on test data. This is done on a separate file like this\r\n```\r\nmodel = MyLightningModule.load_from_checkpoint('ckpt_dir/mymodel-epoch=15-val_loss=44.31.ckpt')\r\ntest_dl = data_utils.DataLoader(test_df, batch_size= 64)\r\ntrainer = pl.Trainer()\r\nresult = trainer.test(model,\r\nckpt_path='ckpt_dir/mymodel-epoch=15-val_loss=44.31.ckpt',dataloaders=test_dl)\r\nprint(result)\r\n```\r\nI expect it to just test on test data and display the result. But I see the model is running once again for a number of epochs. \r\nMay be I am doing something wrong. Please suggest the best way to obtain the test result after restoring a saved model.\r\n\r\n<details>\r\npytorch-lightning==1.4.9\r\n</details>your code looks correct. Just tested on 1.4.9 it's not running from scratch when you call `trainer.test()`. Can you share a minimal example to reproduce your issue? ",
    "meta": {
      "name": "Getting the test score after restoring a pretrained model"
    },
    "answer": "your code looks correct. Just tested on 1.4.9 it's not running from scratch when you call `trainer.test()`. Can you share a minimal example to reproduce your issue? "
  },
  {
    "content": "Is there a way to save the model with the best validation accuracy when using early stopping? I believe right now, the model weights are the weights from the latest snapshot; but i am looking for a way to access the model with the best performance on validation.by on validation you mean while calling `trainer.validate` or validation happening within `trainer.fit` call?",
    "meta": { "name": "Model with best validation accuracy" },
    "answer": "by on validation you mean while calling `trainer.validate` or validation happening within `trainer.fit` call?"
  },
  {
    "content": "I thought the number of on_train_epoch_start and on_train_epoch_end should be equal to the number of epochs. But when I passed the following callback function:\r\n```\r\nclass MyPrintingCallback(Callback):\r\n\r\n    def on_train_epoch_start(self, trainer, pl_module):\r\n        print('Train epoch start for epoch: ', pl_module.current_epoch)\r\n        \r\n    def on_train_epoch_end(self, trainer, pl_module):\r\n        # will run only once in the beginning\r\n        print('Train step end for epoch: ', pl_module.current_epoch)\r\n```\r\n\r\non_train_epoch_end is only called in the 0th epoch:\r\n\r\n> Training: -1it [00:00, ?it/s]Train epoch start for epoch:  0                                                                                                                                                                                                                        \r\nEpoch 0: : 4875it [00:33, 143.57it/s, Train step end for epoch:  0\r\nTrain epoch start for epoch:  1       \r\nEpoch 1: : 0it [00:00, 7096.96it/s, loss=2.13e+09, v_num=37] Train epoch start for epoch:  2\r\nEpoch 2: : 0it [00:00, 11335.96it/s, loss=2.13e+09, v_num=37]Train epoch start for epoch:  3\r\nEpoch 3: : 0it [00:00, 12052.60it/s, loss=2.13e+09, v_num=37]Train epoch start for epoch:  4\r\nEpoch 4: : 0it [00:00, 4301.85it/s, loss=2.13e+09, v_num=37] \r\n\r\n\r\nAny idea why this is happening? It turns out that the issue is because I was chaining many data loaders with itertools.chain() which was called only once by lightning. ",
    "meta": {
      "name": "multiple on_train_epoch_start callbacks but only one on_train_epoch_end?"
    },
    "answer": "It turns out that the issue is because I was chaining many data loaders with itertools.chain() which was called only once by lightning. "
  },
  {
    "content": "Hello I am trying to train a neural network using pytorch lightning. I have run into an issue with the trainer when I try to run the program. I am getting the following issue:\r\n```\r\nGPU available: False, used: False\r\nTPU available: False, using: 0 TPU cores\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/PytorchLightningGRUtraining.py\", line 179, in <module>\r\n    main(args)\r\n\r\n  File \"/home/PytorchLightningGRUtraining.py\", line 171, in main\r\n    trainer.fit(model, dm.train_dataloader(), dm.val_dataloader())\r\n\r\n  File \"/home/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 449, in fit\r\n    self.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\r\n\r\n  File \"/home/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 123, in setup_fit\r\n    self.trainer.callback_connector.attach_model_logging_functions(model)\r\n\r\n  File \"/home/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py\", line 123, in attach_model_logging_functions\r\n    callback.log = model.log\r\n\r\n  File \"/home/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in __getattr__\r\n    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\r\n\r\nAttributeError: 'NeuralNetwork' object has no attribute 'log'\r\n```\r\n\r\nI defined the logger for the trainer after I created an instance of the model class. What am I doing wrong?did you pass in a lightningmodule instance?\r\n```py\r\nclass YourModel(pl.LightningModule): <- here?\r\n    ...\r\n```That was the mistake. Thank you",
    "meta": { "name": "'NeuralNetwork' object has no attribute 'log'" },
    "answer": "did you pass in a lightningmodule instance?\r\n```py\r\nclass YourModel(pl.LightningModule): <- here?\r\n    ...\r\n```"
  },
  {
    "content": "Anyone help me where did these funcs been called in the core parts? I expect it to be called in the loop instance of trainer however not. Quite confused about this.depends upon the configuration but in a general case here: https://github.com/PyTorchLightning/pytorch-lightning/blob/454e93bacecb4f1d6ebc9ebd3bc11b26723ddbec/pytorch_lightning/plugins/training_type/training_type_plugin.py#L217-L230\r\n\r\ncall sequence is `loop -> accelerators -> training_type_plugin -> actual call`\r\n",
    "meta": { "name": "The call of training_step and validation_step .etc." },
    "answer": "depends upon the configuration but in a general case here: https://github.com/PyTorchLightning/pytorch-lightning/blob/454e93bacecb4f1d6ebc9ebd3bc11b26723ddbec/pytorch_lightning/plugins/training_type/training_type_plugin.py#L217-L230\r\n\r\ncall sequence is `loop -> accelerators -> training_type_plugin -> actual call`\r\n"
  },
  {
    "content": "Hi,\r\n\r\nI'm applying Pytorch Lightning module to VAE and our model We first train VAE and give the best checkpoint of pretrained VAE as the initial weight of our model.\r\n\r\n```python\r\n# STEP 1. Train VAE\r\nvae = VAE(...)\r\n\r\ntrainer = Trainer(...)\r\ntrainer.fit(vae)\r\n\r\n# STEP 2.\r\nvae = VAE.load_from_checkpoint(...)\r\n\r\nclass Model(LightningModule):\r\n    def __init__(self, encoder, decoder, learning_rate):\r\n       super().__init__()\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n        \r\n        self.save_hyperparameters(\"learning_rate\")\r\n        ...\r\n\r\nencoder = copy.deepcopy(vae.encoder)\r\ndecoder = copy.deepcopy(vae.decoder)\r\n\r\nmodel = Model(\r\n    encoder=encoder,\r\n    decoder=decoder,\r\n    ...\r\n)\r\ntrainer.fit(model)\r\n```\r\n\r\nThe problem is when I load the **model** after train ends. Since the torch modules are contained in input arguments of **Model**, the common approach \r\n\r\n```python\r\nmodel = Model.load_from_checkpoint(...) \r\n```\r\nyields following error messages. `TypeError: __init__() missing 2 required positional arguments: 'encoder' and 'decoder'\r\n`\r\nSo, what is the best practice for saving and loading the model which uses the pre-trained model?\r\n\r\nsince part of your model is inside arguments, you can randomly initialize your VAE and let the new checkpoint configure it's weights\r\n```py\r\nvae = VAE()\r\nencoder = copy.deepcopy(vae.encoder)\r\ndecoder = copy.deepcopy(vae.decoder)\r\n\r\nmodel = Model.load_from_checkpoint(..., encoder=encoder, decoder=decoder) \r\n```",
    "meta": {
      "name": "How to save and load LightningModule whose input containing the pretrained moduel?"
    },
    "answer": "since part of your model is inside arguments, you can randomly initialize your VAE and let the new checkpoint configure it's weights\r\n```py\r\nvae = VAE()\r\nencoder = copy.deepcopy(vae.encoder)\r\ndecoder = copy.deepcopy(vae.decoder)\r\n\r\nmodel = Model.load_from_checkpoint(..., encoder=encoder, decoder=decoder) \r\n```"
  },
  {
    "content": "I am using Jupyter Lab to run. It has pre-installed tf2.3_py3.6 kernel installed in it. It has 2 GPUS in it.\r\n\r\n```\r\nPyTorch Lightning Version (e.g., 1.3.0): '1.4.6'\r\nPyTorch Version (e.g., 1.8): '1.6.0+cu101'\r\nPython version: 3.6\r\nOS (e.g., Linux): system='Linux'\r\nCUDA/cuDNN version: 11.2\r\nHow you installed PyTorch (conda, pip, source): pip\r\n```\r\n\r\nHere is the screenshot of my model and it got interrupted due to connection issue.\r\n\r\nI am saving the best model in checkpoint.\r\n\r\n`I am doing multi-label classification using Hugging face model. After training the model I want to export the model using ONNX format.`\r\n\r\n**Here is the DataModule Class**\r\n\r\n```\r\n\r\nN_EPOCHS = 30\r\nBATCH_SIZE = 10\r\n\r\nclass  SRDataModule(pl.LightningDataModule):\r\n    \r\n    def __init__(self, X_train,y_train, X_test,y_test, tokenizer, batch_size=8, max_token_len=512):\r\n        super().__init__()\r\n        self.batch_size = batch_size\r\n        self.train_df = X_train\r\n        self.test_df = X_test\r\n        self.train_lab = y_train\r\n        self.test_lab = y_test\r\n        self.tokenizer = tokenizer\r\n        self.max_token_len = max_token_len\r\n\r\n    def setup(self, stage=None):\r\n        self.train_dataset = SRDataset(\r\n          self.train_df,\r\n          self.train_lab,\r\n          self.tokenizer,\r\n          self.max_token_len\r\n        )\r\n\r\n        self.test_dataset = SRDataset(\r\n          self.test_df,\r\n          self.test_lab,\r\n          self.tokenizer,\r\n          self.max_token_len\r\n    )\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n          self.train_dataset,\r\n          batch_size=self.batch_size,\r\n          shuffle=True,\r\n          num_workers=10\r\n        )\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(\r\n          self.test_dataset,\r\n          batch_size=self.batch_size,\r\n          num_workers=10\r\n        )\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(\r\n          self.test_dataset,\r\n          batch_size=self.batch_size,\r\n          num_workers=10\r\n        )\r\n\r\n```\r\n\r\n\r\n**Here is the model class:**\r\n\r\n\r\n```\r\nclass SRTagger(pl.LightningModule):\r\n\r\n  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\r\n    super().__init__()\r\n    self.save_hyperparameters()\r\n    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\r\n    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\r\n    self.n_training_steps = n_training_steps\r\n    self.n_warmup_steps = n_warmup_steps\r\n    self.criterion = nn.BCELoss()\r\n\r\n  def forward(self, input_ids, attention_mask, labels=None):\r\n    output = self.bert(input_ids, attention_mask=attention_mask)\r\n    output = self.classifier(output.pooler_output)\r\n    output = torch.sigmoid(output)    \r\n    loss = 0\r\n    if labels is not None:\r\n        loss = self.criterion(output, labels)\r\n    return loss, output\r\n\r\n  def training_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    \r\n    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\r\n    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\r\n\r\n  def validation_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\r\n    return loss\r\n\r\n  def test_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\r\n    return loss\r\n\r\n  def training_epoch_end(self, outputs):\r\n    \r\n    labels = []\r\n    predictions = []\r\n    for output in outputs:\r\n      for out_labels in output[\"labels\"].detach().cpu():\r\n        labels.append(out_labels)\r\n      for out_predictions in output[\"predictions\"].detach().cpu():\r\n        predictions.append(out_predictions)\r\n\r\n    labels = torch.stack(labels).int()\r\n    predictions = torch.stack(predictions)\r\n\r\n    for i, name in enumerate(LABEL_COLUMNS):\r\n      class_roc_auc = auroc(predictions[:, i], labels[:, i])\r\n      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\r\n\r\n\r\n  def configure_optimizers(self):\r\n\r\n    optimizer = optim.RAdam(self.parameters(), lr=2e-4)\r\n\r\n    scheduler = get_linear_schedule_with_warmup(\r\n      optimizer,\r\n      num_warmup_steps=self.n_warmup_steps,\r\n      num_training_steps=self.n_training_steps\r\n    )\r\n\r\n    return dict(\r\n      optimizer=optimizer,\r\n      lr_scheduler=dict(\r\n        scheduler=scheduler,\r\n        interval='step'\r\n      )\r\n    )\r\n```\r\n\r\n\r\n**Sample Data**\r\n\r\n\r\n```\r\nsample_batch = next(iter(DataLoader(train_dataset, batch_size=10, num_workers=2)))\r\nsample_batch[\"input_ids\"].shape, sample_batch[\"attention_mask\"].shape\r\n\r\n(torch.Size([10, 512]), torch.Size([10, 512]))\r\n```\r\n\r\n```\r\nsample_batch.keys()\r\ndict_keys(['text_data', 'input_ids', 'attention_mask', 'labels'])\r\n```\r\n\r\n**Model**\r\n\r\n```\r\nmodel = SRTagger(\r\n  n_classes=100,\r\n  n_warmup_steps=warmup_steps,\r\n  n_training_steps=total_training_steps \r\n)\r\n\r\n```\r\n**ONNX code**\r\n\r\n```\r\n# # Export the model\r\ntorch.onnx.export(model,                     # model being run\r\n                  ##since model is in the cuda mode, input also need to be\r\n                  (sample_batch[\"input_ids\"],sample_batch[\"attention_mask\"]),              # model input (or a tuple for multiple inputs)\r\n                  \"model_torch_export.onnx\", # where to save the model (can be a file or file-like object)\r\n                  export_params=True,        # store the trained parameter weights inside the model file\r\n                  opset_version=10,          # the ONNX version to export the model to\r\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\r\n                  input_names = ['input'],   # the model's input names\r\n                  output_names = ['output'], # the model's output names\r\n                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\r\n                                'output' : {0 : 'batch_size'}})\r\n```\r\n\r\n**Error**\r\n\r\n```\r\nRuntimeError: output 1 (0\r\n[ CPULongType{} ]) of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.\r\n```\r\nHi\r\n\r\nA google search reveals some help on this issue here: \r\n\r\nhttps://github.com/pytorch/pytorch/issues/31591\r\n\r\nCiting the thread there\r\n\r\n> As the error message indicates, the tracer detected that the output of your model didn't have any relationship to the input. \r\n\r\nIf we look closer at your code, we see that loss=0 and labels=None.\r\n\r\n```py\r\n  def forward(self, input_ids, attention_mask, labels=None):\r\n    output = self.bert(input_ids, attention_mask=attention_mask)\r\n    output = self.classifier(output.pooler_output)\r\n    output = torch.sigmoid(output)    \r\n    loss = 0\r\n    if labels is not None:\r\n        loss = self.criterion(output, labels)\r\n    return loss, output\r\n```\r\n\r\nthe if condition does not hold, so the part of your output (the loss) cannot be traced back to any inputs by onnx. \r\n\r\nChange your code to something like this and try again please:\r\n```py\r\n    if labels is not None:\r\n        loss = self.criterion(output, labels)\r\n        return loss, output\r\n    return output\r\n```\r\n> Hi\r\n> \r\n> A google search reveals some help on this issue here:\r\n> \r\n> [pytorch/pytorch#31591](https://github.com/pytorch/pytorch/issues/31591)\r\n> \r\n> Citing the thread there\r\n> \r\n> > As the error message indicates, the tracer detected that the output of your model didn't have any relationship to the input.\r\n> \r\n> If we look closer at your code, we see that loss=0 and labels=None.\r\n> \r\n> ```python\r\n>   def forward(self, input_ids, attention_mask, labels=None):\r\n>     output = self.bert(input_ids, attention_mask=attention_mask)\r\n>     output = self.classifier(output.pooler_output)\r\n>     output = torch.sigmoid(output)    \r\n>     loss = 0\r\n>     if labels is not None:\r\n>         loss = self.criterion(output, labels)\r\n>     return loss, output\r\n> ```\r\n> \r\n> the if condition does not hold, so the part of your output (the loss) cannot be traced back to any inputs by onnx.\r\n> \r\n> Change your code to something like this and try again please:\r\n> \r\n> ```python\r\n>     if labels is not None:\r\n>         loss = self.criterion(output, labels)\r\n>         return loss, output\r\n>     return output\r\n> ```\r\n\r\n@awaelchli  Ok. This solves my problem. However I have one clarification :\r\n\r\nIn code to save in ONNX format:\r\n\r\n```\r\ntorch.onnx.export(model,                     # model being run\r\n                  ##since model is in the cuda mode, input also need to be\r\n                  (sample_batch[\"input_ids\"],sample_batch[\"attention_mask\"]),              # model input (or a tuple for multiple inputs)\r\n                  \"model_torch_export.onnx\", # where to save the model (can be a file or file-like object)\r\n                  export_params=True,        # store the trained parameter weights inside the model file\r\n                  opset_version=10,          # the ONNX version to export the model to\r\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\r\n                  input_names = ['input'],   # the model's input names\r\n                  output_names = ['output'], # the model's output names\r\n                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\r\n                                'output' : {0 : 'batch_size'}})\r\n```\r\n\r\nIs this correct way to pass ` (sample_batch[\"input_ids\"],sample_batch[\"attention_mask\"])` inputs.  Also what does \r\n\r\n` input_names = ['input'],   # the model's input names\r\n                  output_names = ['output'], # the model's output names\r\n                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\r\n                                'output' : {0 : 'batch_size'}})` means?\r\n\r\n**Secondly, does this saves my best model which is trained above using pytorch lightning??**\r\nGreat! Glad the main issue is resolved. I'm converting this issue to a discussion thread. \r\n\r\nAlso check out the PL docs for the [to_onnx](https://pytorch-lightning.readthedocs.io/en/latest/common/production_inference.html#exporting-to-onnx) method on the LightningModule\r\n\r\n> Is this correct way to pass (sample_batch[\"input_ids\"],sample_batch[\"attention_mask\"]) inputs. \r\n\r\nYes, looks good.\r\n\r\n> Secondly, does this saves my best model which is trained above using pytorch lightning??\r\n\r\nWhen you run trainer.fit(model) it will train but when it finishes you may not end up with the best weights. You can load the model with the best weights by accessing the path \r\n\r\n`trainer.checkpoint_callback.best_model_path`\r\n\r\nCheck the [ModelCheckpoint callback docs ](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint)how to use it :) ",
    "meta": {
      "name": "Exporting PyTorch Lightning model to ONNX format not working"
    },
    "answer": "Hi\r\n\r\nA google search reveals some help on this issue here: \r\n\r\nhttps://github.com/pytorch/pytorch/issues/31591\r\n\r\nCiting the thread there\r\n\r\n> As the error message indicates, the tracer detected that the output of your model didn't have any relationship to the input. \r\n\r\nIf we look closer at your code, we see that loss=0 and labels=None.\r\n\r\n```py\r\n  def forward(self, input_ids, attention_mask, labels=None):\r\n    output = self.bert(input_ids, attention_mask=attention_mask)\r\n    output = self.classifier(output.pooler_output)\r\n    output = torch.sigmoid(output)    \r\n    loss = 0\r\n    if labels is not None:\r\n        loss = self.criterion(output, labels)\r\n    return loss, output\r\n```\r\n\r\nthe if condition does not hold, so the part of your output (the loss) cannot be traced back to any inputs by onnx. \r\n\r\nChange your code to something like this and try again please:\r\n```py\r\n    if labels is not None:\r\n        loss = self.criterion(output, labels)\r\n        return loss, output\r\n    return output\r\n```\r\n"
  },
  {
    "content": "My `LightningModule.training_step` includes calls to `self.log` and finally returns the loss value. What is the best way to run `training_step` outside of a `Trainer` context, for debugging purposes (such as manual gradient inspection, etc)? Without the instrumentation by `Trainer`, logger is not defined and `self.log` calls cause an exception. I was trying to mock the logger to turn them to no-ops, but I wasn't successful.it was updated to warning recently: https://github.com/PyTorchLightning/pytorch-lightning/pull/9733\r\nmight work with recent release or master",
    "meta": { "name": "How to disable logging temporarily?" },
    "answer": "it was updated to warning recently: https://github.com/PyTorchLightning/pytorch-lightning/pull/9733\r\nmight work with recent release or master"
  },
  {
    "content": "Hi,\r\n\r\nI'm using an `IterableDataset` and need to manually split up the data depending on the GPU (I'm using DDP). The lightning docs say that `replace_sampler_ddp` doesn't automatically handle `IterableDatasets`, so I need to do this myself.\r\n\r\nIs it possible to access the global rank or world size in a dataset (aka are there any utility functions available for this purpose)? I saw `_get_rank` [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/24db914093978a0b3fbd61d2d2ca331f0d6a4628/pytorch_lightning/utilities/distributed.py#L55), but ideally I wouldn't use a private helper function.\r\n\r\nI tried using:\r\n```\r\nfile_paths_filtered = file_paths[\r\n    torch.distributed.get_rank() :: torch.distributed.get_world_size()\r\n]\r\n```\r\n\r\ninside the IterableDataset, but it fails because the default process group has not been setup yet (which makes sense):\r\n```\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n```\r\n\r\nIs there any way to access the global rank/world size? If not, how would you recommend I check which GPU the dataset is on without causing an issue with `init_process_group` not being called yet (should I maybe call this manually)?Where are you calling this? If this is inside a DataModule, you could implement this splitting inside your `train_dataloader()` funtion, which is called after the distributed training setup and process group creation. in this case, accessing the torch.distributed ranks should workIssue ended up being `torch.distributed.get_rank()` and `torch.distributed.get_world_size()` aren't available when on a single GPU, which I was testing on locally before sending to multi-GPU server.\r\n\r\nThe following works:\r\n```\r\nif torch.distributed.is_available() and torch.distributed.is_initialized():\r\n    file_paths = file_paths[\r\n        torch.distributed.get_rank() :: torch.distributed.get_world_size()\r\n    ]\r\n```",
    "meta": {
      "name": "Is it possible to access the global rank and world size outside of `LightningModule`?"
    },
    "answer": "Issue ended up being `torch.distributed.get_rank()` and `torch.distributed.get_world_size()` aren't available when on a single GPU, which I was testing on locally before sending to multi-GPU server.\r\n\r\nThe following works:\r\n```\r\nif torch.distributed.is_available() and torch.distributed.is_initialized():\r\n    file_paths = file_paths[\r\n        torch.distributed.get_rank() :: torch.distributed.get_world_size()\r\n    ]\r\n```"
  },
  {
    "content": "I want to test again with ckpt. However I found Trainer resume_from_checkpoint only work for fit. Is there any particular reason for this?\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/f3442db3f0155a32c8bc0ca3bc943d4b0039897f/pytorch_lightning/trainer/trainer.py#L840-L841\r\n```\r\nIn [1]: from pytorch_lightning import seed_everything, Trainer\r\n\r\nIn [2]: from src import MMTSMatcher, WDCDataModule\r\n\r\nIn [3]: trainer = Trainer(resume_from_checkpoint=\"tests/mmtsmatcher_wdcdatamodule/shoes_small_false_32_0808-182020/checkpoints/epoch=05-valid_f1=81.22%.ckpt\", gpus=[4])\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\n\r\nIn [4]: trainer\r\nOut[4]: <pytorch_lightning.trainer.trainer.Trainer at 0x7f2345178fa0>\r\n\r\nIn [5]: model = MMTSMatcher(model_name=\"bert-base-uncased\")\r\n\r\nIn [6]: data = WDCDataModule(training_size=\"small\", cate=\"shoes\")\r\n\r\nIn [7]: data\r\nOut[7]: <src.datamodules.wdcdatamodule.WDCDataModule at 0x7f22ccd258e0>\r\n\r\nIn [10]: trainer.test(model, data)\r\n```Dear @tshu-w,\r\n\r\nThe resume_start will reload all states which might not be required for testing. We are currently refactoring this part of the code.\r\n\r\nBest,\r\nT.C",
    "meta": {
      "name": "Why Trainer resume_from_checkpoint only resume when fit."
    },
    "answer": "Dear @tshu-w,\r\n\r\nThe resume_start will reload all states which might not be required for testing. We are currently refactoring this part of the code.\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "When editing code in VSCode, the `pytorch_lightning/__about__.py` file keeps getting automatically updated\r\n```\r\n__version__ = \"1.5.0dev\" -> __version__ = \"20210827\"\r\n```\r\n\r\nlike in https://github.com/PyTorchLightning/pytorch-lightning/pull/8985/commits/5f4f3c5447a30dde3d78b8727a3ca432ec81131b#r697656347\r\n\r\nDoes anyone know how to stop this from happening? Thanks!Must be an installed extensionAfter a few days of my investigation, it turned out that it's not from any extensions but from our script in `.github/`:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/83ce1bf5158c4c03f7811e385624bbd903cd9c5f/.github/prepare-nightly_version.py#L13\r\n\r\nI locally confirmed that `$ pytest` in the project root directory will run all the scripts matching `*.py` except for those in the excluded directories, and thus it also runs the above file. Not sure why everyone doesn't experience this issue, but I'll submit a fix anyway.",
    "meta": {
      "name": "__about__.py \"version\" field automatically updated (unwanted behavior)"
    },
    "answer": "After a few days of my investigation, it turned out that it's not from any extensions but from our script in `.github/`:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/83ce1bf5158c4c03f7811e385624bbd903cd9c5f/.github/prepare-nightly_version.py#L13\r\n\r\nI locally confirmed that `$ pytest` in the project root directory will run all the scripts matching `*.py` except for those in the excluded directories, and thus it also runs the above file. Not sure why everyone doesn't experience this issue, but I'll submit a fix anyway."
  },
  {
    "content": "For example, part of my model's parameters are frozen, no need to train, no need to saveThis might work:\r\n```py\r\ndef on_save_checkpoint(checkpoint):\r\n    # pop the backbone here using custom logic\r\n    del checkpoint['state_dict'][backbone_keys]\r\n\r\nLitModel.load_from_checkpoint(ckpt_path, strict=False)\r\n```",
    "meta": {
      "name": "How to save/load only part of the weights in the model?"
    },
    "answer": "This might work:\r\n```py\r\ndef on_save_checkpoint(checkpoint):\r\n    # pop the backbone here using custom logic\r\n    del checkpoint['state_dict'][backbone_keys]\r\n\r\nLitModel.load_from_checkpoint(ckpt_path, strict=False)\r\n```"
  },
  {
    "content": "So I have code like this:\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x = batch['src']\r\n        y = batch['label']\r\n        mask = batch['mask']\r\n\r\n        x = self.base_model(x, mask)\r\n        x = self.linear(x).mean(axis=1).squeeze(1)\r\n        \r\n        loss = F.binary_cross_entropy_with_logits(input=x,\r\n                               target=y)\r\n        try:\r\n            auc = roc_auc_score(y_true=y.cpu().numpy(), y_score=torch.sigmoid(x).cpu().numpy())\r\n        except ValueError:\r\n            auc = 0\r\n        metrics = {'test_loss': loss, 'test_auc': auc}\r\n        self.log_dict(metrics)\r\n        return metrics\r\n\r\n\r\nI'm trying to calculate the AUC for the model over a test set,  but I assume this function is only calculating the AUC for a single batch. So I guess my question is whether the test set is treated as a single batch and if not, how would I go about doing this? Would I have to average the AUC over all the batches? If so, I'm not sure how to do that. But more importantly, that doesn't seem like the ideal way to calculate this metric to me, given how much variance there would be over each batch. averaging auc won't give the correct metric over the entire dataset plus you might get some error in case you have all targets of the same label which is possible in case of a single batch. To avoid this you can checkout https://torchmetrics.readthedocs.io/en/stable/references/modules.html#auroc. It will calculate the metrics the right way for you and you don't have to take care of averaging/accumulation or anything. There are different subpackage for each metric: module and functional. For your use-case I'd suggest using module one.Thanks @rohitgr7. I tried to implement this package as follows:\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x = batch['src']\r\n        y = batch['label']\r\n        mask = batch['mask']\r\n\r\n        x = self.base_model(x, mask)\r\n        x = self.linear(x).mean(axis=1).squeeze(1)\r\n        \r\n        loss = F.binary_cross_entropy_with_logits(input=x,\r\n                               target=y)\r\n        return {'loss': loss, 'preds': x, 'target': y}\r\n\r\n    def validation_step_end(self, outputs):\r\n        self.valid_auc(torch.sigmoid(outputs['preds']), outputs['target'].int())\r\n        self.log('valid_auc', self.valid_auc)\r\n        self.log('valid_loss', outputs['loss'])\r\n\r\nDo you think this is correct or do I also need to write explicit code for `validation_epoch_end`?",
    "meta": { "name": "How to calculate AUC over an entire test set?" },
    "answer": "averaging auc won't give the correct metric over the entire dataset plus you might get some error in case you have all targets of the same label which is possible in case of a single batch. To avoid this you can checkout https://torchmetrics.readthedocs.io/en/stable/references/modules.html#auroc. It will calculate the metrics the right way for you and you don't have to take care of averaging/accumulation or anything. There are different subpackage for each metric: module and functional. For your use-case I'd suggest using module one."
  },
  {
    "content": "(Q1) Does PyTorch Lightning enable parallelization across multiple dimension or does it only allow data parallelism?\r\nThe [FlexFlow](https://github.com/flexflow/FlexFlow) implements the parallelism across 4 different dimensions (\"SOAP\": the sample, operator, attribute and parameter dimensions). (Q2) Over which of these does PyTorch-Lightning do parallelization? (Q3) Does PyTorch-Lightning's API give an option to manually control the parallelization for each and every layer individually?Dear @roman955b,\r\n\r\n1 ) Currently, Lightning automatically implement distributed data parallelism. However, we are currently working on making manual parallelization for users who want deeper control of the parallelisation schema. \r\n\r\n2 ) Lightning supports only (S, P) with DeepSpeed, FSDP integrations.\r\n\r\n3 ) Yes, we are currently working on this. Here is an issue to track the conversation https://github.com/PyTorchLightning/pytorch-lightning/issues/9375\r\n\r\nBest,\r\nT.C",
    "meta": {
      "name": "What are ones options for manually defining the parallelization?"
    },
    "answer": "Dear @roman955b,\r\n\r\n1 ) Currently, Lightning automatically implement distributed data parallelism. However, we are currently working on making manual parallelization for users who want deeper control of the parallelisation schema. \r\n\r\n2 ) Lightning supports only (S, P) with DeepSpeed, FSDP integrations.\r\n\r\n3 ) Yes, we are currently working on this. Here is an issue to track the conversation https://github.com/PyTorchLightning/pytorch-lightning/issues/9375\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "Hey pytorch-lightning team,\r\n\r\nI am trying to build an autoencoder architecture via **pytorch-lightning** (newbie), which I observe its advantages while running complex models in comparison to **pytorch implementation**. \r\n\r\nHowever in this **AE vanilla model,** I observe **significant difference in the embedding output of pytorch-lightning** vs **pytorch** where I expect that pytorch-lightning should give the exact UMAP embedding as in pytorch \r\n\r\n### and here is an example:\r\n\r\n### Pytorch implementation\r\n\r\n```\r\n## AE Architecture\r\nclass Autoencoder(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()        \r\n        self.encoder = nn.Sequential(\r\n            #nn.Flatten(),\r\n            nn.Linear(nfeatures_rna, 200),\r\n            nn.LeakyReLU(negative_slope=0.01),\r\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\r\n            nn.Linear(200, 10), # (N, 784) -> (N, 128)\r\n            nn.Dropout(0.2)\r\n        )\r\n        \r\n        self.decoder = nn.Sequential(\r\n            nn.Linear(10, 200), \r\n            nn.LeakyReLU(negative_slope=0.01),\r\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\r\n            nn.Linear(200, nfeatures_rna), # (N, 784) -> (N, 128)\r\n        )\r\n    def forward(self, x):\r\n        x = torch.flatten(x, start_dim=1)\r\n        encoded = self.encoder(x)\r\n        decoded = self.decoder(encoded)\r\n        return decoded\r\nmodelV = Autoencoder()\r\nlr = 1e-2\r\noptimizer = torch.optim.Adam(modelV.parameters(), lr=lr)\r\nnum_epochs = 50\r\noutputs = []\r\nfor epoch in range(num_epochs):\r\n    for x, y in data_loader:\r\n        #x = x.reshape(-1, nfeatures_rna) # -> use for Autoencoder_Linear\r\n        recon = modelV(x)\r\n        \r\n        loss = criterion(recon, x)\r\n        \r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n    print(f'Epoch:{epoch+0}, Loss:{loss.item():.4f}')\r\n    outputs.append((epoch, x, recon))\r\ndef get_encodings(model, dl):\r\n    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    model.eval()\r\n    with torch.no_grad():\r\n        encodings = [model.encoder(x) for x, _ in dl]\r\n    return torch.cat(encodings, dim=0)\r\nencodings = get_encodings(modelV, data_loader)\r\nencodings = encodings.cpu().numpy()\r\nencodings.shape\r\n## UMAP\r\nimport umap\r\nembedding = umap.UMAP(random_state=0).fit_transform(encodings)\r\nplot_df = pd.DataFrame({'A' : []})\r\n#plot_df = metadata.copy()\r\nplot_df[\"UMAP1\"] = embedding[:, 0]\r\nplot_df[\"UMAP2\"] = embedding[:, 1]\r\n```\r\n\r\n### Here is the UMAP plot for pytorch embedding:\r\n\r\n\r\n<img width=\"492\" alt=\"Screenshot 2021-10-08 at 4 08 45 PM\" src=\"https://user-images.githubusercontent.com/29317258/136575302-ef4c94b5-eebf-420b-83db-fdb551c43730.png\">\r\n\r\n### Pytorch-lightning implementation for the same Pytorch AE model above\r\n\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom torch import nn\r\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\r\nlr = 1e-2\r\nclass Autoencoder(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()        \r\n        \r\n        ## Encoder Arch\r\n        \r\n        self.encoder = nn.Sequential(\r\n            # nn.Flatten(),\r\n            nn.Linear(nfeatures_rna, 200),\r\n            nn.LeakyReLU(negative_slope=0.01),\r\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\r\n            nn.Linear(200, 10), # (N, 784) -> (N, 128)\r\n            nn.Dropout(0.2)\r\n        )\r\n        \r\n        ## Decoder Arch\r\n        \r\n        self.decoder = nn.Sequential(\r\n            nn.Linear(10, 200), \r\n            nn.LeakyReLU(negative_slope=0.01),\r\n            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\r\n            nn.Linear(200, nfeatures_rna), # (N, 784) -> (N, 128)\r\n        )\r\n    \r\n    ## Traning step\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        # encode\r\n        #x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n    \r\n    \r\n        # decode\r\n        recons = self.decoder(z)\r\n        \r\n        # reconstruction #1#\r\n        reconstruction_loss_1 = nn.functional.mse_loss(x, recons)\r\n        return reconstruction_loss_1\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=lr)\r\n    \r\n## Fotward step\r\n    \r\n    def forward(self, x):\r\n        z = self.encoder(x)\r\n        recons = self.decoder(z)\r\n        embedding = z\r\n        return embedding \r\nautoencoder = Autoencoder()\r\ntrainer = pl.Trainer(gpus=1, max_epochs=50)\r\n# trainer = pl.Trainer(gpus=1)\r\ntrainer.fit(autoencoder, data_loader)\r\ndef get_encodings(model, dl):\r\n    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    model.eval()\r\n    with torch.no_grad():\r\n        encodings = [model.encoder(x) for x, _ in dl]\r\n    return torch.cat(encodings, dim=0)\r\nmodel = Autoencoder()\r\nencodings = get_encodings(model, data_loader)\r\nencodings = encodings.cpu().numpy()\r\nencodings.shape\r\nembedding = umap.UMAP(random_state=0).fit_transform(encodings) \r\nplot_df = pd.DataFrame({'A' : []})\r\nplot_df[\"UMAP1\"] = embedding[:, 0]\r\nplot_df[\"UMAP2\"] = embedding[:, 1]\r\n##\r\ncustom = sns.scatterplot(data=plot_df, x='UMAP1', \r\n                      y='UMAP2', \r\n                      #hue='Gen_and_RealCells',\r\n                      palette=\"deep\", \r\n                      #style=\"Gen_and_RealCells\"\r\n                     )\r\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\r\n```\r\n\r\n\r\n### Here is the UMAP plot for pytorch-lightning embedding :\r\n<img width=\"399\" alt=\"Screenshot 2021-10-08 at 4 18 59 PM\" src=\"https://user-images.githubusercontent.com/29317258/136575417-f96b8655-cd45-49ab-a5f7-414db3b23d5d.png\">\r\n\r\n\r\nSince I have experience with this data type and the data generative process, which give me confidence to believe in the  pytorch output\r\n\r\nAny hints or explanations would be highly appreciated! Thank you in advance for your help.\r\n\r\nBest wishes,\r\nAbdelrahman \r\n\r\nHi\r\nhere, I think you are reinitializing the model and using the one with random weights.\r\n```py\r\nmodel = Autoencoder()\r\nencodings = get_encodings(model, data_loader)\r\n```\r\n\r\nyou need to load a checkpoint here\r\n```py\r\nmodel = AutoEncoder.load_from_checkpoint(ckpt_path)\r\nencodings = get_encodings(model, data_loader)\r\n```",
    "meta": {
      "name": "pytorch-lightning output embeddings completely differnt than pytorch vanilla"
    },
    "answer": "Hi\r\nhere, I think you are reinitializing the model and using the one with random weights.\r\n```py\r\nmodel = Autoencoder()\r\nencodings = get_encodings(model, data_loader)\r\n```\r\n\r\nyou need to load a checkpoint here\r\n```py\r\nmodel = AutoEncoder.load_from_checkpoint(ckpt_path)\r\nencodings = get_encodings(model, data_loader)\r\n```"
  },
  {
    "content": "Hi!\r\nI work with cyclical learning rates, which I don't want to further reduce. So for annealing the training, I would like to increase the number of grad batches in \"accumulate_grad_batches\" every N epochs. Is there a way to do that?you can try:\r\n```py\r\nfactor = # factor by which you want to increase the accumulate_grad_batches\r\nN = \r\nmax_epochs = \r\ninit_acc_grad_batches = \r\nacc_grad_batches = {ep: init_acc_grad_batches + (i)*factor for i, ep in enumerate(range(0, max_epochs, N))}\r\ntrainer = Trainer(accumulate_grad_batches=acc_grad_batches, max_epochs=max_epochs, ...)\r\n```\r\n`accumulate_grad_batches` can take input as a Dict where the key represents the epoch where this value will be changed and the value represents the `accumulate_grad_batches` to use.\r\n\r\nSo if it's {0: 1, 3: 2, 5: 4}, then\r\nepoch [1-4) -> 1\r\nepoch [4-6) -> 3\r\nepoch [6-max_epochs] -> 4\r\n\r\nlooks like the epoch(key) here should be zero-indexed, which might be a small bug here.",
    "meta": { "name": "Increase grad_batches during training" },
    "answer": "you can try:\r\n```py\r\nfactor = # factor by which you want to increase the accumulate_grad_batches\r\nN = \r\nmax_epochs = \r\ninit_acc_grad_batches = \r\nacc_grad_batches = {ep: init_acc_grad_batches + (i)*factor for i, ep in enumerate(range(0, max_epochs, N))}\r\ntrainer = Trainer(accumulate_grad_batches=acc_grad_batches, max_epochs=max_epochs, ...)\r\n```\r\n`accumulate_grad_batches` can take input as a Dict where the key represents the epoch where this value will be changed and the value represents the `accumulate_grad_batches` to use.\r\n\r\nSo if it's {0: 1, 3: 2, 5: 4}, then\r\nepoch [1-4) -> 1\r\nepoch [4-6) -> 3\r\nepoch [6-max_epochs] -> 4\r\n\r\nlooks like the epoch(key) here should be zero-indexed, which might be a small bug here."
  },
  {
    "content": "hello,\r\n\r\nhow can i update the learning rate of adam optimizer after 10 epochs ?\r\nmy code is like this\r\n```\r\nself.lr_decay_epoch = [15,]\r\nif epoch in self.lr_decay_epoch:\r\n     self.lr = self.lr * 0.1\r\n     self.optimizer = Adam(filter(lambda p: p.requires_grad, self.net.parameters()), lr=self.lr, weight_decay=self.wd)\r\n```you can use [LambdaLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#) where lambda function can be something like:\r\n```py\r\nlambda epoch: return lr * (0.1 if epoch in self.lr_decay_epoch else 1)\r\n```",
    "meta": { "name": "Update Adam learning rate after 10 epochs" },
    "answer": "you can use [LambdaLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#) where lambda function can be something like:\r\n```py\r\nlambda epoch: return lr * (0.1 if epoch in self.lr_decay_epoch else 1)\r\n```"
  },
  {
    "content": "In a previous version of pytorch lightning I could return a dictionary in the method `validation_epoch_end` and then the content of the dictionary would automatically populate `trainer.callback_metrics`. I can then use this in my callbacks. \r\n\r\nHowever, if I try this in 1.4.8, `trainer.callback_metrics` is an empty dictionary.\r\n\r\nDo you suggest any alternative? Calling `self.log` is not an option, because it cannot log np.ndarrays:\r\n\r\n```\r\nself.log(val_output, [[array]])` was called, but `ndarray` values cannot be logged\r\n``` At the end, what I do is to place the dictionary as a new member (e.g. `self.extra_data`). Then, from the callback I can access it `pl_module.extra_data`. I guess it is not clean, but it works. ",
    "meta": { "name": "How to pass arrays to callbacks?" },
    "answer": "At the end, what I do is to place the dictionary as a new member (e.g. `self.extra_data`). Then, from the callback I can access it `pl_module.extra_data`. I guess it is not clean, but it works. "
  },
  {
    "content": "I have some data that I store on my LightningModule during validation.  I want to prevent this from being saved by the model checkpoint.  They are not actually parameters and do not affect the state at all.  I want to maintain other parts of the state, I don't want to use weights only.\r\n\r\nIs it possible to do this?Out of curiosity, what are the states? You could try removing these states within the LightningModule's `on_save_checkpoint` hookHey @jmerkow,\r\n\r\nThe checkpoint is generated from the `dump_checkpoint` function of the CheckpointConnector. One of the latest hooks called is lightning_module.on_save_checkpoint(checkpoint) here: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/connectors/checkpoint_connector.py#L386\r\n\r\nThis is actually done in-place.\r\n\r\nTherefore, you could do the following.\r\n\r\n```py\r\nclass MyModel(LightningModule):\r\n\r\n    def on_save_checkpoint(self, checkpoint):\r\n        # pop the keys you are not interested by\r\n        checkpoint\r\n```",
    "meta": {
      "name": "How to ignore certain \"parameters\" with model checkpointing?"
    },
    "answer": "Hey @jmerkow,\r\n\r\nThe checkpoint is generated from the `dump_checkpoint` function of the CheckpointConnector. One of the latest hooks called is lightning_module.on_save_checkpoint(checkpoint) here: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/connectors/checkpoint_connector.py#L386\r\n\r\nThis is actually done in-place.\r\n\r\nTherefore, you could do the following.\r\n\r\n```py\r\nclass MyModel(LightningModule):\r\n\r\n    def on_save_checkpoint(self, checkpoint):\r\n        # pop the keys you are not interested by\r\n        checkpoint\r\n```"
  },
  {
    "content": "Hi, I'm trying to use multi-node DDP, but my training job never gets past the following errors:\r\n\r\n```\r\nstore_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n1: [10/01/21 13:46:59] INFO: Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n1: [10/01/21 13:46:59] INFO: Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n```\r\n\r\nMy code works fine using DDP with multiple GPUs on a single node.\r\n\r\nI'm defining `NODE_RANK`, `WORLD_SIZE`, `MASTER_PORT`, `MASTER_ADDR` as environment variables, but I'm not defining `LOCAL_RANK`, `GROUP_RANK`, `GLOBAL_RANK` or anything else. I was assuming lightning would handle creating the processes and would then set those variables accordingly. Am I mistaken?\r\n\r\n### Edit:\r\nTo add some more details, it looks like what is happening (in some cases) is that the rank 1 node is setup before the rank 0 node and this causes issues:\r\n```\r\n1: [10/02/21 13:14:57] INFO: Node index: 1\r\n1: initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\r\n1: initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\r\n...\r\n0: [10/02/21 13:15:27] INFO: Node index: 0\r\n0: initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\r\n0: initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\r\n1: [10/02/21 13:15:29] INFO: Added key: store_based_barrier_key:1 to store for rank: 2\r\n1: [10/02/21 13:15:29] INFO: Added key: store_based_barrier_key:1 to store for rank: 3\r\n1: [10/02/21 13:15:39] INFO: Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n1: [10/02/21 13:15:39] INFO: Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n1: [10/02/21 13:15:49] INFO: Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n1: [10/02/21 13:15:49] INFO: Waiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)\r\n```\r\n\r\nHowever, I also get this error if I ensure the rank 0 node is setup first:\r\n```\r\n0: [10/02/21 15:42:10] INFO: Node index: 0\r\n0: GPU available: True, used: True\r\n0: TPU available: False, using: 0 TPU cores\r\n0: initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\r\n...\r\n0: [10/02/21 15:42:12] INFO: num_nodes_to_use: 2\r\n0: [10/02/21 15:42:12] INFO: num_gpus_to_use: 2\r\n0: [10/02/21 15:42:12] INFO: Node index: 0\r\n0: initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\r\n...\r\n1: [10/02/21 15:43:29] INFO: num_nodes_to_use: 2\r\n1: [10/02/21 15:43:29] INFO: num_gpus_to_use: 2\r\n1: [10/02/21 15:43:29] INFO: Node index: 1\r\n1: GPU available: True, used: True\r\n1: TPU available: False, using: 0 TPU cores\r\n1: initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\r\n...\r\n1: [10/02/21 15:44:22] INFO: num_nodes_to_use: 2\r\n1: [10/02/21 15:44:22] INFO: num_gpus_to_use: 2\r\n1: [10/02/21 15:44:22] INFO: Node index: 1\r\n1: initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\r\n0: [10/02/21 16:12:13] INFO: Added key: store_based_barrier_key:1 to store for rank: 0\r\n...\r\nline 199, in _env_rendezvous_handler\r\n0:     store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\r\n0: RuntimeError: connect() timed out.\r\n...\r\n0: [10/02/21 16:12:23] INFO: Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\r\n0: [10/02/21 16:12:33] INFO: Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\r\n0: [10/02/21 16:12:43] INFO: Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=1, timeout=0:30:00)\r\n...\r\nline 199, in _env_rendezvous_handler\r\n1:     store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\r\n1: RuntimeError: connect() timed out.\r\n```This ended up being an issue with the port a node communicated through was changing everytime the script ran (since DDP re-runs the entire script for each GPU). Note that the `MASTER_PORT` environment variable was only set once and remained the same, but since the node kept getting new ports allocated, the original value of `MASTER_PORT` was invalidated.\r\n\r\n### Extra info:\r\nThe SLURM cluster I use has a utility that will allocate you a port. My issue was that `ddp` will re-run the entire script, so it meant if I launched a 2 node x 4 GPU job, each of the two nodes would allocate 4 different ports for communication.\r\n\r\nI originally had:\r\n```python3\r\ndef setup_multinode():\r\n    port = allocate_port_for_node()\r\n    # Set environment variables ...\r\n\r\nif __name__ == \"__main__\":\r\n    setup_multinode()\r\n\r\n    # Initialize a trainer\r\n    trainer = pl.Trainer(\r\n        gpus=2,\r\n        num_nodes=2,\r\n        # Note that you could also set this to \"ddp_spawn\", which uses\r\n        # torch.multiprocessing.spawn, but performance is worse\r\n        accelerator=\"ddp\",\r\n        max_epochs=3,\r\n        progress_bar_refresh_rate=20,\r\n    )\r\n    \r\n    # Train the model\r\n    trainer.fit(ae, train, val)\r\n```\r\n\r\nHowever, I needed to change it so the port is only allocated once per node (not once for every GPU):\r\n```python3\r\ndef setup_multinode():\r\n    if os.environ.get(\"LOCAL_RANK\", None):\r\n        # if this node has already been setup\r\n        return\r\n    port = allocate_port_for_node()\r\n    # Set environment variables ...\r\n\r\nif __name__ == \"__main__\":\r\n    setup_multinode()\r\n\r\n    # Initialize a trainer\r\n    trainer = pl.Trainer(\r\n        gpus=2,\r\n        num_nodes=2,\r\n        # Note that you could also set this to \"ddp_spawn\", which uses\r\n        # torch.multiprocessing.spawn, but performance is worse\r\n        accelerator=\"ddp\",\r\n        max_epochs=3,\r\n        progress_bar_refresh_rate=20,\r\n    )\r\n    \r\n    # Train the model\r\n    trainer.fit(ae, train, val)\r\n```\r\n\r\nThis way each node only allocates a single port to communicate through once.\r\n",
    "meta": { "name": "DDP failing when using multiple nodes" },
    "answer": "This ended up being an issue with the port a node communicated through was changing everytime the script ran (since DDP re-runs the entire script for each GPU). Note that the `MASTER_PORT` environment variable was only set once and remained the same, but since the node kept getting new ports allocated, the original value of `MASTER_PORT` was invalidated.\r\n\r\n### Extra info:\r\nThe SLURM cluster I use has a utility that will allocate you a port. My issue was that `ddp` will re-run the entire script, so it meant if I launched a 2 node x 4 GPU job, each of the two nodes would allocate 4 different ports for communication.\r\n\r\nI originally had:\r\n```python3\r\ndef setup_multinode():\r\n    port = allocate_port_for_node()\r\n    # Set environment variables ...\r\n\r\nif __name__ == \"__main__\":\r\n    setup_multinode()\r\n\r\n    # Initialize a trainer\r\n    trainer = pl.Trainer(\r\n        gpus=2,\r\n        num_nodes=2,\r\n        # Note that you could also set this to \"ddp_spawn\", which uses\r\n        # torch.multiprocessing.spawn, but performance is worse\r\n        accelerator=\"ddp\",\r\n        max_epochs=3,\r\n        progress_bar_refresh_rate=20,\r\n    )\r\n    \r\n    # Train the model\r\n    trainer.fit(ae, train, val)\r\n```\r\n\r\nHowever, I needed to change it so the port is only allocated once per node (not once for every GPU):\r\n```python3\r\ndef setup_multinode():\r\n    if os.environ.get(\"LOCAL_RANK\", None):\r\n        # if this node has already been setup\r\n        return\r\n    port = allocate_port_for_node()\r\n    # Set environment variables ...\r\n\r\nif __name__ == \"__main__\":\r\n    setup_multinode()\r\n\r\n    # Initialize a trainer\r\n    trainer = pl.Trainer(\r\n        gpus=2,\r\n        num_nodes=2,\r\n        # Note that you could also set this to \"ddp_spawn\", which uses\r\n        # torch.multiprocessing.spawn, but performance is worse\r\n        accelerator=\"ddp\",\r\n        max_epochs=3,\r\n        progress_bar_refresh_rate=20,\r\n    )\r\n    \r\n    # Train the model\r\n    trainer.fit(ae, train, val)\r\n```\r\n\r\nThis way each node only allocates a single port to communicate through once.\r\n"
  },
  {
    "content": "In https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/predict_loop.py#L111:\r\n\r\n```py\r\n        predictions = self.trainer.accelerator.predict_step(args)\r\n\r\n        if predictions is None:\r\n            self.warning_cache.warn(\"predict returned None if it was on purpose, ignore this warning...\")\r\n\r\n        self.trainer.call_hook(\"on_predict_batch_end\", predictions, batch, batch_idx, dataloader_idx)\r\n\r\n        if self.should_store_predictions:\r\n            self.predictions[dataloader_idx].append(predictions)\r\n```\r\n\r\nShouldn't `.cpu()` be called on predictions before appending them to `self.predictions` so that predictions accumulate con CPU memory, not GPU memory?\r\n\r\nThis issue has just affected me. I had trained a model using a single GPU, it was tight but no CUDA OOM errors, then I tried getting some predictions through `pytorch_lightning.Trainer` and because of the accumulation on GPU I got those CUDA OOM.\r\n\r\nWould be nice to have a flag (`accumulate_on_cpu`?) that would accumulate predictions on CPU.This was fixed in https://github.com/PyTorchLightning/pytorch-lightning/pull/9085",
    "meta": { "name": "Shouldn't predictions accumulate on CPU, not GPU?" },
    "answer": "This was fixed in https://github.com/PyTorchLightning/pytorch-lightning/pull/9085"
  },
  {
    "content": "Hi, \r\n\r\nThe `test` method of the `Trainer` class, has the input argument `ckpt_path`. According to the docs: \r\n\r\n> ckpt_path (Optional[str]) \u2013 Either best or path to the checkpoint you wish to test. If None and the model instance was passed, use the current weights. Otherwise, the best model from the previous trainer.fit call will be loaded.\r\n\r\nAlso, in the [documentation of PyTorch Lightning for the test set](https://pytorch-lightning.readthedocs.io/en/latest/common/test_set.html), using Trainer, there is the following: \r\n\r\n```python\r\n# run full training\r\ntrainer.fit(model)\r\n\r\n# (1) load the best checkpoint automatically (lightning tracks this for you)\r\ntrainer.test(ckpt_path=\"best\")\r\n```\r\n\r\nMy question is, according to what the \"best\" checkpoint is decided? That is, is the \"best\" decided on maximising or minimising some value? What would be that value? Can someone configure the policy (i.e. minimising or maximising) and the value? How one should use this \"best\" string? \r\n\r\nLinks for reference: \r\n\r\n1. [Test set documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/test_set.html)\r\n2. [Test method of Trainer class documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#test)\r\n\r\nP.S. Please note that I'm not referring to using the `ModelChekpoint` callback, but explicitly to the above, which seems that the `ModelCheckpoint` callback is not used. it actually relies on `ModelCheckpoint` callback only https://github.com/PyTorchLightning/pytorch-lightning/blob/b3e9dff32d842431b067b0ab83e508ffe3262968/pytorch_lightning/trainer/trainer.py#L1268-L1280\r\nso if there is no `ModelCheckpoint` callback it will raise an error@rohitgr7 \r\n\r\nOne more question. Would that work when using `EarlyStopping` callback? ",
    "meta": {
      "name": "String \u201cbest\u201d at argument \u201cckpt_path\u201d for test method of Trainer class"
    },
    "answer": "it actually relies on `ModelCheckpoint` callback only https://github.com/PyTorchLightning/pytorch-lightning/blob/b3e9dff32d842431b067b0ab83e508ffe3262968/pytorch_lightning/trainer/trainer.py#L1268-L1280\r\nso if there is no `ModelCheckpoint` callback it will raise an error"
  },
  {
    "content": "I would like to provide my own learning rate scheduler. What I would love is do is doing this in the lightning style, e.g. implementing some hooks:\r\n```python\r\nclass MyScheduler(pl.LightningScheduler):\r\n    ...\r\n    def on_step(self...):\r\n        ...\r\n    def on_epoch(self...):\r\n        ...\r\n```\r\nIs something like this possible? How do other people handle custom schedulers?\r\n\r\nPS: I asked this question before on the [deprecated forum.]\r\n(https://forums.pytorchlightning.ai/t/custom-scheduler-class/1238)can you please elaborate on the usage more? like what do you want to do inside `on_step` and `on_epoch` methods? ",
    "meta": { "name": "Custom scheduler" },
    "answer": "can you please elaborate on the usage more? like what do you want to do inside `on_step` and `on_epoch` methods? "
  },
  {
    "content": "Hello,\r\n\r\nCurrently, I am working in a Lit Model, which has two encoders. Each of them has its optimizer, scheduler, and loss, as shown below:\r\n\r\n```python\r\nimport importlib\r\n\r\nimport torch\r\nfrom pytorch_lightning.core.lightning import LightningModule\r\nfrom hydra.utils import instantiate\r\n\r\nfrom source.metric.ULMRRMetric import ULMRRMetric\r\n\r\n\r\nclass LitModel(LightningModule):\r\n\r\n    def __init__(self, hparams):\r\n\r\n        super(LitModel, self).__init__()\r\n        self.save_hyperparameters(hparams)\r\n\r\n        # encoders\r\n        self.x1_encoder = instantiate(hparams.x1_encoder)\r\n        self.x2_encoder = instantiate(hparams.x2_encoder)\r\n\r\n        # loss function\r\n        self.x1_loss = instantiate(hparams.x1_loss)\r\n        self.x2_loss = instantiate(hparams.x2_loss)\r\n\r\n\r\n    def forward(self, x1, x2):\r\n        x1_repr = self.x1_encoder(x1)\r\n        x2_repr = self.x2_encoder(x2)\r\n        return x1_repr, x2_repr\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        x1_repr, x2_repr = self(x1, x2)\r\n        x1_loss=self.x1_loss(x1_repr, x2_repr)\r\n        x2_loss = self.x2_loss(x1_repr, x2_repr)\r\n\r\n        # what to return here?\r\n        return\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        x1_repr, x2_repr = self(x1, x2)\r\n        self.log(\"val_x1_LOSS\", self.x1_loss(x1_repr, x2_repr), prog_bar=True)\r\n        self.log(\"val_x2_LOSS\", self.x2_loss(x1_repr, x2_repr), prog_bar=True)\r\n\r\n\r\n\r\n\r\n    # Alternating schedule for optimizer steps\r\n    def optimizer_step(\r\n            self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure,\r\n            on_tpu=False, using_native_amp=False, using_lbfgs=False,\r\n    ):\r\n        # update x1 encoder every even step\r\n        if optimizer_idx == 0:\r\n            if batch_idx % 2 == 0:\r\n                optimizer.step(closure=optimizer_closure)\r\n\r\n        # update x2 encoder every odd step\r\n        if optimizer_idx == 1:\r\n            if batch_idx % 2 != 0:\r\n                optimizer.step(closure=optimizer_closure)\r\n\r\n\r\n\r\n    def configure_optimizers(self):\r\n        # optimizers\r\n        optimizers = [\r\n            torch.optim.AdamW(self.x1_encoder.parameters(), lr=self.hparams.lr, betas=(0.9, 0.999), eps=1e-08,\r\n                              weight_decay=self.hparams.weight_decay, amsgrad=True),\r\n\r\n            torch.optim.AdamW(self.x2_encoder.parameters(), lr=self.hparams.lr, betas=(0.9, 0.999), eps=1e-08,\r\n                              weight_decay=self.hparams.weight_decay, amsgrad=True)\r\n        ]\r\n\r\n        # schedulers\r\n        step_size_up = round(0.03 * self.num_training_steps)\r\n        schedulers = [\r\n            torch.optim.lr_scheduler.CyclicLR(optimizers[0], mode='triangular2', base_lr=self.hparams.base_lr,\r\n                                              max_lr=self.hparams.max_lr, step_size_up=step_size_up,\r\n                                              cycle_momentum=False),\r\n            torch.optim.lr_scheduler.CyclicLR(optimizers[1], mode='triangular2', base_lr=self.hparams.base_lr,\r\n                                              max_lr=self.hparams.max_lr, step_size_up=step_size_up,\r\n                                              cycle_momentum=False)\r\n        ]\r\n\r\n        return optimizers, schedulers\r\n\r\n\r\n    @property\r\n    def num_training_steps(self) -> int:\r\n        \"\"\"Total training steps inferred from datamodule and number of epochs.\"\"\"\r\n        steps_per_epochs = len(self.train_dataloader()) / self.trainer.accumulate_grad_batches\r\n        max_epochs = self.trainer.max_epochs\r\n        return steps_per_epochs * max_epochs\r\n\r\n```\r\nMy intention is to update each encoder in alternate steps (even steps: `x1_encoder`; odd steps: `x2_encoder`).\r\nAfter reading the documentation, it was not clear to me how it would be possible to update the parameters of each encoder from the loss of each one of them. For instance, I would like to update `x1_encoder`'s parameters based on the `x1_loss` value and leveraging `optimizer_1`. Respectively, I would like to update `x2_encoder`'s parameters based on the `x2_loss` value and employing `optimizer_2`.\r\n\r\nI appreciate any help you can provide.\r\nI see two ways. I think your example is quite simple so it does not matter which way you choose in the end:\r\n\r\n## 1) Automatic Optimization:\r\n```py\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        if optimizer_idx == 0;\r\n            x1_repr = self.x1_encoder(x1)\r\n            x1_loss=self.x1_loss(x1_repr, x2_repr)\r\n            return x1_loss\r\n       if optimizer_idx == 1:\r\n           x2_repr = ...\r\n           return x2_loss\r\n\r\n\r\n    def configure_optimizers(self):\r\n        return [\r\n        {\"optimizer\": torch.optim.AdamW(self.x1_encoder.parameters(), ...), \"frequency\": 1}, \r\n        {\"optimizer\": torch.optim.AdamW(self.x2_encoder.parameters(), ...), \"frequency\": 1},\r\n    ]\r\n```\r\n(and delete your overridden optimizer step method)\r\nReference: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers (see frequency description)\r\n\r\n## 2) Manual Optimization:\r\n```py\r\ndef __init__(self, hparams):\r\n\r\n    super().__init__()\r\n    self.automatic_optimization = False\r\n    ...\r\n\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        \r\n        opt0, opt1 = self.optimizers()\r\n        if batch_idx % 2 == 0:\r\n            loss = ...\r\n            opt0.zero_grad()\r\n            loss.backward()\r\n            opt0.step()\r\n        else:\r\n            loss = \r\n            opt1.zero_grad()\r\n            loss.backward()\r\n            opt1.step()\r\n```\r\nReference: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization\r\n\r\nNote: I converted this issue to a GitHub discussion article as this is the primary forum for implementation help questions :) ",
    "meta": { "name": "Optimization in a dual encoder LitModel" },
    "answer": "I see two ways. I think your example is quite simple so it does not matter which way you choose in the end:\r\n\r\n## 1) Automatic Optimization:\r\n```py\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        if optimizer_idx == 0;\r\n            x1_repr = self.x1_encoder(x1)\r\n            x1_loss=self.x1_loss(x1_repr, x2_repr)\r\n            return x1_loss\r\n       if optimizer_idx == 1:\r\n           x2_repr = ...\r\n           return x2_loss\r\n\r\n\r\n    def configure_optimizers(self):\r\n        return [\r\n        {\"optimizer\": torch.optim.AdamW(self.x1_encoder.parameters(), ...), \"frequency\": 1}, \r\n        {\"optimizer\": torch.optim.AdamW(self.x2_encoder.parameters(), ...), \"frequency\": 1},\r\n    ]\r\n```\r\n(and delete your overridden optimizer step method)\r\nReference: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers (see frequency description)\r\n\r\n## 2) Manual Optimization:\r\n```py\r\ndef __init__(self, hparams):\r\n\r\n    super().__init__()\r\n    self.automatic_optimization = False\r\n    ...\r\n\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        \r\n        opt0, opt1 = self.optimizers()\r\n        if batch_idx % 2 == 0:\r\n            loss = ...\r\n            opt0.zero_grad()\r\n            loss.backward()\r\n            opt0.step()\r\n        else:\r\n            loss = \r\n            opt1.zero_grad()\r\n            loss.backward()\r\n            opt1.step()\r\n```\r\nReference: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization\r\n\r\nNote: I converted this issue to a GitHub discussion article as this is the primary forum for implementation help questions :) "
  },
  {
    "content": "Hello! I\u2019m new to PyTorch Lightning. Recently, I am developing a PyTorch project. I want to run an open-source DeepSpeech code in Github and load its checkpoint. I clone their repository and run successfully. Then, I want to add their model to my project.\r\nIn PyTorch, this is easy. I only need to copy their model definition files and checkpoint files to my project directory. However, when I did these, I received an error when I ran load_from_checkpoint.\r\n\r\n> ModuleNotFoundError: No module named deepspeech_pytorch\r\n\r\nThen, I try to directly call `torch.load('librispeech_pretrained_v3.ckpt')`. The error is still raised.\r\nThus, I was wondering if I can load their model in my own directory without copying their whole repository?I assume you are talking about this one, right? \r\nhttps://github.com/SeanNaren/deepspeech.pytorch\r\n\r\nMaybe you are getting\r\n\r\n> ModuleNotFoundError: No module named deepspeech_pytorch\r\n\r\nbecause you didn't install that module into your environment. Follow the instructions on the README page of the repository to install the package, i.e., `pip install -e .` in the cloned repo folder. \r\n\r\nThen in your other projects you can import the deepspeech_pytorch module given that you active the same virtual environment. An so loading a model checkpoint should be able to import that too under the same conditions.\r\n\r\nHope that helps\r\n",
    "meta": { "name": "How to load a released PL model in my directory?" },
    "answer": "I assume you are talking about this one, right? \r\nhttps://github.com/SeanNaren/deepspeech.pytorch\r\n\r\nMaybe you are getting\r\n\r\n> ModuleNotFoundError: No module named deepspeech_pytorch\r\n\r\nbecause you didn't install that module into your environment. Follow the instructions on the README page of the repository to install the package, i.e., `pip install -e .` in the cloned repo folder. \r\n\r\nThen in your other projects you can import the deepspeech_pytorch module given that you active the same virtual environment. An so loading a model checkpoint should be able to import that too under the same conditions.\r\n\r\nHope that helps\r\n"
  },
  {
    "content": "Hello! I\u2019m new to PyTorch Lightning. Recently, I am developing a PyTorch project. I want to run an open-source DeepSpeech code in [Github ](https://github.com/SeanNaren/deepspeech.pytorch/)and load its checkpoint. I clone their repository and run successfully. Then, I want to add their model to my project.\r\nIn PyTorch, this is easy. I only need to copy their model definition files and checkpoint files to my project directory. However, when I did these, I received an error when I ran load_from_checkpoint.\r\n\r\n> ModuleNotFoundError: No module named deepspeech_pytorch\r\n\r\nThen, I try to directly call `torch.load('librispeech_pretrained_v3.ckpt')`. The error is still raised.\r\nThus, I was wondering if I can load their model in my own directory without copying their whole repository?Ok, I got it. It's my fault.",
    "meta": { "name": "How to load a released PL model in my directory?" },
    "answer": "Ok, I got it. It's my fault."
  },
  {
    "content": "## How to apply uniform length batching(smart batching)?  \r\n  \r\n<img src=\"https://user-images.githubusercontent.com/42150335/130908773-73f38a84-041c-4c13-b102-3dba09493785.png\" width=600>. \r\n  \r\nHi all! I have a question about applying a smart batching system like the above picture.  \r\nTo implement smart batching system, I write the code like below:\r\n  \r\n- Dataset Class\r\n\r\n```python\r\nclass ExampleDataset(Dataset):\r\n    def __init__(self, datas, tokenizer):\r\n        super(ExampleDataset, self).__init__()\r\n        self.tokenizer = tokenizer\r\n\r\n        tokenized = [self.tokenize(data) for data in tqdm(datas, desc='Tokenizing..')]\r\n        self.input_ids, self.attention_masks, self.labels = list(zip(*tokenized))\r\n\r\n    def tokenize(self, data):\r\n        encodings_dict = self.tokenizer(data)\r\n        return [\r\n            encodings_dict['input_ids'],\r\n            encodings_dict['attention_mask'],\r\n            encodings_dict['input_ids']\r\n        ]\r\n\r\n    def __len__(self):\r\n        return len(self.input_ids)\r\n\r\n    def __getitem__(self, idx):\r\n        return self.input_ids[idx], self.attention_masks[idx], self.labels[idx]\r\n```\r\n\r\n- Sampler Class\r\n\r\n```python\r\nclass SmartBatchingSampler(Sampler):\r\n    def __init__(self, data_source: torch.utils.data.Dataset, batch_size=1):\r\n        super(SmartBatchingSampler, self).__init__(data_source)\r\n        self.batch_size = batch_size\r\n        self.data_source = data_source\r\n\r\n        sentence_lengths = [len(sentence[0]) for sentence in data_source]\r\n        sentence_indices = [idx for idx in range(len(data_source))]\r\n\r\n        pack_by_length = list(zip(sentence_lengths, sentence_indices))\r\n        sort_by_length = sorted(pack_by_length)\r\n        sentence_lengths, sentence_indices = zip(*sort_by_length)\r\n\r\n        self.bins = [\r\n            sentence_indices[i: i + batch_size]\r\n            for i in range(0, len(sentence_indices), batch_size)\r\n        ]\r\n        self.bins = list(chain.from_iterable(self.bins))\r\n        self.drop_last = drop_last\r\n\r\n    def __iter__(self):\r\n        for ids in self.bins:\r\n            yield ids\r\n\r\n    def __len__(self):\r\n        return len(self.bins)\r\n\r\n    def shuffle(self, epoch):\r\n        np.random.shuffle(self.bins)\r\n```\r\n\r\n- collate_fn function\r\n\r\n```python\r\ndef collate_fn(batch):\r\n    def seq_length_(p):\r\n        return len(p[0])\r\n\r\n    max_seq_sample = max(batch, key=seq_length_)[0]\r\n    max_seq_size = len(max_seq_sample)\r\n\r\n    batch_size = len(batch)\r\n\r\n    input_ids = torch.zeros(batch_size, max_seq_size).fill_(0).long()\r\n    attention_masks = torch.zeros(batch_size, max_seq_size).fill_(0).long()\r\n    labels = torch.zeros(batch_size, max_seq_size).fill_(0).long()\r\n\r\n    for idx in range(batch_size):\r\n        sample = batch[idx]\r\n        sample_input_ids = sample[0]\r\n        sample_attention_masks = sample[1]\r\n        sample_labels = sample[2]\r\n\r\n        input_ids[idx].narrow(0, 0, len(sample_input_ids)).copy_(torch.LongTensor(sample_input_ids))\r\n        attention_masks[idx].narrow(0, 0, len(sample_attention_masks)).copy_(torch.LongTensor(sample_attention_masks))\r\n        labels[idx].narrow(0, 0, len(sample_labels)).copy_(torch.LongTensor(sample_labels))\r\n\r\n    return input_ids, attention_masks, labels\r\n```\r\n\r\n- LightningDataModule\r\n\r\n```python\r\nclass ExampleDataModule(pl.LightningDataModule):\r\n    ...\r\n    ...\r\n\r\n    def train_dataloader(self):\r\n        sampler = SmartBatchingSampler(self.dataset['train'], batch_size=self.batch_size)\r\n        return DataLoader(\r\n            dataset=self.dataset['train'],  # ExampleDataset class\r\n            sampler=sampler,\r\n            collate_fn=collate_fn,\r\n        )\r\n```\r\n\r\nI have three questions.  \r\n  \r\n1. Can I apply it like this?  If not, let me know how to apply it.\r\n2. If it is done in the same way as above, the batch size must be determined in advance. If 'auto_scale_batch_size' is performed, how can I know the determined batch size?\r\n3. If I designate SmartBatchingSampler that the batch size is 32, and 'auto_scale_batch_size' has set the batch size to 128, how does this work?\r\n  \r\nThank you.you can sort the data by `len` initially while creating the dataset itself. now just use a sequential sampler to avoid shuffle by just setting `shuffle=False` inside dataloader. collate_fn looks good, although can be optimized a little bit. apart from that even if you use `auto_scale_batch_size`, it will work just fine since your dataset will already be sorted by length.```\r\nyou can sort the data by len initially while creating the dataset itself.   \r\n```\r\n\r\nHow can I? Can you explain more?",
    "meta": { "name": "How to apply uniform length batching(smart batching)?" },
    "answer": "you can sort the data by `len` initially while creating the dataset itself. now just use a sequential sampler to avoid shuffle by just setting `shuffle=False` inside dataloader. collate_fn looks good, although can be optimized a little bit. apart from that even if you use `auto_scale_batch_size`, it will work just fine since your dataset will already be sorted by length."
  },
  {
    "content": "this is my code:\r\n\r\n```Python\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor='hmean',\r\n    mode='max',\r\n    dirpath='../weights',\r\n    filename='DB-{epoch:02d}-{hmean:.2f}',\r\n    save_last=True,\r\n    save_weights_only=True,\r\n)\r\n```\r\n\r\n```\r\ntrainer = pl.Trainer(\r\n    # open this, must drop last\r\n    benchmark=True,\r\n    checkpoint_callback=True,\r\n    gpus=[0],\r\n    max_epochs=1200,\r\n    min_epochs=300,\r\n    logger=[logger],\r\n    callbacks=[early_stop, checkpoint_callback],\r\n    resume_from_checkpoint='../weights/DB-epoch=130-hmean=0.70.ckpt'\r\n)\r\n```\r\n\r\nwhen i try resume train from checkpoint, i got this error: KeyError: 'Trying to restore training state but checkpoint contains only the model. This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.'if you set `save_weights_only=True` in `ModelCheckpoint` then it won't save optimizer/scheduler states in an ideal case. So assigning this checkpoint to resume training won't work because it needs to restore optimizer/scheduler state as well to actually resume it.",
    "meta": {
      "name": "KeyError: 'Trying to restore training state but checkpoint contains only the model. This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.'"
    },
    "answer": "if you set `save_weights_only=True` in `ModelCheckpoint` then it won't save optimizer/scheduler states in an ideal case. So assigning this checkpoint to resume training won't work because it needs to restore optimizer/scheduler state as well to actually resume it."
  },
  {
    "content": "Hi! I'm currently using Pytorch's weighted random sampler for my multi-class skewed dataset and I've put \"use_ddp_sampler\" to False.\r\nEverything works well with the custom sampler but it's taking a significantly longer time to train my model on multiple GPUs.\r\nI notice that with the default sampler my 2000 iterations scale well over 4 GPUs, 500 iterations/gpu but with the custom sampler, it doesn't scale and stays at 2000 iterations/GPU over 4 GPUs - highly likely explaining the longer training times.\r\n\r\nHow can I speed up my training procedure using a custom sampler?\r\n\r\nEdit: Trainer settings:\r\n```\r\ntrainer = pl.Trainer(   gpus                = args.gpus, \r\n                            num_nodes           = 1,\r\n                            distributed_backend = 'ddp', \r\n                            max_epochs          = args.epochs,\r\n                            weights_save_path   = args.weights_save_path, \r\n                            logger              = True,\r\n                            replace_sampler_ddp = False,\r\n                            checkpoint_callback = checkpoint_callback)\r\n```[WeightedRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler) is not a distributed sampler right?@bartmch I think @rohitgr7 is right here, the base class for both DistributedSampler and WeightedRandomSampler is Sampler but its the DistributedSampler which implements the DDP capabilities.Thanks for your replies! Two silly questions: 1) how can I implement a sampler in Lightning which is ddp and takes care of my unbalanced dataset? 2) weighted sampler is simply giving a sampling probability to each number right? Why doesn't it simply scale on multiple goud? Is it not implemented (yet) or am I understanding it wrong?All you need is a custom sampler here + set `replace_sampler_ddp=False` in `Trainer`. `WeightedRandomSampler` just uses the `weights` which you need to define to sample the batch from the dataset something similar to what boosting algorithms do while sampling. In your use case, you need some kind of `DistributedBalancedSampler` that can do either oversampling or undersampling. There are some discussions [here](https://github.com/pytorch/pytorch/issues/23430) which partially might solve your use-case in the future. But for now neither lightning nor PyTorch has this yet. But you check some custom sampler [here](https://catalyst-team.github.io/catalyst/api/data.html#samplers) that might help.@bartmch I hope the above suggestion might help.Hey @rohitgr7 apologies for the delay - I'll try the solution this week.Hi! Just found this discussion, seems a critical feature to implement directly in lightning! I'm working on my custom implementation for the moment\r\n\r\n@Borda @awaelchli  ",
    "meta": { "name": "iterations/gpu don't scale when using custom sampler" },
    "answer": "All you need is a custom sampler here + set `replace_sampler_ddp=False` in `Trainer`. `WeightedRandomSampler` just uses the `weights` which you need to define to sample the batch from the dataset something similar to what boosting algorithms do while sampling. In your use case, you need some kind of `DistributedBalancedSampler` that can do either oversampling or undersampling. There are some discussions [here](https://github.com/pytorch/pytorch/issues/23430) which partially might solve your use-case in the future. But for now neither lightning nor PyTorch has this yet. But you check some custom sampler [here](https://catalyst-team.github.io/catalyst/api/data.html#samplers) that might help."
  },
  {
    "content": "Is there a way to make lr_find not print anything while searching?Hey @grudloff,\r\n\r\nI don't believe this is supported. You could either capture the logs or contribute the feature :)\r\n\r\nBest,\r\nT.C",
    "meta": { "name": "How to have a silent lr_find()" },
    "answer": "Hey @grudloff,\r\n\r\nI don't believe this is supported. You could either capture the logs or contribute the feature :)\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "this is my val func:\r\n```Python\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        data = batch\r\n\r\n        output = self.forward(data['img'])\r\n        boxes, scores = self.postprocess(output.cpu().numpy(), batch['shape'])\r\n        raw_metric = self.metric(batch, (boxes, scores))\r\n\r\n        return raw_metric\r\n\r\n    def validation_epoch_end(self, outputs):\r\n\r\n        metric = self.metric.gather_measure(outputs)\r\n        self.log('recall', value=metric['recall'].avg)\r\n        self.log('precision', value=metric['precision'].avg)\r\n        self.log('hmean', value=metric['fmeasure'].avg)\r\n        return {'hmean': metric['fmeasure'].avg}\r\n```\r\n\r\nthis is my checkpoint callback:\r\n\r\n```Python\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor='hmean',\r\n    mode='max',\r\n    dirpath='../det_weights',\r\n    filename='{epoch:02d}-{hmean:.2f}',\r\n    save_last=True,\r\n)\r\ncheckpoint_callback.FILE_EXTENSION = '.pt'\r\n```\r\n\r\nuse this callback, i can only get a `epoch=00-hmean=0.00.pt` file, It does not record the value of hemani solvd this problem, thanks...",
    "meta": {
      "name": "Does PL support customizing indicators in checkpoint callback\uff1f"
    },
    "answer": "i solvd this problem, thanks..."
  },
  {
    "content": "Hello,\r\n\r\nI have a single GPU, but I would like to spawn multiple replicas on that single GPU and train a model with DDP. Of course, each replica would have to use a smaller batch size in order to fit in memory. (For my use case, I am not interested in having a single replica with a large batch size).\r\n\r\nI tried to pass `--gpus \"0,0\"` to the Lightning Trainer, and it managed to spawn two processes on the same GPU:\r\n```\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\r\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\r\n```\r\nBut in the end it crashed with `RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:911, invalid usage`.\r\n\r\nPlease, is there any way to split a single GPU into multiple replicas with Lightning?\r\nThanks!\r\n\r\nP.S.: Ray has a really nice support for fractional GPUs: https://docs.ray.io/en/master/using-ray-with-gpus.html#fractional-gpus. I've never used them with Lightning, but maybe it could be a workaround?Hmm interesting use case.\r\n\r\nAFAIU it is not possible, at least with `torch.distributed`. When using GPU, both the `gloo` and `nccl` backend uses https://github.com/NVIDIA/nccl under the hood, which does not support the semantic you described:\r\n\r\nFrom https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html:\r\n> Using the same CUDA device multiple times as different ranks of the same NCCL communicator is not supported and may lead to hangs.\r\n\r\nIt probably can be done if you write a custom gradient sync'ing logic, which moves gradient to RAM before sync'ing and sync them with a `gloo` process group.For reference: it seems to be possible when the backend is `gloo` instead of `nccl`. See discussion here: https://github.com/PyTorchLightning/pytorch-lightning/discussions/8630#discussioncomment-1127286.",
    "meta": { "name": "Emulating multiple devices with a single GPU" },
    "answer": "For reference: it seems to be possible when the backend is `gloo` instead of `nccl`. See discussion here: https://github.com/PyTorchLightning/pytorch-lightning/discussions/8630#discussioncomment-1127286."
  },
  {
    "content": "Hello Lightning folks!\r\n\r\nSuppose I have a base model class that I'd like to inherit from as follows:\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass ParentModel(pl.LightningModule):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.001,\r\n    ):\r\n        super(ParentModel, self).__init__()\r\n        self.lr = lr\r\n\r\nclass ChildModel(ParentModel):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.005,\r\n        loss: str = \"mse\",\r\n    ):\r\n        super(ParentModel, self).__init__()\r\n        self.lr = lr\r\n        self.loss = loss\r\n```\r\n\r\nI would like to be able to access the hyperparameters of `ChildModel` and one way to do that is by including `save_hyperparameters()` in the `__init__` as follows:\r\n\r\n```python\r\nclass ChildModel(ParentModel):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.005,\r\n        loss: str = \"mse\",\r\n    ):\r\n        super(ParentModel, self).__init__()\r\n        self.lr = lr\r\n        self.loss = loss\r\n        self.save_hyperparameters()\r\n```\r\n\r\nHowever, I would like to avoid the need to call `save_hyperparameters()` in every class that inherits from `ParentModel` and I was wondering whether it is possible to do this in PyTorch Lightning somehow? \r\n\r\nOne idea I have in mind is something like a `__post_init__`  that calls `save_hyperparameters()` after the `__init__` is called, but this doesn't seem to be supported.\r\n\r\nThanks!\r\n\r\nYou could do something like this:\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass ParentModel(pl.LightningModule):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.001,\r\n        **kwargs\r\n    ):\r\n        super(ParentModel, self).__init__()\r\n        self.save_hyperparameters()\r\n        self.lr = lr\r\n\r\nclass ChildModel(ParentModel):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.005,\r\n        loss: str = \"mse\",\r\n    ):\r\n        super(ParentModel, self).__init__(lr=lr, loss=loss)\r\n        self.loss = loss\r\n```\r\n\r\nThat would save all hparams passed to the parent model (including the ones passed through the kwargs). If you want to go one step further, you could also include the following there:\r\n```python\r\n\r\nfor k, v in kwargs.items():\r\n    setattr(self, k, v)\r\n\r\n```\r\n\r\nwhich sets all attributes that are passed through kwargs automatically as model attributes.\r\nThat means you could also spare the `self.loss=loss` line in the child model :) ",
    "meta": { "name": "Inheritance and `save_hyperparameters`" },
    "answer": "You could do something like this:\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass ParentModel(pl.LightningModule):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.001,\r\n        **kwargs\r\n    ):\r\n        super(ParentModel, self).__init__()\r\n        self.save_hyperparameters()\r\n        self.lr = lr\r\n\r\nclass ChildModel(ParentModel):\r\n    def __init__(\r\n        self,\r\n        lr: float = 0.005,\r\n        loss: str = \"mse\",\r\n    ):\r\n        super(ParentModel, self).__init__(lr=lr, loss=loss)\r\n        self.loss = loss\r\n```\r\n\r\nThat would save all hparams passed to the parent model (including the ones passed through the kwargs). If you want to go one step further, you could also include the following there:\r\n```python\r\n\r\nfor k, v in kwargs.items():\r\n    setattr(self, k, v)\r\n\r\n```\r\n\r\nwhich sets all attributes that are passed through kwargs automatically as model attributes.\r\nThat means you could also spare the `self.loss=loss` line in the child model :) "
  },
  {
    "content": "Hi, everyone!\r\nI want to load model from checkpoint when start a training, and save it to disk when finished every epoch automatically,\r\nIs there any nice way to do that correctly?\r\nShall we modify the Trianer code, or just use a special hook?\r\n\r\n- For loading the model, are you looking to load just the weights? If so, take a look at LightningModule.load_from_checkpoint: https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html#checkpoint-loading\r\n- Otherwise, if you want to load the whole training state (for example, including the optimizer states), take a look here: https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html#restoring-training-state\r\n\r\n- For saving, take a look at the ModelCheckpoint callback: https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html#automatic-saving",
    "meta": {
      "name": "How can I save and restore the trained model when I call fit() at pytorch_lightning every time?"
    },
    "answer": "- For loading the model, are you looking to load just the weights? If so, take a look at LightningModule.load_from_checkpoint: https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html#checkpoint-loading\r\n- Otherwise, if you want to load the whole training state (for example, including the optimizer states), take a look here: https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html#restoring-training-state\r\n\r\n- For saving, take a look at the ModelCheckpoint callback: https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html#automatic-saving"
  },
  {
    "content": "I want to train my model in 20000 steps and run evaluation on each 1000 steps\r\nI try to define the callback like this:\r\n```\r\nclass IntervalStepValidate(Callback):\r\n    def __init__(self, config):\r\n        self.config = config\r\n        self.total_steps = 20000\r\n        self.validation_interval = 1000\r\n\r\n    def on_batch_end(self, trainer, pl_module):\r\n        if self.total_steps % self.validation_interval == 0:\r\n            trainer.run_evaluation()\r\n```\r\nBut I find out that there is no `run_evaluation()` in the latest version of Pytorch-Lightning :(\r\nHow can I update this code to get the function I want?there is a `val_check_interval` argument for it inside `Trainer`. You can set `Trainer(val_check_interval=1000)`.",
    "meta": {
      "name": "How to define an interval validate callbacks in lightning"
    },
    "answer": "there is a `val_check_interval` argument for it inside `Trainer`. You can set `Trainer(val_check_interval=1000)`."
  },
  {
    "content": "I want to log accuracy to neptune.ai logger. I tried this way\r\n```\r\ndef test_epoch_end(self, outputs):\r\n        pred=torch.cat([x[\"pred\"] for x in outputs]).detach().cpu().numpy()\r\n        label=torch.cat([x[\"label\"] for x in outputs]).detach().cpu().numpy()\r\n        pred=np.argmax(pred,1)\r\n        acc=accuracy_score(label,pred)\r\n        print('acc',acc)\r\n        self.logger.experiment.log_metric(\"acc\", acc)\r\n```\r\nand i am getting this error\r\n```\r\n---------------------------------------------------------------------------\r\nNeptunePossibleLegacyUsageException       Traceback (most recent call last)\r\n<ipython-input-15-a549be9bb6fe> in <module>\r\n----> 1 trainer.test(model)\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in test(self, model, test_dataloaders, ckpt_path, verbose, datamodule)\r\n    577 \r\n    578         # run test\r\n--> 579         results = self._run(model)\r\n    580 \r\n    581         assert self.state.stopped\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in _run(self, model)\r\n    754 \r\n    755         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n--> 756         self.dispatch()\r\n    757 \r\n    758         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in dispatch(self)\r\n    791     def dispatch(self):\r\n    792         if self.evaluating:\r\n--> 793             self.accelerator.start_evaluating(self)\r\n    794         elif self.predicting:\r\n    795             self.accelerator.start_predicting(self)\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py in start_evaluating(self, trainer)\r\n     97 \r\n     98     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n---> 99         self.training_type_plugin.start_evaluating(trainer)\r\n    100 \r\n    101     def start_predicting(self, trainer: 'pl.Trainer') -> None:\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py in start_evaluating(self, trainer)\r\n    146     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n    147         # double dispatch to initiate the test loop\r\n--> 148         self._results = trainer.run_stage()\r\n    149 \r\n    150     def start_predicting(self, trainer: 'pl.Trainer') -> None:\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in run_stage(self)\r\n    802 \r\n    803         if self.evaluating:\r\n--> 804             return self.run_evaluate()\r\n    805         if self.predicting:\r\n    806             return self.run_predict()\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in run_evaluate(self)\r\n   1042 \r\n   1043         with self.profiler.profile(f\"run_{self.state.stage}_evaluation\"):\r\n-> 1044             eval_loop_results = self.run_evaluation()\r\n   1045 \r\n   1046         # remove the tensors from the eval results\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py in run_evaluation(self, on_epoch)\r\n    986 \r\n    987         # lightning module method\r\n--> 988         self.evaluation_loop.evaluation_epoch_end(outputs)\r\n    989 \r\n    990         # hook\r\n\r\nC:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py in evaluation_epoch_end(self, outputs)\r\n    206             if is_overridden('test_epoch_end', model=model):\r\n    207                 model._current_fx_name = 'test_epoch_end'\r\n--> 208                 model.test_epoch_end(outputs)\r\n    209 \r\n    210         else:\r\n\r\n<ipython-input-9-b08c6aa77d3f> in test_epoch_end(self, outputs)\r\n    104         acc=accuracy_score(label,pred)\r\n    105         print('acc',acc)\r\n--> 106         self.logger.experiment.log_metric(\"acc\", acc)\r\n\r\nC:\\anaconda3\\lib\\site-packages\\neptune\\new\\run.py in __getattr__(self, item)\r\n    161     def __getattr__(self, item):\r\n    162         if item in LEGACY_METHODS:\r\n--> 163             raise NeptunePossibleLegacyUsageException()\r\n    164         raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{item}'\")\r\n    165 \r\n```Dear @9157,\r\n\r\nNetpune is currently in the process of switching their client and I believe you are using their latest version while the currently Logger within Lightning users the old client.\r\nPlease, downgrade your netpune version and it should work !\r\n\r\nNote: Netpune is currently in the process of updating the logger within Lightning.\r\n\r\nBest,\r\nT.C",
    "meta": {
      "name": "NeptunePossibleLegacyUsageException  when logging accuracy"
    },
    "answer": "Dear @9157,\r\n\r\nNetpune is currently in the process of switching their client and I believe you are using their latest version while the currently Logger within Lightning users the old client.\r\nPlease, downgrade your netpune version and it should work !\r\n\r\nNote: Netpune is currently in the process of updating the logger within Lightning.\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "## \ud83d\udc1b Bug\r\n\r\nWhen the dataset size is small (i.e. comparable to the minibatch size), it slows training down significantly.\r\n\r\nNo GPU, batch size 64, dataset size 1024: 185 iterations/second\r\nNo GPU, batch size 64, dataset size 100: 47 iterations/second\r\n\r\n1 GPU, batch size 64, dataset size 1024: 110 iterations/second\r\n1 GPU, batch size 64, dataset size 100: 23 iterations/second\r\n\r\n1 GPU, batch size 800, dataset size 1024: 19 iterations/second\r\n1 GPU, batch size 800, dataset size 10000: 90 iterations/second\r\n1 GPU, batch size 64, dataset size 10000: 235 iterations/second\r\n\r\n\r\n\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n```python\r\nimport os, sys\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nfrom torch.utils.data import Dataset, DistributedSampler, DataLoader\r\n\r\nfrom pl_examples import cli_lightning_logo\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.utilities.seed import seed_everything\r\nfrom pytorch_lightning.callbacks.progress import ProgressBar, ProgressBarBase, tqdm, reset, convert_inf\r\n\r\nclass CustomProgressBar(ProgressBar):\r\n    def init_train_tqdm(self) -> tqdm:\r\n        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\r\n        bar = tqdm(\r\n            desc='Training',\r\n            initial=self.trainer.global_step,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n        )\r\n        return bar\r\n    def on_train_start(self, trainer, pl_module):\r\n        super(ProgressBar, self).on_train_start(trainer, pl_module)\r\n        self.main_progress_bar = self.init_train_tqdm()\r\n        self.prev_train_gs = -1\r\n        reset(self.main_progress_bar, self.trainer.max_steps)\r\n\r\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\r\n        super(ProgressBar, self).on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)\r\n        if self.prev_train_gs != self.trainer.global_step and self._should_update(self.trainer.global_step, self.trainer.max_steps):\r\n            self._update_bar(self.main_progress_bar)\r\n            self.main_progress_bar.set_postfix(trainer.progress_bar_dict)\r\n            self.prev_train_gs = self.trainer.global_step\r\n\r\n    def on_train_epoch_start(self, trainer, pl_module):\r\n        super(ProgressBar, self).on_train_epoch_start(trainer, pl_module)\r\n\r\n    def on_train_end(self, trainer, pl_module):\r\n        super(ProgressBar, self).on_train_end(trainer, pl_module)\r\n\r\nclass RandomDataset(Dataset):\r\n    \"\"\"\r\n    >>> RandomDataset(size=10, length=20)  # doctest: +ELLIPSIS\r\n    <...bug_report_model.RandomDataset object at ...>\r\n    \"\"\"\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    \"\"\"\r\n    >>> BoringModel()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    BoringModel(\r\n      (layer): Linear(...)\r\n    )\r\n    \"\"\"\r\n\r\n    def __init__(self, train_data, test_data, bs):\r\n        \"\"\"\r\n        Testing PL Module\r\n\r\n        Use as follows:\r\n        - subclass\r\n        - modify the behavior for what you want\r\n\r\n        class TestModel(BaseTestModel):\r\n            def training_step(...):\r\n                # do your own thing\r\n\r\n        or:\r\n\r\n        model = BaseTestModel()\r\n        model.training_epoch_end = None\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self.layer1 = torch.nn.Linear(32, 32)\r\n        self.layer2 = torch.nn.Linear(32, 32)\r\n        self.layer3 = torch.nn.Linear(32, 2)\r\n\r\n        self.train_data = train_data\r\n        self.test_data = test_data\r\n        self.bs = bs\r\n\r\n    def forward(self, x):\r\n        return self.layer3(torch.relu(self.layer2(torch.relu(self.layer1(x)))))\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self.forward(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.forward(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(list(self.layer1.parameters()) + list(self.layer2.parameters()) + list(self.layer3.parameters()), lr=0.001)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        train_loader = DataLoader(self.train_data, shuffle=True, num_workers=1, batch_size=self.bs)\r\n        return train_loader\r\n\r\nparser = ArgumentParser()\r\nparser.add_argument(\"--gpus\", type=int, default=0)\r\nparser.add_argument(\"--num_processes\", type=int, default=1)\r\nparser.add_argument(\"--dataset_size\", type=int, default=1024)\r\nparser.add_argument(\"--mb_size\", type=int, default=64)\r\nargs = parser.parse_args()\r\n\r\n\r\ndef test_run():\r\n    # data\r\n    train_data = torch.randn(args.dataset_size, 32)\r\n    test_data = torch.randn(256, 32)\r\n\r\n    # model\r\n    model = BoringModel(train_data, test_data, bs=args.mb_size)\r\n    trainer = Trainer(\r\n        gpus=args.gpus,\r\n        logger=False,\r\n        max_steps=5000,\r\n        limit_val_batches=0,\r\n        num_processes=args.num_processes,\r\n        weights_summary=None,\r\n        reload_dataloaders_every_epoch=False,\r\n        callbacks=[CustomProgressBar()]\r\n    )\r\n\r\n    # fit\r\n    trainer.fit(model)\r\n\r\n    print(f\"{trainer.accelerator_backend=}\")\r\n    print(f\"{trainer.gpus=}\")\r\n    print(f\"{trainer.num_processes=}\")\r\n    print(f\"{trainer.global_step=}\")\r\n\r\nif __name__ == \"__main__\":\r\n    test_run()\r\n```\r\n\r\n### To Reproduce\r\n\r\nRun the following command: `python bug_report.py --gpus 1 --dataset_size 10000 --mb_size 64`\r\nfor varying values of gpus, dataset_size, and mb_size.\r\n\r\n### Expected behavior\r\n\r\nIterations/second is unaffected by dataset size.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.8.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.9\r\n\r\n### Additional context\r\n\r\nMy guess is that this is caused by inter-epoch reloading of the dataset. The code should be restructured to pre-load a fixed number of minibatches ahead, rather than caring about the location of epoch boundaries.Dear @jbuckman,\r\n\r\nCould you use profiler=\"simple\" or profiler=\"advanced\" to explore the source of the problem.\r\n\r\nBest,\r\nT.CI would say that the case here is that you would need to assume always some overhead and for a small dataset the initial phase is dominant compare to the full run, you can see a parallel with a car riding 100 or 1000 meters, in both cases, you need to start from zero and as long you go you benefit from no need starting again...Okay, but, you are writing a library, right? And you want it to be efficient? Isn't the whole point of writing a library to fix these issues under the hood, so users don't have to deal with them?\r\n\r\nThis can definitely be fixed by amortizing the overhead. For example, for a dataset of size 100 with minibatches of size 10, naively you will be incurring the overhead every 10 steps, but you can instead incur overhead only every 10000 steps by pre-queuing 1000 epochs (which will easily fit in memory).\r\n\r\nI think you should reopen this issue and fix it.Hey @Borda\r\nSo in case of a small data set, each epoch is short, and thus every epoch beginning could cause a significant overhead.\r\nIs there a way to tell the trainer to \"recycle\" the dataloader and just continue training for a specified number of steps? That is, can we specify the epoch length manually, and then the dataloader would recycle batches?\r\n\r\nThanks",
    "meta": {
      "name": "Training slows down significantly for small dataset sizes"
    },
    "answer": "I would say that the case here is that you would need to assume always some overhead and for a small dataset the initial phase is dominant compare to the full run, you can see a parallel with a car riding 100 or 1000 meters, in both cases, you need to start from zero and as long you go you benefit from no need starting again..."
  },
  {
    "content": "Is there an easy way to make a [Lightning CLI](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_cli.html) work with a [Lightning Module defined by data](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_cli.html)? This seems like a very common design pattern.\r\n\r\nFor example (from the docs) it doesn't appear possible to easily convert the following to a Lightning CLI:\r\n```\r\n# init dm AND call the processing manually\r\ndm = ImagenetDataModule()\r\ndm.prepare_data()\r\ndm.setup()\r\n\r\nmodel = LitModel(out_features=dm.num_classes, img_width=dm.img_width, img_height=dm.img_height)\r\ntrainer.fit(model, dm)\r\n```\r\n\r\nThe current CLI implementation works for model re-loading. However, it requires data-dependent attributes to be specified to the config files prior to fitting, which does not seem ideal.Resolved: see https://github.com/PyTorchLightning/pytorch-lightning/issues/9473",
    "meta": {
      "name": "Lightning CLI is incompatible with models defined by data"
    },
    "answer": "Resolved: see https://github.com/PyTorchLightning/pytorch-lightning/issues/9473"
  },
  {
    "content": "I use pytorch lightning to train a model but it always strangely fail at end: After validations completed, the trainer will start an epoch that bigger that *max_epoch* and causing GPU memory allocation failure (CUDA out of memory) right after this epoch (which should not run) started. For my example, I set max_epoch=5 so there should only be epoch 0-4. But there will always be an additional epoch-5 after 5 validations are done and a few seconds later the CUDA memory error will occur.\r\n\r\nNotebook log:\r\n![log](https://user-images.githubusercontent.com/53288091/130439543-2de5205d-d11f-43b1-9f52-65837e493690.JPG)\r\n\r\nWandb system info:\r\n![image](https://user-images.githubusercontent.com/53288091/130439389-4d93caf7-e467-486e-8ef9-da24e80f569d.png)\r\n\r\nMy dataset should be fine as CUDA memory and system memory are stable during the training period, except the GPU memory surge at the very end. And here are my code for lightning module and training loop which I think may cause this trouble:\r\n\r\n```\r\nclass BaseModel(pl.LightningModule):\r\n    def __init__(self, model_name=params['model'], out_features=params['out_features'], inp_channels=params['inp_channels'], pretrained=True):\r\n        super().__init__()\r\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\r\n        \r\n        # Change output features. Input features keep the same.\r\n        if model_name == 'resnet18d':\r\n            n_features = self.model.fc.in_features\r\n            self.model.fc = nn.Linear(n_features, out_features, bias=True)\r\n            \r\n        if model_name == 'nfnet_f1':\r\n            n_features = self.model.head.fc.in_features\r\n            self.model.head.fc = nn.Linear(n_features, out_features, bias=True)\r\n            \r\n        elif model_name == 'efficientnet_b1':\r\n            n_features = self.model.classifier.in_features\r\n            self.model.classifier = nn.Linear(n_features, out_features, bias=True)\r\n            \r\n        self.criterion = nn.BCEWithLogitsLoss()\r\n        \r\n    def forward(self, x):\r\n        output = self.model(x)\r\n        return output\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        output = self.model(x)\r\n        labels = y.unsqueeze(1)\r\n        loss = self.criterion(output, labels)\r\n\r\n        try:\r\n            auc = roc_auc_score(labels.detach().cpu(), output.sigmoid().detach().cpu())\r\n            self.log('auc', auc, on_step=True, prog_bar=True, logger=True)\r\n            self.log('Train Loss', loss, on_step=True, prog_bar=True, logger=True)\r\n        except:\r\n            pass\r\n\r\n        return {'loss': loss, 'predictions': output, \"labels\": labels}\r\n\r\n    def training_epoch_end(self, outputs):\r\n        preds = []\r\n        labels = []\r\n\r\n        for output in outputs:\r\n            preds += output['predictions'].detach()\r\n            labels += output['labels'].detach()\r\n\r\n        preds = torch.stack(preds)\r\n        labels = torch.stack(labels)\r\n\r\n        train_auc = roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\r\n        self.log('mean_train_auc', train_auc, prog_bar=True, logger=True)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        output = self.model(x)\r\n        labels = y.unsqueeze(1)\r\n        loss = self.criterion(output, labels)\r\n\r\n        self.log('val_loss', loss, on_step=True, prog_bar=True, logger=True)\r\n        return {'predictions': output, 'labels': labels}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        preds = []\r\n        labels = []\r\n\r\n        for output in outputs:\r\n            preds += output['predictions'].detach()\r\n            labels += output['labels'].detach()\r\n\r\n        preds = torch.stack(preds)\r\n        labels = torch.stack(labels)\r\n\r\n        val_auc = roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\r\n        self.log('val_auc', val_auc, prog_bar=True, logger=True)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        output = self(x).sigmoid()\r\n        return output\r\n\r\n    def configure_optimizers(self):\r\n        param_optimizer = list(self.model.named_parameters())\r\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight'] # no decay\r\n        optimizer_parameters = [\r\n            {\r\n                'params': [\r\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\r\n                ], # Do not optimize no decay parameters.\r\n                'weight_decay': params['weight_decay'],\r\n            },\r\n            {\r\n                'params': [\r\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\r\n                ],\r\n                'weight_decay': 0.0,\r\n            }\r\n        ]\r\n\r\n        optimizer = FusedAdam(optimizer_parameters, lr=params['lr'])\r\n\r\n        scheduler = CosineAnnealingLR(optimizer,\r\n                                      T_max=params['T_max'],\r\n                                      eta_min=params['min_lr'],\r\n                                      last_epoch=-1)\r\n\r\n        # Give out optimizer & scheduler for pytorch lightning in python dict format.\r\n        return dict(optimizer=optimizer,\r\n                    lr_scheduler=scheduler) # lr_scheduler for scheduler.  \r\n```\r\n\r\n```\r\nkfolds = StratifiedKFold(n_splits=params['nfolds'], shuffle=True, random_state=params['seed'])\r\n\r\nmodel = BaseModel()\r\n\r\nfor fold, (trn_idx, val_idx) in enumerate(kfolds.split(train_df[\"id\"], train_df['target'])):\r\n    # Run first round.\r\n    if fold != 0:\r\n        continue\r\n    \r\n    # PL + wandb\r\n    wandb_logger = WandbLogger(project='G2Net-steady-exp',\r\n                               config=params,\r\n                               group='Effnet-CQT',\r\n                               job_type='train',\r\n                               name=f'Fold{fold}')\r\n    print(f\"{'='*20} Fold: {fold} {'='*20}\")\r\n    \r\n    # Set up data module.\r\n    train_data = train_df.loc[trn_idx]\r\n    train_sample_data = data_sample(train_data)\r\n    valid_data = train_df.loc[val_idx] # About 65k samples.\r\n    data_module = DataModule(train_sample_data,\r\n                             valid_data,\r\n                             valid_data) # No test data yet.\r\n    data_module.setup()\r\n    \r\n    # Add callbacks.\r\n    early_stopping_callback = EarlyStopping(monitor='val_auc',\r\n                                            mode='max',\r\n                                            patience=5)\r\n    checkpoint_callback = ModelCheckpoint(dirpath='./checkpoints/',\r\n                                          filename= f'fold-{fold}-best' + '-val_auc{val_auc:.3f}',\r\n                                          save_top_k=2,\r\n                                          verbose=True,\r\n                                          monitor='val_auc',\r\n                                          mode='max')\r\n    \r\n    trainer = pl.Trainer(gpus=1,\r\n                         callbacks=[early_stopping_callback,\r\n                                    checkpoint_callback],\r\n                         max_epochs=params['epochs'],\r\n                         precision=params['precision'],\r\n                         progress_bar_refresh_rate=1,\r\n                         stochastic_weight_avg=True,\r\n                         logger=wandb_logger)\r\n    \r\n    trainer.fit(model, data_module)\r\n```\r\n\r\nCan I get any clue about why this would happen and how to avoid it ? I'm new to pytorch lightning so there might be problems I'm not aware of. Thanks a lot!Dear @EMUNES,\r\n\r\nWould you mind sharing your notebook ? This would make investigation much simpler.\r\n\r\nBest,\r\nT.C \r\nI am also facing similar issue. I have 8 GB gpu, and 50 epochs, till epoch 49 my gpu usage is 4 gb, after 49 epoch, i got memory error and my gpu memory reaches to 8 gbSame thing happened with me. I thought downgrading PL version resolved the\nissue, but it didn't work in other notebook\n\nOn Wed, Sep 8, 2021, 3:18 PM EMUNES ***@***.***> wrote:\n\n> Indeed this problem is more complicated than I thought... Today one of my\n> notebook is normal but the next version throws the same error. The only\n> difference I made between those two notebooks is to reduce training samples\n> from 3000 to 1000 (I use small samples just to test whether the pipeline\n> can be working). Now I'm totally confused again...\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/PyTorchLightning/pytorch-lightning/discussions/9048#discussioncomment-1295471>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI5FYOYMI7ICGYP66IEEJMDUA42BBANCNFSM5CULZXXQ>\n> .\n>\nLet's continue discussing in https://github.com/PyTorchLightning/pytorch-lightning/issues/9441\r\n\r\nLocking this thread to avoid discussing in two places.",
    "meta": {
      "name": "Why would GPU memory always surge after training and cause CUDA memory error?"
    },
    "answer": "Let's continue discussing in https://github.com/PyTorchLightning/pytorch-lightning/issues/9441\r\n\r\nLocking this thread to avoid discussing in two places."
  },
  {
    "content": "Hello :)\r\nCurrently I use  `trainer.predict(model=..., dataloaders=...)` which returns the results of `predict_step(...)` in a list where each element in the list corresponds to one batch input to the `predict_step` function which I already implemented. I am looking for an `predict_epoch_end` kind of function to collect to batched predictions into one data structure but only found the possibility to define a callback `on_prediction_epoch_end()` but this has no return possibility. How should one best proceed to collect the batched predictions?\r\n\r\ni.e. each predict_step() returns a tensor of shape batch_size x 10\r\nand lets assume the dataloader gives 5 batches to predict. Then the `trainer.predict()` function will return a list of lenght 5 with each element beeing a tensor of shape batch_size x 10. However I would rather like to receive one tensor of shape 5*batch_size x 10. \r\n\r\nThis is just a simple example to illustrate my problem. My actual return per prediction step is a little bit more involved and I would like to clean up the structure before returning it and also make this cleanup logic part of the module so that I dont have to remember the specifics.\r\n\r\nIs there a way to do that in the current framework and if so how?\r\nThanks in advance for any help!\r\nIssue to track https://github.com/PyTorchLightning/pytorch-lightning/issues/9380",
    "meta": { "name": "How to collect batched predictions?" },
    "answer": "Issue to track https://github.com/PyTorchLightning/pytorch-lightning/issues/9380"
  },
  {
    "content": "The great advantage and convenient feature of PyTorch-Lightning over vanilla Pytorch is the more convenient interface for optimization loops.  Instead of reinventing the wheel every time with manual `train` and `test` function, one can just define\r\n`training_step` and `validation_step`, e.t.c. \r\n\r\nThe setup of the optimizer is done in the `configure_optimizers` method https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers, where one can create optimizers and schedulers.\r\n\r\nHowever, it seems, that this choice fixes the optimizer properties in the definition of the class, and often you would like to try several optimizers and change their params. One way to do it - is to pass additional arguments to the constructor of \r\n`pl.LightningModule` to be processed in `configure_optimizers`, where an appropriate optimizer will be chosen. \r\n\r\nAlso one may try to access directly the optimizers outside the class, but probably it is an unsafe approach.\r\n\r\nIs there some alternative way to reconfigure optimizer, like in `Tensorflow`, where models have `complie` method https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile?Hi @Godofnothing ,\r\n\r\nI am afraid there is no clean way to do so. What I personally do to hack around this (if I need it) is that I have arguments to the model that define what is to be returned by `configure_optimizers` and then call `self.trainer.accelerator.setup_optimizers(self.trainer)` in any of the other hooks. That makes sure `configure_optimizers` is called again and converts everything to the correct types. Inside your `configure_optimizers` function you would have to handle the creation of the optimizers for the different stages in training then.I've come up with this kind of solution (dangerous and breaking the law, but superficially woking one). Add this two methods to the `pl.LightningModule`:\r\n\r\n    @staticmethod\r\n    def _validate_compile_args(args):\r\n\r\n        def _check_single(args):\r\n            if isinstance(args, torch.optim.Optimizer):\r\n                return True\r\n            return False\r\n\r\n        def _check_optimizer_list(args):\r\n            if isinstance(args, list):\r\n                for arg in args:\r\n                    if not _check_single(arg):\r\n                        return False\r\n            return True\r\n\r\n        def _check_dict(args):\r\n            if isinstance(args, dict):\r\n                assert args.get(\"optimizer\"), \"optimizer has to be specified\"\r\n                assert isinstance(args[\"optimizer\"], torch.optim.optimizer.Optimizer)\r\n            return True\r\n\r\n        def _check_tuple_of_dicts(args):\r\n            if isinstance(args, tuple):\r\n                for arg in args:\r\n                    if not _check_dict(arg):\r\n                        return False\r\n            return True\r\n\r\n        def _check_none(args):\r\n            if not args:\r\n                return True\r\n\r\n        if _check_single(args):\r\n            return\r\n        if _check_optimizer_list(args):\r\n            return\r\n        if _check_dict(args):\r\n            return\r\n        if _check_tuple_of_dicts(args):\r\n            return\r\n        if _check_none(args):\r\n            return\r\n    \r\n        raise ValueError(\"Unsupported configuration of optimizers\")\r\n\r\n    def compile(self, *args):\r\n        self._validate_compile_args(args)\r\n\r\n        def _configure_optimizers(self):\r\n            return args\r\n\r\n        self.configure_optimizers = MethodType(_configure_optimizers, self)\r\n        self.configure_optimizers()\r\n\r\nAnd the example of the use case:\r\n\r\n```\r\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\r\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max')\r\n\r\nmodel.compile({\r\n    \"optimizer\" : optimizer,\r\n    \"lr_scheduler\" : {\r\n        \"scheduler\" : scheduler,\r\n        \"monitor\" : \"val/loss\"\r\n    }\r\n})\r\n```",
    "meta": { "name": "How to redefine optimizer for pl.LightningModule?" },
    "answer": "Hi @Godofnothing ,\r\n\r\nI am afraid there is no clean way to do so. What I personally do to hack around this (if I need it) is that I have arguments to the model that define what is to be returned by `configure_optimizers` and then call `self.trainer.accelerator.setup_optimizers(self.trainer)` in any of the other hooks. That makes sure `configure_optimizers` is called again and converts everything to the correct types. Inside your `configure_optimizers` function you would have to handle the creation of the optimizers for the different stages in training then."
  },
  {
    "content": "Hey,\r\nI am not sure if I can currently keep only the best and latest models as a wandb artifact using the `WandbLogger`? That is, I am looking for a behavior similar to `log_model='all'`, but which keeps only the latest and best models and deletes previous checkpoints from the wandb artifacts of the experiment.\r\nMy checkpoints weigh about 1GB and I don't want to keep the entire history of checkpoints with the `log_model='all'` flag, but rather only the best and the latest models. I thought about inheriting from the `WandbLogger` and following the guideline here:\r\nhttps://gitbook-docs.wandb.ai/guides/artifacts/api#cleaning-up-unused-versions\r\n\r\nAny thoughts?\r\nMaybe this should be a feature request?Hey @ohayonguy \r\n\r\nGiven the docs of WandbLogger I think you can just set `log_model=True`. This will then upload the checkpoints at the end of training. And if you have set `save_top_k=n` it will only upload the best n ones.",
    "meta": {
      "name": "Keep only the best and the latest artifacts in wandb logger"
    },
    "answer": "Hey @ohayonguy \r\n\r\nGiven the docs of WandbLogger I think you can just set `log_model=True`. This will then upload the checkpoints at the end of training. And if you have set `save_top_k=n` it will only upload the best n ones."
  },
  {
    "content": "Hello,\r\n\r\nI have been trying to debug OOM after a few iterations on my model.\r\nAfter investigation I saw that in loops/batch/training_batch_loop.py there is this piece of code (L235):\r\n`result = self.training_step_and_backward(split_batch, batch_idx, opt_idx, optimizer, hiddens)`\r\n`  if result is not None:`\r\n`     return_result.update(result)`\r\n`     return return_result.loss`\r\n     \r\nWhat I see is that return_result will keep the computation graph as it contains the loss. So I wonder what is the role of this variable? Also, where is the graph released, I could no go any further than \"_training_step_and_backward_closure\"? I don't understand why my model runs fine for a few iterations then there is some increase in memory.Hi @Jovp \r\n\r\nAre you running on master? We are currently updating our internals regarding the loops there. If not could you give master a try?\r\n\r\nBest,\r\nJustus\r\n\r\ncc @awaelchli who designed the loops/closuresHello Justus!\r\n\r\nIndeed I have seen during my investigations that master changed quite a lot for that part of the code.\r\n\r\nFor the specific concern that I described see:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/issues/9343 and\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/pull/9336\r\n\r\npull request 9336 seems to address exactly the issue that I was referring to.\r\n\r\nMany thanks to the team for the great responsiveness and great work!\r\nJulien",
    "meta": { "name": "return_result role in training_batch_loop.py" },
    "answer": "Hello Justus!\r\n\r\nIndeed I have seen during my investigations that master changed quite a lot for that part of the code.\r\n\r\nFor the specific concern that I described see:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/issues/9343 and\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/pull/9336\r\n\r\npull request 9336 seems to address exactly the issue that I was referring to.\r\n\r\nMany thanks to the team for the great responsiveness and great work!\r\nJulien"
  },
  {
    "content": "Hi,\r\n\r\nDo I need to manually set the random seed to ensure the synchronization of state (e.g. initialized parameters) across processes when using distributed training?No, parameters get broadcast on the first time the model performs forward. This is implemented in the PyTorch [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel) wrapper and so applies to Lightning as well. ",
    "meta": { "name": "Setting random seed before training" },
    "answer": "No, parameters get broadcast on the first time the model performs forward. This is implemented in the PyTorch [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel) wrapper and so applies to Lightning as well. "
  },
  {
    "content": "Hello all - just wanted to discuss a use-case with CPU vs GPU PL install. We do normal training on GPUs, but when deploying for prediction we use CPUs and would like to keep the Docker container size as small as possible. It't not clear if it's possible to install pytorch-lightning with CPU-only torch distribution, which is much smaller. Is there any possible equivalent to `pip install pytorch-lightning[cpu]` . Thanks for suggestions! Hey, if you install pytorch first (cpu only) and then Lightning it will just use that version. ",
    "meta": { "name": "Pytorch-lightning CPU-only installation" },
    "answer": "Hey, if you install pytorch first (cpu only) and then Lightning it will just use that version. "
  },
  {
    "content": "I am defining a simple multi-class BERT classification model and then training it using pytorch-lightning. The code is in https://colab.research.google.com/drive/1os9mz7w7gmLBL_ZDvZ9K1saz9UA3rmD7?usp=sharing under class BertForMulticlassSequenceClassification(BertPreTrainedModel). The issue is that after training when I am loading the classifier model model = ClassTaggerModel.load_from_checkpoint(checkpoint_file) I get\r\n\r\n```\r\nSome weights of BertForMulticlassSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifiers.0.weight', 'classifiers.1.bias', 'classifiers.0.bias', 'classifiers.1.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n```\r\n\r\nThe reason is probably because `pl.LightningModule` module has transformer's `from_pretrained` function that would normally downloads weights from huggingface. This is undesirable behaviour when loading from the trained checkpoint. Is there any feature in Pytroch-lightning that can help having different logic for these two cases (training vs loading). Thanks!It's because lightning instantiates the LightningModel and then loads the weights using load_from_checkpoint and since you have HFModel.from_pretrained in the init it will load the pretrained weights every time. There is a way around for this.\r\n\r\n```python\r\nclass HFLightningModule(LightningModule):\r\n    def __init__(self, ..., model_name=None)\r\n        if model_name is not None:\r\n            self.model = HFModel.from_pretrained(model_name, ...)\r\n        else:\r\n            self.model = HFModel(config, num_classes)\r\n\r\n\r\nmodel = HFLightningModule(..., model_name='bert-base-cased')\r\ntrainer.fit(model, ...)\r\n\r\nmodel = HFLightningModule.load_from_checkpoint(...)\r\n```\r\nAlthough there might be a better solution.",
    "meta": {
      "name": "Loading from checkpoints re-downloads pre-trained BERT model"
    },
    "answer": "It's because lightning instantiates the LightningModel and then loads the weights using load_from_checkpoint and since you have HFModel.from_pretrained in the init it will load the pretrained weights every time. There is a way around for this.\r\n\r\n```python\r\nclass HFLightningModule(LightningModule):\r\n    def __init__(self, ..., model_name=None)\r\n        if model_name is not None:\r\n            self.model = HFModel.from_pretrained(model_name, ...)\r\n        else:\r\n            self.model = HFModel(config, num_classes)\r\n\r\n\r\nmodel = HFLightningModule(..., model_name='bert-base-cased')\r\ntrainer.fit(model, ...)\r\n\r\nmodel = HFLightningModule.load_from_checkpoint(...)\r\n```\r\nAlthough there might be a better solution."
  },
  {
    "content": "> https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html?highlight=on_epoch%20#pytorch_lightning.core.lightning.LightningModule.log.params.on_epoch\r\n\r\nI'm using horovod to train the model. I wonder if `on_step` and `on_epoch` average the metrics across all GPUs automatically. In other words, do we need to explicitly average the metrics in functions like `training_epoch_end` and `validation_epoch_end`?\r\nDear @marsggbo,\r\n\r\nWhen using self.log(..., on_step=True), this will compute the metric per step locally as synchronisation adds performance hit. \r\nWhen using self.log(..., on_step=True, sync_dist=True), this will compute the metric per step across GPUS.\r\nWhen using self.log(..., on_epoch=True), this will compute the metrics across GPUS and epoch batches automatically.\r\n  \r\nBest,\r\nT.C",
    "meta": {
      "name": "What's the difference between `on_step` and `on_epoch` of `pl_module.log`"
    },
    "answer": "Dear @marsggbo,\r\n\r\nWhen using self.log(..., on_step=True), this will compute the metric per step locally as synchronisation adds performance hit. \r\nWhen using self.log(..., on_step=True, sync_dist=True), this will compute the metric per step across GPUS.\r\nWhen using self.log(..., on_epoch=True), this will compute the metrics across GPUS and epoch batches automatically.\r\n  \r\nBest,\r\nT.C"
  },
  {
    "content": " `self.log(on_epoch=True)` computes the metrics across GPUS and epoch batches automatically. How can we get the value of it when training in distributed mode?It should be available in the `trainer.callback_metrics` dictionary",
    "meta": { "name": "Can we get the value of `self.log()`?" },
    "answer": "It should be available in the `trainer.callback_metrics` dictionary"
  },
  {
    "content": "I'm currently running a lot of experiments and in order to track all of them in tensorboard I have to rename each experiment folder by hand (e.g. lightning_logs/version_0 -> lightning_logs/{unique_informative_exp_name}).\r\nHowever, it would be better if I could pass the as an argument to Trainer.\r\nI searched documentation thoroughly, but couldn't find anything related to names of experiments (I found only default_root_dir argument, however, it's not that interesting)\r\n\r\nThe question is follows :\r\nIs there a way to set experiment name on program level? I'm using default logger. You can pass Trainer a custom logger with the version specified.\r\n\r\n```python\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\n\r\nlogger = TensorBoardLogger(\"default_root_dir\", version=\"your_version\", name=\"my_model\")\r\ntrainer = Trainer(logger=logger)\r\n```\r\nHere is the api of [TensorBoardLogger](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.TensorBoardLogger.html#pytorch_lightning.loggers.TensorBoardLogger)",
    "meta": {
      "name": "How to set experiment name such that it can be some unique name instead of version_0, ... etc."
    },
    "answer": "You can pass Trainer a custom logger with the version specified.\r\n\r\n```python\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\n\r\nlogger = TensorBoardLogger(\"default_root_dir\", version=\"your_version\", name=\"my_model\")\r\ntrainer = Trainer(logger=logger)\r\n```\r\nHere is the api of [TensorBoardLogger](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.TensorBoardLogger.html#pytorch_lightning.loggers.TensorBoardLogger)"
  },
  {
    "content": "I wonder how people do Hyperparameter Tuning with Lightning CLI?\r\nAny suggestion of good practices?\r\n\r\nThanks!Personally when I tune hyperparameters (e.g. with optuna or nevergrad), I don't use the Lightning CLI much but use the programmatic way to inject the arguments there (since it's easier for communication across different python libs directly in python and not leaving it to os calls).Dear @tshu-w,\r\n\r\nI was a bit confused with your question. Are you talking using `Trainer.tune` or doing HPO of some parameters.\r\n\r\nIn the later case, here is something you could try out. \r\n\r\n```py\r\nfrom pytorch_lightning.utilities.cli import LightningCLI\r\nfrom unittest import mock\r\nimport optuna\r\n\r\nconfig_path = ...\r\n\r\nclass MyModel(LightningModule):\r\n\r\n    def __init__(self, num_layers):\r\n        ...\r\n\r\ndef objective(trial):\r\n    num_layers = trial.suggest_uniform('num_layers', 10, 100)\r\n\r\n    with mock.patch(\"sys.argv\", [\"any.py\", \"--config\", str(config_path), \"--model.num_layers\", str(num_layers)]):\r\n        cli = LightningCLI(MyModel, MyDataModule)\r\n\r\n    return cli.trainer.model_checkpoint.best_score\r\n\r\nstudy = optuna.create_study()\r\nstudy.optimize(objective, n_trials=100)\r\nstudy.best_params\r\n```\r\n\r\n@carmocca  Any thoughts ?",
    "meta": { "name": "Hyperparameter Tuning in Lightning CLI" },
    "answer": "Personally when I tune hyperparameters (e.g. with optuna or nevergrad), I don't use the Lightning CLI much but use the programmatic way to inject the arguments there (since it's easier for communication across different python libs directly in python and not leaving it to os calls)."
  },
  {
    "content": "I want to apply custom learning rate scheduler like below.  \r\n\r\n```python\r\nclass WarmupLRScheduler(torch.optim.lr_scheduler._LRScheduler):\r\n    \"\"\"\r\n    Warmup learning rate until `total_steps`\r\n\r\n    Args:\r\n        optimizer (Optimizer): wrapped optimizer.\r\n        configs (DictConfig): configuration set.\r\n    \"\"\"\r\n    def __init__(\r\n            self,\r\n            optimizer: Optimizer,\r\n            configs: DictConfig,\r\n    ) -> None:\r\n        super(WarmupLRScheduler, self).__init__(optimizer, configs.lr_scheduler.init_lr)\r\n        if configs.lr_scheduler.warmup_steps != 0:\r\n            warmup_rate = configs.lr_scheduler.peak_lr - configs.lr_scheduler.init_lr\r\n            self.warmup_rate = warmup_rate / configs.lr_scheduler.warmup_steps\r\n        else:\r\n            self.warmup_rate = 0\r\n        self.update_steps = 1\r\n        self.lr = configs.lr_scheduler.init_lr\r\n        self.warmup_steps = configs.lr_scheduler.warmup_steps\r\n\r\n    def step(self, val_loss: Optional[torch.FloatTensor] = None):\r\n        if self.update_steps < self.warmup_steps:\r\n            lr = self.init_lr + self.warmup_rate * self.update_steps\r\n            self.set_lr(self.optimizer, lr)\r\n            self.lr = lr\r\n        self.update_steps += 1\r\n        return self.lr\r\n```\r\n\r\nBut I find that my custom lr schedulers doesn't work in pytorch lightning.  \r\nI set lightning module's `configure_optimizers` like below:\r\n\r\n```python\r\ndef configure_optimizers(self):\r\n    r\"\"\"\r\n    Choose what optimizers and learning-rate schedulers to use in your optimization.\r\n\r\n\r\n    Returns:\r\n        - **Dictionary** - The first item has multiple optimizers, and the second has multiple LR schedulers\r\n            (or multiple ``lr_dict``).\r\n    \"\"\"\r\n    SUPPORTED_OPTIMIZERS = {\r\n        \"adam\": Adam,\r\n        \"adamp\": AdamP,\r\n        \"radam\": RAdam,\r\n        \"adagrad\": Adagrad,\r\n        \"adadelta\": Adadelta,\r\n        \"adamax\": Adamax,\r\n        \"adamw\": AdamW,\r\n        \"sgd\": SGD,\r\n        \"asgd\": ASGD,\r\n        \"novograd\": Novograd,\r\n    }\r\n\r\n    assert self.configs.model.optimizer in SUPPORTED_OPTIMIZERS.keys(), \\\r\n        f\"Unsupported Optimizer: {self.configs.model.optimizer}\\n\" \\\r\n        f\"Supported Optimizers: {SUPPORTED_OPTIMIZERS.keys()}\"\r\n\r\n    self.optimizer = SUPPORTED_OPTIMIZERS[self.configs.model.optimizer](\r\n        self.parameters(),\r\n        lr=self.configs.lr_scheduler.lr,\r\n    )\r\n    scheduler = SCHEDULER_REGISTRY[self.configs.lr_scheduler.scheduler_name](self.optimizer, self.configs)\r\n\r\n    if self.configs.lr_scheduler.scheduler_name == \"reduce_lr_on_plateau\":\r\n        lr_scheduler = {\r\n            'scheduler': scheduler,\r\n            'monitor': 'val_loss',\r\n            'interval': 'epoch',\r\n        }\r\n    elif self.configs.lr_scheduler.scheduler_name == \"warmup_reduce_lr_on_plateau\":\r\n        lr_scheduler = {\r\n            'scheduler': scheduler,\r\n            'monitor': 'val_loss',\r\n            'interval': 'step',\r\n        }\r\n    else:\r\n        lr_scheduler = {\r\n            'scheduler': scheduler,\r\n            'interval': 'step',\r\n        }\r\n\r\n    return {\r\n        'optimizer': self.optimizer,\r\n        'lr_scheduler': lr_scheduler\r\n    }\r\n``` \r\n\r\nIf you fine some weird, please let me know. Thank you.Hi @sooftware, can you share the error or unexpected behaviour you are getting with this configuration?\r\n\r\nThanks :smiley:Dear @sooftware,\r\nThis looks like a bug. I created an issue from this discussion. Would you mind creating a reproducible script using the `BoringModel`?\r\nDear @sooftware,\r\n\r\nAny updates ? We solved some bugs related monitoring.\r\n\r\nBest,\r\nT.CHey @sooftware,\r\n\r\nAwesome work !\r\n\r\nBy the way, do you know about Lightning Flash: https://github.com/PyTorchLightning/lightning-flash.\r\n\r\nI believe there could be some cool synergies between your framework and Flash.\r\n\r\nBest,\r\nT.C",
    "meta": { "name": "I want to apply custom learning rate scheduler." },
    "answer": "Dear @sooftware,\r\n\r\nAny updates ? We solved some bugs related monitoring.\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "Hi, I am working on building a question and answering model using T5(Huggingface) with PyTorch Lightning Module and I am checking my loss and PyTorch Lightning Loss is not being matched.\r\n\r\n```\r\nclass UQAFineTuneModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model = T5ForConditionalGeneration.from_pretrained(\r\n            \"allenai/unifiedqa-t5-small\", return_dict=True\r\n        )\r\n        self.model.train()\r\n    def forward(\r\n        self,\r\n        source_text_input_ids,\r\n        source_text_attention_mask,\r\n        target_text_input_ids=None,\r\n    ):\r\n        output = self.model(\r\n            input_ids=source_text_input_ids,\r\n            attention_mask=source_text_attention_mask,\r\n            labels=target_text_input_ids,\r\n        )\r\n        return output.loss, output.logits\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        source_text_input_ids = batch[\"source_text_input_ids\"]\r\n        source_text_attention_mask = batch[\"source_text_attention_mask\"]\r\n        target_text_input_ids = batch[\"target_text_input_ids\"]\r\n        # labels_attention_mask = batch[\"target_text_attention_mask\"]\r\n        loss, outputs = self(\r\n            source_text_input_ids, source_text_attention_mask, target_text_input_ids\r\n        ) \r\n        loss_mine = None  \r\n        output = self.model(\r\n            input_ids=source_text_input_ids,\r\n            attention_mask=source_text_attention_mask,\r\n            labels=target_text_input_ids,\r\n        ) \r\n        labels = batch[\"target_text_input_ids\"].clone() \r\n        labels[labels == 0] = -100 \r\n        if target_text_input_ids is not None:  \r\n            loss_fct = CrossEntropyLoss(ignore_index=-100) \r\n            loss_mine = loss_fct(output.logits.view(-1, outputs.size(-1)), labels.view(-1)) \r\n            print(f\"loss_Hugginface: {loss.item()}, loss_mine : {loss_mine.item()}\") \r\n        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\r\n        return {\"loss\": loss, \"predictions\": outputs}\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/81796368/131071640-14d4dade-c9ec-4c6a-8ec3-d04f0ed5411c.png)\r\nYou can see the above image, why loss is not same, help is very much needed, I asked the same question on HuggingFace but they told me to ask here, you can view that discussion here.\r\nHey @ayush714,\r\n\r\nI believe this is related to HF and you might get your answers by opening an issue on their repo directly.\r\n\r\nBest,\r\nT.C",
    "meta": {
      "name": "Cross Entropy Loss and loss of PyTorch Lightning does not matches"
    },
    "answer": "Hey @ayush714,\r\n\r\nI believe this is related to HF and you might get your answers by opening an issue on their repo directly.\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "Hi there, \r\nI'm trying to run pytorch-lightning training with deepspeed plugin and activation checkpoints to support bigger batch sizes, based on https://pytorch-lightning.readthedocs.io/en/stable/advanced/advanced_gpu.html#deepspeed-activation-checkpointing. \r\nAs specified in the docs, running the model should be done using the checkpoint function. However, this function seems to return a tensor without gradients. When computing loss based on this value and returning from `training_step`, I'm getting an error\r\n`RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn`\r\n\r\nMinimal code to reproduce\r\n```python\r\nimport os\r\n\r\nimport deepspeed\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom deepspeed.ops.adam import FusedAdam\r\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\r\nfrom torch import nn\r\nfrom pytorch_lightning.utilities.types import STEP_OUTPUT\r\nfrom torch.utils.data import DataLoader, RandomSampler\r\n\r\n\r\nclass PlModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model = nn.Linear(1, 1)\r\n\r\n    def forward(self, batch):\r\n        return self.model(batch)\r\n\r\n    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\r\n        res = deepspeed.checkpointing.checkpoint(self.model, batch)\r\n        return nn.MSELoss()(res, torch.zeros_like(res, device=res.device))\r\n\r\n    def configure_optimizers(self):\r\n        return FusedAdam(self.parameters(), lr=0.1)\r\n\r\n\r\nif __name__ == '__main__':\r\n    trainer = pl.Trainer(gpus=-1, precision=16, plugins=DeepSpeedPlugin(stage=3, partition_activations=True))\r\n    model = PlModel()\r\n    dataset = torch.rand(100, 1)\r\n    dl = torch.utils.data.DataLoader(dataset, batch_size=1, num_workers=os.cpu_count(),\r\n                                     sampler=RandomSampler(dataset))\r\n    trainer.fit(model, dl)\r\n```\r\n\r\npytorch-lightning version: 1.3.3\r\ndeepspeed version: 0.5.0\r\nThanks!@SeanNaren Thanks @nachshonc!\r\n\r\nI've managed to reproduce the same case without Deepspeed using `torch.utils.checkpoint` and our bug report model:\r\n\r\n```python\r\nimport deepspeed\r\nimport torch\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return torch.utils.checkpoint.checkpoint(self.layer, x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        max_epochs=1,\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nI think the issue arises from the fact that the entire model's activations have been removed, with the input tensors not requiring any gradients, thus the autograd engine not being able to infer any gradients.\r\n\r\nFor activation checkpointing, it only makes sense to include it if you have intermediate layers which can create expensive activations. For example, swap the model out to look like this:\r\n\r\n```python\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer_h = torch.nn.Linear(32, 32)\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        x = torch.utils.checkpoint.checkpoint(self.layer_h, x)\r\n        return self.layer(x)\r\n```\r\n\r\nActivation checkpointing just means on the backwards, we'll need to re-compute the activations (unless you do CPU checkpointing with Deepspeed or something, where activations are just transferred to the CPU memory). In this case, there is no point checkpointing the final layer, as the final layer will instantly need to be re-computed.\r\n\r\n```python\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer_h = torch.nn.Linear(32, 32)\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        x = self.layer_h(x)\r\n        return torch.utils.checkpoint.checkpoint(self.layer, x) # no point doing this!\r\n```\r\n\r\nWe should definitely make the docs clearer for this, I'll make this an issue :)\r\n\r\n",
    "meta": { "name": "DeepSpeedPlugin with activation checkpoint fails" },
    "answer": "Thanks @nachshonc!\r\n\r\nI've managed to reproduce the same case without Deepspeed using `torch.utils.checkpoint` and our bug report model:\r\n\r\n```python\r\nimport deepspeed\r\nimport torch\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return torch.utils.checkpoint.checkpoint(self.layer, x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        max_epochs=1,\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nI think the issue arises from the fact that the entire model's activations have been removed, with the input tensors not requiring any gradients, thus the autograd engine not being able to infer any gradients.\r\n\r\nFor activation checkpointing, it only makes sense to include it if you have intermediate layers which can create expensive activations. For example, swap the model out to look like this:\r\n\r\n```python\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer_h = torch.nn.Linear(32, 32)\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        x = torch.utils.checkpoint.checkpoint(self.layer_h, x)\r\n        return self.layer(x)\r\n```\r\n\r\nActivation checkpointing just means on the backwards, we'll need to re-compute the activations (unless you do CPU checkpointing with Deepspeed or something, where activations are just transferred to the CPU memory). In this case, there is no point checkpointing the final layer, as the final layer will instantly need to be re-computed.\r\n\r\n```python\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer_h = torch.nn.Linear(32, 32)\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        x = self.layer_h(x)\r\n        return torch.utils.checkpoint.checkpoint(self.layer, x) # no point doing this!\r\n```\r\n\r\nWe should definitely make the docs clearer for this, I'll make this an issue :)\r\n\r\n"
  },
  {
    "content": "Hi. I have a function which generates a set of random values (hyperparameters) which are then used to create my model. I want to run this function only once, then use it to create my model and then start ddp training on this model.\r\n\r\nHowever, with the current setup, when I start ddp, the randomize function gets called again, so now I have 2 GPU processes, each having initialized the model with different set of hyperparameters. (random values from both calls aren't same)\r\n\r\nIf I add `if os.getenv(\"LOCAL_RANK\",0):` before my randomize function, then there is no way for the second GPU process to access the hyperparameters generated by the first GPU process. How do I go about this ? Thanks.If that isn't to costly to generate, I'd recommend to generate them at every process and then use [ddp broadcasting](https://pytorch.org/docs/stable/distributed.html#collective-functions) to overwrite the values with the ones from the main process (src=0)Hey @Gateway2745,\r\n\r\nHere is an example where I am broadcasting the current checkpoint tmpdir to all processes: https://github.com/PyTorchLightning/pytorch-lightning/blob/522df2b89b35c050b14bb5e9c2ba2c3d1d20ea67/tests/core/test_metric_result_integration.py#L468\r\n\r\nBest,\r\nT.CHello @tchaton. Thank you for the helpful example. I have a quick question. Since I would need to broadcast my hyperparameters (present in a python dictionary) before I even create my model, where would be the best place to do this? I guess I would need to do it before `trainer.fit` since it takes the model as a parameter but the process group is also initiated after this call only. How would I broadcast my parameters before this call? Thank you! Also cc @justusschock :)Hey @Gateway2745,\r\n\r\nYou could do this\r\n\r\n```py\r\nfrom pytorch_lightning.utilities.cli import LightningCLI\r\nfrom unittest import mock\r\nimport optuna\r\n\r\nconfig_path = ...\r\n\r\nclass MyModel(LightningModule):\r\n\r\n    def __init__(self, num_layers):\r\n        ...\r\n\r\ndef objective(trial):\r\n    num_layers = trial.suggest_uniform('num_layers', 10, 100)\r\n\r\n    with mock.patch(\"sys.argv\", [\"any.py\", \"--config\", str(config_path), \"--trainer.accelerator\", \"ddp_spawn\", \"--trainer.gpu\", \"2\", \"--model.num_layers\", str(num_layers)]):\r\n        cli = LightningCLI(MyModel, MyDataModule)\r\n\r\n    return cli.trainer.model_checkpoint.best_score\r\n\r\nstudy = optuna.create_study()\r\nstudy.optimize(objective, n_trials=100)\r\nstudy.best_params\r\n```",
    "meta": {
      "name": "Run specific code only once (which generates randomized values) before starting DDP"
    },
    "answer": "Hey @Gateway2745,\r\n\r\nYou could do this\r\n\r\n```py\r\nfrom pytorch_lightning.utilities.cli import LightningCLI\r\nfrom unittest import mock\r\nimport optuna\r\n\r\nconfig_path = ...\r\n\r\nclass MyModel(LightningModule):\r\n\r\n    def __init__(self, num_layers):\r\n        ...\r\n\r\ndef objective(trial):\r\n    num_layers = trial.suggest_uniform('num_layers', 10, 100)\r\n\r\n    with mock.patch(\"sys.argv\", [\"any.py\", \"--config\", str(config_path), \"--trainer.accelerator\", \"ddp_spawn\", \"--trainer.gpu\", \"2\", \"--model.num_layers\", str(num_layers)]):\r\n        cli = LightningCLI(MyModel, MyDataModule)\r\n\r\n    return cli.trainer.model_checkpoint.best_score\r\n\r\nstudy = optuna.create_study()\r\nstudy.optimize(objective, n_trials=100)\r\nstudy.best_params\r\n```"
  },
  {
    "content": "Hi . I want to calculate the inference time of my model. I am not sure where to the code for measuring the time. I thought its better to do it inside `predict_step` of the `LightningModule`. Something like this \r\n\r\n```\r\nclass LitModule(pl.LightningModule):\r\n        def __init__(self, hparams):\r\n               ....\r\n               self.starter, self.ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\r\n               .....\r\n        ......\r\n        ......\r\n         def predict_step(self, batch, batch_idx, dataloader_idx=None):\r\n                 torch.cuda.empty_cache()\r\n                 minibatch_model_in, _ = batch\r\n                 self.starter.record()\r\n                 _ = self(minibatch_model_in)\r\n                 self.ender.record()\r\n                 # wait for gpu sync\r\n                 torch.cuda.synchronize()\r\n                 inference_time = self.starter.elapsed_time(self.ender)\r\n                 return inference_time*1e-3\r\n```\r\nAnd in the main funtion,\r\n```\r\ninference_metrics = trainer.predict(model=pl_model, datamodule=pl_data)\r\n```\r\nAfter removing the initial measurements (considering GPU warm-up) and taking mean of 200 samples, I get `0.0196` seconds.\r\n\r\nIf I do the measurement outside the LightningModule then I get a different value. This is how I measured \r\n```\r\n# Inferencing\r\npl_data.setup()\r\n# Initializing cuda event loggers\r\nstarter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\r\nmetrics = []\r\npl_model.eval()\r\nwith torch.no_grad():\r\nfor batch in pl_data.train_dataloader():\r\n      torch.cuda.empty_cache()\r\n      minibatch_model_in, _, _, _, _, _ = batch\r\n      starter.record()\r\n        _, _, _ = pl_model(minibatch_model_in)\r\n      # wait for gpu sync\r\n       torch.cuda.synchronize()\r\n       curr_time = starter.elapsed_time(ender)\r\n       metrics.append(curr_time)\r\n```\r\nUsing the above code, I get `0.027` seconds. Please someone tell me if I am doing something wrong. Which method is correct? Why is there a decrease in the inference time when measured within `predict_step`?Hi, the first snippet measures raw prediction time, whereas the second one doesn't seem to be on gpu at all (e.g. your data likely isn't on gpu) or (if it is) can already include the data preparation and host to device transfers.Dear @karthi0804,\r\n\r\nYou could do get the profiling automatically.\r\n\r\n```py\r\ntrainer = Trainer(profiler=\"simple\")\r\ntrainer.predict(model=pl_model, datamodule=pl_data)\r\n```\r\n\r\nFrom `pytorch_lightning/loops/epoch/prediction_epoch_loop.py`\r\n```py\r\n        with self.trainer.profiler.profile(\"predict_batch_to_device\"):\r\n            batch = self.trainer.accelerator.batch_to_device(batch, dataloader_idx=dataloader_idx)\r\n\r\n        self.batch_progress.increment_ready()\r\n\r\n        with self.trainer.profiler.profile(\"predict_step\"):\r\n            self._predict_step(batch, batch_idx, dataloader_idx)\r\n```",
    "meta": { "name": "Calculation of inference time" },
    "answer": "Hi, the first snippet measures raw prediction time, whereas the second one doesn't seem to be on gpu at all (e.g. your data likely isn't on gpu) or (if it is) can already include the data preparation and host to device transfers."
  },
  {
    "content": "I get the below error immediately when I set a CUDA seed or after a few epochs without seed and with val_workers>0. I also find that I get the error when I try reading from a pickled file containing PyTorch tensors (on cpu).\r\n\r\nI have a small dataset, so I load all the data in the `__init__` of my `Dataset` class. I then save it on my disk using pickle so I can save on dataloading time when I run my code again. Now, since I have 2 GPUs, DDP in pytorch-lightning starts 2 processes and each of these processes start reading from the pickle file. Both the training data and validation data are being read from pickle files. \r\n\r\nMy error is quite similar to the the comment mentioned here - https://github.com/pytorch/pytorch/issues/28950#issuecomment-694938684. Note that both the person who commented and I, have `pin_memory=False` although the title says otherwise. \r\n\r\nAny ideas as to why this is happening will surely help me! Thank you.\r\n\r\nPL version = 1.4.2 , torch version = '1.9.0+cu102', CUDA version = 10.2\r\n\r\n```\r\nValidation sanity check: 0it [00:00, ?it/s]/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val \r\ndataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this m\r\nachine) in the `DataLoader` init to improve performance.                                                                                                                      \r\n  rank_zero_warn(                                                                                                                                                             \r\nValidation sanity check:   0%|                                                                                                                          | 0/1 [00:00<?, ?it/s]\r\n/home/usr/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subjec\r\nt to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)                 \r\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)                                                                                           \r\n/home/usr/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subjec\r\nt to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)                 \r\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)                                                                                           \r\nGlobal seed set to 42                                                                                                                                                         \r\nGlobal seed set to 42                                                                                                                                                         \r\nEpoch 0:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                     | 4/5 [00:14<00:02,  2.80s/it, loss=4.33, v_num=d09et\r\nerminate called after throwing an instance of 'c10::CUDAError'                                                                                          | 0/1 [00:00<?, ?it/s]\r\n  what():  CUDA error: initialization error                                                                                                                                   \r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.                                                        \r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.                                                                                                                        \r\nException raised from insert_events at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1089 (most recent call first):                                                              \r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x2b5f7135ca22 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)               \r\nframe #1: <unknown function> + 0x10d7e (0x2b5f710ecd7e in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)                                        \r\nframe #2: c10::cuda::CUDACachingAllocator::raw_delete(void*) + 0x1a7 (0x2b5f710ee027 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)          \r\nframe #3: c10::TensorImpl::release_resources() + 0x54 (0x2b5f713465a4 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)                              \r\nframe #4: <unknown function> + 0xa27e1a (0x2b5f1a569e1a in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libtorch_python.so)                                   \r\n<omitting python frames>                                                                                                                                                      \r\n                                                                                                                                                                              \r\nterminate called after throwing an instance of 'c10::CUDAError'                                                                                                               \r\n  what():  CUDA error: initialization error                                                                                                                                   \r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.                                                        \r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.                                                                                                                        \r\nException raised from insert_events at /pytorch/c10/cuda/CUDACachingAllocator.cpp:1089 (most recent call first):                                                              \r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x2b4b41756a22 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)               \r\nframe #1: <unknown function> + 0x10d7e (0x2b4b414e6d7e in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)                                        \r\nframe #2: c10::cuda::CUDACachingAllocator::raw_delete(void*) + 0x1a7 (0x2b4b414e8027 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)          \r\nframe #3: c10::TensorImpl::release_resources() + 0x54 (0x2b4b417405a4 in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libc10.so)                              \r\nframe #4: <unknown function> + 0xa27e1a (0x2b4aea963e1a in /home/usr/pytorch/lib/python3.8/site-packages/torch/lib/libtorch_python.so)                                   \r\n<omitting python frames>                                                                                                                                                      \r\n                                                                                                                                                                              \r\nTraceback (most recent call last):                                                     \r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 990, in _try_get_data\r\nTraceback (most recent call last):                                             \r\nFile \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 990, in _try_get_data\r\n    data = self._data_queue.get(timeout=timeout)\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 107, in get\r\n    data = self._data_queue.get(timeout=timeout)\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/queues.py\", line 107, in get\r\n    if not self._poll(timeout):\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 257, in poll\r\n    if not self._poll(timeout):\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 257, in poll\r\n    return self._poll(timeout)\r\n    return self._poll(timeout)\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 424, in _poll\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 424, in _poll\r\n    r = wait([self], timeout)\r\n    r = wait([self], timeout)\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\r\n    ready = selector.select(timeout)\r\n    ready = selector.select(timeout)\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/selectors.py\", line 415, in select\r\n  File \"/server/easybuild/software/2020/avx2/Core/python/3.8.10/lib/python3.8/selectors.py\", line 415, in select\r\n    fd_event_list = self._selector.poll(timeout)\r\n    fd_event_list = self._selector.poll(timeout)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3404) is killed by signal: Aborted. \r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/usr/mymodel/run.py\", line 22, in <module>\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3407) is killed by signal: Aborted. \r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/usr/mymodel/run.py\", line 22, in <module>\r\n    main()\r\n  File \"/home/usr/mymodel/run.py\", line 18, in main\r\n   main()\r\n  File \"/home/usr/mymodel/run.py\", line 18, in main\r\n    return train(CFG)\r\n  File \"/scratch/usr/mymodel/src/train.py\", line 110, in train\r\n    return train(CFG)\r\n  File \"/scratch/usr/mymodel/src/train.py\", line 110, in train\r\n    trainer.fit(model,dm)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 553, in fit\r\n    trainer.fit(model,dm)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 553, in fit\r\n    self._run(model)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in _run\r\n    self._run(model)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in _run\r\n    self._dispatch()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _dispatch\r\n    self._dispatch()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\r\n    self.accelerator.start_training(self)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\r\n    return self._run_train()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in start_training\r\n    self.fit_loop.run()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self._results = trainer.run_stage()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\r\n    return self._run_train()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\r\n    epoch_output = self.epoch_loop.run(train_dataloader)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 112, in run\r\n    self.on_advance_end()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 177, in on_advance_end\r\n\r\n   self.fit_loop.run()                                                                                                                                                       \r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self._run_validation()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 256, in _run_validation\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\r\n    epoch_output = self.epoch_loop.run(train_dataloader)\r\n    self.val_loop.run()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 112, in run\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\r\n    self.on_advance_end()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 177, in on_advance_end\r\n    dl_outputs = self.epoch_loop.run(\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self._run_validation()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 256, in _run_validation\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 93, in advance\r\n    self.val_loop.run()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    batch_idx, batch = next(dataloader_iter)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\r\n    dl_outputs = self.epoch_loop.run(\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    data = self._next_data()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1186, in _next_data\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 93, in advance\r\n    batch_idx, batch = next(dataloader_iter)\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\r\n    idx, data = self._get_data()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1152, in _get_data\r\n    data = self._next_data()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1186, in _next_data\r\n    success, data = self._try_get_data()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1003, in _try_get_data\r\n    idx, data = self._get_data()\r\n  File \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1152, in _get_data\r\n    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\r\nRuntimeError: DataLoader worker (pid(s) 3404) exited unexpectedly\r\n    success, data = self._try_get_data()\r\nile \"/home/usr/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1003, in _try_get_data\r\n    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\r\nRuntimeError: DataLoader worker (pid(s) 3407) exited unexpectedly                          \r\n```                                                                      Same as https://github.com/PyTorchLightning/pytorch-lightning/issues/8821.",
    "meta": {
      "name": "Validation crashes when setting seed or with val_num_workers > 0 with CUDA initialization error"
    },
    "answer": "Same as https://github.com/PyTorchLightning/pytorch-lightning/issues/8821."
  },
  {
    "content": "Hi. I have a `ModelCheckpoint` callback where I am monitoring the `val_acc` (validation set accuracy) with `mode=max`. After training my model using `trainer.fit`, I want to get the maximum value of `val_acc`. How can I get this value without re-evaluating my best model on the validation dataset? Thanks.Can be accessed with checkpoint_callback.best_model_score.",
    "meta": {
      "name": "Get best metrics after call to trainer.fit without evaluating best model on val set again"
    },
    "answer": "Can be accessed with checkpoint_callback.best_model_score."
  },
  {
    "content": "The decorator here is leaky: https://github.com/PyTorchLightning/pytorch-lightning/blob/963c26764682fa4cf64c93c5a7572ae0040e9c32/pytorch_lightning/core/decorators.py#L73-L108\r\n\r\nIt assumes it is called with the `model_to_device` method, and that `self` has access to `model` which implements `on_post_move_to_device` and has `parameters` defined.  \r\n\r\nThe hook here: https://github.com/PyTorchLightning/pytorch-lightning/blob/963c26764682fa4cf64c93c5a7572ae0040e9c32/pytorch_lightning/core/hooks.py#L341-L355\r\n\r\nis only called by the TPU backend. Is it intended to be called by other plugins? \r\n\r\nSince this is under the `core/decorators.py`, one might assume that this is more general. Should this be an implementation detail of the TPU plugins instead? @kaushikb11 This hook is intended only for TPU as TPU don't support tying parameters on cpu.\r\n\r\nHowever, I believe we could actually depreciate it once https://github.com/PyTorchLightning/pytorch-lightning/issues/8555 is implemented.\r\n\r\n",
    "meta": { "name": "on_post_move_to_device is leaky" },
    "answer": "This hook is intended only for TPU as TPU don't support tying parameters on cpu.\r\n\r\nHowever, I believe we could actually depreciate it once https://github.com/PyTorchLightning/pytorch-lightning/issues/8555 is implemented.\r\n\r\n"
  },
  {
    "content": "I want to add noise to the gradients in pytorch lightning . Specifically, something similar to this paper: https://arxiv.org/pdf/1511.06807.pdf . Basically, I would compute the gradients and before the call to backward, i want to add noise.\r\n\r\nWhat is the best way to achieve this in pytorch lightning?Does the `on_before_backward` hook work for your use case? https://github.com/PyTorchLightning/pytorch-lightning/blob/24f0124ddd478e18b76d64a79f1d7b341a0f8646/pytorch_lightning/core/hooks.py#L298-L305\r\n\r\nYou can override this in your LightningModule\r\n\r\nIf you need absolute control over the optimization process (forward, backward, optimizer steps), you can use manual optimization: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimizationDear @sebastiangonsal,\r\n\r\nFrom your paper, it seems you might want to call backward which computes the gradients and then add some noise right ?\r\n\r\nTherefore, you could override the `before_optimizer_step` and add noise to all the params.grad.\r\n\r\n```\r\nclass TestModel(LightningModule):\r\n\r\n    def on_before_optimizer_step(self):\r\n        for param in self.parameters():\r\n            param.grad += torch.rand_like(param.grad)\r\n```\r\n\r\nIf you want this re-usable, you could just move this to a callback instead.",
    "meta": { "name": "Changing the computed gradients before backprop" },
    "answer": "Dear @sebastiangonsal,\r\n\r\nFrom your paper, it seems you might want to call backward which computes the gradients and then add some noise right ?\r\n\r\nTherefore, you could override the `before_optimizer_step` and add noise to all the params.grad.\r\n\r\n```\r\nclass TestModel(LightningModule):\r\n\r\n    def on_before_optimizer_step(self):\r\n        for param in self.parameters():\r\n            param.grad += torch.rand_like(param.grad)\r\n```\r\n\r\nIf you want this re-usable, you could just move this to a callback instead."
  },
  {
    "content": "Say I have a lightning model `model = MyLightningModel()` that contains an object that gets created and updated throughout training `model.my_object`. Upon loading the model from the checkpoint `MyLightningModel.load_from_checkpoint(ckpt_path)` I'm noticing the `model.my_object` attribute is not being saved and restored upon loading from checkpoint.\r\n\r\nIs it possible to somehow ensure that these model attributes get saved in the checkpoint file and properly restored when loading the model from checkpoint? \r\n\r\nThanks in advance for your help! Dear @KirillShmilovich.\r\n\r\nYou could use the LightningModule `on_save_checkpoint` and `on_load_checkpoint` hooks.\r\n\r\n```py\r\nclass MyLightningModel(LightningModule):\r\n\r\n    def on_save_checkpoint(self):\r\n        return {\"my_object\": self.my_object}\r\n\r\n    def on_load_checkpoint(self, state_dict):\r\n        self.my_object = state_dict[\"my_object\"]\r\n```\r\n\r\nHowever, pickling objets isn't always the best approach. A slightly better approach is\r\n\r\n```py\r\nclass MyLightningModel(LightningModule):\r\n\r\n    def on_save_checkpoint(self):\r\n        return {\"my_object_state_dict\": self.my_object.state_dict()}\r\n\r\n    def on_load_checkpoint(self, state_dict):\r\n        self.my_object = my_object_cls.from_state_dict(state_dict[\"my_object_state_dict\"])\r\n```",
    "meta": {
      "name": "How to ensure objects saved as model attributes are saved in the checkpoint file?"
    },
    "answer": "Dear @KirillShmilovich.\r\n\r\nYou could use the LightningModule `on_save_checkpoint` and `on_load_checkpoint` hooks.\r\n\r\n```py\r\nclass MyLightningModel(LightningModule):\r\n\r\n    def on_save_checkpoint(self):\r\n        return {\"my_object\": self.my_object}\r\n\r\n    def on_load_checkpoint(self, state_dict):\r\n        self.my_object = state_dict[\"my_object\"]\r\n```\r\n\r\nHowever, pickling objets isn't always the best approach. A slightly better approach is\r\n\r\n```py\r\nclass MyLightningModel(LightningModule):\r\n\r\n    def on_save_checkpoint(self):\r\n        return {\"my_object_state_dict\": self.my_object.state_dict()}\r\n\r\n    def on_load_checkpoint(self, state_dict):\r\n        self.my_object = my_object_cls.from_state_dict(state_dict[\"my_object_state_dict\"])\r\n```"
  },
  {
    "content": "I got this question from someone around installing Lightning with conda\r\n\r\nRunning both `conda install -c conda-forge pytorch-lightning`  and `conda update pytorch-lightning` resulted in the version 0.8.5 being used, instead of the latest (as of now, 1.4.2). https://anaconda.org/conda-forge/pytorch-lightning\r\nI'm curious if anyone else has seen this issue. @Borda ? \r\nConda-forge does not have any latest pytorch (the last is 1.1) which in leads installing pytorch-lightning 0.8 as it was last supporting this PT version... You need to add channel pytorch\r\n`conda install -c pytorch -c conda-forge pytorch-lightning`",
    "meta": { "name": "Installation with conda?" },
    "answer": "Conda-forge does not have any latest pytorch (the last is 1.1) which in leads installing pytorch-lightning 0.8 as it was last supporting this PT version... You need to add channel pytorch\r\n`conda install -c pytorch -c conda-forge pytorch-lightning`"
  },
  {
    "content": "I followed the steps from the documentation when it comes to returning a dictionary of DataLoaders. The problems occurs in the training_step method. I expect the batch parameter to be a dictionary, but in reality it is a string, the first key of the expected dictionary.\r\n\r\n```\r\ndef train_dataloader(self):\r\n    dl1 =  DataLoader(IconDataset(self.train_icons_path),batch_size=128,\r\n              collate_fn=self.triplet_mining.batch_hard_negatives,num_workers=5)\r\n    dl2 = DataLoader(IconDataset(self.train_icons_path),batch_size=128,\r\n              collate_fn=self.triplet_mining.batch_semi_hard_negative,num_workers=5)\r\n    return {\r\n            \"hard\": dl1,\r\n            \"semi-hard\": dl2 }\r\n```\r\n\r\nIn training_step\r\n```\r\ndef training_step(self,batch,batch_idx):\r\n    print(type(batch))   # str\r\n    print(batch)         # hard\r\n```\r\n\r\nI don't know if there is a problem with my collate_fn method. The batching was working all right when one single DataLoader was used.It's so funny how fast one can find the answer to his own question. The problem was related to the validation DataLoaders. I was trying to return a dictionary in a similar manner to the training_dataloader, but the validation loop works with a sequence of DataLoaders. There is a similar question answered here: https://github.com/PyTorchLightning/pytorch-lightning/discussions/8623",
    "meta": {
      "name": "Problem with dictionary of DataLoaders in training_step."
    },
    "answer": "It's so funny how fast one can find the answer to his own question. The problem was related to the validation DataLoaders. I was trying to return a dictionary in a similar manner to the training_dataloader, but the validation loop works with a sequence of DataLoaders. There is a similar question answered here: https://github.com/PyTorchLightning/pytorch-lightning/discussions/8623"
  },
  {
    "content": "Here is DataReader\r\n```\r\nclass DataReader(torch.utils.data.Dataset):\r\n  def __init__(self, df):\r\n        super(DataReader,self).__init__()\r\n        self.df = df\r\n  def __len__(self):\r\n        'Denotes the total number of samples'\r\n        return len(self.df)\r\n\r\n  def __getitem__(self, index):\r\n        'Generates one sample of data'\r\n        # Select sample\r\n        file = self.df.iloc[index,0]\r\n        label=self.df.iloc[index,1]\r\n        return data,label\r\n```\r\nwhen i do \r\n```\r\nx=DataLoader(DataReader(train), batch_size = 2,collate_fn=my_collate)\r\nb=next(iter(x))\r\n```  \r\nit worked. but when I call it inside LightningModule, it throws an error\r\n\r\n```\r\nclass OurModel(LightningModule):\r\n  def __init__(self):\r\n    super(OurModel,self).__init__()\r\n    self.model =cnnmodel()\r\n  def forward(self,x):\r\n    x= self.model(x)\r\n    return x\r\n\r\n  def configure_optimizers(self):\r\n    return torch.optim.AdamW(params=self.parameters(),lr=self.lr )\r\n\r\n  def train_dataloader(self):\r\n    return DataLoader(DataReader(train))\r\n\r\n  def training_step(self,batch,batch_idx):\r\n    return loss\r\n\r\n  def val_dataloader(self):\r\n    ds= DataLoader(DataReader(val))\r\n    print('ds',len(ds))\r\n    return\r\n    \r\n  def validation_step(self,batch,batch_idx):\r\n    print('val step')\r\n    return \r\n```\r\nI am unable to figure out what is the error and how to resolve it.I tried to debug it, val_dataloader function is working, it print `ds 7` but validation_step is not working, it should print val step, but its not printing it\r\n\r\n**EDIT**\r\nThis issue is because of `numworker`, setting numworker to 0, resolve the issue, but I am getting this warning \r\n` Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine)`\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nAttributeError: Can't get attribute 'DataReader' on <module '__main__' (built-in)>\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nAttributeError: Can't get attribute 'DataReader' on <module '__main__' (built-in)>\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nAttributeError: Can't get attribute 'DataReader' on <module '__main__' (built-in)>\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nAttributeError: Can't get attribute 'DataReader' on <module '__main__' (built-in)>\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 872, in _try_get_data\r\n    data = self._data_queue.get(timeout=timeout)\r\n\r\n  File \"C:\\Anaconda3\\lib\\multiprocessing\\queues.py\", line 108, in get\r\n    raise Empty\r\n\r\nEmpty\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-9-5ebd04eb49d1>\", line 101, in <module>\r\n    trainer.fit(model)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 458, in fit\r\n    self._run(model)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 756, in _run\r\n    self.dispatch()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 797, in dispatch\r\n    self.accelerator.start_training(self)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\", line 96, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\", line 144, in start_training\r\n    self._results = trainer.run_stage()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 807, in run_stage\r\n    return self.run_train()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 842, in run_train\r\n    self.run_sanity_check(self.lightning_module)\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1107, in run_sanity_check\r\n    self.run_evaluation()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 949, in run_evaluation\r\n    for batch_idx, batch in enumerate(dataloader):\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 435, in __next__\r\n    data = self._next_data()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1068, in _next_data\r\n    idx, data = self._get_data()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1034, in _get_data\r\n    success, data = self._try_get_data()\r\n\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 885, in _try_get_data\r\n    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\r\n\r\nRuntimeError: DataLoader worker (pid(s) 4984, 20904) exited unexpectedly\r\n```You can disable that warning with the following\r\n\r\n```python\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\", message=\".*does not have many workers which may be a bottleneck.*\")\r\n```do i need to write whole code under `if __name__ ==  \"__main__\"` or just\n`trainer.fit(model)`\n\nOn Mon, Aug 16, 2021 at 2:00 PM thomas chaton ***@***.***>\nwrote:\n\n> Did you try to add\n>\n> if *name* == \"*main*\"\n>\n> to your script ?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/PyTorchLightning/pytorch-lightning/discussions/8898#discussioncomment-1188890>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI5FYO3X4RDTD7LMWYTWCBTT5DHTXANCNFSM5CEXIGTA>\n> .\n>\n",
    "meta": {
      "name": "AttributeError: Can't get attribute 'DataReader' on <module '__main__' (built-in)>"
    },
    "answer": "do i need to write whole code under `if __name__ ==  \"__main__\"` or just\n`trainer.fit(model)`\n\nOn Mon, Aug 16, 2021 at 2:00 PM thomas chaton ***@***.***>\nwrote:\n\n> Did you try to add\n>\n> if *name* == \"*main*\"\n>\n> to your script ?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/PyTorchLightning/pytorch-lightning/discussions/8898#discussioncomment-1188890>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI5FYO3X4RDTD7LMWYTWCBTT5DHTXANCNFSM5CEXIGTA>\n> .\n>\n"
  },
  {
    "content": "Hi! \r\n\r\nI'm a bit confused regarding the trainer's flags. According to Lightning's [documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#amp-level), the default settings are:\r\n- `amp_level='O2'`  (i.e., \"Almost FP16\" Mixed Precision, cf. Nvidia [documentation](https://nvidia.github.io/apex/amp.html#opt-levels))\r\n- `precision=32`\r\n\r\nIsn't it a bit contradictory ? Is the default training mode full precision or mixed precision? \r\n\r\nThanks in advance for your clarification :)Dear @dianemarquette,\r\n\r\n`precision=32` is the default one. However, if you turn on `precision=16` and set `amp_backend=\"apex\"`, then `amp_level=02` is considered as the default one.\r\n\r\nBest,\r\nT.C",
    "meta": { "name": "Trainer flags: amp_level vs. precision" },
    "answer": "Dear @dianemarquette,\r\n\r\n`precision=32` is the default one. However, if you turn on `precision=16` and set `amp_backend=\"apex\"`, then `amp_level=02` is considered as the default one.\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "I am getting this weird error `TypeError: 'Subset' object is not callable`\r\n```\r\nclass OurModel(LightningModule):\r\n    def __init__(self):\r\n        super(OurModel,self).__init__()\r\n        #architecute\r\n        self.model =  timm.create_model('efficientnetv2_rw_s', pretrained=True)\r\n        self.dataset=torchvision.datasets.ImageFolder('./PSL DATA SET')\r\n\r\n        self.train, self.val, self.test = torch.utils.data.random_split(self.dataset, [1009, 250, 250])\r\n\r\n    def forward(self,x):\r\n        x= self.model(x)\r\n        return x\r\n\r\n    def val_dataloader(self):\r\n        print('val_dataloader')\r\n        ds=DataLoader(MyLazyDataset(self.val,aug), batch_size = self.batch_size,num_workers=self.numworker, shuffle=False)\r\n        print(len(ds))\r\n        return ds\r\n\r\n    def validation_step(self,batch,batch_idx):\r\n        print('validation step')\r\n        image,label=batch\r\n        out=self(image).squeeze(1)\r\n        loss=self.criterion(out,label.float())\r\n        self.log('val_loss', loss)#use by early stopping\r\n        return loss\r\n```\r\nprint statement shows `len(ds)` but it does not print 'validation step', so data from `val_dataloader` is not moving to `validation_step`\r\n\r\nComplete logs\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-8edf32f6d94d> in <module>\r\n      8 #trainer.tune(model)\r\n      9 \r\n---> 10 trainer.fit(model)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    458         )\r\n    459 \r\n--> 460         self._run(model)\r\n    461 \r\n    462         assert self.state.stopped\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\r\n    756 \r\n    757         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n--> 758         self.dispatch()\r\n    759 \r\n    760         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n    797             self.accelerator.start_predicting(self)\r\n    798         else:\r\n--> 799             self.accelerator.start_training(self)\r\n    800 \r\n    801     def run_stage(self):\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n     94 \r\n     95     def start_training(self, trainer: 'pl.Trainer') -> None:\r\n---> 96         self.training_type_plugin.start_training(trainer)\r\n     97 \r\n     98     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\r\n    142     def start_training(self, trainer: 'pl.Trainer') -> None:\r\n    143         # double dispatch to initiate the training loop\r\n--> 144         self._results = trainer.run_stage()\r\n    145 \r\n    146     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\r\n    807         if self.predicting:\r\n    808             return self.run_predict()\r\n--> 809         return self.run_train()\r\n    810 \r\n    811     def _pre_training_routine(self):\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n    842             self.progress_bar_callback.disable()\r\n    843 \r\n--> 844         self.run_sanity_check(self.lightning_module)\r\n    845 \r\n    846         self.checkpoint_connector.has_trained = False\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_sanity_check(self, ref_model)\r\n   1110 \r\n   1111             # run eval step\r\n-> 1112             self.run_evaluation()\r\n   1113 \r\n   1114             self.on_sanity_check_end()\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_evaluation(self, on_epoch)\r\n    930 \r\n    931         # enable eval mode + no grads\r\n--> 932         self.evaluation_loop.on_evaluation_model_eval()\r\n    933         # ref model\r\n    934         model = self.lightning_module\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py in on_evaluation_model_eval(self)\r\n     87             model_ref.on_test_model_eval()\r\n     88         else:\r\n---> 89             model_ref.on_validation_model_eval()\r\n     90 \r\n     91     def on_evaluation_model_train(self) -> None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/core/hooks.py in on_validation_model_eval(self)\r\n    195         Sets the model to eval during the val loop\r\n    196         \"\"\"\r\n--> 197         self.trainer.model.eval()\r\n    198 \r\n    199     def on_validation_model_train(self) -> None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in eval(self)\r\n   1291             Module: self\r\n   1292         \"\"\"\r\n-> 1293         return self.train(False)\r\n   1294 \r\n   1295     def requires_grad_(self: T, requires_grad: bool = True) -> T:\r\n```Reason `train` is a method thats why its throwing this error. Renaming `self.train` to `self.trainx` resolved the error. but I think logs should print this message",
    "meta": { "name": "TypeError: 'Subset' object is not callable" },
    "answer": "Reason `train` is a method thats why its throwing this error. Renaming `self.train` to `self.trainx` resolved the error. but I think logs should print this message"
  },
  {
    "content": "how can i solve this problem, my net is DB,  data is coco text det\r\n\r\nthis is error log\r\n\r\n```\r\nFile \"/home/cattree/PycharmProjects/torch-ocr/BoatNumber/ocr_det/train/train.py\", line 38, in <module>\r\n    trainer.fit(model, data)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self._run(model)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 756, in _run\r\n    self.dispatch()\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 797, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 144, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 807, in run_stage\r\n    return self.run_train()\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 869, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 535, in run_training_epoch\r\n    monitor_metrics = deepcopy(self.trainer.logger_connector.callback_metrics)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 230, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 230, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/copy.py\", line 153, in deepcopy\r\n    y = copier(memo)\r\n  File \"/home/cattree/miniconda3/envs/Segmentation-torch/lib/python3.8/site-packages/torch/tensor.py\", line 55, in __deepcopy__\r\n    raise RuntimeError(\"Only Tensors created explicitly by the user \"\r\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment\r\n```What Lightning version are you using?\r\n\r\nThe fastest way for us to be able to help you is if you can adapt [this script](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py) to reproduce the behaviour you are seeing.\r\n\r\nJudging by the stacktrace, it's an error on our side.\r\n\r\nYou can try fixing it by calling `.detach()` on your logged tensors, but it'd be better if you can reproduce it and share the code\r\n\r\n",
    "meta": {
      "name": "Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment"
    },
    "answer": "What Lightning version are you using?\r\n\r\nThe fastest way for us to be able to help you is if you can adapt [this script](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py) to reproduce the behaviour you are seeing.\r\n\r\nJudging by the stacktrace, it's an error on our side.\r\n\r\nYou can try fixing it by calling `.detach()` on your logged tensors, but it'd be better if you can reproduce it and share the code\r\n\r\n"
  },
  {
    "content": "hi  dear friends,\r\nI wanted to train on cpu for debug purpose which can help me understand more about codes. \r\nso in my trainer class, i didn't pass gpus parameter,  the parameter looks like below:\r\n\r\n args of trainer  Namespace(accelerator=None, accumulate_grad_batches=1, adam_epsilon=1e-08, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=10, benchmark=False, bert_config_dir='/Users/i052090/Downloads/segmentation/data/bertmany/bert-base-uncased', bert_dropout=0.2, bert_max_length=128, best_dev_f1=0.0, check_val_every_n_epoch=1, checkpoint_callback=True, data_dir='data/conll03', dataname='conll03', default_root_dir='./conll03/spanner_bert-base-uncased_spMLen_usePruneTrue_useSpLenTrue_useSpMorphTrue_SpWtTrue_value0.5_38274488', deterministic=False, distributed_backend=None, fast_dev_run=False, final_div_factor=10000.0, flush_logs_every_n_steps=100, fp_epoch_result='./conll03/spanner_bert-base-uncased_spMLen_usePruneTrue_useSpLenTrue_useSpMorphTrue_SpWtTrue_value0.5_38274488/epoch_results.txt', gpus=None, gradient_clip_algorithm='norm', gradient_clip_val=1.0, label2idx_list=[('O', 0), ('ORG', 1), ('PER', 2), ('LOC', 3), ('MISC', 4)], limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=1e-05, max_epochs=1, max_spanLen=4, max_steps=None, max_time=None, min_epochs=None, min_steps=None, modelName='spanner_bert-base-uncased_spMLen_usePruneTrue_useSpLenTrue_useSpMorphTrue_SpWtTrue_value0.5', model_dropout=0.2, morph2idx_list=[('isupper', 1), ('islower', 2), ('istitle', 3), ('isdigit', 4), ('other', 5)], morph_emb_dim=100, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', n_class=5, neg_span_weight=0.5, num_nodes=1, num_processes=1, num_sanity_val_steps=2, optimizer='adamw', overfit_batches=0.0, param_name='epoch1_batchsize10_lr1e-5_maxlen128', plugins=None, precision=16, prepare_data_per_node=True, pretrained_checkpoint='', process_position=0, profiler=None, progress_bar_refresh_rate=1, random_int='38274488', reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, spanLen_emb_dim=100, span_combination_mode='x,y', stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, tokenLen_emb_dim=50, tpu_cores=None, track_grad_norm=-1, truncated_bptt_steps=None, use_morph=True, use_prune=True, use_spanLen=True, use_span_weight=True, use_tokenLen=False, val_check_interval=0.25, warmup_steps=0, weight_decay=0.01, weights_save_path=None, weights_summary='top', workers=0)\r\n\r\nBut it would give errors:\r\n File \"trainer.py\", line 572, in main\r\n    trainer = Trainer.from_argparse_args(\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py\", line 207, in from_argparse_args\r\n    return from_argparse_args(cls, args, **kwargs)\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py\", line 52, in from_argparse_args\r\n    return cls(**trainer_kwargs)\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 40, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 319, in __init__\r\n    self.accelerator_connector = AcceleratorConnector(\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 136, in __init__\r\n    self.accelerator = self.select_accelerator()\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 483, in select_accelerator\r\n    precision_plugin=self.precision_plugin,\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 220, in precision_plugin\r\n    self._precision_plugin = self.select_precision_plugin()\r\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 350, in select_precision_plugin\r\n    raise MisconfigurationException(\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: You have asked for native AMP on CPU, but AMP is only available on GPU.\r\n\r\n\r\n\r\n\r\nthanks,Setting `Trainer(precision=16)` is only supported on GPU!\r\n\r\nIf you have them available, you can do: `Trainer(gpus=N, precision=16)`",
    "meta": {
      "name": "Train on cpu but gives error \"You have asked for native AMP on CPU, but AMP is only available on GPU\""
    },
    "answer": "Setting `Trainer(precision=16)` is only supported on GPU!\r\n\r\nIf you have them available, you can do: `Trainer(gpus=N, precision=16)`"
  },
  {
    "content": "Hey gang!\r\n\r\nI have written an Encoder model and a decoder model and I want to train them **separately**. \r\n\r\n```\r\nclass Decoder(pl.LightningModule):\r\n    def __init__(self, encoder_model):#visualize_latent):\r\n        super().__init__()\r\n        self.encoder_model = encoder_model\r\n```\r\n\r\nHowever, when I give my Decoder an Encoder hyperparameter, how do I make sure it will not be trained?\r\nI actually asked that question before, but it wasn't answered for a while and I do not find sufficient support in the docs.\r\n\r\nThanks in advance!Ok, I found out from other forums that one should use `.freeze()`, in this case: `self.encoder_model.freeze()`",
    "meta": { "name": "How to handle pretrained models without training them" },
    "answer": "Ok, I found out from other forums that one should use `.freeze()`, in this case: `self.encoder_model.freeze()`"
  },
  {
    "content": "I have a pytorch lightning module that includes the following section:\r\n\r\n`def training_step(self, batch, batch_idx):\r\n        losses, tensors = self.shared_step(batch)\r\n        return losses\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        losses, tensors = self.shared_step(batch)\r\n        return losses, tensors\r\n\r\n    def training_epoch_end(self, losses):\r\n        my_function(losses)\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        my_function2(outputs)`\r\n\r\nThe `losses` variable is a dictionary that includes the key \"loss\". The input to `validation_epoch_end()` is a list of `(losses, tensors)` tuples from each of the batches as expected. The input to `training_epoch_end()` is a list of the correct length (the number of batches), but every element is the same losses dictionary from the final batch. \r\n\r\nHow can I input the losses from _every_ train batch into the `training_epoch_end()` method (like the inputs to `validation_epoch_end()`)?This issue was reported in https://github.com/PyTorchLightning/pytorch-lightning/issues/8603 - are you able to try Lightning 1.4.1, which contains the fix? \r\n\r\nAnd for broader discussion on these hooks, and alternatives you have to access the per-step outputs, see https://github.com/PyTorchLightning/pytorch-lightning/issues/8731",
    "meta": {
      "name": "training_epoch_end only returning the last train batch"
    },
    "answer": "This issue was reported in https://github.com/PyTorchLightning/pytorch-lightning/issues/8603 - are you able to try Lightning 1.4.1, which contains the fix? \r\n\r\nAnd for broader discussion on these hooks, and alternatives you have to access the per-step outputs, see https://github.com/PyTorchLightning/pytorch-lightning/issues/8731"
  },
  {
    "content": "The default implementation for these is logging a warning that nothing is implemented. https://github.com/PyTorchLightning/pytorch-lightning/blob/963c26764682fa4cf64c93c5a7572ae0040e9c32/pytorch_lightning/core/hooks.py#L529\r\n\r\nWhy isn\u2019t the default implementation to raise a NotImplementedError? This would make errors much clearer in case users forget to override these hooksI believe this is just legacy code. No real reason. It's in the original implementation (to bolts!)\r\n\r\nhttps://github.com/PyTorchLightning/lightning-bolts/commit/797464c421652c4ceb4e8527b9f56ab059382200\r\n\r\nFeel free to try changing it :)",
    "meta": {
      "name": "Why is the default implementation for train_dataloader in DataHooks a warning?"
    },
    "answer": "I believe this is just legacy code. No real reason. It's in the original implementation (to bolts!)\r\n\r\nhttps://github.com/PyTorchLightning/lightning-bolts/commit/797464c421652c4ceb4e8527b9f56ab059382200\r\n\r\nFeel free to try changing it :)"
  },
  {
    "content": "I have some preprocessing logic to run during datamodule setup process, but only in certain situations (mostly to try out different preprocessing steps while experimenting, but at other times, its due to the model I am using with the datamodule).\r\n\r\nIs there a way to specify a set of data preprocessing steps to perform using callbacks? Reading the documentation, I could not find the correct hook to use.Dear @brijow,\r\n\r\nI wondered if something like that could fit your use-case ?\r\n\r\n```py\r\nclass MyDataModule(LightningDataModule):\r\n\r\n    def __init__(self):\r\n\r\n        self._processed_train_dataset = None\r\n\r\n    def setup(self):\r\n        self.train_dataset = ...\r\n\r\n    @property\r\n    def processed_train_dataset(self):\r\n        return self._processed_train_dataset or self.train_dataset\r\n\r\n    @processed_train_dataset.setter\r\n    def processed_train_dataset(self, processed_train_dataset):\r\n        self._processed_train_dataset = processed_train_dataset\r\n    \r\n    def train_dataloader(self):\r\n        return DataLoader(self.processed_train_dataset)\r\n\r\n\r\nclass Preprocessing1(Callback):\r\n\r\n    def preprocess_function(self, dataset):\r\n        # your preprocessing logic\r\n\r\n    def on_train_start(self, trainer, pl_module):\r\n       #\u00a0apply processing\r\n        trainer.datamodule.processed_train_dataset = self.preprocess_function(trainer.datamodule.train_dataset)\r\n        \r\n        # force dataloader reload\r\n        # trainer.reset_train_dataloader(pl_module)\r\n\r\n\r\ntrainer = Trainer(callbacks=Preprocessing1())\r\ntrainer.fit(model, dm)\r\n```\r\n",
    "meta": {
      "name": "Using callbacks for datamodule setup preprocessing logic?"
    },
    "answer": "Dear @brijow,\r\n\r\nI wondered if something like that could fit your use-case ?\r\n\r\n```py\r\nclass MyDataModule(LightningDataModule):\r\n\r\n    def __init__(self):\r\n\r\n        self._processed_train_dataset = None\r\n\r\n    def setup(self):\r\n        self.train_dataset = ...\r\n\r\n    @property\r\n    def processed_train_dataset(self):\r\n        return self._processed_train_dataset or self.train_dataset\r\n\r\n    @processed_train_dataset.setter\r\n    def processed_train_dataset(self, processed_train_dataset):\r\n        self._processed_train_dataset = processed_train_dataset\r\n    \r\n    def train_dataloader(self):\r\n        return DataLoader(self.processed_train_dataset)\r\n\r\n\r\nclass Preprocessing1(Callback):\r\n\r\n    def preprocess_function(self, dataset):\r\n        # your preprocessing logic\r\n\r\n    def on_train_start(self, trainer, pl_module):\r\n       #\u00a0apply processing\r\n        trainer.datamodule.processed_train_dataset = self.preprocess_function(trainer.datamodule.train_dataset)\r\n        \r\n        # force dataloader reload\r\n        # trainer.reset_train_dataloader(pl_module)\r\n\r\n\r\ntrainer = Trainer(callbacks=Preprocessing1())\r\ntrainer.fit(model, dm)\r\n```\r\n"
  },
  {
    "content": "Hi everyone, I wonder what is the best practice to share a massive CPU tensor over multiple processes in pytorch-lightning DDP mode (read-only + single machine)?\r\n\r\nI think [torch.Storage.from_file](https://pytorch.org/docs/stable/storage.html?highlight=from_file#torch.DoubleStorage.from_file) with `share=True` may suit my needs, but I can\u2019t find a way to save storage and read it as a tensor. (see [here](https://discuss.pytorch.org/t/how-to-save-a-storage-file-that-can-be-loaded-using-torch-storage-from-file/127737) for details)\r\n\r\nI also tried to copy training data to /dev/shm ([reference](https://pytorch-lightning.readthedocs.io/en/stable/benchmarking/performance.html#preload-data-into-ram)) and run DDP with 8 GPUs, but nothing is different. The memory usage when running with 8 GPUs is the same as before, but I tested with a single process, loading the dataset may occupy more than 1 GB of memory. Am I missing something here?\r\n\r\nFor `torch.shared_memory`, how should I pass the same reference to all processes in pytorch-lightning pure DDP mode?\r\n\r\nThank you.Dear @siahuat0727,\r\n\r\nLightning doesn't support shared tensors yet, but there is some work being done around it. \r\nYou can track this issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/8230 and help if you feel like :)\r\n\r\nBest,\r\nTCI found that `torch.Storage.from_file` suits my needs and it can reduce the memory usage in my Lightning DDP program.\r\nFor the way to create a storage file, see [here](https://discuss.pytorch.org/t/how-to-save-a-storage-file-that-can-be-loaded-using-torch-storage-from-file/127737).",
    "meta": {
      "name": "What is the best practice to share a massive CPU tensor over multiple processes in pytorch-lightning DDP mode (read-only + single machine)?"
    },
    "answer": "I found that `torch.Storage.from_file` suits my needs and it can reduce the memory usage in my Lightning DDP program.\r\nFor the way to create a storage file, see [here](https://discuss.pytorch.org/t/how-to-save-a-storage-file-that-can-be-loaded-using-torch-storage-from-file/127737)."
  },
  {
    "content": "I am looking to implement `n` parallel independent ensembles. My idea is the following:\r\n\r\n``` python\r\nclass DeepEnsemble(LightningModule):\r\n    def __init__(self, cfg):\r\n        super().__init__(cfg)\r\n        self.net = nn.ModuleList([configure_network(self.cfg) for _ in range(self.cfg.METHOD.ENSEMBLE)])\r\n\r\n    def configure_optimizers(self):\r\n        return [torch.optim.Adam(net.parameters(), lr=self.cfg.SOLVER.LR) for net in self.net]\r\n\r\n    def forward(self, x):\r\n        x = [net.forward(x) for net in self.net]\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        image, label = batch[\"image\"], batch[\"label\"]\r\n        logits = self.forward(image)\r\n        loss = [self.criterion(logit, label) for logit in logits]\r\n\r\n        mean_logit = torch.stack(logits, dim=-1).mean(dim=-1)\r\n\r\n        metrics = self.log_metrics(mean_logit, label, 'train')\r\n\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        image, label = batch[\"image\"], batch[\"label\"]\r\n        logits = self.forward(image)\r\n\r\n        mean_logit = torch.stack(logits, dim=-1).mean(dim=-1)\r\n\r\n        metrics = self.log_metrics(mean_logit, label, 'val')\r\n\r\n        return metrics[self.cfg.CKPT.MONITOR]\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        pass\r\n```\r\n\r\nI have `n` networks and `n` optimisers. My solution works (I think), but the `training_step` gets called with a new `optimizer_idx` every time, which indicates that Pytorch Lightning expects to only train 1 network per training_step. Therefore, my solution is very inefficient, because n^2 forward passes are executed instead of n. If I only do the forward pass for the *i*th network, then I can't compute metrics based on all ensembles (e.g. disagreement) unless I write some very inelegant if statements.\r\n\r\nIn addition It would be nice to have all forward passes done in parallel instead of sequential like in this list comprehension.\r\n\r\nSo what is the most elegant way to train an ensemble and still access all predictions for metric logging together?I see two potential options.\r\n\r\n1. cache the forward output for a specific batch idx. Check the automatic optimization flow: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization\r\n\r\n2. Use manual optimization. https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimizationHere a solution with caching predictions:\r\n``` python\r\nclass DeepEnsemble(pl.LightningModule):\r\n    def __init__(self, cfg):\r\n        super().__init__(cfg)\r\n        self.net = nn.ModuleList([configure_network(self.cfg) for _ in range(self.cfg.METHOD.ENSEMBLE)])\r\n        self.cache_preds = []\r\n        \r\n    def configure_optimizers(self):\r\n        return [torch.optim.Adam(net.parameters(), lr=self.cfg.SOLVER.LR) for net in self.net]\r\n\r\n    def forward(self, x, idx = None):\r\n        if idx is None:\r\n            x = torch.stack([net.forward(x) for net in self.net], dim=-1)\r\n        else:\r\n            x = self.net[idx].forward(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        image, label = batch[\"image\"], batch[\"label\"]\r\n        logits = self.forward(image, optimizer_idx)\r\n        self.cache_preds.append(logits.detach())\r\n\r\n        loss = self.criterion(logits, label)\r\n        \r\n        if optimizer_idx == self.cfg.METHOD.ENSEMBLE - 1:\r\n            logits = torch.stack(self.cache_preds, dim=-1)\r\n            mean_logit = logits.mean(dim=-1)\r\n            all_loss = self.log_loss(mean_logit, label, 'train')\r\n            metrics = self.log_metrics(mean_logit, label, 'train')\r\n            \r\n            self.cache_preds, self.cache_loss = [], []\r\n\r\n        return loss\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        image, label = batch[\"image\"], batch[\"label\"]\r\n        logits = self.forward(image)\r\n\r\n        mean_logit = logits.mean(dim=-1)\r\n\r\n        loss = self.log_loss(mean_logit, label, 'val')\r\n        metrics = self.log_metrics(mean_logit, label, 'val')\r\n        return metrics[self.cfg.CKPT.MONITOR]\r\n\r\n```",
    "meta": { "name": "How to implement a deep ensemble" },
    "answer": "I see two potential options.\r\n\r\n1. cache the forward output for a specific batch idx. Check the automatic optimization flow: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization\r\n\r\n2. Use manual optimization. https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization"
  },
  {
    "content": "Greetings,\r\n\r\nI can only show metrics of variables calculated on training step but can't show validation step metrics on the progress bar. How can show a metric in the validation step ? `self.log(...., prog_bar=True)` does not work. Hi\r\n\r\nIt works fine for me. Have a look at this running example (latest PL version 1.2.6:\r\n\r\n```python\r\n\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self.layer(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        self.log(\"VALIDATION_STEP\", loss, prog_bar=True)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x['x'] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ndef test_run():\r\n\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n\r\n    # model\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        max_epochs=3,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model, train_data, val_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    test_run()\r\n\r\n```\r\nHi. It seems that you run the program in pycharm.\r\n\r\nI would also encounter that problem as below.\r\n\r\n``` bash\r\nEpoch 0:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c       | 1/2 [00:00<00:00, 793.10it/s, loss=1.28, v_num=0]\r\nValidating: 0it [00:00, ?it/s]\r\nEpoch 0: 100%|\u2588| 2/2 [00:00<00:00, 547.39it/s, loss=1.28, v_num=0, VALIDATION_ST\r\nEpoch 1:  50%|\u258c| 1/2 [00:00<00:00, 1027.76it/s, loss=4.07, v_num=0, VALIDATION_S\r\nValidating: 0it [00:00, ?it/s]\r\nEpoch 1: 100%|\u2588| 2/2 [00:00<00:00, 621.22it/s, loss=4.07, v_num=0, VALIDATION_ST\r\nEpoch 2:  50%|\u258c| 1/2 [00:00<00:00, 1037.94it/s, loss=3.73, v_num=0, VALIDATION_S\r\nValidating: 0it [00:00, ?it/s]\r\nEpoch 2: 100%|\u2588| 2/2 [00:00<00:00, 634.89it/s, loss=3.73, v_num=0, VALIDATION_ST\r\nEpoch 2: 100%|\u2588| 2/2 [00:00<00:00, 475.49it/s, loss=3.73, v_num=0, VALIDATION_ST\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\nI found this solution, but didn't try yet.\r\n- https://stackoverflow.com/questions/59455268/how-to-disable-progress-bar-in-pytorch-lightning/66731318#66731318",
    "meta": { "name": "Progress Bar Variables from Validation Step" },
    "answer": "Hi\r\n\r\nIt works fine for me. Have a look at this running example (latest PL version 1.2.6:\r\n\r\n```python\r\n\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self.layer(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        self.log(\"VALIDATION_STEP\", loss, prog_bar=True)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x['x'] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ndef test_run():\r\n\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n\r\n    # model\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        max_epochs=3,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model, train_data, val_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    test_run()\r\n\r\n```\r\n"
  },
  {
    "content": "```python\r\nclass MyNet(pl.LightningModule):\r\n    def __init__(self):\r\n        self.m1 = MyMod1()\r\n        self.m2 = MyMod2()\r\n```\r\nIf I implement different `configure_optimizers` for different submodules `MyMod`(also `pl.LightningModule`), is it correct that parameters in each `MyMod` will be updated by their own optimizers returned by `configure_optimizers`? If I only implement `configure_optimizers` for the top module, is it correct that parameters in submodules will be optimized by the same optimizer returned by `configure_optimizers` of the top module?When you have nested LightningModules, their `configure_optimizers` will never be called unless you explicitly call it in the top-level `configure_optimizers`.\r\n\r\nThat being said, if you call, merge and return the optimizers created there, these optimizers should only contain parameters from the respective submodule (if implemented correctly)",
    "meta": { "name": "Optimizers for nested modules" },
    "answer": "When you have nested LightningModules, their `configure_optimizers` will never be called unless you explicitly call it in the top-level `configure_optimizers`.\r\n\r\nThat being said, if you call, merge and return the optimizers created there, these optimizers should only contain parameters from the respective submodule (if implemented correctly)"
  },
  {
    "content": "I have a training regime that is disk-speed bound, because instances are loaded from disk.\r\n\r\nI would like to train multiple models with one dataloader. That way, I can do model selection over many models, but reduce the number of disk reads. Is this possible?Dear @turian,\r\n\r\nYes, it is possible.\r\n\r\nYou could do something like this.\r\n\r\n```\r\nclass MultiModels(LightningModule):\r\n\r\n    def __init__(self, models: List[nn.Module]):\r\n        self.models = models\r\n\r\n    def compute_loss(self, model, batch):\r\n        loss = ...\r\n        return loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = sum(compute_loss(model, batch) for model in self.models)\r\n        return loss\r\n\r\n\r\nmodel = MultiModels([resnet50_model, alexnet_model, ...])\r\ndm = ...\r\ntrainer.fit(model, dm)\r\n```\r\n\r\nDoes this answer your questions ?\r\n",
    "meta": { "name": "Multiple models, one dataloader?" },
    "answer": "Dear @turian,\r\n\r\nYes, it is possible.\r\n\r\nYou could do something like this.\r\n\r\n```\r\nclass MultiModels(LightningModule):\r\n\r\n    def __init__(self, models: List[nn.Module]):\r\n        self.models = models\r\n\r\n    def compute_loss(self, model, batch):\r\n        loss = ...\r\n        return loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = sum(compute_loss(model, batch) for model in self.models)\r\n        return loss\r\n\r\n\r\nmodel = MultiModels([resnet50_model, alexnet_model, ...])\r\ndm = ...\r\ntrainer.fit(model, dm)\r\n```\r\n\r\nDoes this answer your questions ?\r\n"
  },
  {
    "content": "Say I have a callback that changes a hyper-parameter of the underlying model before every epoch.\r\n\r\n```python\r\nclass ChangeHyperParam(pl.Callback):\r\n    def on_train_epoch_start(self, trainer, pl_module):\r\n         pl_module.hyper_param1 = func(trainer.current_epoch)\r\n```\r\n\r\nWhat happens when I use this callback in DDP mode. Will this call be called on every GPU automatically? Or do I have to do something else?Callbacks run on all ranks, so this would be called on each GPU in distributed training. As long as `func` returns the same value given the inputs, this should be fine",
    "meta": { "name": "How are callback calls handled in multi-gpu mode?" },
    "answer": "Callbacks run on all ranks, so this would be called on each GPU in distributed training. As long as `func` returns the same value given the inputs, this should be fine"
  },
  {
    "content": "I'm migrating my repository to pytorch-lightning and I get the following error: \r\n\r\n```bash\r\nRuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.\r\n```\r\n\r\nThe CNNLSTM model seems to be the problem, what should I do?\r\n\r\n[My repository]\r\nKeiku/Action-Recognition-CNN-LSTM: Action recognition tutorial using UCF-101 dataset. https://github.com/Keiku/Action-Recognition-CNN-LSTMHi @Keiku This error happens if you try to call backward on something twice in a row without calling optimizer.step. Are you able to share your LightningModule code? It looks like the code in your repo just uses vanilla pytorch.\r\n\r\nThanks :smiley:",
    "meta": {
      "name": "RuntimeError: Trying to backward through the graph a second time"
    },
    "answer": "Hi @Keiku This error happens if you try to call backward on something twice in a row without calling optimizer.step. Are you able to share your LightningModule code? It looks like the code in your repo just uses vanilla pytorch.\r\n\r\nThanks :smiley:"
  },
  {
    "content": "I know that parameters are indirectly synced in multi-gpu via grad-syncing.  But how to sync buffers that are not updated via gradient? \r\n\r\nI find that I can use all_reduce() or all_gather() method manually in ddp doc, but what pytorch-lightning does under the hood? Does it sync buffers automatically or offer an interface to do that? \r\n\r\nConsidering pl handles single and multi-gpu without any need to change Modules, do it manually will result in ugly judgement code whether the training is in ddp mode inside an independent module.Dear @MeteorsHub,\r\n\r\nGreat question. Lightning doesn't do any magic there. It relies on PyTorch DistributedDataParallel to handle multi-gpu via grad-syncing.\r\n\r\nThe buffers are being synced on start using `_sync_params_and_buffers`: https://github.com/pytorch/pytorch/blob/3e3acf8a9ac005db3094f23bd41a5fbc0c3c154b/torch/nn/parallel/distributed.py#L570\r\n\r\nIf you need to access this private function for any reason, you can do the following:\r\n\r\n```\r\n\r\ntrainer.training_type_plugin.model._sync_params_and_buffers(authoritative_rank=0)\r\n```\r\n",
    "meta": { "name": "How to sync buffers in multi-gpu training" },
    "answer": "Dear @MeteorsHub,\r\n\r\nGreat question. Lightning doesn't do any magic there. It relies on PyTorch DistributedDataParallel to handle multi-gpu via grad-syncing.\r\n\r\nThe buffers are being synced on start using `_sync_params_and_buffers`: https://github.com/pytorch/pytorch/blob/3e3acf8a9ac005db3094f23bd41a5fbc0c3c154b/torch/nn/parallel/distributed.py#L570\r\n\r\nIf you need to access this private function for any reason, you can do the following:\r\n\r\n```\r\n\r\ntrainer.training_type_plugin.model._sync_params_and_buffers(authoritative_rank=0)\r\n```\r\n"
  },
  {
    "content": "I am training an image classifier on tpu and am getting errors after execution of the first epoch.\r\n\r\nhttps://colab.research.google.com/drive/1Lgz0mF6UiLirsDltPQH5HtctmVvb7gHm?usp=sharingI don't see an error in the provided link.\r\n\r\nIs this still relevant?No. I think I change the notebook. I don't even remember what was the error.\n\nOn Fri, 23 Jul, 2021, 9:15 AM Carlos Mochol\u00ed, ***@***.***>\nwrote:\n\n> I don't see an error in the provided link.\n>\n> Is this still relevant?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/PyTorchLightning/pytorch-lightning/discussions/8339#discussioncomment-1040381>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APKB7QYHMLNNOQTHEIARM53TZDQURANCNFSM5ABI7EKQ>\n> .\n>\n",
    "meta": { "name": "Getting error after completion of 1st epoch" },
    "answer": "I don't see an error in the provided link.\r\n\r\nIs this still relevant?"
  },
  {
    "content": "I am trying to create multiple model using loop as below.\r\n\r\n```\r\nfor client in clients:\r\n    t.manual_seed(10)\r\n    client['model'] = LinearNN(learning_rate = args.lr, i_s = args.input_size,  h1_s = args.hidden1, h2_s = args.hidden2, n_c = args.output, client=client)\r\n    client['optim'] = optim.Adam(client['model'].parameters(), lr= args.lr)\r\n```\r\n\r\nHowever, ```trainer.fit()``` is an async method. To train multiple models, I need to put ```trainer.fit()``` in a loop as follows \r\n\r\n```\r\nfor client in clients:\r\n     trainer = pl.Trainer(\r\n     max_epochs=args.epochs+1,\r\n     progress_bar_refresh_rate=20,\r\n     )\r\n     trainer.fit(client['model'])\r\n```\r\n\r\nAs this is an async method, it gives an error \r\n\r\n> AttributeError: can't set attribute\r\n\r\nas it doesn't wait for finishing ```trainer.fit()```.\r\n\r\nIs there any way to do that? \r\n\r\nThanks in advance.Hi!\r\n\r\nI'm not entirely sure, but I don't see why the code snippets you shared would not work.\r\n\r\n>  is an async method\r\n\r\nWhat do you mean exactly by \"async\" method?\r\n\r\n>  it gives an error\r\n\r\nCan you share the full error stacktrace and your Lightning version?",
    "meta": { "name": "how to put ```trainer.fit()``` in for loop?" },
    "answer": "Hi!\r\n\r\nI'm not entirely sure, but I don't see why the code snippets you shared would not work.\r\n\r\n>  is an async method\r\n\r\nWhat do you mean exactly by \"async\" method?\r\n\r\n>  it gives an error\r\n\r\nCan you share the full error stacktrace and your Lightning version?"
  },
  {
    "content": "I'm getting the following error after setting up an EC2 instance p3.8xlarge (so 4 GPUs) and setting gpus=4:\r\n\r\n```python\r\n/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:524: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `accelerator=\"ddp_spawn\"` for you.\r\n  'You requested multiple GPUs but did not specify a backend, e.g.'\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 79, in <module>\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/tuning.py\", line 197, in lr_find\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 688, in tune\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/tuning.py\", line 54, in _tune\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/lr_finder.py\", line 250, in lr_find\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/tuner/tuning.py\", line 64, in _run\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 758, in _run\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 799, in dispatch\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 122, in start_training\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 179, in start_processes\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 112, in start\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/context.py\", line 284, in _Popen\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 20, in __init__\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/reduction.py\", line 60, in dump\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 328, in reduce_storage\r\nRuntimeError: unable to open shared memory object </torch_91130_1372465664> in read-write mode\r\n```\r\n\r\nMy code runs fine on a single gpu instance. Any idea what I need to look at here?Some quick googling \ud83d\udd0d \r\nhttps://github.com/facebookresearch/maskrcnn-benchmark/issues/103\r\n\r\nThis issue is not Lightning related, so if the fixes mentioned there do not help, then you should try asking on PyTorch discussions.",
    "meta": {
      "name": "RuntimeError: unable to open shared memory object </torch_91130_1372465664> in read-write mode"
    },
    "answer": "Some quick googling \ud83d\udd0d \r\nhttps://github.com/facebookresearch/maskrcnn-benchmark/issues/103\r\n\r\nThis issue is not Lightning related, so if the fixes mentioned there do not help, then you should try asking on PyTorch discussions."
  },
  {
    "content": "I'm trying to run code with PL+Deepspeed. I set my scheduler to be `WarmupDecayLR`, but get a warning `RuntimeWarning: You are using LearningRateMonitor callback with models that have no learning rate schedulers. Please see documentation for configure_optimizers method.`\r\nIn the CometML logging I can't see learning rate logs. Is this the expected behaviour?\r\nThank you! :) I encountered the same problem and reported in the issue: \r\n- https://github.com/PyTorchLightning/pytorch-lightning/issues/8520Thanks for reporting! Closing in favor of the linked issue",
    "meta": { "name": "Does LearningRateMonitor work with deepspeed?" },
    "answer": "Thanks for reporting! Closing in favor of the linked issue"
  },
  {
    "content": "Hi! \r\n\r\nI'm training my neural network with Pytorch Lightning and [MONAI](https://monai.io/) (a PyTorch-based framework for deep learning in healthcare imaging). Because my training dataset is small, I need to perform data augmentation using random transforms. \r\n\r\n**Context** \r\n\r\nI use MONAI's `CacheDataset` (basically, a PyTorch `Dataset` with cache mechanism). [From what I understood](https://github.com/Project-MONAI/MONAI/discussions/2608), , `CacheDataset` will cache the consistent result of the transforms until the first random transform, then reuse the cache content and apply the remaining random transforms (such as Gaussian noise, random intensity shift, etc.) for every epoch. As a result, the dataloader trains the network with a different dataset at each epoch. \r\n\r\nIn the [video](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#reload-dataloaders-every-n-epochs) presenting the `reload_dataloaders_every_epoch` flag, William Falcon mentions that:\r\n\r\n> By default, Lighthning only loads your dataset once (so that you don't occur the cost of downloading that data and process it every single time). On every epoch, Lightning shuffles the data and feeds it into the training loop.\r\n\r\n**My questions** \r\n\r\nDid Lightning add a cache mechanism to load the data once? Must I use the `reload_dataloader_every_epoch` flag to do data augmentation or else my random transforms will only be applied once (therefore defeating my data augmentation goal)? \r\n\r\nThanks in advance for your explanation :)> Did Lightning add a cache mechanism to load the data once?\r\n\r\nNo, the flag just means that we call `LightningModule.train_dataloader()` every epoch if enabled, thus creating a new `DataLoader` instance.\r\n\r\n>  Must I use the reload_dataloader_every_epoch flag to do data augmentation or else my random transforms will only be applied once \r\n\r\nIf I understand correctly, no.\r\nThe transformations are applied directly in the `Dataset`, so every time an item is consumed from it, the random transforms should be applied regardless of whether the `DataLoader` has or hasn't been recreated.",
    "meta": { "name": "Data augmentation and reload_dataloaders_every_epoch" },
    "answer": "> Did Lightning add a cache mechanism to load the data once?\r\n\r\nNo, the flag just means that we call `LightningModule.train_dataloader()` every epoch if enabled, thus creating a new `DataLoader` instance.\r\n\r\n>  Must I use the reload_dataloader_every_epoch flag to do data augmentation or else my random transforms will only be applied once \r\n\r\nIf I understand correctly, no.\r\nThe transformations are applied directly in the `Dataset`, so every time an item is consumed from it, the random transforms should be applied regardless of whether the `DataLoader` has or hasn't been recreated."
  },
  {
    "content": "Is it possible to use shared filesystem for DDP init_group in pytorch lighting? If so how what should I do to the Trainer?\r\n\r\nThanks!I'm not quite sure about what you want to do, but if it's about customizing `DDP`, you can do the following:\r\n\r\n```python\r\nfrom pytorch_lightning.plugins import DDPPlugin\r\n\r\nclass MyCustomDDP(DDPPlugin):\r\n    ...\r\n\r\ntrainer = Trainer(plugins=[MyCustomDDP()])\r\n```",
    "meta": { "name": "DDP with shared file system" },
    "answer": "I'm not quite sure about what you want to do, but if it's about customizing `DDP`, you can do the following:\r\n\r\n```python\r\nfrom pytorch_lightning.plugins import DDPPlugin\r\n\r\nclass MyCustomDDP(DDPPlugin):\r\n    ...\r\n\r\ntrainer = Trainer(plugins=[MyCustomDDP()])\r\n```"
  },
  {
    "content": "Hi,\r\n\r\nI\u2019m trying to train a model on 2 GPUs. I do this by specifying Trainer(..., gpus=2). ddp_spawn should automatically be selected for the method, but I instead get the following message + error:\r\n\r\n```\r\nUserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(\r\naccelerator=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `accelerator=\"ddp_spawn\"` for you.\r\n  'You requested multiple GPUs but did not specify a backend, e.g.'\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nTraceback (most recent call last):\r\n File \"train.py\", line 186, in <module>\r\n    main(sys.argv[1:])\r\n  File \"train.py\", line 173, in main\r\n    print(f\"Logs for this experiment are being saved to {trainer.log_dir}\")\r\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/trainer/properties.py\", line 137, in log_dir\r\n    dirpath = self.accelerator.broadcast(dirpath)\r\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/accelerators/accelerator.py\", line 436, in broadcast\r\n    return self.training_type_plugin.broadcast(obj, src)\r\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 275, in broadcast\r\n    return self.dist.broadcast(obj)\r\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/distributed/dist.py\", line 33, in broadcast\r\n    broadcast_object_list(obj, 0, group=group or _group.WORLD)\r\n  File \".../pypi__torch_python3_deps/torch/distributed/distributed_c10d.py\", line 1700, in broadcast_object_list\r\n    my_rank = get_rank()\r\n  File \".../pypi__torch_python3_deps/torch/distributed/distributed_c10d.py\", line 725, in get_rank\r\n    default_pg = _get_default_group()\r\n  File \".../pypi__torch_python3_deps/torch/distributed/distributed_c10d.py\", line 358, in _get_default_group\r\n    raise RuntimeError(\"Default process group has not been initialized, \"\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n```\r\n\r\nI looked at the [source code](https://github.com/PyTorchLightning/pytorch-lightning/blob/51ea84222ba07b45d01d94348f8e671e50fccc7e/pytorch_lightning/plugins/training_type/ddp_spawn.py#L259-L261) of ddp_spawn and it looks like it should print out a message when initializing ddp, but it didn\u2019t.\r\n\r\nCould I please have advice on how to correct this error.\r\n\r\nThank you!Found a relevant [discussion](https://github.com/PyTorchLightning/pytorch-lightning/discussions/7573), but I don't think it's applicable here because I construct the dataloader with:\r\n\r\n```python\r\nclass CustomDataModule(pl.LightningDataModule):\r\n    def train_dataloader(self):\r\n        train_dataset = CustomDataset(\r\n            params=self.params,\r\n            data_params=self.data_params,\r\n            num_workers=self.num_workers,\r\n        )\r\n        return DataLoader(\r\n            train_dataset,\r\n            timeout=self.data_loader_timeout,\r\n            num_workers=self.num_workers,\r\n            batch_size=self.batch_size,\r\n            worker_init_fn=worker_init_fn,\r\n        )\r\n```\r\n\r\nand here's what the order of operations looks like:\r\n```python\r\ndata_module = CustomDataModule(...)\r\nmodel = CustomLightningModule(...)\r\ntb_logger = TensorBoardLogger(...)\r\ncheckpoint_callback = ModelCheckpoint(...)\r\ntrainer = Trainer.from_argparse_args(\r\n    args,\r\n    logger=tb_logger,\r\n    default_root_dir=args.output_dir,\r\n    profiler=\"pytorch\", # tried removing this and it doesn't make a difference\r\n    callbacks=[checkpoint_callback],\r\n    gpus=args.gpus\r\n)\r\ntrainer.fit(model, data_module)\r\n```The issue comes from the line\r\n\r\n```python\r\n  File \"train.py\", line 173, in main\r\n    print(f\"Logs for this experiment are being saved to {trainer.log_dir}\")\r\n```\r\n\r\nwhich tries to access `trainer.log_dir` outside of the trainer scope.\r\n\r\n`trainer.log_dir` tries to `broadcast` the directory but fails as DDP hasn\u2019t been initialized yet.\r\n\r\n```python\r\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/trainer/properties.py\", line 137, in log_dir\r\n    dirpath = self.accelerator.broadcast(dirpath)\r\n```\r\n\r\nThis is fixed in the 1.4 release as `broadcast` becomes a no-op in that case",
    "meta": {
      "name": "`init_process_group` not called when training on multiple-GPUs"
    },
    "answer": "The issue comes from the line\r\n\r\n```python\r\n  File \"train.py\", line 173, in main\r\n    print(f\"Logs for this experiment are being saved to {trainer.log_dir}\")\r\n```\r\n\r\nwhich tries to access `trainer.log_dir` outside of the trainer scope.\r\n\r\n`trainer.log_dir` tries to `broadcast` the directory but fails as DDP hasn\u2019t been initialized yet.\r\n\r\n```python\r\n  File \".../pypi__pytorch_lightning_python3_deps/pytorch_lightning/trainer/properties.py\", line 137, in log_dir\r\n    dirpath = self.accelerator.broadcast(dirpath)\r\n```\r\n\r\nThis is fixed in the 1.4 release as `broadcast` becomes a no-op in that case"
  },
  {
    "content": "Just wondering when backward gets called when you are accumulating over (say 8) batches. I had put a breakpoint in `on_after_backward()` and that seemed to be only getting called on the 8th iteration of training. According to [this answer](https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903/20?u=alband), in order to save on GPU memory, it seems its best to call `loss.backward()` on each iteration.Dear @sachinruk,\r\n\r\nThis behaviour was a bug and should have been resolved on master and on_after_backward should be called after each backward call. In the case of accumulating over (say 8) batches, you should see `on_after_backward ` called 8 times on master, and only 1 time previously.\r\n\r\nBest,\r\nT.C",
    "meta": {
      "name": "When does loss.backward() get called when accumulating gradients?"
    },
    "answer": "Dear @sachinruk,\r\n\r\nThis behaviour was a bug and should have been resolved on master and on_after_backward should be called after each backward call. In the case of accumulating over (say 8) batches, you should see `on_after_backward ` called 8 times on master, and only 1 time previously.\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "Hi, I'm trying to use integrate pytorch lightning into my current pipeline. But I'm having some difficulties in using multiple dataloaders. In my current use case, let's say I have 10 dataloaders in my pipeline, but for each training step, I'm only sampling data from 5 of them. Is it doable in pytorch lightning? I can sample from 10 dataloaders each step but that would be a waste to system IO and GPU memory. Thanks for any help!Hey @YuShen1116,\r\n\r\nHere is the pseudo code to get it working\r\n\r\n```\r\nfrom typing import List\r\nimport itertools\r\nfrom pytorch_lightning.trainer.supporters import CombinedLoader\r\nfrom pytorch_lightning.utilities.apply_func import apply_to_collection\r\n\r\n\r\nclass CyclingLoader(object):\r\n\r\n    def __init__(self, combined_loaders: List[CombinedLoader]):\r\n        self.combined_loaders = combined_loaders\r\n        self._dataloader_idx_cycle = itertools.cycle(range(len(combined_loaders)))\r\n\r\n    def __iter__(self):\r\n        self._iterators = apply_to_collection(self.combined_loaders, CombinedLoader, iter)\r\n        self._dataloader_idx_cycle_iter = iter(self._dataloader_idx_cycle)\r\n        return self\r\n\r\n    def __next__(self):\r\n        iterator_idx = next(self._dataloader_idx_cycle_iter)\r\n        return next(self._iterators[iterator_idx])\r\n\r\n\r\nclass MyDataModule(DataModule):\r\n\r\n\r\n    def train_dataloader(self):\r\n        ds_1, .... ds_10 = create_dataloaders()\r\n\r\n        ds_1_5 = CombinedLoader([ds_1, ... ds_5])\r\n        ds_6_10 = CombinedLoader([ds_6, ... ds_10])\r\n\r\n        return CyclingLoader([ds_1_5, ds_6_10])\r\n\r\n\r\nclass Model(LightningModule):\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        if batch_idx % 2 == 0:\r\n            # batches from dataloaders from 1 - 5\r\n        elif batch_idx % 2 == 1:\r\n            # batches from dataloaders from 6 - 10\r\n```",
    "meta": { "name": "Multiple dataloaders in training" },
    "answer": "Hey @YuShen1116,\r\n\r\nHere is the pseudo code to get it working\r\n\r\n```\r\nfrom typing import List\r\nimport itertools\r\nfrom pytorch_lightning.trainer.supporters import CombinedLoader\r\nfrom pytorch_lightning.utilities.apply_func import apply_to_collection\r\n\r\n\r\nclass CyclingLoader(object):\r\n\r\n    def __init__(self, combined_loaders: List[CombinedLoader]):\r\n        self.combined_loaders = combined_loaders\r\n        self._dataloader_idx_cycle = itertools.cycle(range(len(combined_loaders)))\r\n\r\n    def __iter__(self):\r\n        self._iterators = apply_to_collection(self.combined_loaders, CombinedLoader, iter)\r\n        self._dataloader_idx_cycle_iter = iter(self._dataloader_idx_cycle)\r\n        return self\r\n\r\n    def __next__(self):\r\n        iterator_idx = next(self._dataloader_idx_cycle_iter)\r\n        return next(self._iterators[iterator_idx])\r\n\r\n\r\nclass MyDataModule(DataModule):\r\n\r\n\r\n    def train_dataloader(self):\r\n        ds_1, .... ds_10 = create_dataloaders()\r\n\r\n        ds_1_5 = CombinedLoader([ds_1, ... ds_5])\r\n        ds_6_10 = CombinedLoader([ds_6, ... ds_10])\r\n\r\n        return CyclingLoader([ds_1_5, ds_6_10])\r\n\r\n\r\nclass Model(LightningModule):\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        if batch_idx % 2 == 0:\r\n            # batches from dataloaders from 1 - 5\r\n        elif batch_idx % 2 == 1:\r\n            # batches from dataloaders from 6 - 10\r\n```"
  },
  {
    "content": "I have a loss module which is part of my lightning module with its own inner pretrained vgg network. \r\nThe problem comes when I am trying to use the checkpoint (that is saved automatically) to resume training or to test my model. \r\nThen I get an error ` unexpected key(s) in state_dict pytorch lightning` which points to the keys of the network which is part of the loss function.\r\n\r\nIs there a way to load my model properly?Dear @vasl12,\r\n\r\nYou have multiple options:\r\n* pass `strict=False`\r\n* drop the key before saving the weights. Use `on_save_checkpoint` hook to access the checkpoint and drop the associated key.\r\n* re-create the missing module in your model\r\n\r\nBest,\r\nT.C\r\nDear @tchaton,\r\n\r\nthank you very much for your answer. Regarding the first option, I am not sure in which function to pass it, since I am trying to load the checkpoint using the `Trainer`. This makes me also uncertain on how to implement the third option, since the code fails when I am trying to load the model, and in fact I have not changed the model itself.\r\n\r\nI appreciate your help! Thank you very much!",
    "meta": { "name": "Loss Module with inner Network" },
    "answer": "Dear @vasl12,\r\n\r\nYou have multiple options:\r\n* pass `strict=False`\r\n* drop the key before saving the weights. Use `on_save_checkpoint` hook to access the checkpoint and drop the associated key.\r\n* re-create the missing module in your model\r\n\r\nBest,\r\nT.C\r\n"
  },
  {
    "content": "```\r\nThis is my training and validation steps  \r\n def training_step(self,batch,batch_idx):\r\n    self.log('train/loss', loss, on_epoch=True,prog_bar=True)\r\n    self.log('train/iou', iou, on_epoch=True,prog_bar=True)\r\n    return loss  \r\n\r\ndef validation_step(self,batch,batch_idx):\r\n    self.log('val/loss', loss, on_epoch=True,prog_bar=True)\r\n    self.log('val/iou', iou, on_epoch=True,prog_bar=True)\r\n    return loss\r\n\r\n```\r\nThis is what logs displayed\r\n```\r\n360/361 [16:07<00:02, 2.69s/it, loss=0.148, v_num=0, val/loss=0.664, val/iou=0.234, train/loss_step=0.129, train/iou_step=0.379, train/loss_epoch=0.210, train/iou_epoch=0.371]\r\n```\r\nI am confused what is the difference between `loss` and `val/loss`? If there is `val/loss` which makes sense as I output it, then what is `loss`. There is `val/iou`, but there is not any `train/iou`, and `train/loss` as in val case. why? Is `train/iou_epoch` is for epoch or for all previous epochs?\r\n\r\nHey @talhaanwarch,\r\n\r\nLightning automatically takes your `training_step` output and applies a running mean on it and add it to the progress bar using the key loss.\r\n\r\nAs you may see, there is `train/iou_step=0.379` and `train/loss_epoch=0.210`. \r\n\r\nHere is why:\r\nFor training, Lightning logs automatically set on_step=True and you provided on_epoch=True. Therefore, to prevent values to be logged under the same key, Lightning adds a _step and _epoch suffix to differentiate them.\r\n\r\n`_epoch` is used to describe the value compute on the previous epoch, until the new reduction is performed. If you use a logger, you will clearly see the `_epoch` value logged at the right epoch.\r\n\r\nBest,\r\nT.C",
    "meta": { "name": "Interpret the output (logs) during training" },
    "answer": "Hey @talhaanwarch,\r\n\r\nLightning automatically takes your `training_step` output and applies a running mean on it and add it to the progress bar using the key loss.\r\n\r\nAs you may see, there is `train/iou_step=0.379` and `train/loss_epoch=0.210`. \r\n\r\nHere is why:\r\nFor training, Lightning logs automatically set on_step=True and you provided on_epoch=True. Therefore, to prevent values to be logged under the same key, Lightning adds a _step and _epoch suffix to differentiate them.\r\n\r\n`_epoch` is used to describe the value compute on the previous epoch, until the new reduction is performed. If you use a logger, you will clearly see the `_epoch` value logged at the right epoch.\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "Hei everyone\r\n\r\nI am working on a small framework based on PyTorch lightning to perform some experiments. \r\nOur models always are a composition out of two networks (backbone and some header) as we want to be able to replace them independently.\r\n\r\n```\r\nclass EncoderHeaderModel(pl.LightningModule):\r\n\r\n        def __init__(self, backbone: Union[pl.LightningModule, torch.nn.Module],\r\n                 header: Union[pl.LightningModule, torch.nn.Module]):\r\n        super().__init__()\r\n\r\n        # sanity check if the last layer of the backbone is compatible with the first layer of the header\r\n\r\n        self.backbone = backbone\r\n        self.header = header\r\n\r\n    def forward(self, x):\r\n        x = self.backbone(x)\r\n        x = self.header(x)\r\n        return x\r\n```\r\n\r\nNow we have the problem with saving the state_dict of these two models separately. To store the whole model we are using the `model_checkpoint` callback which works fine. \r\n\r\nIs there an easy way to save the models each time `model_checkpoint` would save the whole model (I am already experimenting with a subclass of `model_checkpoint`)? Or should we after the training just extract the state_dicts out of the checkpoint? Any idea how to make this in a \"clean\" way?\r\n\r\nThanksI was able to solve my problem by overwriting `_del_model` and `_save_model` to perform the desired actions.",
    "meta": { "name": "Save model into separate files" },
    "answer": "I was able to solve my problem by overwriting `_del_model` and `_save_model` to perform the desired actions."
  },
  {
    "content": "Hi, everyone.\r\n\r\nI have a repo whose link is [https://github.com/zhoufengfan/pytorch-lightning-cifar10](https://github.com/zhoufengfan/pytorch-lightning-cifar10).\r\n\r\nWhen I run `python train.py`, it returns this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 21, in <module>\r\n    trainer.fit(model, cifar10_dm)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self._run(model)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 756, in _run\r\n    self.dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 797, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 144, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 807, in run_stage\r\n    return self.run_train()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 842, in run_train\r\n    self.run_sanity_check(self.lightning_module)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1107, in run_sanity_check\r\n    self.run_evaluation()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 962, in run_evaluation\r\n    output = self.evaluation_loop.evaluation_step(batch, batch_idx, dataloader_idx)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 174, in evaluation_step\r\n    output = self.trainer.accelerator.validation_step(args)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 226, in validation_step\r\n    return self.training_type_plugin.validation_step(*args)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 322, in validation_step\r\n    return self.model(*args, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 619, in forward\r\n    output = self.module(*inputs[0], **kwargs[0])\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 57, in forward\r\n    output = self.module.validation_step(*inputs, **kwargs)\r\n  File \"/root/code/test-of-pytorch-lightning/backbone.py\", line 42, in validation_step\r\n    self.evaluate(batch, 'val')\r\n  File \"/root/code/test-of-pytorch-lightning/backbone.py\", line 32, in evaluate\r\n    logits = self(x)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/code/test-of-pytorch-lightning/backbone.py\", line 20, in forward\r\n    out = self.model(x)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 220, in forward\r\n    return self._forward_impl(x)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/models/resnet.py\", line 215, in _forward_impl\r\n    x = self.fc(x)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 93, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\", line 1690, in linear\r\n    ret = torch.addmm(bias, input, weight.t())\r\nRuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`\r\n```\r\nBut after I change the `AVAIL_GPUS ` in the `hyper_var.py` file to `min(1, torch.cuda.device_count())`, the bug disappear()(see the comment in the `hyper_var.py` file).\r\n\r\nWhen I change the  `AVAIL_GPUS ` in the `hyper_var.py` file to `\"1,2\"`, it returns this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 21, in <module>\r\n    trainer.fit(model, cifar10_dm)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self._run(model)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 753, in _run\r\nTraceback (most recent call last):\r\n  File \"/root/code/test-of-pytorch-lightning/train.py\", line 21, in <module>\r\n    self.pre_dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 778, in pre_dispatch\r\n    trainer.fit(model, cifar10_dm)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self.accelerator.pre_dispatch(self)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 108, in pre_dispatch\r\n    self.training_type_plugin.pre_dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 279, in pre_dispatch\r\n    self._run(model)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 753, in _run\r\n    self.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 286, in barrier\r\n    torch_distrib.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1960, in barrier\r\n    self.pre_dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 778, in pre_dispatch\r\n    self.accelerator.pre_dispatch(self)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 108, in pre_dispatch\r\n    work = _default_pg.barrier()\r\n    self.training_type_plugin.pre_dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 279, in pre_dispatch\r\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1607370172916/work/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8\r\n    self.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 286, in barrier\r\n    torch_distrib.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1960, in barrier\r\n    work = _default_pg.barrier()\r\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1607370172916/work/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8\r\n```\r\n\r\nThe hardware parameters of my server are:\r\n```\r\nNVIDIA GeForce 3090\r\nCUDA Version: 11.2\r\nDriver Version: 460.39\r\n```\r\nCan anyone help me to fix the bug?\r\n\r\nThanks.This error can happen due to a number of reasons, could you try launching your script as:\r\n```CUDA_LAUNCH_BLOCKING=1 python script.py args```\r\nto hopefully get a more meaningfull error.However, after I change the `AVAIL_GPUS` to \"1,2\" and I run `CUDA_LAUNCH_BLOCKING=1 python train.py`, it returns this error:\r\n\r\n```\r\nGlobal seed set to 7\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nGlobal seed set to 7\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\r\nGlobal seed set to 7\r\nGlobal seed set to 7\r\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\r\nLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2]\r\nTraceback (most recent call last):\r\n  File \"/root/code/test-of-pytorch-lightning/train.py\", line 21, in <module>\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 21, in <module>\r\n    trainer.fit(model, cifar10_dm)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    trainer.fit(model, cifar10_dm)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self._run(model)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 753, in _run\r\n    self._run(model)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 753, in _run\r\n    self.pre_dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 778, in pre_dispatch\r\n    self.pre_dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 778, in pre_dispatch\r\n    self.accelerator.pre_dispatch(self)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 108, in pre_dispatch\r\n    self.training_type_plugin.pre_dispatch()\r\n    self.accelerator.pre_dispatch(self)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 279, in pre_dispatch\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 108, in pre_dispatch\r\n    self.training_type_plugin.pre_dispatch()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 279, in pre_dispatch\r\n    self.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 286, in barrier\r\n    self.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 286, in barrier\r\n    torch_distrib.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1960, in barrier\r\n    torch_distrib.barrier()\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1960, in barrier\r\n    work = _default_pg.barrier()\r\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1607370172916/work/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8\r\n    work = _default_pg.barrier()\r\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1607370172916/work/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8\r\n```OK, I know the reason. The reason is that I haven't installed NCCL on my machine.\r\n\r\nAfter I installed NCCL on my machine, the errors disappeared.\r\n\r\nThanks to all the people who have replied to me. @awaelchli @SkafteNicki ",
    "meta": {
      "name": "RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
    },
    "answer": "OK, I know the reason. The reason is that I haven't installed NCCL on my machine.\r\n\r\nAfter I installed NCCL on my machine, the errors disappeared.\r\n\r\nThanks to all the people who have replied to me. @awaelchli @SkafteNicki "
  },
  {
    "content": "It's weired. I have a single machine with 8 GPUSs and now 0~4 are full load.\r\nI would like to use parallel training on 5 6 7 GPUs but cannot assign the task to them.\r\n\r\nSome of my codes:\r\n\r\n`    parser = argparse.ArgumentParser(description='Solver')\r\n    parser.add_argument('--config', required=True, type=str)\r\n    fargs = parser.parse_args()\r\n    args = parse_config(fargs.config)\r\n\r\n    data = DDPMData(args.Data)\r\n    data.train_dataloader()\r\n    data.test_dataloader()\r\n\r\n    num_cls = data.n_classes\r\n    shape = data.data_shapes\r\n    args.Model.Unet.kwargs.num_classes = num_cls\r\n    args.Model.DiscreteDiffusion.kwargs.num_class = num_cls\r\n    args.Model.DiscreteDiffusion.kwargs.shape = shape\r\n    model = DDDPM(args)\r\n\r\n    wandb_logger = WandbLogger()\r\n\r\n    callbacks = []\r\n    callbacks.append(ModelCheckpoint(save_last=True,every_n_train_steps=600))\r\n    callbacks.append(LearningRateMonitor(logging_interval='step'))\r\n\r\n    trainer = pl.Trainer(callbacks=callbacks,max_steps=args.max_steps, \r\n                    accelerator='dp', gpus=[5,6,7],\r\n                    logger = wandb_logger, check_val_every_n_epoch=120,\r\n                    num_sanity_val_steps=0)\r\n    trainer.fit(model, data)\r\n`\r\nAnd now nvidia-smi shows:\r\n\r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  On   | 00000000:04:00.0 Off |                  N/A |\r\n| 63%   72C    P2   151W / 200W |   7997MiB /  8119MiB |     85%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA GeForce ...  On   | 00000000:06:00.0 Off |                  N/A |\r\n| 71%   77C    P2   134W / 200W |   8109MiB /  8119MiB |     98%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  NVIDIA GeForce ...  On   | 00000000:07:00.0 Off |                  N/A |\r\n| 82%   83C    P2   143W / 200W |   7405MiB /  8119MiB |     89%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  NVIDIA GeForce ...  On   | 00000000:08:00.0 Off |                  N/A |\r\n| 80%   83C    P2   158W / 200W |   7263MiB /  8119MiB |     59%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  NVIDIA GeForce ...  On   | 00000000:0C:00.0 Off |                  N/A |\r\n| 78%   82C    P2   148W / 200W |   5719MiB /  8119MiB |    100%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  NVIDIA GeForce ...  On   | 00000000:0D:00.0 Off |                  N/A |\r\n|  0%   24C    P8     7W / 200W |      4MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  NVIDIA GeForce ...  On   | 00000000:0E:00.0 Off |                  N/A |\r\n|  0%   34C    P8     8W / 200W |      4MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  NVIDIA GeForce ...  On   | 00000000:0F:00.0 Off |                  N/A |\r\n|  0%   25C    P8     7W / 200W |      4MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      4082      C   python                           7993MiB |\r\n|    1   N/A  N/A     13619      C   python                           8105MiB |\r\n|    2   N/A  N/A      4082      C   python                           7401MiB |\r\n|    3   N/A  N/A      4082      C   python                           7259MiB |\r\n|    4   N/A  N/A     24206      C   python                           5715MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\nand when I run the codes, it will assign task to GPU:2 , I don't know why..\r\n`\r\nRuntimeError: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 2; 7.93 GiB total capacity; 117.19 MiB already allocated; 45.50 MiB free; 120.00 MiB reserved in total by PyTorch)`\r\n\r\nAny idea..?\r\n\r\n\r\n##############################\r\ntorch = 1.7.1\r\npl = 1.3.8Problemed solved.\r\nI forgot to map device when I load a ckpt file as data...",
    "meta": { "name": "I cannot assign the GPU index by ddp or dp backends.." },
    "answer": "Problemed solved.\r\nI forgot to map device when I load a ckpt file as data..."
  },
  {
    "content": "Hi,\r\n\r\nWhen calling `train.fit(model)` I get the following TypeError. I got a couple of more similar errors but could find solutions to them (mainly due to the newer version of pl), but I could not find any fix for following error:\r\n\r\n```\r\n  File \"train.py\", line 540, in <module>\r\n    main(args)\r\n  File \"train.py\", line 506, in main\r\n    logger=logger,\r\n  File \"/Structure-Aware-BART/src/lightning_base.py\", line 700, in generic_train\r\n    trainer.fit(model)\r\n  File \"/anaconda3/envs/s-bart/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self._run(model)\r\n  File \"/anaconda3/envs/s-bart/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 713, in _run\r\n    self.call_setup_hook(model)  # allow user to setup lightning_module in accelerator environment\r\n  File \"/anaconda3/envs/s-bart/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1161, in call_setup_hook\r\n    model.setup(stage=fn)\r\nTypeError: setup() got an unexpected keyword argument 'stage'\r\n```\r\n\r\nAny pointers would be much appreciated. ThxCan you share your LightningModule code? Are you overriding the `setup` function? If so, are you overriding it with this signature? https://github.com/PyTorchLightning/pytorch-lightning/blob/03bb389b218084f365b922f2a50133a5aaaadaf0/pytorch_lightning/core/hooks.py#L395",
    "meta": {
      "name": "TypeError: setup() got an unexpected keyword argument 'stage'"
    },
    "answer": "Can you share your LightningModule code? Are you overriding the `setup` function? If so, are you overriding it with this signature? https://github.com/PyTorchLightning/pytorch-lightning/blob/03bb389b218084f365b922f2a50133a5aaaadaf0/pytorch_lightning/core/hooks.py#L395"
  },
  {
    "content": "When I execute some program in PyTorch-Lightning, it implements very fast. But, at the last part, I got a message as below\r\n\r\n```\r\n/home/mydirectory/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: cleaning up ddp environment...\r\n\r\nwarnings.warn(*args, **kwargs)\r\n```\r\n\r\nAfter this message, all the system stops. I can't see the result that I want stopped by this message.\r\n\r\nCan anyone tell me how to solve it? I'm in Hurry \u3160.\u3160\r\n\r\n(I am using it with `wandb` & `HuggingFace`.)@data-weirdo mind share some sample code to reproduce? I have been using DDP in some of our examples and all is fine :rabbit: ",
    "meta": { "name": "UserWarning: cleaning up ddp environment..." },
    "answer": "@data-weirdo mind share some sample code to reproduce? I have been using DDP in some of our examples and all is fine :rabbit: "
  },
  {
    "content": "The device of the metric return by validation_step is GPU, related code is\r\n```python\r\ndef validation_step(self, batch, batch_idx):\r\n    x, y = batch\r\n    if y.device != self.device:\r\n        y = y.to(self.device)\r\n    y_hat = self(x)\r\n    loss = self.loss(y_hat, y)    # loss.device is cuda.\r\n    self.log('valid loss', loss.item())\r\n    return loss\r\n```\r\n\r\nAfter an epoch of validation compeleted when using earlystopping, follow error occured: \r\n```\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 871, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\", line 584, in run_training_epoch\r\n    self.trainer.run_evaluation(on_epoch=True)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1011, in run_evaluation\r\n    self.evaluation_loop.on_evaluation_end()\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py\", line 102, in on_evaluation_end\r\n    self.trainer.call_hook('on_validation_end', *args, **kwargs)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1228, in call_hook\r\n    trainer_hook(*args, **kwargs)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\callback_hook.py\", line 227, in on_validation_end\r\n    callback.on_validation_end(self, self.lightning_module)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py\", line 173, in on_validation_end\r\n    self._run_early_stopping_check(trainer)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py\", line 193, in _run_early_stopping_check\r\n    should_stop, reason = self._evalute_stopping_criteria(current, trainer)\r\n  File \"E:\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py\", line 226, in _evalute_stopping_criteria\r\n    elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\r\n```\r\n\r\nThis error didn't appear until I updated the version of pytorch_lightning.Looking into it in #8295",
    "meta": {
      "name": "Earlystopping callback metrics in different devices with single gpu"
    },
    "answer": "Looking into it in #8295"
  },
  {
    "content": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nImport error after installing pytorch-lightning on google colab TPU instance.\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\nColab link: https://colab.research.google.com/drive/1ssH3PwBh6skcIze440LwHn5R5dUnih7Q?usp=sharing\r\n\r\n\r\nCode to reproduce:\r\n\r\n```\r\n!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\r\n!pip install pytorch-lightning\r\n\r\nimport pytorch_lightning as pl\r\n```\r\n\r\nConsole output:\r\n\r\n```\r\nWARNING:root:Waiting for TPU to be start up with version pytorch-1.8...\r\nWARNING:root:Waiting for TPU to be start up with version pytorch-1.8...\r\nWARNING:root:TPU has started up successfully with version pytorch-1.8\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-efd8697dde72> in <module>()\r\n----> 1 import pytorch_lightning as pl\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/torch_xla/__init__.py in <module>()\r\n    126 import torch\r\n    127 from ._patched_functions import _apply_patches\r\n--> 128 import _XLAC\r\n    129 \r\n    130 \r\n\r\nImportError: /usr/local/lib/python3.7/dist-packages/_XLAC.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN2at11result_typeERKNS_6TensorEN3c106ScalarE\r\n```\r\n\r\nI followed the instructions from the docs: https://pytorch-lightning.readthedocs.io/en/stable/advanced/tpu.html. I also tried xla version 1.7, 1.6, and pl version 1.1.8.@kaushikb11 I guess it is unrelated to PL, right?Hi @BryanWBear, this error is raised when the PyTorch and PyTorch xla are not of the same versions.\r\nYou could verify using `pip list | grep torch` and could install the latest versions for both!",
    "meta": { "name": "ImportError: _XLAC.cpython: undefined symbol" },
    "answer": "Hi @BryanWBear, this error is raised when the PyTorch and PyTorch xla are not of the same versions.\r\nYou could verify using `pip list | grep torch` and could install the latest versions for both!"
  },
  {
    "content": "I want to marry lightning and https://pytorch-geometric.readthedocs.io/en/latest/ or in particular https://pytorch-geometric-temporal.readthedocs.io/en/latest/\r\n\r\nWhen following the basic examples on their website such as for the ChickenpoxDatasetLoader() a RecurrentGCN is constructed. For me being a total newbie for lightning it is already pretty clear how t convert that to a regular lightning module - kudos to the easy API so far.\r\n\r\nHowever, it is rather unclear for me how to put the training loop into a lightning compatible trainer:\r\n\r\n```py\r\nfrom tqdm import tqdm\r\nmodel = RecurrentGCN(node_features = 4) # chickenpox\r\nmodel\r\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\r\nmodel.train()\r\nfor epoch in tqdm(range(200)):\r\n    cost = 0\r\n    for time, snapshot in enumerate(train_dataset):\r\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\r\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\r\n    cost = cost / (time+1)\r\n    cost.backward()\r\n    optimizer.step()\r\n    optimizer.zero_grad() \r\n```\r\n\r\nWould I need a custom trainer in lightning?\r\nIn particular, I need to be able to handle temporal graphs with snapshots over time i.e. think of a social network of people who can perform a hobby at a certain location (lat, long ) and timestamp. I guess it would be fine to discretize the time to i.e. weekly or monthly slices, but the graph is dynamic.\r\n\r\n```py\r\nimport pandas as pd\r\ndf = pd.DataFrame({'person_1':[], 'person_2':[], 'time':[], 'lat':[], 'long':[], 'hobby':[]})\r\ndisplay(df)\r\n```\r\n\r\nI want to perform link prediction - i.e. recommend new friends based on similar hobbies in similar locations & time ranges.\r\n\r\nWith that being said: in the pytorch-geometric-temporal framework they denote snapshots over time (this is not meant as batches, currently they assume that a snapshot contains all the data in a single batch for that particular span of time).\r\n\r\nHowever, the default trainer does not offer this functionality to iterate over snapshots and to me it is unclear how to include it.just found https://github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/examples/lightning_example.py  - seems to be the answer to my question - almost, except it is not covering how to handle the iteration over the temporal snapshots.I dont know much about pytorch geometric but are you trying to have an additional state (tensor) that is passed in between steps?",
    "meta": { "name": "How to customize training loop?" },
    "answer": "just found https://github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/examples/lightning_example.py  - seems to be the answer to my question - almost, except it is not covering how to handle the iteration over the temporal snapshots."
  },
  {
    "content": "I am trying to predict on my test data but it throws an error:\r\nTypeError: conv2d(): argument 'input' (position 1) must be Tensor, not list\r\n\r\nTo help you get context:\r\nI have called a pre-trained model and am trying to predict on the model parameters. The model is resnet18().\r\n\r\nThe `test_loader` is a dataloader that loads images from a test image folder. The folder has labelled folders of the test files.\r\n   \r\n```py\r\n# test dataloader\r\ntest_dataset = torchvision.datasets.ImageFolder(\r\n    'path/to/test_data_labelled_folders',\r\n    transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\r\n)\r\n\r\ntestloader = torch.utils.data.DataLoader(\r\n    test_dataset, num_workers=4, batch_size=4)\r\n\r\n# loading previously trained model \r\nmodel = LitClassifier.load_from_checkpoint('path/to/checkpoint.ckpt') #LitClassifier is a Lightningmodule\r\n\r\n# calling predict function\r\ntrainer = pl.Trainer()\r\ntrainer.predict(model=model, dataloaders=[testloader], return_predictions=True)\r\n```\r\n\r\nDid you overwrite the `predict_step`? By default it just feeds the whole batch through `forward` (which with the image folder also includes the label and therefore is a list)\r\n\r\nSo you have two choices: Remove the labels from you predict data or overwrite the predict step to ignore them :)",
    "meta": { "name": "Error with predict()" },
    "answer": "Did you overwrite the `predict_step`? By default it just feeds the whole batch through `forward` (which with the image folder also includes the label and therefore is a list)\r\n\r\nSo you have two choices: Remove the labels from you predict data or overwrite the predict step to ignore them :)"
  },
  {
    "content": "Hi all, I am running a script that performs some calculations and then does a testing loop using multi GPU using a single node with DDP.\r\nThe code looks like this:\r\n\r\n```py\r\nsome prepocesing code\r\n...\r\nmodel = LitModel(path_weights)\r\n\r\ndataloader = DataLoader(dataset, batch_size=512,\r\n                        shuffle=False, num_workers=32)\r\ntrainer = pl.Trainer(accelerator='ddp', gpus=8)\r\ntrainer.test(model, dataloader)\r\n```\r\nThe issue that I have is that it seems the the whole script is being computed several times, I can see that the preprocessing code is being called 8 times. It seems to be the same issue as posted in SO (https://stackoverflow.com/questions/66261729/pytorch-lightning-duplicates-main-script-in-ddp-mode)\r\nAm I missing something in my code?\r\n\r\nThanks!after reading more carefully I realized that it is what it is supposed to do, I would like to know however, if is there a way to avoid replicating some heavy calculations several times, say I want to do the postprocessing only ones and share the results with all the other processes? i",
    "meta": { "name": "ddp replicates whole script" },
    "answer": "after reading more carefully I realized that it is what it is supposed to do, I would like to know however, if is there a way to avoid replicating some heavy calculations several times, say I want to do the postprocessing only ones and share the results with all the other processes? i"
  },
  {
    "content": "I train a model with 2 GPUs\uff0c when running ```predictions = trainer.predict(model, datamodule)``` in a script, what I get is not a single variable ```predictions``` as desired, instead, I get two independent variables on two GPUs. I tried to use ```on_predict_end``` and ```on_predict_batch_end``` to aggregate the predictions on different GPUs, but it seems they change the behavior of the method ```trainer.predict```\uff0c i.e. I can't get an aggregated ```predicitons```.  What should I do to aggregate predictions defined in ```predict_step``` when using multiple GPUs?\r\n\r\n---\r\nTo see the issue, simple create a ```LightningModule``` with the ```predict_step``` method \r\n```py\r\nclass Model(pl.LightningModule):\r\n    ...\r\n    def predict_step(self, batch, batch_idx, dataloader_idx):\r\n        y = self(x)\r\n        return {\"predict\":y}\r\n \r\nm = Modle(...)\r\n```\r\nand use any ```pl.DataModule``` with a ```predict_dataloader``` method \r\n```py\r\ntrainer = pl.Trainer(gpus=2, accelerator=ddp)\r\npredictions = trainer.predict(model=m, datamodule=dm)\r\n```\r\nbut we'll get two ```predictions``` on two GPUs(they are actually the outputs of two scripts that ```pytorch_lightning``` creates for us anyway...), and we can't use it as a single object for later processing in the script. \r\nYou can either sync them by writing them both to disk (we have a `PredictionWriter` for that) or you can use `self.all_gather` inside your `predict_step` to sync across GPUs (note that this can lead to GPU OOM!)",
    "meta": {
      "name": "predict with multiple GPUs doesn't aggregate the predictions even with on_predict_end or on_predict_batch_end"
    },
    "answer": "You can either sync them by writing them both to disk (we have a `PredictionWriter` for that) or you can use `self.all_gather` inside your `predict_step` to sync across GPUs (note that this can lead to GPU OOM!)"
  },
  {
    "content": "How to get predictions \r\n```py\r\n  class OurModel(LightningModule):\r\n      def __init__(self):\r\n        super(OurModel,self).__init__()\r\n        self.layer = MyModelV3()\r\n      def forward(self,x):\r\n        return self.layer(x)\r\n\r\n  def train_dataloader(self):\r\n    return DataLoader(DataReader(train_df))\r\n\r\n  def training_step(self,batch,batch_idx):\r\n    return loss\r\n\r\n  def test_dataloader(self):\r\n    return DataLoader(DataReader(test_df))\r\n    \r\n  def test_step(self,batch,batch_idx):\r\n    image,label=batch\r\n    out=self(image)\r\n    loss=self.criterion(out,label)\r\n    return loss\r\n\r\n  def predict(self, batch):\r\n        return self(batch) \r\n ```\r\n\r\nI am not sure, how to use predict function. How to define data loader for predict function. I want to get predictions for `test_df`. But now idea how to do this.\r\n Hi @talhaanwarch, in order to get predictions from a data loader you need to implement predict_step in your `LightningModule` (docs here: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#predict-step). You would then be able to call `Trainer.predict` with the dataloader you want use following the API here: https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer.predict\r\n\r\nHope that helps :smiley:Dear @talhaanwarch,\r\n\r\nTo create a dataloder, just do `DataLoader(predict_dataset)`.\r\n\r\n```py\r\npredict_dataloader = DataLoader(predict_dataset)\r\n\r\nclass Model(LightningModule):\r\n\r\n    def __init__(self, ...):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.model = ...\r\n\r\n    def forward(self, batch):\r\n        return self.model(batch)\r\n\r\n    def predict_step(self, batch, batch_idx, dataloder_idx = None):\r\n        return self(batch)\r\n\r\ntrainer = Trainer(...)\r\npredictions = trainer.predict(model, dataloaders=predict_dataloader)\r\n```\r\n\r\n",
    "meta": { "name": "How to use predict function to return predictions" },
    "answer": "Hi @talhaanwarch, in order to get predictions from a data loader you need to implement predict_step in your `LightningModule` (docs here: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#predict-step). You would then be able to call `Trainer.predict` with the dataloader you want use following the API here: https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer.predict\r\n\r\nHope that helps :smiley:"
  },
  {
    "content": "Hello,\r\n\r\nI have a jitted function within which I need to use the output of a neural network (trained using PyTorch Lightning). The pseudo code will make this clearer:\r\n```py\r\nwhile True:\r\n    x = sample_from_model() # \u2190 numpy type, hence compatible with numba\r\n    out = NN(torch.Tensor(x)) # \u2190 incompatible with numba\r\n```\r\nIs there a way to circumvent this problem? First thing that comes to mind is to manually extract the weights and compute the forward pass.\r\n\r\nThanks in advance,\r\nPetarHi Petar,\r\n\r\nI'm not that familiar with numba, but if it runs with numpy types, you should be able to do this via ONNX export. \r\n\r\nyou should be able to simply get this with `my_lightning_model.to_onnx()`.\r\n\r\nNote: This dumps it to disk and you can use the onnx runtime for prediction then.",
    "meta": { "name": "NN output within a numba jitted function" },
    "answer": "Hi Petar,\r\n\r\nI'm not that familiar with numba, but if it runs with numpy types, you should be able to do this via ONNX export. \r\n\r\nyou should be able to simply get this with `my_lightning_model.to_onnx()`.\r\n\r\nNote: This dumps it to disk and you can use the onnx runtime for prediction then."
  },
  {
    "content": "I am trying to run trainer.fit with only models passed. However, I am getting the following error.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 116, in <module>\r\n    main_func(args)\r\n  File \"train.py\", line 55, in main_func\r\n    trainer.fit(model, train_dataloader=train_dataloader, val_dataloaders=val_dataloader)\r\n  File \"/netscratch/gsingh/EVN/Anaconda/envs/lighting_neptune/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 443, in fit\r\n    self.model_connector.copy_trainer_model_properties(model)\r\n  File \"/netscratch/gsingh/EVN/Anaconda/envs/lighting_neptune/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/model_connector.py\", line 39, in copy_trainer_model_properties\r\n    m.precision = self.trainer.precision\r\n  File \"/netscratch/gsingh/EVN/Anaconda/envs/lighting_neptune/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 982, in __setattr__\r\n    raise TypeError(\"cannot assign '{}' as child module '{}' \"\r\nTypeError: cannot assign 'int' as child module 'precision' (torch.nn.Module or None expected)\r\n```\r\n\r\n\r\nI am unable to figure this out. Any help would be appreciated\r\n\r\n```py\r\n    trainer = Trainer(\r\n        logger=logger, resume_from_checkpoint=args.resume_from_checkpoint, max_epochs=args.num_epochs,\r\n        accumulate_grad_batches=args.gradient_accumulation_steps, \r\n        default_root_dir=args.experiment_path, checkpoint_callback=ckpt, num_sanity_val_steps=args.num_sanity_val_steps, \r\n        gradient_clip_val=args.gradient_clip_val, gpus=1, auto_select_gpus=True, sync_batchnorm=args.sync_batchnorm\r\n    )\r\n                        \r\n    criterion = torch.nn.CrossEntropyLoss()\r\n    criterion_val = torch.nn.CrossEntropyLoss()\r\n    \r\n    model = Model(args, tr_dl=train_dataloader, val_dl=val_dataloader, test_dl=test_dataloader, criterion=criterion, criterion_val=criterion_val)\r\n    trainer.fit(model)\r\n```\r\nHere precision value is the default.\r\n\r\nThank YouDo you have any properties in your `Model` class named `precision` ? Having the same issue here. Seems to be no problem if I comment out [model_connecter](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/connectors/model_connector.py#L30) here",
    "meta": {
      "name": "cannot assign 'int' as child module 'precision' (torch.nn.Module or None expected) Error"
    },
    "answer": "Do you have any properties in your `Model` class named `precision` ? "
  },
  {
    "content": "how to use ```LightningDataModule```  to process tfrecords data, anyone can give a Tutorial How you use python and pytorch to handle tfrecords data is how you use it in `LightningDataModule`. pytorchlightning is just a wrapper.Depends on what data you have inside TFRecord, but you can see usage:\r\n https://github.com/vahidk/tfrecord#reading-tfexample-records-in-pytorch\r\n\r\n```py\r\ndef train_dataloader():\r\n    # index_path = None\r\n    # tfrecord_path = \"/tmp/data.tfrecord\"\r\n    # description = {\"image\": \"byte\", \"label\": \"float\"}\r\n    dataset = TFRecordDataset(tfrecord_path, index_path, description)\r\n    loader = torch.utils.data.DataLoader(dataset, batch_size=32)\r\n    return loader\r\n```\r\n\r\nalso check how to work with dataset - https://discuss.pytorch.org/t/read-dataset-from-tfrecord-format/16409/15",
    "meta": { "name": "how to use pl to process tfrecords data?" },
    "answer": "Depends on what data you have inside TFRecord, but you can see usage:\r\n https://github.com/vahidk/tfrecord#reading-tfexample-records-in-pytorch\r\n\r\n```py\r\ndef train_dataloader():\r\n    # index_path = None\r\n    # tfrecord_path = \"/tmp/data.tfrecord\"\r\n    # description = {\"image\": \"byte\", \"label\": \"float\"}\r\n    dataset = TFRecordDataset(tfrecord_path, index_path, description)\r\n    loader = torch.utils.data.DataLoader(dataset, batch_size=32)\r\n    return loader\r\n```\r\n\r\nalso check how to work with dataset - https://discuss.pytorch.org/t/read-dataset-from-tfrecord-format/16409/15"
  },
  {
    "content": "I need to plot confusion_matrix in tensotboard ,how to do it ?https://stackoverflow.com/questions/65498782/how-to-dump-confusion-matrix-using-tensorboard-logger-in-pytorch-lightning",
    "meta": {
      "name": "how to plot confusion_matrix with lightning tensorboard?"
    },
    "answer": "https://stackoverflow.com/questions/65498782/how-to-dump-confusion-matrix-using-tensorboard-logger-in-pytorch-lightning"
  },
  {
    "content": "Hi! I have found a weird behavior when using ModelCheckpoint, if I have a metric that I want to save in my filename and it has a \"/\" on it it will create nested directories. For example\r\n\r\n```py\r\ncheckpoint_callback = ModelCheckpoint(\r\n        monitor='val/acc',\r\n        dirpath=checkpoints_dir,\r\n        filename='checkpoint_{epoch:02d}-{val/acc}',\r\n        save_top_k=-1,\r\n     )\r\n```\r\nThis one will create one extra folder per checkpoint:\r\n```bash\r\ncheckpoints/base_lstm/checkpoint_epoch=00-val/acc=0.04-v1.ckp\r\ncheckpoints/base_lstm/checkpoint_epoch=00-val/acc=0.05-v1.ckp\r\n```\r\nIs there any way to make the modelcheckpoint callback store the \"val/acc\" value while not using the string \"val/acc\" to reference it? Something like:\r\ncheckpoints/base_lstm/checkpoint_epoch=00-valAcc=0.04-v1.ckp\r\nI think is quite standard to use \"/\" on tensorboard to be able to use the inbuilt tabs to better group metrics.hi, can you provide also your model sample, in particular, the metrics section\r\nI guess that the problem is with `/` as it is interpreted as a normal folder path, as you can see that `'val/acc'` is not replaced by a number either... :rabbit: ",
    "meta": { "name": "ModelCheckpoint creating unexpected subfolders" },
    "answer": "hi, can you provide also your model sample, in particular, the metrics section\r\nI guess that the problem is with `/` as it is interpreted as a normal folder path, as you can see that `'val/acc'` is not replaced by a number either... :rabbit: "
  },
  {
    "content": "I am experimenting with the following repository. [Keiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning](https://github.com/Keiku/PyTorch-Lightning-CIFAR10)\r\n\r\nIt is implemented as follows. I was able to train/validation. But test has not been implemented yet.\r\n\r\n```python\r\nimport os\r\n\r\nimport hydra\r\nimport torch\r\nfrom hydra.core.hydra_config import HydraConfig\r\nfrom omegaconf import DictConfig, OmegaConf\r\nfrom pytorch_lightning import Trainer, seed_everything\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\r\n\r\nfrom datamodule import LitCIFAR10DataModule\r\nfrom model import LitCIFAR10Model\r\n\r\n\r\n@hydra.main(config_path=\"./configs\", config_name=\"default.yaml\")\r\ndef main(cfg: DictConfig) -> None:\r\n\r\n    if \"experiments\" in cfg.keys():\r\n        cfg = OmegaConf.merge(cfg, cfg.experiments)\r\n\r\n    seed_everything(0)\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = cfg.runs.gpu_id\r\n\r\n    if cfg.runs.logger == \"wandb\":\r\n        logger = WandbLogger(name=cfg.model.classifier, project=\"cifar10\")\r\n    elif cfg.runs.logger == \"tensorboard\":\r\n        logger = TensorBoardLogger(cfg.train.tensorboard_dir, name=cfg.model.classifier)\r\n\r\n    checkpoint = ModelCheckpoint(monitor=\"acc/val\", mode=\"max\", save_last=True)\r\n\r\n    trainer = Trainer(\r\n        fast_dev_run=cfg.runs.dev,\r\n        logger=logger if not (cfg.runs.dev or cfg.runs.evaluate) else None,\r\n        gpus=-1,\r\n        deterministic=True,\r\n        weights_summary=None,\r\n        log_every_n_steps=1,\r\n        max_epochs=cfg.train.num_epochs,\r\n        checkpoint_callback=checkpoint,\r\n        precision=cfg.runs.precision,\r\n        resume_from_checkpoint=cfg.train.checkpoint,\r\n    )\r\n\r\n    datamodule = LitCIFAR10DataModule(cfg)\r\n    model = LitCIFAR10Model(cfg)\r\n\r\n    if cfg.runs.evaluate:\r\n        model = LitCIFAR10Model.load_from_checkpoint(checkpoint_path=cfg.test.checkpoint)\r\n        trainer.test(model, datamodule.test_dataloader())\r\n    else:\r\n        trainer.fit(model, datamodule)\r\n        trainer.test()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nI tried using `load_from_checkpoint` for test, but I get the following error. How can I solve it?\r\n\r\n```bash\r\n\u22ca> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main \u2a2f\r\npipenv run python train.py +experiments=test_exp01 hydra.run.dir=outputs/test_exp01\r\nGlobal seed set to 0\r\nGPU available: True, used: True\r\nTPU available: None, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 48, in main\r\n    model = LitCIFAR10Model.load_from_checkpoint(checkpoint_path=cfg.test.checkpoint)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\", line 159, in load_from_checkpoint\r\n    model = cls._load_model_state(checkpoint, strict=strict, **kwargs)\r\n  File \"/home/anasys/.local/share/virtualenvs/PyTorch-Lightning-CIFAR10-fAnnMMRx/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\", line 199, in _load_model_state\r\n    model = cls(**_cls_kwargs)\r\nTypeError: __init__() missing 1 required positional argument: 'cfg'\r\n\r\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\r\n\u22ca> /m/n/k/c/PyTorch-Lightning-CIFAR10 on main \u2a2f\r\n```I was able to infer test by setting as follows. I think the documentation is missing, so please refer to my implementation.\r\n\r\n```python\r\n    if cfg.runs.evaluate:\r\n        hparams = OmegaConf.load(cfg.test.hparams)\r\n        model = LitCIFAR10Model.load_from_checkpoint(checkpoint_path=cfg.test.checkpoint, **hparams)\r\n        trainer.test(model, datamodule.test_dataloader())\r\n    else:\r\n        trainer.fit(model, datamodule)\r\n        trainer.test()\r\n```",
    "meta": {
      "name": "In load_from_checkpoint, \"TypeError: __init__ () missing 1 required positional argument:'cfg'\""
    },
    "answer": "I was able to infer test by setting as follows. I think the documentation is missing, so please refer to my implementation.\r\n\r\n```python\r\n    if cfg.runs.evaluate:\r\n        hparams = OmegaConf.load(cfg.test.hparams)\r\n        model = LitCIFAR10Model.load_from_checkpoint(checkpoint_path=cfg.test.checkpoint, **hparams)\r\n        trainer.test(model, datamodule.test_dataloader())\r\n    else:\r\n        trainer.fit(model, datamodule)\r\n        trainer.test()\r\n```"
  },
  {
    "content": "Hi there,\r\n\r\nI am trying to implement a data module however I keep getting an error that I cannot understand. I normally setup my data module as:\r\n```python\r\nclass dataModule(pl.LightningDataModule):\r\n    def __init__(self, batch_size, csv_file, data_dir):\r\n        super().__init__()\r\n        self.csv_file = csv_file\r\n        self.data_dir = data_dir\r\n        self.batch_size = batch_size\r\n        self.preprocess = None\r\n        self.transform = None\r\n        self.train_set = None\r\n        self.val_set = None\r\n        self.test_set = None\r\n    \r\n    def get_augmentation_transform(self):\r\n        augment = tio.Compose([\r\n            tio.RandomAffine(),\r\n            tio.RandomFlip(p = 0.25),\r\n            tio.RandomGamma(p=0.25),\r\n            tio.RandomNoise(p=0.25),\r\n            tio.RandomMotion(p=0.1),\r\n            tio.RandomBiasField(p=0.25),\r\n        ])\r\n        return augment\r\n\r\n    def setup(self, stage=None):\r\n        \r\n        subjList = fmriDataset(csv_file = self.csv_file,\r\n                              root_dir = self.data_dir)\r\n    \r\n        train_size, val_size = int(0.7 * len(subjList)), int(0.2 * len(subjList))\r\n        test_size = len(subjList) - train_size - val_size\r\n        \r\n        if stage == 'fit' or stage is None:\r\n            self.train_dataset, self.val_dataset, _ = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n\r\n        if stage == 'test' or stage is None:\r\n            _, _, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n\r\n        augment = self.get_augmentation_transform()\r\n\r\n        self.train_set = tio.SubjectsDataset(self.train_dataset, transform=augment)\r\n        self.val_set = tio.SubjectsDataset(self.val_dataset, transform=None)\r\n        self.test_set = tio.SubjectsDataset(self.test_dataset, transform=None)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.train_set, self.batch_size, shuffle=True, num_workers=27)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.val_set, self.batch_size, num_workers=27)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.test_set, self.batch_size, num_workers=27)\r\n```\r\n\r\nHowever, when I call `trainer.tune(model = model, datamodule = data)` I get the error:\r\n```python\r\nAttributeError: 'dataModule' object has no attribute 'test_dataset'\r\n```\r\n<br><br><br><br>\r\nHowever, if I change these lines\r\n```python\r\nif stage == 'fit' or stage is None:\r\n    self.train_dataset, self.val_dataset, _ = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n\r\nif stage == 'test' or stage is None:\r\n    _, _, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n```\r\nto a single line:\r\n```python\r\nself.train_dataset, self.val_dataset, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n```\r\neverything works. Is there something basic that I have missed?\r\n<br><br><br><br>\r\nFor completeness:\r\n- Pytorch version: 1.9.0\r\n- Pytorch-lightning version: 1.3.7\r\n\r\nAnd I initialise the data module/model/trainer with:\r\n```python\r\ndata = dataModule(data_dir = '/home/data/', csv_dir = '/home/scanList.csv', batch_size = 24)\r\n\r\nmodel = cnnRnnClassifier()\r\n\r\nearly_stop_callback = Earlystopping(\r\n    monitor = 'val_loss',\r\n    min_delta = 1e-4,\r\n    patience = 10,\r\n    Verbose = True,\r\n    mode = 'min')\r\n\r\ntrainer = Trainer(\r\n    gpus =  1,\r\n    fast_dev_run = False,\r\n    max_epochs = 100,\r\n    weights_summary = 'full',\r\n    callbacks = [early_stop_callback],\r\n    auto_lr_find = True,\r\n    precision = 16)\r\n\r\ntrainer.tune(model = model, datamodule = data)\r\ntrainer.fit(model = model, datamodule = data)\r\ntrainer.test(model = model, datamodule = data)\r\n```\r\n\r\nThanks in advance for your help!The problem is the combination of this line:\r\n\r\n```python\r\n if stage == 'test' or stage is None:\r\n            _, _, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n\r\n```\r\n\r\nand this line:\r\n```python\r\nself.test_set = tio.SubjectsDataset(self.test_dataset, transform=None)\r\n```\r\n\r\nas you can see, `self.test_dataset` is only defined if the condition above applies. With this hint you should be able to figure it out now. Let me know :) ",
    "meta": { "name": "Help understanding data module error" },
    "answer": "The problem is the combination of this line:\r\n\r\n```python\r\n if stage == 'test' or stage is None:\r\n            _, _, self.test_dataset = torch.utils.data.random_split(subjList, [train_size, val_size, test_size])\r\n\r\n```\r\n\r\nand this line:\r\n```python\r\nself.test_set = tio.SubjectsDataset(self.test_dataset, transform=None)\r\n```\r\n\r\nas you can see, `self.test_dataset` is only defined if the condition above applies. With this hint you should be able to figure it out now. Let me know :) "
  },
  {
    "content": "What's the difference between `on_fit_start` and `on_train_start` hooks in LightningModule?\r\n![image](https://user-images.githubusercontent.com/13477956/123500499-1a8c8f80-d671-11eb-9598-ebfc6f25c809.png)\r\nI think the document [here](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#hooks) has answered your question very well.",
    "meta": {
      "name": "What's the difference between on_fit_start and on_train_start in LightningModule?"
    },
    "answer": "I think the document [here](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#hooks) has answered your question very well."
  },
  {
    "content": "Hi,\r\n\r\nSince the `1.3.3`, I cannot use the flag `accumulate_grad_batches` for the training of GANs.\r\nIt is immediately reproducible on the GAN example given in `pl_examples.domain_templates.generative_adversarial_net`.\r\n\r\nFor any version higher than `1.3.2`, in the `Trainer`, if `accumulate_grad_batches` is set to any value higher than `1`, after one batch (`batch_idx == 1`) the following error arises:\r\n```\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n```\r\nThis is due to the fact that the `g_loss` does not require gradients, because the generator's output does not requires any gradients.\r\n\r\nIs it intended?\r\nIf yes, what is the workaround to train GANs while still accumulating gradients?\r\n\r\nThanks in advance,\r\n\r\nGuillaumeHey, I believe this is the same bug I fixed here: https://github.com/PyTorchLightning/pytorch-lightning/pull/8284 \r\nWill try to put it into the next bugfix release. \r\n\r\nAs a workaround, you can try to manually call `self.untoggle_optmizer(optimizer)` in your LM.",
    "meta": {
      "name": "How to accumulate grad batches for GANs in `pytorch_lightning >= 1.3.3`?"
    },
    "answer": "Hey, I believe this is the same bug I fixed here: https://github.com/PyTorchLightning/pytorch-lightning/pull/8284 \r\nWill try to put it into the next bugfix release. \r\n\r\nAs a workaround, you can try to manually call `self.untoggle_optmizer(optimizer)` in your LM."
  },
  {
    "content": "Hi there!\r\n\r\nI am trying to build a very basic CNN for a binary classification task however I am getting an odd dimensionality issue.\r\n\r\nMy CNN is:\r\n\r\n```python\r\nclass convNet(nn.Module):\r\n    def __init__(self):\r\n        \r\n        super().__init__()\r\n        self.conv2d_1 = nn.Conv2d(193, 193, kernel_size=3)\r\n        self.conv2d_2 = nn.Conv2d(193, 193, kernel_size=3)\r\n        self.conv2d_3 = nn.Conv2d(193, 193, kernel_size=3)\r\n        self.maxpool = nn.MaxPool2d(2)\r\n    \r\n    def forward(self, x):\r\n        x = self.conv2d_1(x)\r\n        x = F.relu(x)\r\n        x = self.maxpool(x)\r\n        x = self.conv2d_2(x)\r\n        x = F.relu(x)\r\n        x = self.maxpool(x)\r\n        x = self.conv2d_3(x)\r\n        return F.relu(x)\r\n```\r\n\r\nAnd my lightning module is:\r\n```python\r\n\r\nclass classifier(pl.LightningModule):\r\n    \r\n    def __init__(self, learning_rate = float):\r\n        super().__init__()\r\n        \r\n        self.learning_rate = learning_rate\r\n        self.cnn =covNet()\r\n        self.flat = nn.Flatten()\r\n        \r\n        self.fc1 = nn.Linear(450076, 100)\r\n        self.fc2 = nn.Linear(100, 10)\r\n        self.fc3 = nn.Linear(10, 1)\r\n        \r\n        self.dropout = nn.Dropout(p = 0.2)\r\n        \r\n        self.criterion = nn.BCEWithLogitsLoss()\r\n        \r\n        self.accuracy = tm.Accuracy()\r\n\r\n    def prepare_batch(self, batch):\r\n        img = batch['image'][tio.DATA]\r\n        img = torch.squeeze(img)\r\n        diagnosis = batch['diagnosis']\r\n        return img, diagnosis\r\n\r\n    def forward(self, x):\r\n        cnn_out = self.cnn(x)\r\n        flat = self.flat(cnn_out)\r\n        fc1_out = self.dropout(F.relu(self.fc1(flat)))\r\n        fc2_out = self.dropout(F.relu(self.fc2(fc1_out)))\r\n        fc3_out = F.relu(self.fc3(fc2_out))\r\n        return fc3_out\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        x, y = self.prepare_batch(batch)\r\n        y = y.view(y.size(0), -1)\r\n        y = y.type(torch.float)\r\n        y_hat = self.forward(x)\r\n        train_loss = self.criterion(y_hat, y)\r\n        self.log('train_loss', train_loss, prog_bar = True)\r\n        return train_loss\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = self.prepare_batch(batch)\r\n        y = y.view(y.size(0), -1)\r\n        y = y.type(torch.float)\r\n        y_hat = self.forward(x)\r\n        val_loss = self.criterion(y_hat, y)\r\n        self.log('val_loss', val_loss, prog_bar = True)\r\n        return val_loss\r\n        \r\n    def test_step(self, batch, batch_idx):\r\n        x, y = self.prepare_batch(batch)\r\n        y = y.view(y.size(0), -1)\r\n        y = y.type(torch.float)\r\n        y_hat = self.forward(x)\r\n        testAcc = self.accuracy(y_hat, y)\r\n        self.log_dict({'test_acc': testAcc})\r\n        return testAcc\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\r\n        return optimizer\r\n```\r\n\r\nThe odd part is that the validation sanity completes and I get 75% through the first epoch when the validation loop starts before I get the error:\r\n\r\n<img width=\"613\" alt=\"Screen Shot 2021-07-01 at 3 33 38 pm\" src=\"https://user-images.githubusercontent.com/57671726/124141934-b824fc00-da81-11eb-9dba-4197b7220117.png\">\r\n\r\nAnd I have checked that all my inputs have the same dimensions of `[193, 229, 193]`. Have I missed something obvious? (sorry if it is obvious)\r\n\r\nAny help would be greatly appreciated!\r\n\r\nFor completeness:\r\n- pytorch version: 1.9.0\r\n- pytorch lightning version: 1.3.7\r\nTo me it seems like you have forgotten the batch dimension. 2D convolutions expect input to have shape `[N, C, H, W]` where `C=193`, `H=229` and `W=193` (is it correct that you have the same amount of channels as the width?). If you only want to feed in a single image you can do `sample.unsqueeze(0)` to add the extra batch dimension in front.",
    "meta": { "name": "CNN dimension error" },
    "answer": "To me it seems like you have forgotten the batch dimension. 2D convolutions expect input to have shape `[N, C, H, W]` where `C=193`, `H=229` and `W=193` (is it correct that you have the same amount of channels as the width?). If you only want to feed in a single image you can do `sample.unsqueeze(0)` to add the extra batch dimension in front."
  },
  {
    "content": "I am experimenting with the following repository. [Keiku/PyTorch-Lightning-CIFAR10: \"Not too complicated\" training code for CIFAR-10 by PyTorch Lightning](https://github.com/Keiku/PyTorch-Lightning-CIFAR10)\r\n\r\nI have implemented two methods, one is to load CIFAR-10 from torchvision and the other is to load CIFAR-10 as a custom dataset. Also, I have implemented two models: a lightweight model (eg scratch resnet18, timm MobileNet V3, etc.) and a relatively heavy model (eg scratch resnet50, timm resnet152).\r\n\r\nAfter some experiments, I found the following.\r\n\r\n* GPU usage remains high (nearly 100%) on any model when loading CIFAR-10 with torchvision\r\n* When loading CIFAR-10 as a custom dataset, GPU usage remains relatively high (still temporarily zero) for heavy models\r\n* When loading CIFAR-10 as a custom dataset, GPU usage remains low (going back and forth between 0% and 100%) for lightweight models (resnet18, MobileNetV3)\r\n\r\nIn this situation, is there a problem with the implementation code of the custom dataset? Also, please let me know if there is a way to increase GPU usage even for lightweight models.\r\n\r\nI am experimenting in the following EC2 g4dn.xlarge environment.\r\n\r\n```\r\n\u22ca> ~ lsb_release -a                                                    (base) 21:45:51\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 18.04.5 LTS\r\nRelease:        18.04\r\nCodename:       bionic\r\n\u22ca> ~ nvidia-container-cli info                                         (base) 21:48:20\r\nNVRM version:   450.80.02\r\nCUDA version:   11.0\r\n\r\nDevice Index:   0\r\nDevice Minor:   0\r\nModel:          Tesla T4\r\nBrand:          Tesla\r\nGPU UUID:       GPU-ba54be15-066e-e7e5-87d0-84b8ac2672c6\r\nBus Location:   00000000:00:1e.0\r\nArchitecture:   7.5\r\n```I got a replay from ptrblck.\r\n\r\nhttps://discuss.pytorch.org/t/gpu-usage-does-not-remain-high-for-lightweight-models-when-loaded-cifar-10-as-a-custom-dataset/125738",
    "meta": {
      "name": "GPU usage does not remain high for lightweight models when loaded CIFAR-10 as a custom dataset"
    },
    "answer": "I got a replay from ptrblck.\r\n\r\nhttps://discuss.pytorch.org/t/gpu-usage-does-not-remain-high-for-lightweight-models-when-loaded-cifar-10-as-a-custom-dataset/125738"
  },
  {
    "content": "Hello i managed to implement training accuracy per epoch with training_epoch_end but i want to do the same with validation accuracy with validation_epoch_end but i get an error of \"too many indices for tensor of dimension 0\" when train.fit() . I am using pytorch-lightning==1.2.8  . Thanks in advance.\r\n\r\n\r\nError is: \r\n\r\n```\r\n<ipython-input-31-41c968828bca> in validation_epoch_end(self, outputs)\r\n     39     predictions = []\r\n     40     for output in outputs:\r\n---> 41       for out_labels in output[\"labels\"].detach().cpu():\r\n     42         labels.append(out_labels)\r\n     43       for out_predictions in output[\"predictions\"].detach().cpu():\r\n\r\nIndexError: too many indices for tensor of dimension 0\r\n```\r\n\r\n\r\n ```\r\n def validation_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\r\n    return loss\r\n\r\n  def validation_epoch_end(self, outputs):\r\n\r\n    labels = []\r\n    predictions = []\r\n    for output in outputs:\r\n      for out_labels in output[\"labels\"].detach().cpu():\r\n        labels.append(out_labels)\r\n      for out_predictions in output[\"predictions\"].detach().cpu():\r\n        predictions.append(out_predictions)\r\n\r\n    labels = torch.stack(labels).int()\r\n    predictions = torch.stack(predictions)\r\n\r\n    validation_acc = accuracy(predictions, labels)\r\n    self.logger.experiment.add_scalar(\"Validation Accuracy\", validation_acc, self.current_epoch)  \r\n\r\n\r\n\r\n```I am a fool , i forgot to add return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels} in the validation step.",
    "meta": { "name": "Getting Validation Accuracy with validation_epoch_end" },
    "answer": "I am a fool , i forgot to add return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels} in the validation step."
  },
  {
    "content": "Hi, can you please tell me if I am doing something wrong here, especially the manual update for the generator and the critic:\r\n\r\n```python\r\nclass Generator(nn.Module):\r\n    def __init__(self, latent_dim=64, img_shape=None):\r\n        super().__init__()\r\n        self.img_shape = img_shape\r\n        self.init_size = 8 #self.img_shape[1] // 4\r\n\r\n        self.l1 = nn.Sequential(\r\n            nn.Linear(latent_dim, 64*self.init_size**2), nn.LeakyReLU(0.2, inplace=True))\r\n        self.conv_blocks = nn.Sequential(\r\n            nn.BatchNorm2d(64),\r\n            nn.Upsample(scale_factor=2),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0),\r\n            nn.BatchNorm2d(64),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Upsample(scale_factor=2),\r\n            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=0),\r\n            nn.BatchNorm2d(32),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Upsample(scale_factor=2),\r\n            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=0),\r\n            nn.BatchNorm2d(16),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Upsample(scale_factor=2),\r\n            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, padding=1),\r\n            nn.BatchNorm2d(8),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=8, out_channels=img_shape[0], kernel_size=3, padding=1),\r\n            nn.Tanh()\r\n        )\r\n    \r\n    def forward(self, z):\r\n        out = self.l1(z)\r\n        out = out.view(out.shape[0], 64, self.init_size, self.init_size)\r\n        img = self.conv_blocks(out)\r\n        return img\r\n\r\nclass Critic(nn.Module):\r\n    def __init__(self, img_shape):\r\n        super().__init__()\r\n        self.disc = nn.Sequential(\r\n            nn.Conv2d(in_channels=img_shape[0], out_channels=16, kernel_size=4, stride=2),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(16, 32, kernel_size=4, stride=2),\r\n            nn.BatchNorm2d(32),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n            nn.BatchNorm2d(64),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(64, 128, kernel_size=4, stride=2),\r\n            nn.BatchNorm2d(128),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n        )\r\n\r\n        # The height and width of downsampled image\r\n        #\r\n        ds_size = 2 ** 4\r\n\r\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size, 1))\r\n    def forward(self, img):\r\n        out = self.disc(img)\r\n        # import pdb; pdb.set_trace()\r\n        out = out.view(out.shape[0], -1)\r\n        validity = self.adv_layer(out)\r\n        return validity       \r\n\r\nclass WGANGP(pl.LightningModule):\r\n    def __init__(self, latent_dim=128, lr=0.0002, lambda_pen=10, crit_repeats=5):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.latent_dim = latent_dim\r\n        self.lr = lr\r\n        self.lambda_pen = lambda_pen\r\n        self.crit_repeats = crit_repeats\r\n        self.b1 = 0.0\r\n        self.b2 = 0.9\r\n\r\n        ### initializing networks\r\n        img_shape = (1, 100, 100)\r\n        self.generator = Generator(self.latent_dim, img_shape)\r\n        self.critic = Critic(img_shape)\r\n\r\n        # application of weight\r\n        self.generator.apply(self.weights_init)\r\n        self.critic.apply(self.weights_init)\r\n        #\r\n        self.validation_z = torch.randn(10, self.latent_dim)\r\n        self.example_input_array = torch.zeros(10, self.latent_dim)\r\n\r\n        # Important: This property activates manual optimization.\r\n        self.automatic_optimization = False # True - Auto // # False - Manual update\r\n\r\n    def forward(self, z):\r\n        return self.generator(z)\r\n\r\n    ### weight initialization\r\n    def weights_init(self, m):\r\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n        if isinstance(m, nn.BatchNorm2d):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n            torch.nn.init.constant_(m.bias, 0)\r\n        if isinstance(m, nn.Linear):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n            torch.nn.init.constant_(m.bias, 0)\r\n        \r\n    def training_step(self, batch, batch_idx):\r\n        imgs = batch\r\n\r\n        # # sample noise\r\n        # z = torch.randn(imgs.shape[0], self.latent_dim)\r\n        # z = z.type_as(imgs)\r\n\r\n        # optimizers, manual access\r\n        g_opt, c_opt = self.optimizers()\r\n\r\n        # update critic\r\n        mean_iteration_critic_loss = 0\r\n        for _ in range(self.crit_repeats):\r\n            c_opt.zero_grad()\r\n            # sample noise\r\n            z = torch.randn(imgs.shape[0], self.latent_dim).type_as(imgs)\r\n            # fake image\r\n            fake = self(z)\r\n            crit_fake_pred = self.critic(fake.detach())\r\n            crit_real_pred = self.critic(imgs)\r\n            # eps\r\n            epsilon = torch.rand(len(imgs), 1, 1, 1, device=self.device, requires_grad=True)\r\n            # gradient penalty\r\n            gp = self.gradient_penalty(self.critic, imgs, fake, epsilon)\r\n\r\n            # critic loss\r\n            critic_loss = torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + self.lambda_pen * gp\r\n\r\n            # Keep track of the average critic loss in this batch\r\n            mean_iteration_critic_loss += critic_loss.item() / crit_repeats\r\n\r\n            # Update gradients\r\n            self.manual_backward(critic_loss)\r\n            # Update optimizer\r\n            c_opt.step()\r\n\r\n        # log critic average loss\r\n        self.log('c_loss_mean', mean_iteration_critic_loss, prog_bar=True)\r\n        \r\n        # update generator\r\n        g_opt.zero_grad()\r\n        # sample new noise\r\n        z_new = torch.randn(imgs.shape[0], self.latent_dim).type_as(imgs)\r\n        # new fake image\r\n        fake_new = self(z_new)\r\n        crit_fake_pred = self.critic(fake_new)\r\n        # generator loss\r\n        gen_loss = -torch.mean(crit_fake_pred)\r\n\r\n        # Update gradients\r\n        self.manual_backward(gen_loss)\r\n        # Update optimizer\r\n        g_opt.step()\r\n\r\n        # log generator average loss\r\n        self.log('g_loss', gen_loss, prog_bar=True)\r\n\r\n    def gradient_penalty(self, crit, real, fake, epsilon):\r\n        # mix/interpolate images\r\n        mixed_images = real * epsilon + fake * (1 - epsilon)\r\n\r\n        # Calculate the critic's scores on the mixed images\r\n        mixed_scores = crit(mixed_images)\r\n\r\n        # Take the gradient of the scores with respect to the images\r\n        gradient = torch.autograd.grad(\r\n            inputs=mixed_images,\r\n            outputs=mixed_scores,\r\n            grad_outputs=torch.ones_like(mixed_scores),\r\n            create_graph=True,\r\n            retain_graph=True,\r\n        )[0]\r\n\r\n        # Flatten the gradients so that each row captures one image\r\n        gradient = gradient.view(len(gradient), -1)\r\n\r\n        # Calculate the magnitude of every row\r\n        gradient_norm = gradient.norm(2, dim=1)\r\n\r\n        # Penalize the mean squared distance of the gradient norms from 1\r\n        gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\r\n\r\n        return gradient_penalty\r\n    \r\n    def configure_optimizers(self):\r\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.b1, self.b2))\r\n        opt_c = torch.optim.Adam(self.critic.parameters(), lr=self.lr, betas=(self.b1, self.b2))\r\n        return opt_g, opt_c\r\n\r\n    def on_epoch_end(self):\r\n        z = self.validation_z.to(self.device)\r\n        # log sampled images\r\n        sample_imgs = self(z)\r\n        grid = torchvision.utils.make_grid(sample_imgs)\r\n        self.logger.experiment.add_image('generated_images', grid, self.current_epoch)\r\n\r\n# defining the hyperparameters\r\nn_epochs = 1000\r\nz_dim = 50\r\nbatch_size = 64\r\nlr = 0.0002\r\nc_lambda = 10\r\ncrit_repeats = 5\r\n```\r\n\r\ndesired output\r\ncurrent output\r\n![image](https://user-images.githubusercontent.com/54369128/120057409-9efadc80-bff7-11eb-90f6-5cd421891557.png)\r\n\r\n*SOLVED* \r\nSolved.",
    "meta": { "name": "WGANGP not working as expected" },
    "answer": "Solved."
  },
  {
    "content": "The validating log does not remain in the console. How can I print and display it?\r\n\r\nThe current `validation_step` is as follows.\r\n\r\n```python\r\n    def validation_step(self, batch, batch_nb):\r\n        loss, accuracy = self.forward(batch)\r\n        self.log(\"loss/val\", loss)\r\n        self.log(\"acc/val\", accuracy)\r\n        return loss\r\n```\r\n\r\nIt will be displayed for a moment as shown below, but it will not remain as a log.\r\n\r\n```bash\r\nGlobal seed set to 0\r\nGPU available: True, used: True\r\nTPU available: None, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nFiles already downloaded and verified\r\nFiles already downloaded and verified\r\nEpoch 26:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 114/116 [00:25<00:00,  4.50it/s, loss=0.623, v_num=0]\r\nValidating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 19/19 [00:01<00:00, 18.68it/s]\r\n```\r\n\r\n```\r\nGlobal seed set to 0\r\nGPU available: True, used: True\r\nTPU available: None, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nFiles already downloaded and verified\r\nFiles already downloaded and verified\r\nEpoch 28:   3%|\u258c                 | 4/116 [00:01<00:42,  2.62it/s, loss=0.588, v_num=0]\r\n```Yes that's correct, it's a UI design choice. \r\nIf you wish to see your values in the progress bar, you can log with `self.log(\"loss/val\", loss, prog_bar=True)` and it will show up in the main bar.\r\n",
    "meta": { "name": "The validating log does not remain in the console." },
    "answer": "Yes that's correct, it's a UI design choice. \r\nIf you wish to see your values in the progress bar, you can log with `self.log(\"loss/val\", loss, prog_bar=True)` and it will show up in the main bar.\r\n"
  },
  {
    "content": "My training_step returns {\u2018loss\u2019: loss, \u2018num\u2019: len(batch)}, so I could calculate mean loss at the end of epoch. But now I get this warning, that my training_step returns None and loss doesn't display in the progress bar. How can I return dict from training_step and display loss in the progress bar at the same tim?> Can training step return dictionary?\r\n\r\nYes, check the [doc](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step) of `LightningModule.training_step`.\r\n\r\n> How can I return dict from training_step and display loss in the progress bar at the same time?\r\n\r\njust return dict like what you do now and use `self.log(loss, prog_bar=True)` to display loss in prog_bar.\r\n\r\n> so I could calculate mean loss at the end of epoch\r\n\r\nBTW, If you just want mean loss, I think pytorch_lightning have already done for you and you don't need to modified training_step_end(not sure)\r\n\r\n",
    "meta": { "name": "Can training step return dictionary?" },
    "answer": "> Can training step return dictionary?\r\n\r\nYes, check the [doc](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step) of `LightningModule.training_step`.\r\n\r\n> How can I return dict from training_step and display loss in the progress bar at the same time?\r\n\r\njust return dict like what you do now and use `self.log(loss, prog_bar=True)` to display loss in prog_bar.\r\n\r\n> so I could calculate mean loss at the end of epoch\r\n\r\nBTW, If you just want mean loss, I think pytorch_lightning have already done for you and you don't need to modified training_step_end(not sure)\r\n\r\n"
  },
  {
    "content": "Hey, as the title says, I want to access the DataModule or the DataLoader inside the on fit start hook. Is this possible and how can I do it? To be more specifc I want to access my model, when I have access to my DataModule, to get a batch of data, then use it to apply some pruning algorithm on my model. `self.datamodule` or `self.trainer.train_dataloader` in the LightningModule",
    "meta": {
      "name": "Accessing DataModule or DataLoaders within model hooks"
    },
    "answer": "`self.datamodule` or `self.trainer.train_dataloader` in the LightningModule"
  },
  {
    "content": "There is no train.test () in the lower version. How to test the test data set?\r\nmy version: 0.4.6\r\n\r\nHi\r\nPyTorch Lightning 0.4.6 is extremely old. You should consider upgrading. \r\nThat being said, you can always test your model as you would in plain pytorch, because the LightningModule is also just a nn.Module:\r\n\r\n```python\r\nfor inp, target in test_dataloader:\r\n    pred = model(inp)\r\n    test_loss = loss(pred, target)\r\n    ...\r\n```",
    "meta": { "name": "How to test in low version" },
    "answer": "Hi\r\nPyTorch Lightning 0.4.6 is extremely old. You should consider upgrading. \r\nThat being said, you can always test your model as you would in plain pytorch, because the LightningModule is also just a nn.Module:\r\n\r\n```python\r\nfor inp, target in test_dataloader:\r\n    pred = model(inp)\r\n    test_loss = loss(pred, target)\r\n    ...\r\n```"
  },
  {
    "content": "My dataset is large, with total CPU memory usage of 20 GB. I train on 2 nodes with 8 GPU. And I use slurm to train it. But I found that each process will consume 20 GB memory, which is equivelence to 80 GB each node. That's not what I want. I want a node to consume only 20GB in total. Is there a way to do that?\r\n```python\r\nclass DataModule(LightningDataModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.batch_size = 1\r\n        self.CT_dataset=np.load(\"./CT_dataset.npy\")#shape:(7000,1,512,512)\r\n        self.MR_dataset=np.load(\"./MR_dataset.npy\")#shape:(7000,1,512,512)\r\n        self.batch_size=1\r\n        self.CT_dataset = torch.from_numpy(self.CT_dataset)\r\n        self.CT_dataset = self.CT_dataset.float()\r\n        self.MR_dataset = torch.from_numpy(self.MR_dataset)\r\n        self.MR_dataset = self.MR_dataset.float()\r\n        self.train_dataset, self.test_dataset = random_split(TensorDataset(self.MR_dataset,self.CT_dataset), [len(self.CT_dataset)-100, 100])\r\n    def train_dataloader(self):\r\n        return DataLoader(self.train_dataset, batch_size=self.batch_size)\r\n    def test_dataloader(self):\r\n        return DataLoader(self.test_dataset, batch_size=self.batch_size)\r\nmodel = CycleGAN()\r\nds = DataModule()\r\nlogger = TensorBoardLogger(save_dir=\"./run\")\r\ntrainer = pl.Trainer(max_epochs=1,fast_dev_run=False,profiler=\"pytorch\",overfit_batches=8,gpus=4,logger=logger,accelerator='ddp',num_nodes=2,auto_scale_batch_size='power',weights_summary='full')\r\ntrainer.fit(model, ds)\r\ntrainer.test(model,datamodule=ds)\r\n```\r\nMy code will raise `MemoryError: Unable to allocate 7.00 GiB for an array with shape (1879572480,) and data type int32`, I can't think of a way to solve it.\r\nSince your data is in one single binary file, it won't be possible to reduce the memory footprint. Each ddp process is independent from the others, there is no shared memory. You will have to save each dataset sample individually, so each process can access a subset of these samples through the dataloader and sampler. \r\n",
    "meta": { "name": "how to load dataset only once on the same machine?" },
    "answer": "Since your data is in one single binary file, it won't be possible to reduce the memory footprint. Each ddp process is independent from the others, there is no shared memory. You will have to save each dataset sample individually, so each process can access a subset of these samples through the dataloader and sampler. \r\n"
  },
  {
    "content": "Can I set the epoch/step initial value to 1? Now the initial default is 0, it feels awkward when watching Tensorboard.\r\nIn addition, can I bring plots in the training and valid in one tersorboard plot?Dear @yuys0602,\r\n\r\nWe are starting at 0 as we are counting the number of epoch and before starting, you actually haven't completed any epoch.\r\nIf this is really an issue for you, I believe there might be a hacky solution.\r\n\r\nI believe this should be working if you are using master. @carmocca Could you confirm ?\r\n\r\n```\r\ndef training_step()\r\n\r\n    self.log(\"epoch\", self.trainer.current_epoch + 1)\r\n```\r\n\r\nConcerning, `can I bring plots in the training and valid in one tersorboard plot?` \r\nUnfortunately, I believe this is a feature provided or not by Logger and Lightning can't do much about it.\r\n\r\nBest,\r\nT.C\r\n",
    "meta": { "name": "Can I set the epoch/step initial value to 1?" },
    "answer": "Dear @yuys0602,\r\n\r\nWe are starting at 0 as we are counting the number of epoch and before starting, you actually haven't completed any epoch.\r\nIf this is really an issue for you, I believe there might be a hacky solution.\r\n\r\nI believe this should be working if you are using master. @carmocca Could you confirm ?\r\n\r\n```\r\ndef training_step()\r\n\r\n    self.log(\"epoch\", self.trainer.current_epoch + 1)\r\n```\r\n\r\nConcerning, `can I bring plots in the training and valid in one tersorboard plot?` \r\nUnfortunately, I believe this is a feature provided or not by Logger and Lightning can't do much about it.\r\n\r\nBest,\r\nT.C\r\n"
  },
  {
    "content": "when i try to write a model, i got `forward() takes 1 positional argument but 2 were given` error, this is my code, i want to know the wrong plcace, thanks!!\r\n\r\ni guess the error is in UpSample place, but i don't know why...\r\n\r\n```Python\r\nclass DownSample(nn.Module):\r\n\r\n    def __init__(self, in_planes: int, out_planes: int, kernel_size: int):\r\n        super(DownSample, self).__init__()\r\n\r\n        self.down = nn.Sequential(\r\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=2, padding=1),\r\n            nn.BatchNorm2d(out_planes),\r\n            nn.LeakyReLU()\r\n        )\r\n\r\n        init_weight.initialize(self)\r\n\r\n    def forward(self, x):\r\n        return self.down(x)\r\n\r\n\r\nclass UpSample(nn.Module):\r\n\r\n    def __init__(self, in_planes: int, out_planes: int,\r\n                 kernel_size: int, padding: int, output_padding: int,\r\n                 apply_dropout: bool = False):\r\n        super(UpSample, self).__init__()\r\n\r\n        self.up = nn.ModuleList()\r\n\r\n        self.up.append(\r\n            nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride=2,\r\n                               padding=padding, output_padding=output_padding),\r\n        )\r\n        self.up.append(nn.BatchNorm2d(out_planes))\r\n        if apply_dropout:\r\n            self.up.append(nn.Dropout())\r\n        self.up.append(nn.LeakyReLU())\r\n\r\n        init_weight.initialize(self)\r\n\r\n    def forward(self, inputs):\r\n        return self.up(inputs)\r\n```\r\n\r\n```Python\r\nclass MyEncoder(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MyEncoder, self).__init__()\r\n\r\n        down_stack = [\r\n            pix2pix.DownSample(3, 64, 4),\r\n            pix2pix.DownSample(64, 128, 4),\r\n            pix2pix.DownSample(128, 256, 4),\r\n            pix2pix.DownSample(256, 512, 4),\r\n            pix2pix.DownSample(512, 512, 4),\r\n            pix2pix.DownSample(512, 512, 4),\r\n            pix2pix.DownSample(512, 512, 4),\r\n            pix2pix.DownSample(512, 512, 4),\r\n        ]\r\n\r\n        self.encoder = nn.ModuleList()\r\n\r\n        for item in down_stack:\r\n            self.encoder.append(item)\r\n\r\n    def forward(self, inputs):\r\n        feat = inputs\r\n        for i in range(len(self.encoder)):\r\n            feat = self.encoder[i](feat)\r\n\r\n        return feat\r\n\r\n\r\nclass MyDecoder(nn.Module):\r\n    def __init__(self):\r\n        super(MyDecoder, self).__init__()\r\n\r\n        up_stack = [\r\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\r\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\r\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\r\n            pix2pix.UpSample(512, 512, 4, 1, 1, True),\r\n            pix2pix.UpSample(512, 256, 4, 1, 1, True),\r\n            pix2pix.UpSample(256, 128, 4, 1, 1, True),\r\n            pix2pix.UpSample(256, 128, 4, 1, 1, True),\r\n            pix2pix.UpSample(128, 64, 4, 1, 1, True),\r\n        ]\r\n\r\n        self.up = nn.ModuleList()\r\n\r\n        for item in up_stack:\r\n            self.up.append(item)\r\n\r\n    def forward(self, inputs):\r\n\r\n        return self.up(inputs)\r\n\r\n\r\nclass MyNet(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super(MyNet, self).__init__()\r\n\r\n        self.encoder = MyEncoder()\r\n        self.decoder = MyDecoder()\r\n\r\n    def forward(self, inputs):\r\n        feat = self.encoder(inputs)\r\n        feat = self.decoder(feat)\r\n        return feat\r\n\r\n\r\nif __name__ == '__main__':\r\n    from torchsummaryX import summary\r\n\r\n    import torch\r\n\r\n    x = torch.ones((1, 3, 512, 512))\r\n    u = UNet()\r\n\r\n    summary(model=u, x=x)\r\n```Hi, have a look at the full stack trace so you know which of the forward methods of these different `nn.Modules` is meant. \r\n\r\nhave you verified that\r\n`u(x)` \r\nworks?Adrian W\u00e4lchli ***@***.***>\u4e8e2021\u5e746\u670825\u65e5 \u5468\u4e9408:48\u5199\u9053\uff1a\n\n> Hi, have a look at the full stack trace so you know which of the forward\n> methods of these different nn.Modules is meant.\n>\n> have you verified that\n> u(x)\n> works?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/PyTorchLightning/pytorch-lightning/discussions/8091#discussioncomment-918111>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIPI3S7TJJXVU55SGJAM3ZLTUPG7NANCNFSM47E3RCPA>\n> .\n>\nHi, thanks for your reply,i resolved this problem. \ud83d\ude00\n",
    "meta": {
      "name": "forward() takes 1 positional argument but 2 were given"
    },
    "answer": "Hi, have a look at the full stack trace so you know which of the forward methods of these different `nn.Modules` is meant. \r\n\r\nhave you verified that\r\n`u(x)` \r\nworks?"
  },
  {
    "content": "Hello. Please forgive the very basic question!\r\n\r\nI really like the perks and freebies that come with letting the Lightning Trainer handle training. However, for some applications I **do not need or want a dataloader**, and so far I've not been able to figure out how to use the Trainer without fooling it by defining a dummy DataLoader that does nothing. :roll_eyes: \r\n\r\nI would like to know if there is a 'nice' way to use the 'standard' Lightning setup of a LightningModule + Trainer that bypasses any dependence on a data loader.\r\n\r\nThe context is that model (normalizing flow) inputs are generated on-demand by sampling from some known distribution, and the loss function is the KL divergence with respect to some other distribution whose un-normalised density function is known.\r\n\r\nGrateful for any suggestions!\r\n\r\nCheers,\r\nJoe.Hi Joe,\r\n\r\nYou don't need a DataLoader. You just need something we can iterate on, that produces batches. \r\nThat being said, some features (like automatically adapting the sampler for distributed training) won't work in that case, but it sounds like you don't need them anyways.\r\n\r\nAnother possibility would be to use the `IterableDataset` from PyTorch and wrap that one into a loader for convencience.\r\n\r\nBest,\r\nJustus@justusschock is it worth us updating the typehints on the dataloader APIs to reflect that users only need to provide an iterable? or should we continue to rely on ducktyping?",
    "meta": {
      "name": "Best way to bypass requirement of a DataLoader when inputs are generated stochastically on-demand"
    },
    "answer": "Hi Joe,\r\n\r\nYou don't need a DataLoader. You just need something we can iterate on, that produces batches. \r\nThat being said, some features (like automatically adapting the sampler for distributed training) won't work in that case, but it sounds like you don't need them anyways.\r\n\r\nAnother possibility would be to use the `IterableDataset` from PyTorch and wrap that one into a loader for convencience.\r\n\r\nBest,\r\nJustus"
  },
  {
    "content": "Hi, I have 4 gpus on my machine. I want to select the third one, that has index 2. How do I pass an argument from CLI? \r\nI call \r\n`python 4_pretrain_encoder.py --gpus 2 --max_epochs 5`\r\nbut it runs one script on two gpus instead of the third.`python 4_pretrain_encoder.py --gpus 2, --max_epochs 5`\r\n\r\nMaybe you should search first\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/discussions/7461",
    "meta": { "name": "Selecting one gpu from cli" },
    "answer": "`python 4_pretrain_encoder.py --gpus 2, --max_epochs 5`\r\n\r\nMaybe you should search first\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/discussions/7461"
  },
  {
    "content": "I'm implementing the same functionality for _validation_step_ and _test_step_.\r\nCurrently, I have implemented it by calling to a shared function (_val_and_test_step_)\r\n```\r\n    def val_and_test_step(self, data_batch, batch_nb):\r\n        output = shared_functionality(data_batch, batch_nb)\r\n        return output\r\n\r\n    def validation_step(self, data_batch, batch_nb):\r\n        return self.val_and_test_step(data_batch, batch_nb)\r\n\r\n    def test_step(self, data_batch, batch_nb):\r\n        return self.val_and_test_step(data_batch, batch_nb) \r\n```\r\nIs there a more _pythonic_ way to implement the above?Dear @ItamarKanter,\r\n\r\nI believe this great and pretty pythonic ! \r\n\r\nYou could do this to make it slightly cleaner.\r\n```\r\nclass Model(LightningModule)\r\n\r\n    def common_step(self, batch, batch_idx, stage):\r\n        logits = self(batch[0])\r\n        loss = self.compute_loss(logits, batch[1])\r\n        self.log(f\"{state}_loss\", loss)\r\n        return loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self.common_step(batch, batch_idx, \"train\")\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        self.common_step(batch, batch_idx, \"val\")\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        self.common_step(batch, batch_idx, \"test\")\r\n```\r\n\r\nBest,\r\nT.C",
    "meta": { "name": "share steps among test and validation steps" },
    "answer": "Dear @ItamarKanter,\r\n\r\nI believe this great and pretty pythonic ! \r\n\r\nYou could do this to make it slightly cleaner.\r\n```\r\nclass Model(LightningModule)\r\n\r\n    def common_step(self, batch, batch_idx, stage):\r\n        logits = self(batch[0])\r\n        loss = self.compute_loss(logits, batch[1])\r\n        self.log(f\"{state}_loss\", loss)\r\n        return loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self.common_step(batch, batch_idx, \"train\")\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        self.common_step(batch, batch_idx, \"val\")\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        self.common_step(batch, batch_idx, \"test\")\r\n```\r\n\r\nBest,\r\nT.C"
  },
  {
    "content": "this is my model `__init__` func\r\n```Python\r\ndef __init__(self, num_classes: int, image_channels: int = 3, drop_rate: int = 0.5,\r\n                 filter_config: tuple = (64, 128, 256, 512, 512), attention=False):\r\n```\r\n\r\nthis is my load code:\r\n```Python\r\nm = SegNet(num_classes=1)\r\nmodel = m.load_from_checkpoint('checkpoints/epoch=99-step=312499.ckpt')\r\n```\r\nwhen i try to load a checkpoint model, i got this error:\r\n\r\n```\r\nTypeError: __init__() missing 1 required positional argument: 'num_classes'\r\n```\r\n\r\nwho can help me?Dear @morestart,\r\n\r\nGreat question ! You should use save_hyperparameters function, so Lightning can save your init arguments inside the checkpoint for future reload.\r\n\r\nHere is the associated doc: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html?highlight=save_hyperparameters#save-hyperparameters\r\n\r\n```\r\nclass SegNet(LightningModule)\r\n\r\n    def __init__(\r\n        self,\r\n        num_classes: int,\r\n        image_channels: int = 3,\r\n        drop_rate: int = 0.5,\r\n        filter_config: tuple = (64, 128, 256, 512, 512),\r\n        attention=False\r\n    ):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n        ....\r\n```\r\n\r\nBest,\r\nT.C\r\n        \r\n",
    "meta": { "name": "load checkpoint model error" },
    "answer": "Dear @morestart,\r\n\r\nGreat question ! You should use save_hyperparameters function, so Lightning can save your init arguments inside the checkpoint for future reload.\r\n\r\nHere is the associated doc: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html?highlight=save_hyperparameters#save-hyperparameters\r\n\r\n```\r\nclass SegNet(LightningModule)\r\n\r\n    def __init__(\r\n        self,\r\n        num_classes: int,\r\n        image_channels: int = 3,\r\n        drop_rate: int = 0.5,\r\n        filter_config: tuple = (64, 128, 256, 512, 512),\r\n        attention=False\r\n    ):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n        ....\r\n```\r\n\r\nBest,\r\nT.C\r\n        \r\n"
  },
  {
    "content": "I wanted to set shuffle to False. So, i tried\r\n```\r\nsampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=False)\r\ndataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\r\n```\r\nI am getting an error \r\n```\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n```\r\nPlease anyone tell me how to use custom sampler.You should set `shuffle=True` in your DataLoader directly\r\n\r\nRegarding the error, can you provide a script to reproduce it?\r\n\r\nYou can adapt https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.pyAdding on to the existing answer:\r\n\r\n`DataLoader(shuffle, sampler)` are mutually exclusive, i.e., if you set `shuffle=True` you will get a `RandomSampler` and if it is set to False you get a `SequentialSampler`.\r\n\r\nWhen using DDP, Lightning takes your dataloader and replaces it with the following\r\n`DataLoader(sampler=DistributedDampler(shuffle=True), ...)`, however ONLY if the sampler is not already a distributed sampler. If it is, no changes are done. \r\n\r\nSo OP can do the following:\r\n\r\n```python\r\ndef train_dataloader(self):\r\n    return DataLoader(sampler=DistributedSampler(shuffle=False), ...)`\r\n```\r\n    \r\nreturning their own sampler. Note this needs to be done in a place where the distributed group is already initialized, so basically in any hook after (including) `setup()`. OP got this error\r\n```\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n```\r\nBecause they probably did the following:\r\n\r\n```python\r\ndataloader = DataLoader(sampler=DistributedSampler(shuffle=False), ...) # fails here, because distributed not init yet\r\ntrainer = Trainer()\r\ntrainer.fit(dataloader)\r\n```\r\n",
    "meta": { "name": "Error while using custom DistributedSampler" },
    "answer": "Adding on to the existing answer:\r\n\r\n`DataLoader(shuffle, sampler)` are mutually exclusive, i.e., if you set `shuffle=True` you will get a `RandomSampler` and if it is set to False you get a `SequentialSampler`.\r\n\r\nWhen using DDP, Lightning takes your dataloader and replaces it with the following\r\n`DataLoader(sampler=DistributedDampler(shuffle=True), ...)`, however ONLY if the sampler is not already a distributed sampler. If it is, no changes are done. \r\n\r\nSo OP can do the following:\r\n\r\n```python\r\ndef train_dataloader(self):\r\n    return DataLoader(sampler=DistributedSampler(shuffle=False), ...)`\r\n```\r\n    \r\nreturning their own sampler. Note this needs to be done in a place where the distributed group is already initialized, so basically in any hook after (including) `setup()`. OP got this error\r\n```\r\nRuntimeError: Default process group has not been initialized, please make sure to call init_process_group.\r\n```\r\nBecause they probably did the following:\r\n\r\n```python\r\ndataloader = DataLoader(sampler=DistributedSampler(shuffle=False), ...) # fails here, because distributed not init yet\r\ntrainer = Trainer()\r\ntrainer.fit(dataloader)\r\n```\r\n"
  },
  {
    "content": "Hello,\r\n\r\nI have written a small class derived from `Callback` in order to log some outputs during training. Initially, a directory is created, for the corresponding files/outputs to be stored:\r\n\r\n```\r\nclass epochCallback(pl.callbacks.Callback):\r\n\r\n    @pl.utilities.rank_zero_only\r\n    def on_train_start(self, trainer, pl_module):\r\n        if not restart:\r\n            print(\"create output directory (rank {})\".format(self.model.global_rank))\r\n                if not isdir(aiOutputDir +\"outputs_train\"):\r\n                    os.mkdir(aiOutputDir +\"outputs_train\")\r\n```\r\nMy intention is to create the directory only once, for the main rank/GPU, that's why I use the `rank_zero_only` decorator.\r\nWhen I try it on multiple GPUs (ddp), the code hangs. If I create an empty directory in advance, then it runs! It seems to me like a barrier is needed before creating the dir... If this is the case, how do I add this in lightning? I have also noticed that the print statement is printed for all GPUs, so I am not sure if the decorator works as it should... Should I use os.environ() instead of the decorator, and specify explicitly the global ID rank which will create the directory?\r\n\r\nAny ideas? What is the standard practice here?\r\n\r\nThanks in advance,\r\nNikos\r\nCan you try to remove the decorator and instead do this?\r\n\r\n```python\r\ndef on_train_start(self, trainer, pl_module):\r\n        print(\"global_rank is\", trainer.global_rank)\r\n        if trainer.is_global_zero:\r\n            print(\"create output directory (rank {})\".format(self.model.global_rank))\r\n                if not isdir(aiOutputDir +\"outputs_train\"):\r\n                    os.mkdir(aiOutputDir +\"outputs_train\")\r\n```",
    "meta": { "name": "code hangs when creating a dir" },
    "answer": "Can you try to remove the decorator and instead do this?\r\n\r\n```python\r\ndef on_train_start(self, trainer, pl_module):\r\n        print(\"global_rank is\", trainer.global_rank)\r\n        if trainer.is_global_zero:\r\n            print(\"create output directory (rank {})\".format(self.model.global_rank))\r\n                if not isdir(aiOutputDir +\"outputs_train\"):\r\n                    os.mkdir(aiOutputDir +\"outputs_train\")\r\n```"
  },
  {
    "content": "Hi. I trained a model several times,\r\nand I found that Pytorch-lightning sometimes appends version, e.g. model-v1.ckpt, and sometimes doesn't append.\r\n(I don't know the reason because the codes of the two situations are the same)\r\n\r\nHere, I want to let PyTorch-lightning do not append any version. Is there any solution?Hi, it happens when the checkpoint tries to save the file but a file with that name already exists. It is to prevent accidentally overwriting/deleting a checkpoint. \r\n\r\nYou can prevent that for example by setting the filename to include epoch and step, e.g., \r\n\r\n```python\r\nModelCheckpoint(monitor=..., filename=\"your-filename-{epoch}-{step}\")\r\n```",
    "meta": {
      "name": "How to remove version number when saving model checkpoint?"
    },
    "answer": "Hi, it happens when the checkpoint tries to save the file but a file with that name already exists. It is to prevent accidentally overwriting/deleting a checkpoint. \r\n\r\nYou can prevent that for example by setting the filename to include epoch and step, e.g., \r\n\r\n```python\r\nModelCheckpoint(monitor=..., filename=\"your-filename-{epoch}-{step}\")\r\n```"
  },
  {
    "content": "`v_num` in progress bar means the version number of this running, and the log file's save directory is `version_{v_num}`.\r\n\r\nHow can i customize the directory's name, such as `version_GAT`, `version_GCN`.\r\nyou change it by passing your logger:\r\n```python\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\nlogger = TensorBoardLogger(\"tb_logs\", name=\"my_model\", version=\"GAT\")\r\ntrainer = Trainer(logger=logger)\r\n```",
    "meta": { "name": "How to customize the version or name of log file" },
    "answer": "you change it by passing your logger:\r\n```python\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\nlogger = TensorBoardLogger(\"tb_logs\", name=\"my_model\", version=\"GAT\")\r\ntrainer = Trainer(logger=logger)\r\n```"
  },
  {
    "content": "**Question:** \r\nI am trying to run the EpicKitchen codes from https://github.com/epic-kitchens/C1-Action-Recognition-TSN-TRN-TSM\r\nI am getting this Typeerror may be related to older and new versions of lightining module and i was not able to resolve it. \r\n**Error:**\r\nTraceback (most recent call last):\r\n  File \"src/test.py\", line 145, in <module>\r\n    main(parser.parse_args())\r\n  File \"src/test.py\", line 139, in main\r\n    trainer = Trainer(**cfg.trainer, callbacks=[saver])\r\n  File \"/home/code-base/.intdoc/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 41, in overwrite_by_env_vars\r\n    return fn(self, **kwargs)\r\nTypeError: __init__() got an unexpected keyword argument 'row_log_interval'\r\n\r\n**Code:**   test.py file \r\n\r\n\r\n```\r\nfrom collections import defaultdict\r\nimport argparse\r\nimport logging\r\nimport os\r\nimport pickle\r\nfrom pathlib import Path\r\nimport colorlog\r\nimport torch\r\nimport numpy as np\r\nfrom omegaconf import OmegaConf\r\nfrom pytorch_lightning import Callback, Trainer\r\nfrom typing import Any, Dict, List, Sequence, Union\r\nfrom systems import EpicActionRecogintionDataModule\r\nfrom systems import EpicActionRecognitionSystem\r\n\r\nparser = argparse.ArgumentParser(\r\n    description=\"Test model\", formatter_class=argparse.ArgumentDefaultsHelpFormatter\r\n)\r\nparser.add_argument(\"checkpoint\", type=Path)\r\nparser.add_argument(\"results\", type=Path)\r\nparser.add_argument(\"--split\", choices=[\"val\", \"test\"], default=\"test\")\r\nparser.add_argument(\r\n    \"--n-frames\",\r\n    type=int,\r\n    help=\"Overwrite number of frames to feed model, defaults to the \"\r\n    \"data.test_frame_count or data.frame_count if the former is not present\",\r\n)\r\nparser.add_argument(\r\n    \"--batch-size\",\r\n    type=int,\r\n    help=\"Overwrite the batch size for loading data, defaults to learning.batch_size\",\r\n)\r\nparser.add_argument(\r\n    \"--datadir\",\r\n    default=None,\r\n    help=\"Overwrite data directory in checkpoint. Useful when testing a checkpoint \"\r\n    \"trained on a different machine.\",\r\n)\r\n\r\nLOG = logging.getLogger(\"test\")\r\nclass ResultsSaver(Callback):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.results: Dict[str, Dict[str, List[Any]]] = dict()\r\n    def on_test_batch_end(\r\n        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx\r\n    ):\r\n        self._store_batch_results(\"test\", outputs)\r\n\r\n    def _store_batch_results(\r\n        self, dataset_name: str, batch_outputs: Dict[str, Sequence[Any]]\r\n    ):\r\n        if dataset_name not in self.results:\r\n            self.results[dataset_name] = {k: [] for k in batch_outputs.keys()}\r\n\r\n        for k, vs in batch_outputs.items():\r\n            if isinstance(vs, torch.Tensor):\r\n                vs = vs.detach().cpu().numpy()\r\n            self.results[dataset_name][k].extend(vs)\r\n\r\n    def save_results(self, dataset_name: str, filepath: Union[str, Path]):\r\n        filepath = Path(filepath)\r\n        filepath.parent.mkdir(parents=True, exist_ok=True)\r\n        results_dict = self.results[dataset_name]\r\n        new_results_dict = {\r\n            k: np.stack(vs)\r\n            for k, vs in results_dict.items()\r\n        }\r\n\r\n        with open(filepath, \"wb\") as f:\r\n            pickle.dump(new_results_dict, f)\r\n\r\n\r\ndef main(args):\r\n    logging.basicConfig(level=logging.INFO)\r\n\r\n    handler = colorlog.StreamHandler()\r\n    handler.setFormatter(\r\n        colorlog.ColoredFormatter(\"%(log_color)s%(levelname)s:%(name)s:%(message)s\")\r\n    )\r\n\r\n    logger = colorlog.getLogger(\"example\")\r\n    logger.addHandler(handler)\r\n\r\n    ckpt = torch.load(args.checkpoint, map_location=lambda storage, loc: storage)\r\n    # Publicly released checkpoints use dicts for longevity, so we need to wrap them\r\n    # up in an OmegaConf object as this is what EpicActionRecognitionSystem expects.\r\n    cfg = OmegaConf.create(ckpt[\"hyper_parameters\"])\r\n    OmegaConf.set_struct(cfg, False)  # allow writing arbitrary keys without raising\r\n    # exceptions\r\n    cfg.data._root_gulp_dir = os.getcwd()  # set default root gulp dir to prevent\r\n    # exceptions on instantiating the EpicActionRecognitionSystem\r\n\r\n    system = EpicActionRecognitionSystem(cfg)\r\n    system.load_state_dict(ckpt[\"state_dict\"])\r\n    if not cfg.get(\"log_graph\", True):\r\n        # MTRN can't be traced due to the model stochasticity so causes a JIT tracer\r\n        # error, we allow you to prevent the tracer from running to log the graph when\r\n        # the summary writer is created\r\n        try:\r\n            delattr(system, \"example_input_array\")\r\n        except AttributeError:\r\n            pass\r\n\r\n    if args.n_frames is not None:\r\n        cfg.data.test_frame_count = args.n_frames\r\n    if args.batch_size is not None:\r\n        cfg.learning.batch_size = args.batch_size\r\n    if args.datadir is not None:\r\n        data_dir_key = f\"{args.split}_gulp_dir\"\r\n        cfg.data[data_dir_key] = args.datadir\r\n\r\n    # Since we don't support writing results when using DP or DDP\r\n    LOG.info(\"Disabling DP/DDP\")\r\n    cfg.trainer.accelerator = None\r\n\r\n    n_gpus = 1\r\n    LOG.info(f\"Overwriting number of GPUs to {n_gpus}\")\r\n    cfg.trainer.gpus = n_gpus\r\n    cfg[\"test.results_path\"] = str(args.results)\r\n\r\n    data_module = EpicActionRecogintionDataModule(cfg)\r\n    if args.split == \"val\":\r\n        dataloader = data_module.val_dataloader()\r\n    elif args.split == \"test\":\r\n        dataloader = data_module.test_dataloader()\r\n    else:\r\n        raise ValueError(\r\n            f\"Split {args.split!r} is not a recognised dataset split to \" f\"test on.\"\r\n        )\r\n\r\n    saver = ResultsSaver()\r\n    trainer = Trainer(**cfg.trainer, callbacks=[saver])\r\n    trainer.test(system, test_dataloaders=dataloader)\r\n    saver.save_results(\"test\", args.results)\r\nif __name__ == \"__main__\":\r\n    main(parser.parse_args())\r\n\r\n```\r\n**Versions:** \r\nPython3.6, lightining-1.1.8, pytorch-1.7.1, Running on an linux machine \r\n\r\nI am new to this library, any help would be appreciated\r\nThanks in advance> TypeError: init() got an unexpected keyword argument 'row_log_interval'\r\n\r\n`row_log_interval` got deprecated and removed. You must have taken this from an old version of the docs. Use [`log_every_n_steps`](https://pytorch-lightning.readthedocs.io/en/1.3.5/common/trainer.html#log-every-n-steps) instead. ",
    "meta": {
      "name": "TypeError: __init__() got an unexpected keyword argument 'row_log_interval'"
    },
    "answer": "> TypeError: init() got an unexpected keyword argument 'row_log_interval'\r\n\r\n`row_log_interval` got deprecated and removed. You must have taken this from an old version of the docs. Use [`log_every_n_steps`](https://pytorch-lightning.readthedocs.io/en/1.3.5/common/trainer.html#log-every-n-steps) instead. "
  },
  {
    "content": "From the [training_step_end()](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step-end) docs it says:\r\n\r\n> If you later switch to ddp or some other mode, this will still be called so that you don\u2019t have to change your code\r\n\r\nWhen using dp or ddp2 the first dimension is equal to the number of GPUs, and it has the per-GPU results like `gpu_n_pred = training_step_outputs[n]['pred']` as shown in the example in the docs. This makes sense since there is a gather in the forward pass. For DDP though there does not need to be a gather / sync / barrier in the forward pass, only for the gradients in the backward pass.\r\n\r\nSo does this just pass through the single-GPU output in the DDP case? E.g. in the same example, is `training_step_outputs` just the dictionary from that single GPU, like `gpu_pred = training_step_outputs['pred']`? \r\n\r\nOr if I define this method does it add a barrier / gather as if doing `outputs = self.all_gather(outputs)`, such that all of the GPU results are actually available like `gpu_n_pred = training_step_outputs[n]['pred']` as in the DP/DDP2 case? \r\n\r\nI just want to make sure I'm not slowing down my code if I define this method for the dp / ddp2 case but then almost always use standard ddp. Sorry if this was already asked or is in the docs, I tried my best to find the answer. Thanks!I'll answer my own question. Short answer is **no**, there is no barrier / gather before `training_step_end()` when using DDP.\r\n\r\nI could be wrong, but it appears these methods just get called using the normal callback mechanism, e.g. PyTorch-Lightning doesn't post-process the output beyond what DP / DDP will do. So in the DP case the outputs are automatically aggregated by concatenating the first dimension, and in the DDP case the outputs are just passed through.\r\n\r\nI tried returning a dictionary from `training_step_end()` containing (1) a scalar, e.g. `loss`, and (2) a tensor of output predictions e.g. `pred_outs`, shape (N, K) for batch size N and number of classes K.\r\n\r\nThe results were as follows, using 2 GPUs for DP / DDP with a batch size of 128 for DP, and 64 for DDP (maintaining the effective batch size of 128):\r\n\r\nRough setup:\r\n\r\n```python\r\ndef training_step(\r\n    self,\r\n    batch: Tuple[torch.Tensor, torch.Tensor],\r\n    batch_idx: int\r\n) -> Dict[str, torch.Tensor]:\r\n    inputs, targets = batch\r\n    pred_odds = self.forward(inputs)\r\n    log_probs = F.log_softmax(pred_odds, dim=1)\r\n    loss = F.nll_loss(log_probs, targets)\r\n    return {'pred_odds': pred_odds, 'loss': loss}\r\n\r\ndef training_step_end(\r\n    self,\r\n    step_outputs: Dict[str, torch.Tensor]\r\n) -> Dict[str, torch.Tensor]:\r\n    print(step_outputs['loss'])\r\n    print(step_outputs['loss'].shape)\r\n\r\n    print(step_outputs['pred_odds'].shape)\r\n    print(step_outputs['pred_odds'].device)\r\n```\r\n\r\nGave the following results:\r\n\r\n```text\r\n---------------------------\r\nSINGLE GPU\r\n---------------------------\r\nSCALAR OUTPUT:\r\n\r\nstep_outputs['loss']\r\ntensor([4.8134], device='cuda:0')\r\n\r\nstep_outputs['loss'].shape\r\ntorch.Size([1])\r\n\r\nTENSOR OUTPUT:\r\n\r\nstep_outputs['pred_odds'].shape\r\ntorch.Size([128, 100])\r\n\r\nstep_outputs['pred_odds'].device\r\ndevice(type='cuda', index=0)\r\n\r\n---------------------------\r\nDP\r\n---------------------------\r\nSCALAR OUTPUT:\r\n\r\nstep_outputs['loss']\r\ntensor([4.8472, 4.8262], device='cuda:0')\r\n\r\nstep_outputs['loss'].shape\r\ntorch.Size([2])\r\n\r\nTENSOR OUTPUT:\r\n\r\nstep_outputs['pred_odds'].shape\r\ntorch.Size([128, 100])\r\n\r\nstep_outputs['pred_odds'].device\r\ndevice(type='cuda', index=0)\r\n\r\n-----------------------------\r\nDDP\r\n-----------------------------\r\nSCALAR OUTPUT:\r\n\r\nstep_outputs['loss']\r\ntensor(4.8477, device='cuda:0')\r\n\r\nstep_outputs['loss'].shape\r\ntorch.Size([])\r\n\r\nTENSOR_OUTPUT:\r\n\r\nstep_outputs['pred_odds'].shape\r\ntorch.Size([64, 100])\r\n\r\nstep_outputs['pred_odds'].device\r\ndevice(type='cuda', index=0)\r\n```\r\n\r\nSo if you're using a tensor (the only case that I actually needed, since the loss will be computed in `training_step_end()` anyway), you can just use the tensor as you normally would in `training_step_end()` as if it were coming from a single GPU in `training_step()`. \r\n\r\nIf you're using a scalar, it will get converted into a 1D tensor equal to the number of GPUs in the no-backend and DP case, but it will be still be a scalar tensor in the DDP case (see the size above). That could throw you off, but again, you're probably not passing scalars to `training_step_end()` anyway. \r\n\r\nHope this helps someone. Cheers.",
    "meta": {
      "name": "Is there an all_gather before training_step_end when using DDP?"
    },
    "answer": "I'll answer my own question. Short answer is **no**, there is no barrier / gather before `training_step_end()` when using DDP.\r\n\r\nI could be wrong, but it appears these methods just get called using the normal callback mechanism, e.g. PyTorch-Lightning doesn't post-process the output beyond what DP / DDP will do. So in the DP case the outputs are automatically aggregated by concatenating the first dimension, and in the DDP case the outputs are just passed through.\r\n\r\nI tried returning a dictionary from `training_step_end()` containing (1) a scalar, e.g. `loss`, and (2) a tensor of output predictions e.g. `pred_outs`, shape (N, K) for batch size N and number of classes K.\r\n\r\nThe results were as follows, using 2 GPUs for DP / DDP with a batch size of 128 for DP, and 64 for DDP (maintaining the effective batch size of 128):\r\n\r\nRough setup:\r\n\r\n```python\r\ndef training_step(\r\n    self,\r\n    batch: Tuple[torch.Tensor, torch.Tensor],\r\n    batch_idx: int\r\n) -> Dict[str, torch.Tensor]:\r\n    inputs, targets = batch\r\n    pred_odds = self.forward(inputs)\r\n    log_probs = F.log_softmax(pred_odds, dim=1)\r\n    loss = F.nll_loss(log_probs, targets)\r\n    return {'pred_odds': pred_odds, 'loss': loss}\r\n\r\ndef training_step_end(\r\n    self,\r\n    step_outputs: Dict[str, torch.Tensor]\r\n) -> Dict[str, torch.Tensor]:\r\n    print(step_outputs['loss'])\r\n    print(step_outputs['loss'].shape)\r\n\r\n    print(step_outputs['pred_odds'].shape)\r\n    print(step_outputs['pred_odds'].device)\r\n```\r\n\r\nGave the following results:\r\n\r\n```text\r\n---------------------------\r\nSINGLE GPU\r\n---------------------------\r\nSCALAR OUTPUT:\r\n\r\nstep_outputs['loss']\r\ntensor([4.8134], device='cuda:0')\r\n\r\nstep_outputs['loss'].shape\r\ntorch.Size([1])\r\n\r\nTENSOR OUTPUT:\r\n\r\nstep_outputs['pred_odds'].shape\r\ntorch.Size([128, 100])\r\n\r\nstep_outputs['pred_odds'].device\r\ndevice(type='cuda', index=0)\r\n\r\n---------------------------\r\nDP\r\n---------------------------\r\nSCALAR OUTPUT:\r\n\r\nstep_outputs['loss']\r\ntensor([4.8472, 4.8262], device='cuda:0')\r\n\r\nstep_outputs['loss'].shape\r\ntorch.Size([2])\r\n\r\nTENSOR OUTPUT:\r\n\r\nstep_outputs['pred_odds'].shape\r\ntorch.Size([128, 100])\r\n\r\nstep_outputs['pred_odds'].device\r\ndevice(type='cuda', index=0)\r\n\r\n-----------------------------\r\nDDP\r\n-----------------------------\r\nSCALAR OUTPUT:\r\n\r\nstep_outputs['loss']\r\ntensor(4.8477, device='cuda:0')\r\n\r\nstep_outputs['loss'].shape\r\ntorch.Size([])\r\n\r\nTENSOR_OUTPUT:\r\n\r\nstep_outputs['pred_odds'].shape\r\ntorch.Size([64, 100])\r\n\r\nstep_outputs['pred_odds'].device\r\ndevice(type='cuda', index=0)\r\n```\r\n\r\nSo if you're using a tensor (the only case that I actually needed, since the loss will be computed in `training_step_end()` anyway), you can just use the tensor as you normally would in `training_step_end()` as if it were coming from a single GPU in `training_step()`. \r\n\r\nIf you're using a scalar, it will get converted into a 1D tensor equal to the number of GPUs in the no-backend and DP case, but it will be still be a scalar tensor in the DDP case (see the size above). That could throw you off, but again, you're probably not passing scalars to `training_step_end()` anyway. \r\n\r\nHope this helps someone. Cheers."
  },
  {
    "content": "I am using CLI to train my model. Instead of specifying parameters directly, I provide a yaml file with variables defined.\r\n\r\nSince I'm using several loggers, they have a common name parameter. So in order to start a new experiment I have to change this parameter in each logger. This raises a question is there a way to create global variable in yaml file while using CLI?@Serega6678 You can try jsonnet format config file which supports global variables. https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.core.ArgumentParser\r\nTo use jsonnet format, you can init CLI by passing `{\"parser_mode\": \"jsonnet\"}` to `parser_kwargs`.",
    "meta": { "name": "Global parameters yaml file" },
    "answer": "@Serega6678 You can try jsonnet format config file which supports global variables. https://jsonargparse.readthedocs.io/en/stable/#jsonargparse.core.ArgumentParser\r\nTo use jsonnet format, you can init CLI by passing `{\"parser_mode\": \"jsonnet\"}` to `parser_kwargs`."
  },
  {
    "content": "Hi ! \r\n\r\nI'm currently working on a segmentation network using PyTorch Lightning and MONAI. \r\n\r\n_Context_\r\n- In my LightningDataModule, I preprocess my images by applying transforms (e.g., to resample them) before feeding them to my DataLoaders. For the transforms, I use a 3rd party framework called MONAI. It stores the context information of all the applied transforms in a structured nested dictionary. \r\n- After predicting the labels of my predict dataset, I would like to invert these transforms (e.g., to recover the original pixel dimensions) in my `predict_step`. \r\n\r\n_Problem_\r\n- MONAI's inverting logic requires the context information to be numpy data or **CPU** tensors (**GPU** tensors are not supported). However, by default, Lightning moves all the data of my batch (including its context info) to GPU. It causes the bug I reproduced in this test example [notebook](https://github.com/dianemarquette/Invertd_demo/blob/master/test_invertd.ipynb) (cf. last cell). \r\n\r\n_My question_\r\n**How can I tell Lightning to only move the images and their labels to GPU and keep the context info on the CPU?**\r\n\r\nFor further details, please refer to the following discussion Project-MONAI/MONAI#2348.\r\nThanks for your help !Hi,\r\n\r\nyour `LightningModule` has a hook `def transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any:` that you can override and which should be a perfect fit for that. Just make sure to only use the provided device.",
    "meta": {
      "name": "How to tell Lightning which data to load onto the GPU (for 3rd party compatibility)"
    },
    "answer": "Hi,\r\n\r\nyour `LightningModule` has a hook `def transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any:` that you can override and which should be a perfect fit for that. Just make sure to only use the provided device."
  },
  {
    "content": "Hi! \r\n\r\nIn my LightningDataModule, I apply preprocessing transforms to my input data before feeding it to my dataloader. In the same datamodule, I also defined the postprocessing transforms to apply after the inference. \r\n\r\n```\r\nself.post_transforms = Compose([\r\n            AsDiscreted(keys=\"pred\", argmax=True, to_onehot=False, n_classes=3),\r\n            Invertd(\r\n                keys=\"pred\",  # invert the `pred` data field, also support multiple fields\r\n                transform=val_transforms,\r\n                loader=val_dataloader,\r\n                orig_keys=\"img\",  # get the previously applied pre_transforms information on the `img` data field,\r\n                                  # then invert `pred` based on this information. we can use same info\r\n                                  # for multiple fields, also support different orig_keys for different fields\r\n                meta_keys=\"pred_meta_dict\",  # key field to save inverted meta data, every item maps to `keys`\r\n                orig_meta_keys=\"img_meta_dict\",  # get the meta data from `img_meta_dict` field when inverting,\r\n                                                 # for example, may need the `affine` to invert `Spacingd` transform,\r\n                                                 # multiple fields can use the same meta data to invert\r\n                meta_key_postfix=\"meta_dict\",  # if `meta_keys=None`, use \"{keys}_{meta_key_postfix}\" as the meta key,\r\n                                               # if `orig_meta_keys=None`, use \"{orig_keys}_{meta_key_postfix}\",\r\n                                               # otherwise, no need this arg during inverting\r\n                nearest_interp=True,  # change to use \"nearest\" mode in interpolation when inverting\r\n                to_tensor=True,  # convert to PyTorch Tensor after inverting\r\n            ),\r\n            SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=\"./out\", output_postfix=\"seg\", resample=False),\r\n        ])\r\n```\r\nI want to apply these post_transforms to my inference outputs in `predict_step()`. What would be the best \"Lightning way\" to give access to my datamodule.post_transforms attribute to `predict_step`?\r\n\r\n```\r\ndef predict_step(self, batch: Any, batch_idx: int):\r\n        batch[\"pred\"] = self.forward(batch)\r\n        post_transforms(batch)\r\n```\r\n\r\nThanks in advance for your suggestions :)Hi, you should be able to access the datamodule inside the LightningModule. \r\nTry \r\n\r\n```python\r\ndef predict_step(self, batch: Any, batch_idx: int):\r\n        batch[\"pred\"] = self(batch)\r\n        self.datamodule.post_transforms(batch)\r\n```\r\nAlso, another tipp: Better use `self()` instead of `self.forward()` (generally in PyTorch).  ",
    "meta": {
      "name": "Processing in predict_step() requires access to a DataModule attribute"
    },
    "answer": "Hi, you should be able to access the datamodule inside the LightningModule. \r\nTry \r\n\r\n```python\r\ndef predict_step(self, batch: Any, batch_idx: int):\r\n        batch[\"pred\"] = self(batch)\r\n        self.datamodule.post_transforms(batch)\r\n```\r\nAlso, another tipp: Better use `self()` instead of `self.forward()` (generally in PyTorch).  "
  },
  {
    "content": "#### Systems\r\nThe [style guide](https://pytorch-lightning.readthedocs.io/en/latest/starter/style_guide.html) encourages to use systems, like this one\r\n\r\n```python\r\nclass LitModel(LightningModule):\r\n    def __init__(self, encoder: nn.Module = None, decoder: nn.Module = None):\r\n        super().__init__()\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n```\r\n\r\nI have problems loading the checkpoint for such modules. \r\nBelow example fails. How can I make it work?\r\n\r\n#### Minimal example\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import Dataset\r\nfrom pytorch_lightning import Trainer, LightningModule\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass SystemModel(LightningModule):\r\n\r\n    def __init__(self, encoder: nn.Module = None, decoder: nn.Module = None, multiplier=10):\r\n        super().__init__()\r\n        self.save_hyperparameters('multiplier')\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n        self.multiplier = multiplier\r\n        print(\"type of hparams\", type(self.hparams))\r\n        print(\"class of hparams type\", self.hparams.__class__.__name__)\r\n\r\n    def forward(self, x):\r\n        return self.multiplier * self.decoder(self.encoder(x))\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self.forward(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.forward(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self.forward(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x['x'] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self.forward(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ntrain_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\nval_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\ntest_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n\r\n# model\r\nmodel = SystemModel(torch.nn.Linear(32, 16), torch.nn.Linear(16, 2), multiplier=15)\r\ntrainer = Trainer(\r\n    default_root_dir=os.getcwd(),\r\n    limit_train_batches=1,\r\n    limit_val_batches=1,\r\n    max_epochs=1,\r\n    weights_summary=None,\r\n)\r\ntrainer.fit(model, train_data, val_data)\r\n\r\nckpt_path = trainer.checkpoint_callback.best_model_path\r\n\r\n# Try to load\r\n# Loading fails....\r\nmodel = SystemModel.load_from_checkpoint(ckpt_path) # Fails\r\n\r\n# Edit: answer by Adrian\r\n# the correct way to load is \r\nmodel = SystemModel.load_from_checkpoint(ckpt_path, encoder=torch.nn.Linear(32, 16), decoder=torch.nn.Linear(16, 2))  \r\n\r\nprint(\"multiplier:\", model.multiplier)\r\n```Could you share the error message / stack trace for the failure? When you reload you need to specify the missing arguments for instantiation: \r\n```python\r\nmodel = SystemModel.load_from_checkpoint(ckpt_path, encoder=..., decoder=...) \r\n``` ",
    "meta": {
      "name": "Loading checkpoint for LightningModule that defines a system"
    },
    "answer": "When you reload you need to specify the missing arguments for instantiation: \r\n```python\r\nmodel = SystemModel.load_from_checkpoint(ckpt_path, encoder=..., decoder=...) \r\n``` "
  },
  {
    "content": "I have 2 losses for my model.\r\nAnd I need the grads of the first loss to compute the second one.\r\nThe pseudocode in pytorch is like:\r\n```python\r\noptimizer.zero_grad()\r\nhidden_value = model.part1(input)\r\noutput = model.part2(hidden_value)\r\nloss1 = criterion(output, label)\r\nloss1.backward(retain_graph=True)\r\nloss2 = criterion2(hidden_value.grad, label2)\r\nloss2.backward()\r\noptimizer.step()\r\n```\r\nI found an API named manual_backward() which may fit my problem.\r\nHowever, I build this model on a project based on pytorch_lighting 0.6.0, and it doesn\u2019t have this API.\r\nSo, my questions are:\r\n1.How can I implement my operation using pytorch_lightning 0.6.0?\r\n2.If I can\u2019t implement it in pytorch_lightning 0.6.0, which lighting version should I chose? (Please recommend a close version which may cause less error after I update the lightning.)I don't think this is really possible in 0.6. This version is too old and manual optimization was introduced to cover your exact use case. I can only recommend the latest version because manual backward underwent many changes and bugfixes so believe it is worth it to invest the time to get that code updated. ",
    "meta": { "name": "Backward twice in one training_step" },
    "answer": "I don't think this is really possible in 0.6. This version is too old and manual optimization was introduced to cover your exact use case. I can only recommend the latest version because manual backward underwent many changes and bugfixes so believe it is worth it to invest the time to get that code updated. "
  },
  {
    "content": "Hey, \r\nim wondering whether the patience parameter is or can be reset once we have started improving again. Reading the docs it sounds like the patience parameter is the absolute number of steps the loss is allowed to not decrease. Is it possible to have it such that the patience counter is reset to its original value whenever we have improved? \r\nThis is exactly how patience is supposed to work. As you can see [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/7b531ac7ac86141237f758a7e7446930964b91a9/pytorch_lightning/callbacks/early_stopping.py#L230), the counter resets to 0 upon improvement. ",
    "meta": {
      "name": "Patience reset in EarlyStopping once loss has improved"
    },
    "answer": "This is exactly how patience is supposed to work. As you can see [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/7b531ac7ac86141237f758a7e7446930964b91a9/pytorch_lightning/callbacks/early_stopping.py#L230), the counter resets to 0 upon improvement. "
  },
  {
    "content": "#### What is your question?\r\nFor some learning rate schedulers, there is a required steps_per_epoch parameter. One example is the OneCycleLR scheduler. On a CPU or single GPU, this parameter should be set to the length of the train dataloader. My question is, how should this parameter be set on a multi-GPU machine using DDP. Does this parameter need to be updated to `len(self.train_dataloader()) / num_gpus`? Or is this done automatically?\r\n\r\n#### What have you tried?\r\nI've tried manually dividing the steps_per_epoch of the OneCycleLR scheduler by the number of GPUs when training on a multi-GPU machine. The LR doesn't seem to be following the expected update pattern and I think the scheduler may be the source of the problem.\r\n\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\n - Packaging: conda\r\n - Version: 0.7.6\r\nAfter some more investigation, it seems like dividing the dataloader size by the number of GPUs is the correct way. The documentation could be more clear on this, but I'm closing this now.",
    "meta": {
      "name": "How do I set the steps_per_epoch parameter of a lr scheduler in multi-GPU environment?"
    },
    "answer": "After some more investigation, it seems like dividing the dataloader size by the number of GPUs is the correct way. The documentation could be more clear on this, but I'm closing this now."
  },
  {
    "content": "I have a PyTorch Lightning DataModule instance that defines train_dataloader, val_dataloader, and test_dataloader.\r\n\r\nCurrently using a custom callback to reload the train_dataloader that will resample the data.\r\n\r\nI saw that there is a Trainer flag called reload_dataloaders_every_epoch and soon to be reload_dataloaders_every_n_epochs.\r\n\r\nDo these just reload the train_dataloader, or do the do all 3?Only the train and validation dataloaders:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/e4f3a8d3dd534d4ec2fe094280272513e652fba9/pytorch_lightning/trainer/training_loop.py#L168-L170\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/e4f3a8d3dd534d4ec2fe094280272513e652fba9/pytorch_lightning/trainer/training_loop.py#L203-L207",
    "meta": { "name": "Clarification on reload_dataloaders_every_epoch" },
    "answer": "Only the train and validation dataloaders:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/e4f3a8d3dd534d4ec2fe094280272513e652fba9/pytorch_lightning/trainer/training_loop.py#L168-L170\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/e4f3a8d3dd534d4ec2fe094280272513e652fba9/pytorch_lightning/trainer/training_loop.py#L203-L207"
  },
  {
    "content": "I used torchmetrics.AveragePrecision as the metric in PL module. This metric takes a lot of gpu memory as it save all data in buffer. \r\nSo I wanted to move the metric to cpu and set move_metrics_to_cpu True in PL Trainer. But the metric's buffer was still on gpu. \r\nAny ideas of this?I think `move_metrics_to_cpu` does not move the state of the metric modules but just the logged values.\r\n\r\ncc @tchaton for confirmation.\r\n\r\n@SkafteNicki @tchaton should we maybe extend this? I see that this can be misleading.\r\n\r\n@MeteorsHub The issue we do it that way is mainly to avoid having the same thing in memory twice. But I think especially for metrics like AP the only option would be to manually move it to cpu (since this will be moved to gpu together with the model). This will introduce some synchronization points though and might slow down your training!\r\n\r\n",
    "meta": { "name": "move_metrics_to_cpu does not work" },
    "answer": "I think `move_metrics_to_cpu` does not move the state of the metric modules but just the logged values.\r\n\r\ncc @tchaton for confirmation.\r\n\r\n@SkafteNicki @tchaton should we maybe extend this? I see that this can be misleading.\r\n\r\n@MeteorsHub The issue we do it that way is mainly to avoid having the same thing in memory twice. But I think especially for metrics like AP the only option would be to manually move it to cpu (since this will be moved to gpu together with the model). This will introduce some synchronization points though and might slow down your training!\r\n\r\n"
  },
  {
    "content": "How do I correctly override `gather` behaviour?\r\nIdeally, I would have a single place where I should specify new method and it would work with `dp`, `ddp` and `ddp_spawn`.I'm currently using `accelerator=\"dp\"` and getting the following error:\r\n```\r\n...\r\n    return trainer.predict(\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:631: in predict\r\n    results = self._run(model)\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:756: in _run\r\n    self.dispatch()\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:795: in dispatch\r\n    self.accelerator.start_predicting(self)\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:102: in start_predicting\r\n    self.training_type_plugin.start_predicting(trainer)\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:152: in start_predicting\r\n    self._results = trainer.run_stage()\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:806: in run_stage\r\n    return self.run_predict()\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1083: in run_predict\r\n    self.predict_loop.predict_step(batch, batch_idx, dataloader_idx)\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/pytorch_lightning/trainer/predict_loop.py:111: in predict_step\r\n    predictions = self.trainer.accelerator.predict_step(args)\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:265: in predict_step\r\n    return self.training_type_plugin.predict_step(*args)\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/dp.py:107: in predict_step\r\n    return self.model(*args, **kwargs)\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:168: in forward\r\n    return self.gather(outputs, self.output_device)\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:180: in gather\r\n    return gather(outputs, output_device, dim=self.dim)\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:76: in gather\r\n    res = gather_map(outputs)\r\n../../miniconda3/envs/ntf/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:71: in gather_map\r\n    return type(out)(map(gather_map, zip(*outputs)))\r\nTypeError: 'MyModel' object is not iterable\r\n```\r\nI need trainer to use my custom `gather_map` instead of the one in `torch`.Managed to get this working for `dp` by subclassing `DataParallelPlugin` and passing it to trainer. Although this approach requires individual plugins for each accelerator, it would be nice to have a way to set this for all accelerators at the same time.",
    "meta": { "name": "Custom gather method" },
    "answer": "Managed to get this working for `dp` by subclassing `DataParallelPlugin` and passing it to trainer. Although this approach requires individual plugins for each accelerator, it would be nice to have a way to set this for all accelerators at the same time."
  },
  {
    "content": "## \ud83d\udc1b Bug\r\n\r\nIf there are unsued parameters in the model, should we still explicitly mentioned the trainer as follows?\r\n\r\n**plugins=[DDPPlugin(find_unused_parameters=True)]**\r\n\r\n` trainer = pl.Trainer.from_argparse_args(\r\nargs, weights_summary=None, \r\ncallbacks=[logging_callback],  logger=logger,        \r\nplugins=[DDPPlugin(find_unused_parameters=True)],\r\n**train_params,)`No, it is set to True by default and you want to turn it off unless you need it :) \r\n\r\nReference:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.3.3/benchmarking/performance.html#when-using-ddp-set-find-unused-parameters-falseThanks. Git it",
    "meta": {
      "name": "find_unused_parameters in the lightning trainer (1.3.2)"
    },
    "answer": "No, it is set to True by default and you want to turn it off unless you need it :) \r\n\r\nReference:\r\nhttps://pytorch-lightning.readthedocs.io/en/1.3.3/benchmarking/performance.html#when-using-ddp-set-find-unused-parameters-false"
  },
  {
    "content": "The CLI has a flag --gpus\r\n\r\nIn a system with more than 1 GPU, is there a way to select the GPU you want to run on from CLI?\r\n\r\nI tried --gpus [1] to select cuda:1 but it doesn't work.\r\n\r\nAlso the auto gpu selection didn't work for me.  It tried to put the job on cuda:0, but cuda:0 didn't have enough memory to run it.\r\n\r\nin the end I resorted to CUDA_VISIBLE_DEVICES, but that seems silly.\r\n\r\nThanks!I'm assuming you are referring to the `LightningCLI`\r\n\r\nIn that case, just do `python yourscript.py --trainer.gpus=[1]`for those who use zsh, maybe we need `\\[1\\]`",
    "meta": { "name": "Select GPU from cli" },
    "answer": "I'm assuming you are referring to the `LightningCLI`\r\n\r\nIn that case, just do `python yourscript.py --trainer.gpus=[1]`"
  },
  {
    "content": "After `.fit()` concludes, I run first `.validate(ckpt_path=None)` and then `.test(ckpt_path=None)`. I use the exact same code for both of them: `validation_step` is identical to `test_step`, `validation_epoch_end` matches `test_epoch_end`, and `val_dataloader` is identical to `test_dataloader`. My trainer also has `limit_val_batches=1.0`, running the whole dataset. Yet, the two values I get are dramatically different: test error is far lower, typically up to an order of magnitude. What is going on in the test phase that could cause this?Maybe because there is some state that persists and influences one method after the other. Have you seen the same results if you switch the order? Maybe a bug on our side. Is it reproducible in our simple bug report model?In the end it appears to have been some sort of seed issue -- I still don't understand quite what was happening, but seed_everything(1) fixed it.",
    "meta": {
      "name": "What are all of the differences between `.validate()` and `.test()`?"
    },
    "answer": "In the end it appears to have been some sort of seed issue -- I still don't understand quite what was happening, but seed_everything(1) fixed it."
  },
  {
    "content": "Hi, I don't think this is a bug but I'm doing something wrong. I want to use my val_dice as hp_metric tabular AND also see the graph on \"show metric\" (radio button) under the Tensorboard HPARAMS tab:\r\n![image](https://user-images.githubusercontent.com/10415569/100072674-002dd180-2e56-11eb-80e0-2dcb94be0ae5.png)\r\nTo achieve this I'm logging using `self.log('hp_metric', mean_dice)` (for the graph) and `self.logger.log_hyperparams(params=self.hparams, metrics={'hp_metric': mean_val_dice})` (for the hparams value), both in the function `validation_epoch_end`\r\nHow do I get rid of the initial -1 value? How can I fix my graph so it doesn't draw any points at x=0? (zoomed in version)\r\n![image](https://user-images.githubusercontent.com/10415569/100073169-9bbf4200-2e56-11eb-86d8-709ee3288fee.png)\r\nThat's a great question ! Why do you want to see the metric under hparams tab ?As @bartmch, I use `hp_metric` to compare hyper parameters (is that not the purpose of this placeholder ?). I also have this negative initial value, but I didn't notice it before. I suppose it is not possible to remove this initial value but is there a way to customize it ?You can set `default_hp_metric` in `TensorBoardLogger` to False to prevent the initial hp_metirc score.\r\n`TensorBoardLogger(save_dir='tb_logs', default_hp_metric=False)`Yes but then the `hp_metric` is not correctly tracked in the hparams (https://github.com/PyTorchLightning/pytorch-lightning/pull/2974)...I already use your first way but setting `default_hp_metric` to `False` makes `hp_metric` be removed from \"hparams\" tab (this tab isn't there at all even if I have set some hyper parameters). Adding the final `log_hyperparams` creates the hparams tab but the graph of `hp_metric` gets a final value at iteration 0 instead of final iteration), and this step will also be skipped if the job is killed.\r\n\r\nHere are what I've tried so far:\r\n- `default_hp_metric=True`: hparams tab visible in Tensorboad with `hp_metric` updated during training, `hp_metric` wrong initial value that makes the corresponding graph unsuitable with log scale and smoothing activated.\r\n- `default_hp_metric=False`: no hparams tab in TensorBoard, `hp_metric` graph OK\r\n- `default_hp_metric=False` and final `log_hyperparams`: no hparams tab in TensorBoard during training, `hp_metric` graph OK until the end of the training where final point is associated to iteration #0. No hparams tab at all if job is killed.\r\n\r\nI will try to calculate metric before training start to appropriately populate it, or to delay `log_hyperparams` at the first validation...",
    "meta": {
      "name": "How to remove hp_metric initial -1 point and x=0 points?"
    },
    "answer": "I already use your first way but setting `default_hp_metric` to `False` makes `hp_metric` be removed from \"hparams\" tab (this tab isn't there at all even if I have set some hyper parameters). Adding the final `log_hyperparams` creates the hparams tab but the graph of `hp_metric` gets a final value at iteration 0 instead of final iteration), and this step will also be skipped if the job is killed.\r\n\r\nHere are what I've tried so far:\r\n- `default_hp_metric=True`: hparams tab visible in Tensorboad with `hp_metric` updated during training, `hp_metric` wrong initial value that makes the corresponding graph unsuitable with log scale and smoothing activated.\r\n- `default_hp_metric=False`: no hparams tab in TensorBoard, `hp_metric` graph OK\r\n- `default_hp_metric=False` and final `log_hyperparams`: no hparams tab in TensorBoard during training, `hp_metric` graph OK until the end of the training where final point is associated to iteration #0. No hparams tab at all if job is killed.\r\n\r\nI will try to calculate metric before training start to appropriately populate it, or to delay `log_hyperparams` at the first validation..."
  },
  {
    "content": "Hello,\r\n\r\nI'm designing a multi-task architecture for depth estimation and semantic segmentation. Because these are very similar tasks, I can use my existing `nn.Module` to learn both of them. I wrapped the `nn.Module` in a `pl.LightningModule` and got a working segmentation. Now I want to extend the `LightningModule` to be able to do the depth estimation. \r\nI want to reuse as much code as possible to reduce errors. But the validation for both tasks is very different, because I'm using many metrics and extensive W&B logging. I first thought of subclassing the `pl.LightningModule` to a segmentation and a depth estimation module, but I want to be able to use both segmentation and depth in a single `LightningModule` later on, when the network should learn both tasks.\r\nThus I created methods for the specific validation steps and set them to variables in the classes init. These get called from the overwritten methods of the class. \r\nHeres some pseudo-code from my structure:\r\n\r\n```python\r\nclass LitModel(pl.LightningModel):\r\n def __init__(self, model: nn.Module,  hparams):\r\n        super().__init__()\r\n        # default for segmentation and depth\r\n        self.hparams.update(hparams)\r\n        self.model = model\r\n\r\n        # SEGMENTATION STUFF\r\n        if segmentation:\r\n            \r\n            self._batch_target_name = 'label'\r\n\r\n            # VAL STUFF\r\n            self._loss_func = CrossEntropyLoss\r\n            self._val_loss_func = CrossEntropyLoss\r\n            self._confusion_matrix = ConfusionMatrix(num_classes=num_classes, compute_on_step=False)\r\n            self._best_metric = 0\r\n            self._best_metric_name = 'mIoU'\r\n\r\n            self._val_reset_metric_list = [self._val_loss_func, self._confusion_matrix]\r\n\r\n            self._validation_step = self.seg_validation_step\r\n            self._validation_epoch_end = self.seg_validation_epoch_end\r\n\r\n        # DEPTH STUFF\r\n        elif depth:\r\n            \r\n            self._batch_target_name = 'depth'\r\n\r\n            # VAL STUFF\r\n            self._loss_func = L1\r\n            self._val_loss_func = MSE\r\n           \r\n            self._best_metric = 0\r\n            self._best_metric_name = 'RMSE'\r\n\r\n            self._val_reset_metric_list = [self._val_loss_func]\r\n\r\n            self._validation_step = self.dpt_validation_step\r\n            self._validation_epoch_end = self.dpt_validation_epoch_end\r\n\r\n    def forward(self, sample):\r\n        return self.model(sample)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # disassemble batch/samples\r\n        image = batch['image']\r\n        target_scales = [batch[self._batch_target_name]]\r\n\r\n        # predict input images\r\n        pred_scales = self.model(image)\r\n\r\n        # calculate losses\r\n        losses = self._loss_func(pred_scales, target_scales)\r\n        summed_loss = sum(losses)\r\n        self.log('train/loss', summed_loss, on_step=False, on_epoch=True)  # logs mean of all losses during that epoch\r\n        return {'loss': summed_loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        # disassemble batch/samples\r\n        image = batch['image']\r\n        gt = batch[self._batch_target_name]\r\n\r\n        # predict input images\r\n        prediction = self.model(image)\r\n\r\n        return self._validation_step(batch, batch_idx, gt, prediction)\r\n\r\n    def seg_validation_step(self, batch, batch_idx, gt, prediction):\r\n        # calculate segmentation validation losses\r\n\r\n    def dpt_validation_step(self, batch, batch_idx, gt, prediction):...\r\n        # calculate depth validation losses\r\n\r\n    def on_validation_epoch_start(self) -> None:\r\n        for f in self._val_reset_metric_list:\r\n            f.reset()\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        metric = self._validation_epoch_end(outputs)\r\n        if self._best_metric < metric:\r\n            self.log(f'eval/best_{self._best_metric_name}', metric)\r\n            self.log('eval/best_epoch', self.current_epoch)\r\n            self._best_metric = metric\r\n\r\n    def seg_validation_epoch_end(self, outputs) -> metric:\r\n        # calculate confusion matrix and mIoU\r\n        return mIoU\r\n\r\n    def dpt_validation_epoch_end(self, outputs) -> metric:\r\n        # calculate RMSE\r\n        return RMSE\r\n\r\n    def configure_optimizers(self):...\r\n```\r\nTo me, this seems as really bad practice and I hope it doesn't hurt you to much seeing this.\r\nWhat would be a better/the best way to achieve this? I'm looking for the best practice.\r\n\r\nI'm coming from TensorFlow and I really really like how clean PyTorch Lightning is. It feels so good to work with it. But what I wrote up there just feels wrong, but it works \ud83d\ude28 First of all: I'm glad you are enjoying lightning :)\r\n\r\nComing to your code: It actually doesn't look so bad to me. You're separating functionality for different use cases in different functions (which is perfectly fine). What you could do ( if you really want to) is something like this:\r\n\r\n```python\r\nclass BaseModel(LightningModule):\r\n   ... # implements all the logic to be shared between the models such as the module logic or something like this\r\n\r\nclass SegmentationModel(BaseModel):\r\n    ... # adds all the segmentation-only logic\r\n\r\nclass DepthModel(BaseModel):\r\n    ... # adds all the depth-only logic\r\n\r\nclass CombinedModel(LightningModel):\r\n    def __init__(self, model, hparams):\r\n        if depth:\r\n            model = DepthModel(model, hparams)\r\n        else:\r\n            model = SegmentationModel(model, hparams)\r\n        self.model = model\r\n\r\n    def training_step(self, *args, **kwargs):\r\n        return self.model.training_step(*args, **kwargs)\r\n\r\n    # do the same for other methods and hooks\r\n```\r\n\r\nThat way your classes would be a bit more separated and self-contained. That being said, I still think your current approach is perfectly fine\r\n",
    "meta": { "name": "How to design a multi-task architecture?" },
    "answer": "First of all: I'm glad you are enjoying lightning :)\r\n\r\nComing to your code: It actually doesn't look so bad to me. You're separating functionality for different use cases in different functions (which is perfectly fine). What you could do ( if you really want to) is something like this:\r\n\r\n```python\r\nclass BaseModel(LightningModule):\r\n   ... # implements all the logic to be shared between the models such as the module logic or something like this\r\n\r\nclass SegmentationModel(BaseModel):\r\n    ... # adds all the segmentation-only logic\r\n\r\nclass DepthModel(BaseModel):\r\n    ... # adds all the depth-only logic\r\n\r\nclass CombinedModel(LightningModel):\r\n    def __init__(self, model, hparams):\r\n        if depth:\r\n            model = DepthModel(model, hparams)\r\n        else:\r\n            model = SegmentationModel(model, hparams)\r\n        self.model = model\r\n\r\n    def training_step(self, *args, **kwargs):\r\n        return self.model.training_step(*args, **kwargs)\r\n\r\n    # do the same for other methods and hooks\r\n```\r\n\r\nThat way your classes would be a bit more separated and self-contained. That being said, I still think your current approach is perfectly fine\r\n"
  },
  {
    "content": "I want the checkpoint and the logs stay in the same place while only the logs are uploaded to wandb server.cc @borisdayma Yes, it will be fixed with #6231",
    "meta": { "name": "How to stop wandblogger uploading the checkpoint?" },
    "answer": "Yes, it will be fixed with #6231"
  },
  {
    "content": "Hi all, I use multi-GPU to train the model, this error happens when saving the checkpoint. But using a single GPU to train the model, everything works fine. \r\n\r\n```\r\nTypeError: cannot pickle '_thread.lock' object\r\n```\r\n\r\nI wonder if it is because I nest the trainer inside another model, the codes for a nested trainer is as followed:\r\n\r\n```\r\nself.model.trainer = self.trainer\r\n```\r\nSo many thanks in advance.\r\n\r\nHi @eddiecong, the trainer is not pickleable and so you're probably correct that it's to do with your nested trainer code. You could override the `on_save_checkpoint` hook and then delete the trainer from the checkpoint as suggested by this answer: https://github.com/PyTorchLightning/pytorch-lightning/issues/3444#issuecomment-797652313\r\n\r\nHope that helps :smiley:",
    "meta": { "name": "save checkpoint issue in nested trainer" },
    "answer": "Hi @eddiecong, the trainer is not pickleable and so you're probably correct that it's to do with your nested trainer code. You could override the `on_save_checkpoint` hook and then delete the trainer from the checkpoint as suggested by this answer: https://github.com/PyTorchLightning/pytorch-lightning/issues/3444#issuecomment-797652313\r\n\r\nHope that helps :smiley:"
  },
  {
    "content": "I'm using the below callback \r\n`checkpoint_callback = pl.callbacks.ModelCheckpoint(\r\n        dirpath=other_arguments.output_dir,\r\n        monitor=\"val_loss\",\r\n        save_top_k=other_arguments.save_top_k,\r\n        save_last=other_arguments.save_last,\r\n        mode='min'\r\n    )\r\n\r\n    train_params = dict(\r\n        accumulate_grad_batches=other_arguments.gradient_accumulation_steps,\r\n        gpus=training_arguments.n_gpu,\r\n        deterministic=True,\r\n        max_epochs=other_arguments.num_train_epochs,\r\n        precision=16 if training_arguments.fp_16 else 32,\r\n        amp_level=training_arguments.opt_level,\r\n        gradient_clip_val=training_arguments.max_grad_norm,\r\n        checkpoint_callback=checkpoint_callback,\r\n        fast_dev_run=other_arguments.do_fast_dev_run,\r\n    )`\r\n\r\n\r\nThe output dir is empty which means the checkpoints are not getting saved. There is no error as wellPass `checkpoint_callback` to the `callbacks` argument. `checkpoint_callback` is a boolean only flag to indicate whether checkpointing should be enabled or not.",
    "meta": { "name": "Checkpoints not getting saved" },
    "answer": "Pass `checkpoint_callback` to the `callbacks` argument. `checkpoint_callback` is a boolean only flag to indicate whether checkpointing should be enabled or not."
  },
  {
    "content": "By default, in Lightning everything that is returned by a dataset is collated by the data loader and shipped to the same device.\r\nHowever, I am frequently in the situation where I have let's say `x, y` which are tensors and something like `y_semantic` which is in principle related to y but of higher data type, say a dictionary with some meta information about augmentations or so.\r\nI don't need 'y_semantic' to be on the GPU. Is there some flag or some method that I can overwrite so that some variables stay on CPU?\r\n\r\nSomething like\r\n```python\r\ndef ship_batch(self, batch):\r\n    batch[0] = batch[0].to(self.device)\r\n    # ...\r\n    batch[2] = batch[2].cpu() # just for illustration here\r\n```Dear @Haydnspass,\r\n\r\nYou have several ways to do this:\r\n\r\n1. Create a custom Data / Batch Object and implement the .to function to move only what is required.\r\n\r\n2. Simpler: Override LightningModule.transfer_batch_to_device hook and add your own logic to move only x, y to the right device.\r\n\r\nBest,\r\nT.C\r\n",
    "meta": { "name": "How to put all but some vars to GPU" },
    "answer": "Dear @Haydnspass,\r\n\r\nYou have several ways to do this:\r\n\r\n1. Create a custom Data / Batch Object and implement the .to function to move only what is required.\r\n\r\n2. Simpler: Override LightningModule.transfer_batch_to_device hook and add your own logic to move only x, y to the right device.\r\n\r\nBest,\r\nT.C\r\n"
  },
  {
    "content": "In my code, I would like to synchronize a tensor across all the gpus in `train_step`, which is a temporary variable. Is it allowed to call `torch.distributed.all_reduce` in this case? Or there is a specific function in `pytorch_lightning` that does the job?Hey @sandylaker! You can use `torch.distributed.all_reduce`. There is also within the `LightningModule` this function, however it may be better to expose this within lightning to make it easier to access:\r\n\r\n```python\r\nx = self.trainer.accelerator.training_type_plugin.reduce(x)\r\n```",
    "meta": {
      "name": "Is it possible to call dist.all_reduce manually in train_step?"
    },
    "answer": "Hey @sandylaker! You can use `torch.distributed.all_reduce`. There is also within the `LightningModule` this function, however it may be better to expose this within lightning to make it easier to access:\r\n\r\n```python\r\nx = self.trainer.accelerator.training_type_plugin.reduce(x)\r\n```"
  },
  {
    "content": "![image](https://user-images.githubusercontent.com/17240858/118919797-117a0700-b8ea-11eb-90e4-5a9f057697c6.png)\r\nIt seems that if I don't pass in an optimizer to my manual backward, my gradient clipping passes through a None object. I thought passing an optimizer to manual backward was not required. It turns out, passing an optimizer doesn't solve the bug.Can you reproduce it and open an issue about it?\r\n\r\nDiscussions is more for help/questions",
    "meta": {
      "name": "Not passing optimizer to manual backward causes downstream error"
    },
    "answer": "Can you reproduce it and open an issue about it?\r\n\r\nDiscussions is more for help/questions"
  },
  {
    "content": "Hi!\r\nWhen I use no multi-gpu settings and just a single GPU and the following parameters:\r\n```python\r\nbatch_size = 16\r\naccumulate_grad_batches=2\r\n```\r\nmy effective batch size is 32. My question is how `accumulate_grad_batches` and DDP interact.\r\nIf I am using 2 GPUS that are on the same machine and i use the parameters\r\n```\r\nbatch_size = 16\r\naccumulate_grad_batches=2\r\naccelerator='ddp'\r\ngpus=[0,1]\r\n```\r\nis my effective batch size now 64?\r\nThanks for the help!\r\nYes: https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html#batch-size",
    "meta": { "name": "accumulate_grad_batches and DDP" },
    "answer": "Yes: https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html#batch-size"
  },
  {
    "content": "Hi.\r\nIt seems like `validation_step` runs simultaneously at the end of training.\r\nAs soon as `validation_step` start, the percentage of gpu memory allocated is shooting up and `RuntimeError: CUDA out of memory` occur.\r\nHow can i fix it?\r\nThanks in advance!> validation_step runs simultaneously at the end of training.\r\n\r\nValidation is considered part of the fitting procedure but it never runs concurrent to training.\r\n\r\n> As soon as validation_step start, the percentage of gpu memory allocated is shooting up and RuntimeError: CUDA out of memory occur. How can i fix it?\r\n\r\nIt's definitely caused by a bug either on your end or ours. Can you try and reproduce it?\r\n\r\nYou can adapt https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py to do it\r\n\r\n",
    "meta": {
      "name": "How to hold 'validation_step' until training one epoch finished"
    },
    "answer": "> validation_step runs simultaneously at the end of training.\r\n\r\nValidation is considered part of the fitting procedure but it never runs concurrent to training.\r\n\r\n> As soon as validation_step start, the percentage of gpu memory allocated is shooting up and RuntimeError: CUDA out of memory occur. How can i fix it?\r\n\r\nIt's definitely caused by a bug either on your end or ours. Can you try and reproduce it?\r\n\r\nYou can adapt https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py to do it\r\n\r\n"
  },
  {
    "content": "I\u2019m currently attempting to make a Multi-GPU-supported CLIP training script, but am hitting a wall. I need to compute two matrices that are composed of whole batch statistics before I can compute loss. Namely, I need to compute the image and text embeddings of an entire batch. Only then can I compute the sub batch losses.\r\n\r\nHow can I first calculate and share the whole batch matrices across GPUs before computing losses?The LightningModule method `all_gather(Tensor)` solved it all!",
    "meta": { "name": "Compute Loss After Sharing Tensor Across GPUs" },
    "answer": "The LightningModule method `all_gather(Tensor)` solved it all!"
  },
  {
    "content": "I really liked @auto_move_data as it gave me a nice and easy way to pass (a single) input to a model, without caring about moving data structures to devices, but now it will be removed \ud83d\ude22 \r\nI realize that you want people to use trainer.predict() but this requires a dataloader, right?, which at least for me oftentimes is overly complicated while developing.\r\nAs far as I see the dataloader/collate/transforms also have to be different from the normal ones, since these typically provide batches with inputs and targets, whereas the forward function only takes inputs.\r\nLikely I'm missing something, can you maybe give some hints about this?\r\nAfter writing this I realized that I can use something like this: \r\n```\r\nmodel = model.eval()\r\ni, _ = next(iter(data.test_dataloader()))\r\nwith th.no_grad():\r\n    outputs = model.forward(model.transfer_batch_to_device(i))\r\n```\r\nBut I'm still curious about the intended future ways of doing this....",
    "meta": {
      "name": "what will be the closest replacement for @auto_move_data?"
    },
    "answer": "After writing this I realized that I can use something like this: \r\n```\r\nmodel = model.eval()\r\ni, _ = next(iter(data.test_dataloader()))\r\nwith th.no_grad():\r\n    outputs = model.forward(model.transfer_batch_to_device(i))\r\n```\r\nBut I'm still curious about the intended future ways of doing this...."
  },
  {
    "content": "Hi,\r\nI was wondering if algorithms implemented with pytorch lightning can take advantage of deep learning boost  hardware: \r\n\r\nhttps://www.intel.com/content/www/us/en/artificial-intelligence/deep-learning-boost.html\r\nhttps://github.com/oneapi-src/oneDNN\r\n\r\nAs far as I know it should be compatible with vanilla pytorch\r\nIf there is a speed-up on vanilla PyTorch ops, there should be one when using Lightning",
    "meta": { "name": "Intel deep learning boost compatibility" },
    "answer": "If there is a speed-up on vanilla PyTorch ops, there should be one when using Lightning"
  },
  {
    "content": "When I use an `IterableDataset`, the `Trainer.fit()` gives something like:\r\n\r\n    Epoch 0: : 21it [02:41,  7.69s/it, loss=0.663, v_num=18]  \r\n\r\nWhich does not display a progress. Even if I specify a validation interval, there is no progress shown. Is there a way I can specify an \"epoch length\", or something like that for `IterableDataset`?Hi\r\nIterableDatasets don't have a lengh defined. They can either be infinite in size or raise StopIteration at an arbitrary point. Hence, there is no notion of epochs and we cannot anticipate when it finishes iterations. \r\n\r\nIf you really need to have epochs, I suggest you try to rewrite your dataset logic into a regular Dataset.",
    "meta": { "name": "How to display progress on IterableDataset?" },
    "answer": "Hi\r\nIterableDatasets don't have a lengh defined. They can either be infinite in size or raise StopIteration at an arbitrary point. Hence, there is no notion of epochs and we cannot anticipate when it finishes iterations. \r\n\r\nIf you really need to have epochs, I suggest you try to rewrite your dataset logic into a regular Dataset."
  },
  {
    "content": "Hey, \r\nIm wondering why the patience parameter has been removed from the Trainer class.\r\nIm pretty sure that there once was this parameter and I used it a lot. \r\nHas it just been moved to another class, or is there a workaround such that I can still use patience when training my Model?You can use [EarlyStopping Callback](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.callbacks.early_stopping.html?highlight=early_stopping).",
    "meta": { "name": "Patience Parameter in Trainer" },
    "answer": "You can use [EarlyStopping Callback](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.callbacks.early_stopping.html?highlight=early_stopping)."
  },
  {
    "content": "Howdy! :cowboy_hat_face: I came across something in the PytorchVideo repository that I wasn't so sure about. They're using 2 accuracy metric classes in their `LightningModule` for train and val. I've been using 1 for everything and its making me wonder if I was wrong for some reason?\r\n\r\nHaven't dove into that code in a while...are the metrics being accumulated separately across the different loops, or are they accumulated together?\r\n\r\n[Here's ](https://github.com/facebookresearch/pytorchvideo/blob/6539ed63a1e48289636c00fcb6aa326749028819/tutorials/video_classification_example/train.py#L61-L71)the snippet in question...\r\n\r\n```python\r\n class VideoClassificationLightningModule(pytorch_lightning.LightningModule): \r\n     def __init__(self, args): \r\n         \"\"\" \r\n         This LightningModule implementation constructs a PyTorchVideo ResNet, \r\n         defines the train and val loss to be trained with (cross_entropy), and \r\n         configures the optimizer. \r\n         \"\"\" \r\n         self.args = args \r\n         super().__init__() \r\n         self.train_accuracy = pytorch_lightning.metrics.Accuracy() \r\n         self.val_accuracy = pytorch_lightning.metrics.Accuracy() \r\n```\r\n\r\nI would normally just have `self.accuracy` and use that in both the train and val steps. Is that incorrect?> I would normally just have self.accuracy and use that in both the train and val steps. Is that incorrect?\r\n\r\nI would say this is incorrect. You don't want to share the same metric state across training and validation. Otherwise you're mixing data, which will give misleading results compared to having dedicated train and val metric instances. Additionally, depending on how you log the metric value, the same instance could get reset at the end of the training and validation epoch. if you're running validation more frequently (e.g. using `val_check_interval`) this will be another problem. \r\n\r\ncc @SkafteNicki @Borda @edenafek should we add this to the documentation for metrics in lightning? are there ways we can make this clearer in the API itself? ",
    "meta": {
      "name": "Do you need to have more than one accuracy metric for different splits?"
    },
    "answer": "> I would normally just have self.accuracy and use that in both the train and val steps. Is that incorrect?\r\n\r\nI would say this is incorrect. You don't want to share the same metric state across training and validation. Otherwise you're mixing data, which will give misleading results compared to having dedicated train and val metric instances. Additionally, depending on how you log the metric value, the same instance could get reset at the end of the training and validation epoch. if you're running validation more frequently (e.g. using `val_check_interval`) this will be another problem. \r\n\r\ncc @SkafteNicki @Borda @edenafek should we add this to the documentation for metrics in lightning? are there ways we can make this clearer in the API itself? "
  },
  {
    "content": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen trainer run_test() called, the results from test cannot properly handle a 1D tensor in the results dictionary.\r\n\r\nSuch error will happen:\r\n\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_test(self)\r\n    708                 for k, v in result.items():\r\n    709                     if isinstance(v, torch.Tensor):\r\n--> 710                         result[k] = v.cpu().item()\r\n    711 \r\n    712         return eval_loop_results\r\n\r\nValueError: only one element tensors can be converted to Python scalars\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\nTo reproduce with BoringModel, only need to replace the test_epoch_end.\r\n\r\n```python\r\ndef test_epoch_end(self, outputs) -> None:\r\n    torch.stack([x[\"y\"] for x in outputs]).mean()\r\n    f1_score = torch.tensor([1,1,1,1])\r\n    return {'f1_score': f1_score}\r\n```\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n```python\r\ndef run_test(self):\r\n\r\n        # remove the tensors from the eval results\r\n        for i, result in enumerate(eval_loop_results):\r\n            if isinstance(result, dict):\r\n                for k, v in result.items():\r\n                    if isinstance(v, torch.Tensor):\r\n                        # should check if you can call .item()\r\n                        result[k] = v.cpu().item()\r\n```\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.8\r\n\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\nHi\r\n\r\nYou should see somewhere a warning:\r\n\r\n`UserWarning: The testing_epoch_end should not return anything as of 9.1. To log, use self.log(...) or self.write(...) directly in the LightningModule`\r\n\r\nOnly scalar tensors are supported. > Hi\r\n> \r\n> You should see somewhere a warning:\r\n> \r\n> `UserWarning: The testing_epoch_end should not return anything as of 9.1. To log, use self.log(...) or self.write(...) directly in the LightningModule`\r\n> \r\n> Only scalar tensors are supported.\r\n\r\nI think returning from testing_epoch_end is a very useful functionality when you want to do any post-processing with the results.\r\nWhy is thie not recommended? Is there any potential bug to do so?Because for example in multi gpu mode, if we would allow the user to return, then we're missing the information what to do with the data, how the data is collected and synced, or reduced or whatever. \r\nThe logging api offers reduction and sync, by specifying the custom arguments how to do so.\r\nOn the other hand, `self.write` offers a way to collect all results.\r\nThere will also be a prediction api in 1.2. #5752 \r\ncc @tchaton Thank for the explaination. I will try the ```self.write```. Could you please refer me to the page of ```self.write```? Sorry, it's not called \"write\", the warning seems to have the wrong name. There is currently `write_prediction` and `write_prediction_dict`\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#write-predictionHey @lzrpotato,\r\n\r\nFrom 1.2, Trainer and Lightning will have a predict function. It is still in `BETA`. We will soon update the doc.\r\n\r\nBest,\r\nT.C",
    "meta": {
      "name": "Trainer cannot handle 1d tensor when return results from test_epoch_end"
    },
    "answer": "Because for example in multi gpu mode, if we would allow the user to return, then we're missing the information what to do with the data, how the data is collected and synced, or reduced or whatever. \r\nThe logging api offers reduction and sync, by specifying the custom arguments how to do so.\r\nOn the other hand, `self.write` offers a way to collect all results.\r\nThere will also be a prediction api in 1.2. #5752 \r\ncc @tchaton "
  },
  {
    "content": "\r\nBut I keep getting this error. I have no what it's about. A hint will be really helpful. Thanks in advance.\r\n\r\n\r\n\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-147-594f245450d7> in <module>\r\n----> 1 trainer.fit(LitPlant, train_dataloader=train_loader, val_dataloaders=valid_loader)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    447         # ----------------------------\r\n    448         # setup data, etc...\r\n--> 449         self.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\r\n    450 \r\n    451         # hook\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in setup_fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    112         # clean hparams\r\n    113         if hasattr(model, \"hparams\"):\r\n--> 114             parsing.clean_namespace(model.hparams)\r\n    115 \r\n    116         # links data to the trainer\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/parsing.py in clean_namespace(hparams)\r\n     73         hparams_dict = hparams.__dict__\r\n     74 \r\n---> 75     del_attrs = [k for k, v in hparams_dict.items() if not is_picklable(v)]\r\n     76 \r\n     77     for k in del_attrs:\r\n\r\nAttributeError: 'property' object has no attribute 'items'What's the type of LitPlant? Are you passing an instance of the class or the class type to trainer.fit() ? ",
    "meta": { "name": "trying to train  Efficient net" },
    "answer": "What's the type of LitPlant? Are you passing an instance of the class or the class type to trainer.fit() ? "
  },
  {
    "content": "Hi, I'm currently trying to finetune a pretrained BERT model for intent classification using Huggingface's Transformers library and Pytorch Lightning.\r\nThe structure is simple where a linear classifier is simply put on the BERT encoder.\r\nI want to get the same result at the same seed setting, but although the whole setting including the seed is identical, the result changes.\r\nI thought if I pre-fix the seed using `seed_everything` and set the flag `workers=True`, I can get the exact same result, but I don't know what the problem is.\r\nThe fun fact is that all executions save the exact same best checkpoint with identical valid accuracy and actually the flow of the training seems also the same.\r\nBut after executing the test, the results are not same.\r\n\r\nThe main codes are as follows.\r\n```python\r\nfrom transformers import BertConfig, BertTokenizer, BertModel\r\nfrom pytorch_lightning import Trainer, seed_everything\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\n\r\ndef run(args):\r\n    # For directory setting\r\n    ...\r\n\r\n    # Tokenizer & Model => This model is a pre-trained encoder, so I think there is no need to fix a random seed.\r\n    config = BertConfig.from_pretrained('bert-base-uncased')\r\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n    model = BertModel.from_pretrained('bert-base-uncased')\r\n\r\n    ...\r\n    \r\n    print(\"Loading datasets...\")\r\n    # For data loading\r\n    train_set = Dataset(...)\r\n    valid_set = Dataset(...)\r\n    test_set = Dataset(...)\r\n\r\n    total_train_steps = int(len(train_set) / batch_size * num_epochs)\r\n    warmup_steps = int(total_train_steps * warmup_prop)\r\n    \r\n    # Random seed fixing for intent classification layer\r\n    seed_everything(0, workers=True)\r\n    \r\n    # Lightning Module setting  \r\n    module = TrainModule(model)\r\n\r\n    # Dataloaders\r\n    ppd = PadCollate(...)\r\n    \r\n    # Reset random seed for data shuffle\r\n    seed_everything(0, workers=True)\r\n\r\n    train_loader = DataLoader(train_set, collate_fn=ppd.pad_collate, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\r\n    valid_loader = DataLoader(valid_set, collate_fn=ppd.pad_collate, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\r\n    test_loader = DataLoader(test_set, collate_fn=ppd.pad_collate, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\r\n\r\n    print(\"Setting pytorch lightning callback & trainer...\")\r\n    # Model checkpoint callback\r\n    filename = \"best_ckpt_{epoch}_{train_all_acc:.4f}_{valid_all_acc:.4f}\"\r\n    monitor = \"valid_all_acc\"\r\n    \r\n    checkpoint_callback = ModelCheckpoint(\r\n        dirpath=save_dir,\r\n        filename=filename,\r\n        verbose=True,\r\n        monitor=monitor,\r\n        mode='max',\r\n        every_n_val_epochs=1,\r\n    )\r\n    \r\n    # Trainer setting\r\n    trainer = Trainer(\r\n        check_val_every_n_epoch=1,\r\n        gpus=gpu,\r\n        auto_select_gpus=True,\r\n        num_nodes=num_nodes,\r\n        max_epochs=num_epochs,\r\n        gradient_clip_val=max_grad_norm,\r\n        num_sanity_val_steps=0,\r\n        deterministic=True,\r\n        callbacks=[checkpoint_callback]\r\n    )\r\n    \r\n    print(\"Train starts.\")\r\n    trainer.fit(model=module, train_dataloader=train_loader, val_dataloaders=valid_loader)\r\n    print(\"Training done.\")\r\n    \r\n    print(\"Test starts.\")\r\n    trainer.test(model=module, test_dataloaders=test_loader, ckpt_path='best')\r\n    \r\n    print(\"GOOD BYE.\")\r\n```\r\n\r\nAlso, `TrainModule` is designed like below.\r\n```python\r\nfrom torch import nn as nn\r\nimport pytorch_lightning as pl\r\n\r\nclass TrainModule(pl.LightningModule):\r\n    def __init__(self, args, encoder):\r\n        super().__init__()\r\n        \r\n        self.args = args\r\n        self.save_hyperparameters(args)\r\n        \r\n        self.encoder = encoder\r\n        self.output_layer = IntentDetection(args)\r\n        self.output_layer.init_params()\r\n        \r\n        self.loss_func = nn.CrossEntropyLoss()\r\n        \r\n    def forward(self, input_ids, padding_masks=None):  # input_ids: (B, L), padding_masks: (B, L)\r\n        hidden_states = self.encoder(input_ids=input_ids, attention_mask=padding_masks)[0]  # (B, L, d_h)\r\n            \r\n        return self.output_layer(hidden_states[:, 0])  # (B, L, C) or  (B, C)\r\n    \r\n    def training_step(self, batch, batch_idx): \r\n        ...\r\n\r\nclass IntentDetection(nn.Module):\r\n    def __init__(self, args):\r\n        super(IntentDetection, self).__init__()\r\n        \r\n        self.hidden_size = args.hidden_size\r\n        self.num_classes = args.num_classes\r\n        \r\n        self.linear = nn.Linear(self.hidden_size, self.num_classes)\r\n        \r\n    def forward(self, hiddens):\r\n        # hiddens: (B, d_h)\r\n        \r\n        return self.linear(hiddens)  # (B, C)\r\n    \r\n    def init_params(self):\r\n        nn.init.xavier_uniform_(self.linear.weight)  \r\n```\r\n\r\nI also post the current training environment.\r\n- OS: Ubuntu 18.04.5 LTS\r\n- Python version: 3.8.5\r\n- Pytorch version: 1.7.1+cu110\r\n- Pytorch Lightning version: 1.3.0\r\n- GPU: A100-SXM4-40GB (DGX)\r\n- CUDA version: 11.0\r\n\r\nAnd I got 3 different results when I run the same codes 3 times.\r\n![image](https://user-images.githubusercontent.com/16731987/117425613-e4c70680-af5d-11eb-90c8-cb3dd990d6bc.png)\r\n\r\nThis is odd, since when I run the exact same code above in different environment, I could get the identical results from a same seed, even the number of workers in a dataloader is different.\r\nI also attach the environment which I was able to get the perfect reproducibility.\r\n- OS: CentOs Linux release 7.9.2009 (Core)\r\n- Python version: 3.7.4\r\n- Pytorch version: 1.7.1+cu110\r\n- Pytorch Lightning version: 1.3.0\r\n- GPU: RTX 3090\r\n- CUDA version: 11.2\r\n\r\nIt will be really great if anyone can help me to solve this issue...\r\nThank you very much.Can you try seeding again right before the `trainer.test` call?\r\n\r\nNot saying you should need to but to know if that makes any difference\r\n\r\nI think your best bet is to try to create a reproducible snippet. You can get started with the https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py",
    "meta": { "name": "How to get the perfect reproducibility" },
    "answer": "Can you try seeding again right before the `trainer.test` call?\r\n\r\nNot saying you should need to but to know if that makes any difference\r\n\r\nI think your best bet is to try to create a reproducible snippet. You can get started with the https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py"
  },
  {
    "content": "I am using [MLFlowLogger](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.MLFlowLogger.html#pytorch_lightning.loggers.MLFlowLogger) to log my experiment into Azure ML. Everything works fine but I noticed that when I ask the logger to store a metric every step (instead of every epoch), the logger does not increase the step number but instead keeps overwriting the current step in the batch. That is, if I have 5 minibatches for each epoch, and I have 10 epochs, the logger will overwrite step 1-5 continuously, instead of logging step 1-50. Something I noticed is that MLFlow/AzureML log metrics according to steps, regardless of whether it is a epoch or not, perhaps this is causing some issues.\r\n\r\nWould you be able to help?\r\n\r\nAn example here (7 steps only for `train_loss` but I ran more than 70 epochs)\r\n![image](https://user-images.githubusercontent.com/18400141/117634100-e6bbe000-b17e-11eb-991f-5985e58278b7.png)\r\n![image](https://user-images.githubusercontent.com/18400141/117635314-fee02f00-b17f-11eb-9ce5-703b36debeb0.png)\r\n\r\nI got it. By default the logger logs every 50 steps (https://pytorch-lightning.readthedocs.io/en/1.3.0/extensions/logging.html#control-logging-frequency). It so happened that my dataset was very small so there were way more epochs than training steps. If I increase the frequency `trainer = Trainer(log_every_n_steps=1)` then I do get a log for every steps in the training process. \r\n![image](https://user-images.githubusercontent.com/18400141/117647064-cbf06800-b18c-11eb-9ac8-354eb5d285db.png)\r\n\r\n",
    "meta": { "name": "MLFlow logger step vs epoch" },
    "answer": "I got it. By default the logger logs every 50 steps (https://pytorch-lightning.readthedocs.io/en/1.3.0/extensions/logging.html#control-logging-frequency). It so happened that my dataset was very small so there were way more epochs than training steps. If I increase the frequency `trainer = Trainer(log_every_n_steps=1)` then I do get a log for every steps in the training process. \r\n![image](https://user-images.githubusercontent.com/18400141/117647064-cbf06800-b18c-11eb-9ac8-354eb5d285db.png)\r\n\r\n"
  },
  {
    "content": "Good day, I'm currently working on two models which train on the same data. I'd like to integrate the two pre-trained models into one and use it for transfer learning. The combination is written as such (you can copy paste to run it). \r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\n\r\nclass MyModelA(pl.LightningModule):\r\n    def __init__(self, hidden_dim = 10):\r\n        super(MyModelA, self).__init__()\r\n        self.fc1 = torch.nn.Linear(hidden_dim, 2)\r\n        self.save_hyperparameters()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\r\n        return optimizer\r\n        \r\n    def forward(self, x):\r\n        x = self.fc1(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x,y = batch\r\n        return F.mse_loss(self.forward(x), y)\r\n    \r\nclass MyModelB(pl.LightningModule):\r\n    def __init__(self, hidden_dim = 10):\r\n        super(MyModelB, self).__init__()\r\n        self.fc1 = torch.nn.Linear(hidden_dim, 2)\r\n        self.save_hyperparameters()\r\n    \r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\r\n        return optimizer\r\n        \r\n    def forward(self, x):\r\n        x = self.fc1(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x,y = batch\r\n        return F.mse_loss(self.forward(x), y)\r\n\r\nclass MyEnsemble(pl.LightningModule):\r\n    def __init__(self, modelA, modelB):\r\n        super(MyEnsemble, self).__init__()\r\n        self.modelA = modelA\r\n        self.modelB = modelB\r\n        self.modelA.freeze()\r\n        self.modelB.freeze()\r\n        self.classifier = torch.nn.Linear(4, 2)\r\n\r\n        #self.save_hyperparameters() # Uncomment to show error\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\r\n        return optimizer\r\n        \r\n    def forward(self, x):\r\n        x1 = self.modelA(x)\r\n        x2 = self.modelB(x)\r\n        x = torch.cat((x1, x2), dim=1)\r\n        x = self.classifier(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        return F.mse_loss(self.forward(x), y)\r\n\r\ndl = DataLoader(TensorDataset(torch.randn(1000, 10), \r\n                              torch.randn(1000, 2)), \r\n                batch_size = 10)\r\n\r\nmodelA = MyModelA()\r\nmodelB = MyModelB()\r\n\r\n# pretrained modelA and modelB\r\ntrainerA = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\r\ntrainerA.fit(modelA, dl)\r\ntrainerB = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\r\ntrainerB.fit(modelB, dl)\r\n\r\n# modelA and modelB contains pretrained weights\r\nmodel = MyEnsemble(modelA, modelB)\r\n\r\ntrainer = pl.Trainer(gpus = 0, max_epochs = 5, progress_bar_refresh_rate = 50)\r\ntrainer.fit(model, dl)\r\n```\r\nAt first the code worked fine. However, I would like to save the hyperparameters of the ensemble module, but adding `self.save_hyperparameters()` at the end of the ensemble module `__init__` return this error.\r\n\r\n    ValueError: dictionary update sequence element #0 has length 1; 2 is required\r\n\r\nHence my question is how can I combine two or more lightning modules in a single module and save its hyperparameters? Or is there any alternative way to do so?\r\n\r\nThanks in advance!\r\n\r\nEDIT: Code updated to show both modelA and modelB are pretrained.Hi\r\nIt turns out you don't actually have any hyperparameters. The only inputs you have are entire models, these can't be saved like this (yes we could try to improve the error message). \r\n\r\nAllow me to modify your example a bit :))\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\n\r\n\r\nclass MyModelA(pl.LightningModule):\r\n    def __init__(self, hidden_size_a=10):\r\n        super(MyModelA, self).__init__()\r\n        self.fc1 = torch.nn.Linear(hidden_size_a, 2)\r\n\r\n    def forward(self, x):\r\n        x = self.fc1(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        return F.mse_loss(self.forward(x), y)\r\n\r\n\r\nclass MyModelB(pl.LightningModule):\r\n    def __init__(self, hidden_size_b=10):\r\n        super(MyModelB, self).__init__()\r\n        self.fc1 = torch.nn.Linear(hidden_size_b, 2)\r\n\r\n    def forward(self, x):\r\n        x = self.fc1(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        return F.mse_loss(self.forward(x), y)\r\n\r\n\r\nclass MyEnsemble(pl.LightningModule):\r\n    def __init__(self, hidden_size_a, hidden_size_b):\r\n        super(MyEnsemble, self).__init__()\r\n        self.save_hyperparameters()\r\n        self.modelA = MyModelA(hidden_size_a)\r\n        self.modelB = MyModelB(hidden_size_b)\r\n        self.modelA.freeze()\r\n        self.modelB.freeze()\r\n        self.classifier = torch.nn.Linear(4, 2)\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n    def forward(self, x):\r\n        x1 = self.modelA(x)\r\n        x2 = self.modelB(x)\r\n        x = torch.cat((x1, x2), dim=1)\r\n        x = self.classifier(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        return F.mse_loss(self.forward(x), y)\r\n\r\nmodel = MyEnsemble(hidden_size_a=10, hidden_size_b=10)\r\n\r\ndl = DataLoader(TensorDataset(torch.randn(1000, 10),\r\n                              torch.randn(1000, 2)),\r\n                batch_size=10)\r\n\r\nprint(\"my hyperparameters are:\")\r\nprint(model.hparams)\r\n\r\ntrainer = pl.Trainer(gpus=0, max_epochs=5, progress_bar_refresh_rate=50)\r\ntrainer.fit(model, dl)\r\n```\r\n\r\nIs this more in the direction you were thinking? Note the changes are in ModelA and ModelB init and mainly instantiating the modules inside the Ensamble module. Good day.\r\n\r\nAfter several trial and error, I come up with three ways on how to save hyperparameters for combined lightning module. Since this will be a long post, I will reply the solutions with codes and their respective cons under this comment thread.\r\n\r\n**TLDR:**\r\n1. Pass both hyperparameters and parameters/weights of the pretrained models to the `Ensemble` module and instantiating the pretrained modules inside the `Ensemble` module. This one is sort of similar to @awaelchli 's answer with additional tweaks.\r\n2. Pass the path of the pretrained models to the `Ensemble` module and instantiating the pretrained modules inside the `Ensemble`module.\r\n3. Construct the pretrained models using `torch.nn.Module` and pretrain them in `LightningModule`. Then, pass the pretrained models to the `Ensemble` module in `torch.nn.Module` form. \r\n\r\nLet me know your thoughts. Also, I hope that this discussion can be continued as I'm still not sure whether these solutions are correct or best in practice.\r\n\r\nAny other solutions are still welcomed.\r\n\r\nThanks in advance. I have finally came out with the final solution which can be obtained [here](https://github.com/PyTorchLightning/pytorch-lightning/issues/7447#issuecomment-835695726).\r\n\r\nThank you for anyone who read and participate in this discussion.",
    "meta": {
      "name": "How to combine multiple lightning module and save hyperparameters"
    },
    "answer": "I have finally came out with the final solution which can be obtained [here](https://github.com/PyTorchLightning/pytorch-lightning/issues/7447#issuecomment-835695726).\r\n\r\nThank you for anyone who read and participate in this discussion."
  },
  {
    "content": "Hi! \r\n\r\nI've trained a model successfully. I now want to have a look at the model predictions. I've overridden the `predict_dataloader` method in my DataModule:\r\n```\r\ndef predict_dataloader(self):\r\n        pred_loader = torch.utils.data.DataLoader(\r\n            self.val_ds, batch_size=1, num_workers=4)\r\n        return pred_loader\r\n```\r\n\r\nThen, I initialize the model using my checkpoint and call the `predict` method:\r\n```\r\ncheckpoint_dir = os.path.join(root_dir, \"logs2/epoch=199-val_loss=0.26-val_dice=1.67.ckpt\")\r\n\r\n# initialize the data module \r\ndata = KeriDataModule(data_dir=data_dir, pix_dim=(0.6, 0.6, 0.937))\r\n\r\n# initialize the LightningModule (from checkpoint)\r\nnet = Net.load_from_checkpoint(checkpoint_path=checkpoint_dir)\r\n\r\n# initialize Lightning's trainer\r\ntrainer = pl.Trainer(gpus=[0])\r\n\r\nresults = trainer.predict(net, data)\r\n```\r\nUnfortunately, I get the following error\r\n```\r\nPredicting: 0it [43:22, ?it/s]\r\n\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n\r\n\r\nPredicting: 0it [00:00, ?it/s]\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-12-4bd0b31315bd> in <module>\r\n     10 trainer = pl.Trainer(gpus=[0])\r\n     11 \r\n---> 12 results = trainer.predict(net, data)\r\n     13 print(results)\r\n     14 \r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in predict(self, model, dataloaders, datamodule, return_predictions)\r\n    629         self.data_connector.attach_data(model, predict_dataloaders=dataloaders, datamodule=datamodule)\r\n    630 \r\n--> 631         results = self._run(model)\r\n    632 \r\n    633         assert self.state.stopped\r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\r\n    754 \r\n    755         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n--> 756         self.dispatch()\r\n    757 \r\n    758         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n    793             self.accelerator.start_evaluating(self)\r\n    794         elif self.predicting:\r\n--> 795             self.accelerator.start_predicting(self)\r\n    796         else:\r\n    797             self.accelerator.start_training(self)\r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in start_predicting(self, trainer)\r\n    100 \r\n    101     def start_predicting(self, trainer: 'pl.Trainer') -> None:\r\n--> 102         self.training_type_plugin.start_predicting(trainer)\r\n    103 \r\n    104     def pre_dispatch(self, trainer: 'pl.Trainer') -> None:\r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_predicting(self, trainer)\r\n    150     def start_predicting(self, trainer: 'pl.Trainer') -> None:\r\n    151         # double dispatch to initiate the predicting loop\r\n--> 152         self._results = trainer.run_stage()\r\n    153 \r\n    154     def training_step(self, *args, **kwargs):\r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\r\n    804             return self.run_evaluate()\r\n    805         if self.predicting:\r\n--> 806             return self.run_predict()\r\n    807         return self.run_train()\r\n    808 \r\n\r\n~/miniconda3/envs/monai-lightning-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_predict(self)\r\n   1071             dataloader = self.accelerator.process_dataloader(dataloader)\r\n   1072             dl_max_batches = self.predict_loop.max_batches[dataloader_idx]\r\n-> 1073             for batch_idx, batch in enumerate(dataloader):\r\n   1074                 if batch is None:\r\n   1075                     continue\r\n\r\nTypeError: 'KeriDataModule' object is not iterable\r\n```\r\n\r\nI don't understand what I messed up ^^'. Any help / tip would be greatly appreciated :)Sorry! We did not add support for `trainer.predict(model, datamodule)`. We'll do it asap!\r\n\r\nYou need to do `trainer.predict(model, datamodule=datamodule)`",
    "meta": {
      "name": "\"Error: 'MyDataModule' object is not iterable\" when calling trainer.predict(model, data)"
    },
    "answer": "Sorry! We did not add support for `trainer.predict(model, datamodule)`. We'll do it asap!\r\n\r\nYou need to do `trainer.predict(model, datamodule=datamodule)`"
  },
  {
    "content": "Hi, I know that there is EarlyStopping if validation metrics are deteriorating. But I was wondering if it was possible to stop training if after say epoch 10, the accuracy hasn\u2019t reached say 20%. If such a callback doesn\u2019t exist, any thoughts on how I can get started on the implementation of it?\r\n\r\nFor context I am running a distributed hyper-parameter optimizer and I know that the \u201cgood\u201d hyper-parameter set will get me to 50% accuracy by epoch 5.You could write a callback similar to early stopping which checks your metric for the target value by whatever epoch. if the metric isn't good enough, you can signal to the trainer to stop, like this: https://github.com/PyTorchLightning/pytorch-lightning/blob/490cc57809ebeba19003b4101393a8a058217c31/pytorch_lightning/callbacks/early_stopping.py#L194-L196",
    "meta": {
      "name": "Stop training if high enough accuracy isn\u2019t reached"
    },
    "answer": "You could write a callback similar to early stopping which checks your metric for the target value by whatever epoch. if the metric isn't good enough, you can signal to the trainer to stop, like this: https://github.com/PyTorchLightning/pytorch-lightning/blob/490cc57809ebeba19003b4101393a8a058217c31/pytorch_lightning/callbacks/early_stopping.py#L194-L196"
  },
  {
    "content": "Hi there\r\n\r\nPytorch docs recommends using channels last when training vision models in mixed precision. To enable, you need to do [two changes](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html#converting-existing-models):\r\n1. Move you model to channel last format: `model = model.to(memory_format=torch.channels_last) # Replace with your model`. This can be done in `on_fit_start` callback hook as `model.to` performs an in place modification: \r\n```\r\nclass ChannelsLast(pl.Callback):\r\n    def on_fit_start(self, trainer, pl_module: pl.LightningModule) -> None:\r\n        # Inplace model modification\r\n        pl_module.to(memory_format=torch.channels_last)\r\n```\r\n2. Move input data to channel last format before feeding to the model: `input = input.to(memory_format=torch.channels_last)`. \r\n\r\nMy problem is in step 2. I don't find any PyTorch lightning hook that allows me to make this modification to the batch :/. The only options left are to add it as data transforms (that must be used in conjunction with the callback) or doing all channel last related logic inside the LightningModule. I would prefer to avoid this last solution as it could clutter the LightningModule with unnecessary code.\r\n\r\nDo you know a to do step 2 in a callback?You can use any of the following if done inside the LightningModule:\r\n\r\n```python\r\nbatch = self.on_before_batch_transfer(batch, dataloader_idx)\r\nbatch = self.transfer_batch_to_device(batch, device)\r\nbatch = self.on_after_batch_transfer(batch, dataloader_idx)\r\n```\r\n\r\nIf you really need to do it in the Callback, I guess you could use `on_train_batch_start` since the modification is in-place. But I wouldn't recommend it.",
    "meta": { "name": "How to implement channels last memory format callback" },
    "answer": "You can use any of the following if done inside the LightningModule:\r\n\r\n```python\r\nbatch = self.on_before_batch_transfer(batch, dataloader_idx)\r\nbatch = self.transfer_batch_to_device(batch, device)\r\nbatch = self.on_after_batch_transfer(batch, dataloader_idx)\r\n```\r\n\r\nIf you really need to do it in the Callback, I guess you could use `on_train_batch_start` since the modification is in-place. But I wouldn't recommend it."
  },
  {
    "content": "## [1.3.0] - 2021-05-06\r\n\r\n### Added\r\n\r\n- Added support for the `EarlyStopping` callback to run at the end of the training epoch (#6944)\r\n- Added synchronization points before and after `setup` hooks are run (#7202)\r\n- Added a `teardown` hook to `ClusterEnvironment` (#6942)\r\n- Added utils for metrics to scalar conversions (#7180)\r\n- Added utils for NaN/Inf detection for gradients and parameters (#6834)\r\n- Added more explicit exception message when trying to execute `trainer.test()` or `trainer.validate()` with `fast_dev_run=True` (#6667)\r\n- Added `LightningCLI` class to provide simple reproducibility with minimum boilerplate training CLI (#4492, #6862, #7156, #7299)\r\n- Added `gradient_clip_algorithm` argument to Trainer for gradient clipping by value (#6123).\r\n- Added a way to print to terminal without breaking up the progress bar (#5470)\r\n- Added support to checkpoint after training steps in `ModelCheckpoint` callback (#6146)\r\n- Added `TrainerStatus.{INITIALIZING,RUNNING,FINISHED,INTERRUPTED}` (#7173)\r\n- Added `Trainer.validate()` method to perform one evaluation epoch over the validation set (#4948)\r\n- Added `LightningEnvironment` for Lightning-specific DDP (#5915)\r\n- Added `teardown()` hook to LightningDataModule (#4673)\r\n- Added `auto_insert_metric_name` parameter to `ModelCheckpoint` (#6277)\r\n- Added arg to `self.log` that enables users to give custom names when dealing with multiple dataloaders (#6274)\r\n- Added `teardown` method to `BaseProfiler` to enable subclasses defining post-profiling steps outside of `__del__` (#6370)\r\n- Added `setup` method to `BaseProfiler` to enable subclasses defining pre-profiling steps for every process (#6633)\r\n- Added no return warning to predict (#6139)\r\n- Added `Trainer.predict` config validation (#6543)\r\n- Added `AbstractProfiler` interface (#6621)\r\n- Added support for including module names for forward in the autograd trace of `PyTorchProfiler` (#6349)\r\n- Added support for the PyTorch 1.8.1 autograd profiler (#6618)\r\n- Added `outputs` parameter to callback's `on_validation_epoch_end` & `on_test_epoch_end` hooks (#6120)\r\n- Added `configure_sharded_model` hook (#6679)\r\n- Added support for `precision=64`, enabling training with double precision (#6595)\r\n- Added support for DDP communication hooks (#6736)\r\n- Added `artifact_location` argument to `MLFlowLogger` which will be passed to the `MlflowClient.create_experiment` call (#6677)\r\n- Added `model` parameter to precision plugins' `clip_gradients` signature (#6764, #7231)\r\n- Added `is_last_batch` attribute to `Trainer` (#6825)\r\n- Added `LightningModule.lr_schedulers()` for manual optimization  (#6567)\r\n- Added `MpModelWrapper` in TPU Spawn (#7045)\r\n- Added `max_time` Trainer argument to limit training time (#6823)\r\n- Added `on_predict_{batch,epoch}_{start,end}` hooks (#7141)\r\n- Added new `EarlyStopping` parameters `stopping_threshold` and `divergence_threshold` (#6868)\r\n- Added `debug` flag to TPU Training Plugins (PT_XLA_DEBUG) (#7219)\r\n- Added new `UnrepeatedDistributedSampler` and `IndexBatchSamplerWrapper` for tracking distributed predictions (#7215)\r\n- Added `trainer.predict(return_predictions=None|False|True)` (#7215)\r\n- Added `BasePredictionWriter` callback to implement prediction saving (#7127)\r\n- Added `trainer.tune(scale_batch_size_kwargs, lr_find_kwargs)` arguments to configure the tuning algorithms (#7258)\r\n- Added `tpu_distributed` check for TPU Spawn barrier (#7241)\r\n- Added device updates to TPU Spawn for Pod training (#7243)\r\n- Added warning when missing `Callback` and using `resume_from_checkpoint` (#7254)\r\n- DeepSpeed single file saving (#6900)\r\n- Added Training type Plugins Registry (#6982, #7063, #7214, #7224)\r\n- Add `ignore` param to `save_hyperparameters` (#6056)\r\n\r\n### Changed\r\n\r\n- Changed `LightningModule.truncated_bptt_steps` to be property (#7323)\r\n- Changed `EarlyStopping` callback from by default running `EarlyStopping.on_validation_end` if only training is run. Set `check_on_train_epoch_end` to run the callback at the end of the train epoch instead of at the end of the validation epoch (#7069)\r\n- Renamed `pytorch_lightning.callbacks.swa` to `pytorch_lightning.callbacks.stochastic_weight_avg` (#6259)\r\n- Refactor `RunningStage` and `TrainerState` usage (#4945, #7173)\r\n    * Added `RunningStage.SANITY_CHECKING`\r\n    * Added `TrainerFn.{FITTING,VALIDATING,TESTING,PREDICTING,TUNING}`\r\n    * Changed `trainer.evaluating` to return `True` if validating or testing\r\n- Changed `setup()` and `teardown()` stage argument to take any of `{fit,validate,test,predict}` (#6386)\r\n- Changed profilers to save separate report files per state and rank (#6621)\r\n- The trainer no longer tries to save a checkpoint on exception or run callback's `on_train_end` functions (#6864)\r\n- Changed `PyTorchProfiler` to use `torch.autograd.profiler.record_function` to record functions (#6349)\r\n- Disabled `lr_scheduler.step()` in manual optimization  (#6825)\r\n- Changed warnings and recommendations for dataloaders in `ddp_spawn` (#6762)\r\n- `pl.seed_everything` will now also set the seed on the `DistributedSampler` (#7024)\r\n- Changed default setting for communication of multi-node training using `DDPShardedPlugin` (#6937)\r\n- `trainer.tune()` now returns the tuning result (#7258)\r\n- `LightningModule.from_datasets()` now accepts `IterableDataset` instances as training datasets. (#7503)\r\n- Changed `resume_from_checkpoint` warning to an error when the checkpoint file does not exist (#7075)\r\n- Automatically set `sync_batchnorm` for `training_type_plugin` (#6536)\r\n- Allowed training type plugin to delay optimizer creation (#6331)\r\n- Removed ModelSummary validation from train loop on_trainer_init (#6610)\r\n- Moved `save_function` to accelerator (#6689)\r\n- Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)\r\n- Improved verbose logging for `EarlyStopping` callback (#6811)\r\n- Run ddp_spawn dataloader checks on Windows (#6930)\r\n- Updated mlflow with using `resolve_tags` (#6746)\r\n- Moved `save_hyperparameters` to its own function (#7119)\r\n- Replaced `_DataModuleWrapper` with `__new__` (#7289)\r\n- Reset `current_fx` properties on lightning module in teardown (#7247)\r\n- Auto-set `DataLoader.worker_init_fn` with `seed_everything` (#6960)\r\n- Remove `model.trainer` call inside of dataloading mixin (#7317)\r\n- Split profilers module (#6261)\r\n- Ensure accelerator is valid if running interactively (#5970)\r\n- Disabled batch transfer in DP mode (#6098)\r\n\r\n### Deprecated\r\n\r\n- Deprecated `outputs` in both `LightningModule.on_train_epoch_end` and `Callback.on_train_epoch_end` hooks (#7339)\r\n- Deprecated `Trainer.truncated_bptt_steps` in favor of `LightningModule.truncated_bptt_steps` (#7323)\r\n- Deprecated `outputs` in both `LightningModule.on_train_epoch_end` and `Callback.on_train_epoch_end` hooks (#7339)\r\n- Deprecated `LightningModule.grad_norm` in favor of `pytorch_lightning.utilities.grads.grad_norm` (#7292)\r\n- Deprecated the `save_function` property from the `ModelCheckpoint` callback (#7201)\r\n- Deprecated `LightningModule.write_predictions` and `LightningModule.write_predictions_dict` (#7066)\r\n- Deprecated `TrainerLoggingMixin` in favor of a separate utilities module for metric handling (#7180)\r\n- Deprecated `TrainerTrainingTricksMixin` in favor of a separate utilities module for NaN/Inf detection for gradients and parameters (#6834)\r\n- `period` has been deprecated in favor of `every_n_val_epochs` in the `ModelCheckpoint` callback (#6146)\r\n- Deprecated `trainer.running_sanity_check` in favor of `trainer.sanity_checking` (#4945)\r\n- Deprecated `Profiler(output_filename)` in favor of `dirpath` and `filename` (#6621)\r\n- Deprecated `PytorchProfiler(profiled_functions)` in favor of `record_functions` (#6349)\r\n- Deprecated `@auto_move_data` in favor of `trainer.predict` (#6993)\r\n- Deprecated `Callback.on_load_checkpoint(checkpoint)` in favor of `Callback.on_load_checkpoint(trainer, pl_module, checkpoint)` (#7253)\r\n- Deprecated metrics in favor of `torchmetrics` (#6505, #6530, #6540, #6547, #6515, #6572, #6573, #6584, #6636, #6637, #6649, #6659, #7131)\r\n- Deprecated the `LightningModule.datamodule` getter and setter methods; access them through `Trainer.datamodule` instead (#7168)\r\n- Deprecated the use of `Trainer(gpus=\"i\")` (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)\r\n\r\n### Removed\r\n\r\n- Removed the `exp_save_path` property from the `LightningModule` (#7266)\r\n- Removed training loop explicitly calling `EarlyStopping.on_validation_end` if no validation is run (#7069)\r\n- Removed `automatic_optimization` as a property from the training loop in favor of `LightningModule.automatic_optimization` (#7130)\r\n- Removed evaluation loop legacy returns for `*_epoch_end` hooks (#6973)\r\n- Removed support for passing a bool value to `profiler` argument of Trainer (#6164)\r\n- Removed no return warning from val/test step (#6139)\r\n- Removed passing a `ModelCheckpoint` instance to `Trainer(checkpoint_callback)` (#6166)\r\n- Removed deprecated Trainer argument `enable_pl_optimizer` and `automatic_optimization` (#6163)\r\n- Removed deprecated metrics (#6161)\r\n    * from `pytorch_lightning.metrics.functional.classification` removed `to_onehot`, `to_categorical`, `get_num_classes`, `roc`, `multiclass_roc`, `average_precision`, `precision_recall_curve`, `multiclass_precision_recall_curve`\r\n    * from `pytorch_lightning.metrics.functional.reduction` removed `reduce`, `class_reduce`\r\n- Removed deprecated `ModelCheckpoint` arguments `prefix`, `mode=\"auto\"` (#6162)\r\n- Removed `mode='auto'` from `EarlyStopping` (#6167)\r\n- Removed `epoch` and `step` arguments from `ModelCheckpoint.format_checkpoint_name()`, these are now included in the `metrics` argument (#7344)\r\n- Removed legacy references for magic keys in the `Result` object (#6016)\r\n- Removed deprecated `LightningModule` `hparams` setter (#6207)\r\n- Removed legacy code to log or include metrics in the progress bar by returning them in a dict with the `\"log\"/\"progress_bar\"` magic keys. Use `self.log` instead (#6734)\r\n- Removed `trainer.fit()` return value of `1`. It has no return now (#7237)\r\n- Removed `logger_connector` legacy code (#6733)\r\n- Removed unused mixin attributes (#6487)\r\n\r\n### Fixed\r\n\r\n- Fixed NaN errors in progress bars when training with iterable datasets with no length defined (#7306)\r\n- Fixed attaching train and validation dataloaders when `reload_dataloaders_every_epoch=True` and `num_sanity_val_steps=0` (#7207)\r\n- Added a barrier in the accelerator `teardown` to synchronize processes before execution finishes (#6814)\r\n- Fixed multi-node DDP sub-process launch by using `local_rank` instead of `global_rank` for main process assertion (#7061)\r\n- Fixed incorrect removal of `WORLD_SIZE` environment variable in DDP training when launching with torch distributed/torchelastic (#6942)\r\n- Made the `Plugin.reduce` method more consistent across all Plugins to reflect a mean-reduction by default (#6011)\r\n- Move lightning module to correct device type when using LightningDistributedWrapper (#6070)\r\n- Do not print top-k verbose log with `ModelCheckpoint(monitor=None)` (#6109)\r\n- Fixed `ModelCheckpoint(save_top_k=0, save_last=True)` not saving the `last` checkpoint (#6136)\r\n- Fixed `.teardown(stage='fit')` and `.on_fit_{start,end}()` getting called during `trainer.test` (#6386)\r\n- Fixed LightningModule `all_gather` on cpu tensors (#6416)\r\n- Fixed torch distributed not available in setup hook for DDP (#6506)\r\n- Fixed `trainer.tuner.{lr_find,scale_batch_size}` not setting the `Trainer` state properly (#7258)\r\n- Fixed bug where the learning rate schedulers did not follow the optimizer frequencies (#4868)\r\n- Fixed pickle error checker to now check for `pickle.PickleError` to catch all pickle errors (#6917)\r\n- Fixed a bug where the outputs object passed to `LightningModule.training_epoch_end` was different from the object passed to the `on_train_end_epoch` hook (#6969)\r\n- Fixed a bug where the outputs passed to `train_batch_end` would be listed even when using a single optimizer and no truncated backprop through time steps (#6969)\r\n- Fixed bug for trainer error handling which would cause hang for distributed training (#6864)\r\n- Fixed `self.device` not returning the correct device in replicas of data-parallel (#6414)\r\n- Fixed `lr_find` trying beyond `num_training` steps and suggesting a too high learning rate (#7076)\r\n- Fixed logger creating incorrect version folder in DDP with repeated `Trainer.fit` calls (#7077)\r\n- Fixed metric objects passed directly to `self.log` not being reset correctly (#7055)\r\n- Fixed `CombinedLoader` in distributed settings for validation / testing (#7102)\r\n- Fixed the save_dir in `WandbLogger` when the run was initiated externally (#7106)\r\n- Fixed `num_sanity_val_steps` affecting reproducibility of training data shuffling (#7014)\r\n- Fixed resetting device after `fitting/evaluating/predicting` (#7188)\r\n- Fixed bug where `trainer.tuner.scale_batch_size(max_trials=0)` would not return the correct batch size result (#7262)\r\n- Fixed metrics not being properly logged with `precision=16` and `manual_optimization` (#7228)\r\n- Fixed `BaseFinetuning` properly reloading `optimizer_states` when using `resume_from_checkpoint` (#6891)\r\n- Fixed `parameters_to_ignore` not properly set to DDPWrapper (#7239)\r\n- Fixed parsing of `fast_dev_run=True` with the built-in `ArgumentParser` (#7240)\r\n- Fixed handling an `IterableDataset` that fails to produce a batch at the beginning of an epoch (#7294)\r\n- Fixed `LightningModule.save_hyperparameters()` when attempting to save an empty container (#7268)\r\n- Fixed `apex` not properly instantiated when running with `ddp` (#7274)\r\n- Fixed optimizer `state` not moved to `GPU` (#7277)\r\n- Fixed custom init args for `WandbLogger` (#6989)\r\n- Fixed a bug where an error would be raised if the train dataloader sometimes produced None for a batch (#7342)\r\n- Fixed examples (#6600, #6638, #7096, #7246, #6357, #6476, #6294, #6373, #6088, #7398)\r\n- Resolved schedule step bug for PyTorch Profiler (#6674, #6681)\r\n- Updated logic for checking TPUs availability (#6767)\r\n- Resolve TPU miss rendezvous (#6781)\r\n- Fixed auto-scaling mode when calling tune method on trainer (#7321)\r\n- Fixed finetuning complex models correctly unfreezes (#6880)\r\n- Ensure we set the eval/train flag correctly on accelerator model (#6877)\r\n- Set better defaults for `rank_zero_only.rank` when training is launched with SLURM and torchelastic (#6802)\r\n- Fixed matching the number of outputs of backward with forward for AllGatherGrad (#6625)\r\n- Fixed the `gradient_clip_algorithm` has no effect (#6928)\r\n- Fixed CUDA OOM detection and handling (#6934)\r\n- Fixed `unfreeze_and_add_param_group` expects `modules` rather than `module` (#6822)\r\n- Fixed DPP + SyncBN when move on device (#6838)\r\n- Fixed missing arguments in `lr_find` call (#6784)\r\n- Fixed `set_default_tensor_type` to `torch.DoubleTensor` with precision=64 (#7108)\r\n- Fixed `NeptuneLogger.log_text(step=None)` (#7194)\r\n- Fixed importing torchtext batch (#6365, #6323, #6211)\r\n\r\n\r\n### Contributors\r\n\r\n@akihironitta, @alessiobonfiglio, @amisev, @amogkam, @ananthsub, @ArvinZhuang, @ashleve, @asnorkin, @awaelchli, @BloodAxe, @bmahlbrand, @Borda, @borisdayma, @camruta, @carmocca, @ceshine, @dbonner, @dhkim0225, @EdwardJB, @EliaCereda, @EricCousineau-TRI, @ethanwharris, @FlorianMF, @hemildesai, @ifsheldon, @kaushikb11, @mauvilsa, @maxfrei750, @mesejo, @ramonemiliani93, @rohitgr7, @s-rog, @sadiqj, @scart97, @SeanNaren, @shuyingsunshine21, @SkafteNicki, @SpontaneousDuck, @stllfe, @tchaton, @THasthika, @vballoli\r\n\r\n_If we forgot someone due to not matching commit email with GitHub account, let us know :]_\n\n<hr /><em>This discussion was created from the release <a href='https://github.com/PyTorchLightning/pytorch-lightning/releases/tag/1.3.0'>Lightning CLI, PyTorch Profiler, Improved Early Stopping</a>.</em>Epic!",
    "meta": {
      "name": "Lightning CLI, PyTorch Profiler, Improved Early Stopping"
    },
    "answer": "Epic!"
  },
  {
    "content": "Hi\r\nI'm trying to calculate some metrics and generate some images to save at the end of each **epoch**.\r\nI put this code in the on_train_epoch_end() (I also tried using a custom callback) but the function seems to be called in the middle of the epochs, approximately 3-4 times per epoch. \r\n\r\nSurely this isn't intended behaviour? Could it be to do with me using a combined loader?@inigoval thanks for reporting this. Could you provide a snippet code so that we can reproduce ?",
    "meta": { "name": "on_train_epoch_end() runs int he middle of epochs" },
    "answer": "@inigoval thanks for reporting this. Could you provide a snippet code so that we can reproduce ?"
  },
  {
    "content": "I'm looking to train on chunks of my entire dataset at a time per epoch (preferably over every n epochs, but this is not yet implemented officially), since the size of my dataset exceeds my total RAM. I'd therefore like to update the data in the DataLoaders every epoch. From all the examples I've seen, the actual data content is saved in memory a class attribute (in the following code snippet, it's saved in `self.mnist_train` and `self.mnist_val`). It seems that `train_dataloader()` and `val_dataloader()` only read `self.mnist_train` and `self.mnist_val` into DataLoaders. \r\n\r\nFrom my understanding, `reload_dataloaders_every_epoch=True` calls `train_dataloader()` and `val_dataloader()` at every epoch, but I don't see the point of doing this if `self.mnist_test` and `self.mnist_train` aren't actually being changed. How do I make `train_dataloader()` and `test_dataloader()` return new data every epoch? \r\n\r\n```\r\nclass MyDataModule(pl.LightningDataModule):\r\n\r\n    def __init__(\r\n        self,\r\n        batch_size: int = 32,\r\n    ):\r\n        super().__init__()\r\n        dataset = MNIST(_DATASETS_PATH, train=True, download=True, transform=transforms.ToTensor())\r\n        self.mnist_test = MNIST(_DATASETS_PATH, train=False, download=True, transform=transforms.ToTensor())\r\n        self.mnist_train, self.mnist_val = random_split(dataset, [55000, 5000])\r\n        self.batch_size = batch_size\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\r\n```> since the size of my dataset exceeds my total RAM\r\n\r\nThat's not unusual. Often datasets don't fit into the RAM and that's fine. DataLoaders are designed to asynchronously load the data from your hard disk into ram and then onto the GPU.\r\n\r\n> From my understanding, reload_dataloaders_every_epoch=True calls train_dataloader() and val_dataloader() at every epoch, but I don't see the point of doing this if self.mnist_test and self.mnist_train aren't actually being changed.\r\n\r\nYes but first of all this is a toy example and doesn't really do anything interesting. Second even though the dataset does not change, the dataloader will be constructed newly every epoch. You could return  dataloader with a new or growing dataset every epoch or do something crazy like increase the batch size every epoch or turn shuffle on and off haha. ",
    "meta": {
      "name": "What is the purpose of reload_dataloaders_every_epoch?"
    },
    "answer": "> since the size of my dataset exceeds my total RAM\r\n\r\nThat's not unusual. Often datasets don't fit into the RAM and that's fine. DataLoaders are designed to asynchronously load the data from your hard disk into ram and then onto the GPU.\r\n\r\n> From my understanding, reload_dataloaders_every_epoch=True calls train_dataloader() and val_dataloader() at every epoch, but I don't see the point of doing this if self.mnist_test and self.mnist_train aren't actually being changed.\r\n\r\nYes but first of all this is a toy example and doesn't really do anything interesting. Second even though the dataset does not change, the dataloader will be constructed newly every epoch. You could return  dataloader with a new or growing dataset every epoch or do something crazy like increase the batch size every epoch or turn shuffle on and off haha. "
  },
  {
    "content": "## \ud83d\udc1b Bug\r\n\r\nI get an error `The metric `val_acc_0_step/epoch_0` does not contain a single element thus it cannot be converted to float.` when running training loop.\r\n\r\n###  Full stack trace \r\n```\r\n File \"main.py\", line 76, in <module>\r\n    run_training()\r\n  File \"main.py\", line 69, in run_training\r\n    trainer.fit(model, dm)\r\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py\", line 484, in safe_patch_function\r\n    patch_function(call_original, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py\", line 241, in patch_with_managed_run\r\n    result = patch_function(original, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py\", line 296, in fit\r\n    return _run_and_log_function(self, original, args, kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py\", line 288, in _run_and_log_function\r\n    result = original(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py\", line 440, in call_original\r\n    original_result = original(*og_args, **og_kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\r\n    self.dispatch()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\r\n    self._results = trainer.run_train()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 577, in run_training_epoch\r\n    self.trainer.run_evaluation(on_epoch=True)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 732, in run_evaluation\r\n    self.evaluation_loop.log_evaluation_step_metrics(output, batch_idx)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 336, in log_evaluation_step_metrics\r\n    self.__log_result_step_metrics(step_log_metrics, step_pbar_metrics, batch_idx)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 351, in __log_result_step_metrics\r\n    self.trainer.logger_connector.log_metrics(metrics_by_epoch, {}, step=batch_idx)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py\", line 222, in log_metrics\r\n    scalar_metrics = self.trainer.metrics_to_scalars(metrics)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/logging.py\", line 51, in metrics_to_scalars\r\n    f\"The metric `{k}` does not contain a single element\"\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: The metric `val_acc_0_step/epoch_0` does not contain a single element thus it cannot be converted to float. Found `tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n        0.])`\r\n```\r\n\r\n### To Reproduce\r\n\r\nI train the model below, which is multi-class / multi-dimensional classifier (have multiple class categories trained in one hierarchical model). The error seem to come from the following piece (running in loop is so that I can specify number of classes, which is different for different dimensions):\r\n```\r\nfor i in range(len(y_true)):\r\n            self.valid_acc[i](preds[i], y_true[i])\r\n            self.log(f'val_acc_{i}', self.valid_acc[i], on_step=True, on_epoch=True)    \r\n```\r\nI was getting the same error when trying to save confusion matrix.\r\n\r\n#### Code sample\r\n\r\n```\r\n\r\nclass OntologyTaggerModel(pl.LightningModule):\r\n    def __init__(self,\r\n                 num_classes,\r\n                 model_name='bert-base-cased',\r\n                 learning_rate=3e-6,\r\n                 **kwargs):\r\n\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.learning_rate = learning_rate\r\n        self.model = BertForMulticlassSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\r\n        self.valid_acc = nn.ModuleList([torchmetrics.Accuracy(average=None, num_classes=num_class) for num_class in torch.tensor(num_classes)])\r\n        self.valid_f1 = torchmetrics.F1(multiclass=True, mdmc_average='samplewise')\r\n        self.cm = nn.ModuleList([torchmetrics.ConfusionMatrix(num_classes=num_class) for num_class in torch.tensor(num_classes)])\r\n        \r\n\r\n    def forward(self, *input, **kwargs):\r\n        return self.model(*input, **kwargs)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y_true = batch\r\n        loss, _ = self(x, labels=y_true)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y_true = batch\r\n        _, y_preds = self(x, labels=y_true)\r\n        preds = [torch.argmax(y_pred, axis=1) for y_pred in y_preds]\r\n        for i in range(len(y_true)):\r\n            self.valid_acc[i](preds[i], y_true[i])\r\n            self.log(f'val_acc_{i}', self.valid_acc[i], on_step=True, on_epoch=True)        \r\n        self.valid_f1(torch.stack(preds), torch.stack(y_true))\r\n        self.log('f1', self.valid_f1, on_step=True, on_epoch=True)\r\n\r\n    def configure_optimizers(self):\r\n        'Prepare optimizer and schedule (linear warmup and decay)'\r\n        return torch.optim.Adam(params=self.parameters(), lr=self.learning_rate)\r\n\r\n    def training_epoch_end(self, training_step_outputs):\r\n        avg_loss = torch.tensor([x['loss']\r\n                                 for x in training_step_outputs]).mean()\r\n        self.log('train_loss', avg_loss)\r\n\r\n        print(f'###score: train_loss### {avg_loss}')\r\n\r\n    def validation_epoch_end(self, val_step_outputs):\r\n        for i in range(len(self.valid_acc)):\r\n            acc = self.valid_acc[i].compute()\r\n            self.log(f'val_score_{i}', acc)\r\n        f1 = self.valid_f1.compute()\r\n        self.log('f1', f1)\r\n        print(f'###score: val_score### {acc}')\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nMetrics should be rendered irrespective of dimension\r\n\r\n### Environment\r\n\r\npytorch-lightning==1.2.7\r\ndatasets==1.4.1\r\nmlflow==1.16.0\r\ntorchmetrics=0.3.1\r\ntorch=1.7.1\r\npython=3.6\r\n\r\n\r\n@sivakhno according to the `torchmetrics` documentation, the [`Accuracy`](https://torchmetrics.readthedocs.io/en/latest/references/modules.html#accuracy) object can be instantiated with an explicit flag `multiclass`. Is there any specific reason why you are not doing in that way ? /cc @Borda @SkafteNickisince you set `average=None` when you initialize the `Accuracy` metric, the output will be the accuracy score calculated per class. As this is a non-scalar tensor and `self.log` is only intended to be used with scalar tensors, you get this error.",
    "meta": { "name": "Can not log metric which is a tensor" },
    "answer": "since you set `average=None` when you initialize the `Accuracy` metric, the output will be the accuracy score calculated per class. As this is a non-scalar tensor and `self.log` is only intended to be used with scalar tensors, you get this error."
  },
  {
    "content": "Hello!\r\n\r\nI'm working on an inference engine where I don't have any data before I start training. The idea is that at each epoch, a new data point is generated by a simulator and that data is fed to the NN only at that epoch. \r\n\r\nI'm wondering if there is a way to do this with pytorch-lightning. I looked at the IterableDataset class but the only thing I came up with is to generate the data beforehand and then iterate over it.\r\n\r\nThanks!hi @pjovanovski1 in this case you could follow a similar strategy proposed in this example of [lightning-bolts](https://lightning-bolts.readthedocs.io/en/latest/) for RL\r\nhttps://github.com/PyTorchLightning/lightning-bolts/blob/master/pl_bolts/models/rl/reinforce_model.py#L254",
    "meta": { "name": "Generating new data while training." },
    "answer": "hi @pjovanovski1 in this case you could follow a similar strategy proposed in this example of [lightning-bolts](https://lightning-bolts.readthedocs.io/en/latest/) for RL\r\nhttps://github.com/PyTorchLightning/lightning-bolts/blob/master/pl_bolts/models/rl/reinforce_model.py#L254"
  },
  {
    "content": "Hi\r\nI need to compute some metrics that are not quick to compute (eg. Frechet Inception Distance).\r\n\r\nIt is too expensive to compute them every validation epoch. Instead I would like to compute the metric once after every training epoch (or after some arbitrary number of steps). To do so, **I need to be able to access the training dataset at the end of every training epoch**, compute the metric and log it.\r\n\r\nIt is not obvious how to do this, as I cannot access the training data-set during \u201con_epoch_end\u201d or one of the other end of epoch hooks.\r\n\r\nIs there a good solution for this?\r\nThanks\r\nInigo.\r\n\r\nSolved by using `self.trainer.datamodule`",
    "meta": {
      "name": "Computing expensive metrics less frequently than using validation_step()"
    },
    "answer": "Solved by using `self.trainer.datamodule`"
  },
  {
    "content": "Once the model is trained, what would be the best way to use it to do a forward pass in a large dataset (that must be divided in batches not to fill GPU memory) and obtain the output?\r\n\r\nWhat I normally do is to use the LightningModule as a torch Module. However, that forces me to: write a `for`, explicitly transfer to the device each batch, make sure to cal `.eval()`, concatenate the output of each batch... all the things that pytorch-lightning saves me from doing during training. But I guess I am missing an obvious simpler option?In 1.3 there will be a [predict](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#predict) function that does all that for you!\r\nYou can already use it by installing 1.3.0rc1. ",
    "meta": { "name": "Forward run: best practices" },
    "answer": "In 1.3 there will be a [predict](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#predict) function that does all that for you!\r\nYou can already use it by installing 1.3.0rc1. "
  },
  {
    "content": "Hi,\r\nI want to save a copy of the running script of each experiment.\r\nI have tried to apply the following callback:\r\n\r\n```\r\nimport os\r\nfrom pathlib import Path\r\n\r\nclass MyCopyingCallback(pl.Callback):\r\n\r\n    def on_init_end(self, trainer):\r\n        log_dir = trainer.logger.log_dir\r\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\r\n        shutil.copy2(os.path.realpath(__file__), os.path.join(log_dir, os.path.basename(os.path.realpath(__file__))))\r\n``` \r\nThe problem is that I ended up with two copies of my running script under two different folders (I'm using the _ddp_ backend)\r\n\r\nAny ideas on how to solve this issue?Hey!\r\nYou can use the rank id to determine in which process you are, like so:\r\n\r\n```python\r\nif trainer.global_rank == 0:\r\n    # do something only on first process\r\n```\r\n\r\nIn your example, this could make sense:\r\n\r\n```python\r\nimport os\r\nfrom pathlib import Path\r\n\r\nclass MyCopyingCallback(pl.Callback):\r\n\r\n    def on_init_end(self, trainer):\r\n        if trainer.global_rank != 0:\r\n            return \r\n        log_dir = trainer.logger.log_dir\r\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\r\n        shutil.copy2(os.path.realpath(__file__), os.path.join(log_dir, os.path.basename(os.path.realpath(__file__))))\r\n```",
    "meta": {
      "name": "How to save a copy of the running script into the log folder?"
    },
    "answer": "Hey!\r\nYou can use the rank id to determine in which process you are, like so:\r\n\r\n```python\r\nif trainer.global_rank == 0:\r\n    # do something only on first process\r\n```\r\n\r\nIn your example, this could make sense:\r\n\r\n```python\r\nimport os\r\nfrom pathlib import Path\r\n\r\nclass MyCopyingCallback(pl.Callback):\r\n\r\n    def on_init_end(self, trainer):\r\n        if trainer.global_rank != 0:\r\n            return \r\n        log_dir = trainer.logger.log_dir\r\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\r\n        shutil.copy2(os.path.realpath(__file__), os.path.join(log_dir, os.path.basename(os.path.realpath(__file__))))\r\n```"
  },
  {
    "content": "to be continued ...",
    "meta": { "name": "To delete" },
    "answer": "to be continued ..."
  },
  {
    "content": "I see that there is an option to return just an optimizer in the `configure_optimizers` function. What will be the default scheduler in that case?\r\nBTW I tried very hard to find the answer in the docs and could not find it..There's no LR scheduler applied in this case: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\r\n\r\n",
    "meta": { "name": "what is the default scheduler?" },
    "answer": "There's no LR scheduler applied in this case: https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\r\n\r\n"
  },
  {
    "content": "I would like to have a step called before the first training step, and that yet necessitates the dataloader\r\n\r\ne.g. (mock code)\r\n\r\n```python\r\nclass Scaler(nn.Module):\r\n    '''center target data'''\r\n     def __init__(self, dims):\r\n         self.mean = nn.Parameter(torch.tensor(dims))\r\n         self.n = nn.Parameters(torch.zeros(1))\r\n\r\n     def forward(self, batch):\r\n          input, target = batch\r\n          if self.training:\r\n              self.mean += target.mean(0)\r\n              self.n += 1\r\n          else:\r\n              return input, (target - self.mean)/self.n\r\n\r\nclass MySystem(pl.LightningModule):\r\n    def __init__(self, scaler_dims, model_dims):\r\n        self.model = nn.Linear(**model_dims)\r\n        self.scaler = Scaler(self.dims).train()\r\n\r\n    def on_first_epoch(self, dataloader):  # <---- not sure where this should live\r\n         # learn to scale the dataset\r\n         for batch in dataloader:\r\n               self.scaler(batch)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n         self.scaler.eval()\r\n         input, target = self.scaler(batch)\r\n         pred = self.model(input)\r\n         loss = F.l1_loss(pred, target)\r\n         return loss\r\n      \r\n\r\ndm = MyDataModule()\r\nsystem = MySystem()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(system, dm) \r\n```\r\n\r\nI'm not clear on how to do this with PL's API: `nn.LithningModule.setup()` does not have access to the dataloader. \r\n\r\nAny advice?\r\n\r\nThanks!In this instance would it be simpler to iterate through the dataset outside of Lightning, prior to starting training? thanks @ananthsub , I think the point of lightning is to try to keep everything in the same system.\r\n\r\ngoing through the doc, I think the best is either \r\n- move the pl.DataLightningModule to the pl.LightningModule  and setup such scaler with `self._prepare_data` (which is called once in distributed, as opposed to `self.setup`)\r\n- using a callback `on_init_start': https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html",
    "meta": { "name": "where to add preprocessing initialization" },
    "answer": "thanks @ananthsub , I think the point of lightning is to try to keep everything in the same system.\r\n\r\ngoing through the doc, I think the best is either \r\n- move the pl.DataLightningModule to the pl.LightningModule  and setup such scaler with `self._prepare_data` (which is called once in distributed, as opposed to `self.setup`)\r\n- using a callback `on_init_start': https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html"
  },
  {
    "content": "Why\r\n```\r\nnew_model = MyModel.load_from_checkpoint(checkpoint_path=\"example.ckpt\")\r\n```\r\ninstead of\r\n```\r\nMyModel.load_from_checkpoint(checkpoint_path=\"example.ckpt\")\r\n```\r\nThe former one is not friendly to native PyTorch users. And cost me an afternoon to find the bug.`load_from_checkpoint` is a class function which instantiates the object with the `hyper_parameters` in the checkpoint and then loads the state_dict.\r\nIf done like in your proposal `my_model.load_from_checkpoint(checkpoint_path=\"example.ckpt\")` similar to pytorch's `load_state_dict` you'd have to create the `my_model` object first with the `hyper_parameters` parsed from the checkpoint and then load the weights. `load_from_checkpoint` combines both.\r\nMaybe adding a way to load the `state_dict` from a checkpoint into an already instantiated object without using a classmethod would make sense. Something like `load_state_dict_from_checkpoint`. \r\nThe alternative is to load the checkpoint and then call `model.load_state_dict(checkpoint['state_dict'], strict=strict)`.\r\n",
    "meta": { "name": "why load_from_checkpoint is not defaultly inplace?" },
    "answer": "`load_from_checkpoint` is a class function which instantiates the object with the `hyper_parameters` in the checkpoint and then loads the state_dict.\r\nIf done like in your proposal `my_model.load_from_checkpoint(checkpoint_path=\"example.ckpt\")` similar to pytorch's `load_state_dict` you'd have to create the `my_model` object first with the `hyper_parameters` parsed from the checkpoint and then load the weights. `load_from_checkpoint` combines both.\r\nMaybe adding a way to load the `state_dict` from a checkpoint into an already instantiated object without using a classmethod would make sense. Something like `load_state_dict_from_checkpoint`. \r\nThe alternative is to load the checkpoint and then call `model.load_state_dict(checkpoint['state_dict'], strict=strict)`.\r\n"
  },
  {
    "content": "Hey!\r\n\r\nI tried to change to using the new way of logging through `MetricCollection` and `self.log_dict` instead of logging every metric through `self.log` on `training step` and `test_epoch_end`. However, each metric is then logged as `[metric_name]_epoch_[epoch_number]` which creates a new graph for every epoch instead of allowing me to use epoch on the x-axis of my graphs (on Comet, if that is relevant). Is there a way to control this behaviour of` log_dic`t, or do I just have to keep logging \"manually\" to control log name?Hi @FluidSense,\r\n\r\nCould you try just updating the `MetricCollection` during the `_step` method and then log in `_epoch_end` method. Something like:\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    logits = self(x)\r\n    self.train_metrics.update(logits, y)\r\n\r\ndef train_epoch_end(self, outputs):\r\n    self.log_dict(self.train_metrics.compute())\r\n```",
    "meta": { "name": "Control log_dict's on_epoch log name" },
    "answer": "Hi @FluidSense,\r\n\r\nCould you try just updating the `MetricCollection` during the `_step` method and then log in `_epoch_end` method. Something like:\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    logits = self(x)\r\n    self.train_metrics.update(logits, y)\r\n\r\ndef train_epoch_end(self, outputs):\r\n    self.log_dict(self.train_metrics.compute())\r\n```"
  },
  {
    "content": "I have a huge dataset, so I have set val_check_interval parameter to be 0.25. I want the scheduler to be called after each validation. So, during one epoch, there should be four validation checks. I want the scheduler to be updated based on these 4 intermediate values. For example, if the accuracy doesn't increase for two consecutive checks, I want the learning rate to be halved.\r\n\r\nI was hoping that the following code should be enough to accomplish this behavior, but it doesn't seem to be working. Any help is appreciated. Thanks.\r\n\r\n  ```\r\n      scheduler = {\r\n            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optim,\r\n                                                                    mode='max', factor=0.75,\r\n                                                                    patience=2, verbose=True),\r\n            'interval': 'step',\r\n            'frequency': len(self.val_dataloader()) + 1,\r\n            'strict': True,\r\n            'monitor': 'val_acc_epoch',\r\n        }\r\n```To me it looks like you just need to set the `frequency` argument to 1/4 of the size of your training data:\r\n```python\r\n    scheduler = {\r\n          'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optim,\r\n                                                                  mode='max', factor=0.75,\r\n                                                                  patience=2, verbose=True),\r\n          'interval': 'step',\r\n          'frequency': int(len(self.train_dataloader()) * 0.25),\r\n          'strict': True,\r\n          'monitor': 'val_acc_epoch',\r\n      }\r\n```",
    "meta": { "name": "How to use scheduler correctly ?" },
    "answer": "To me it looks like you just need to set the `frequency` argument to 1/4 of the size of your training data:\r\n```python\r\n    scheduler = {\r\n          'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optim,\r\n                                                                  mode='max', factor=0.75,\r\n                                                                  patience=2, verbose=True),\r\n          'interval': 'step',\r\n          'frequency': int(len(self.train_dataloader()) * 0.25),\r\n          'strict': True,\r\n          'monitor': 'val_acc_epoch',\r\n      }\r\n```"
  },
  {
    "content": "It took me quite some time playing with PyTorch Lightning and reading through the docs before I realized I can attach the data-related methods (`[train|val]_dataloader`, `prepare_data`) directly to `LightningModule` and then pass only the module to the trainer, rather than having to create a separate `LightningDataModule` class or passing the data loaders explicitly. When thinking of `LightningModule` as a task, rather than a model, this makes actually perfect sense. After all, things like batch size and learning rate are inherently related. But since I haven't seen it really explicitly documented, I wonder whether the use of data methods directly on the `LightningModule` is considered standard?Hi, could you explain what you mean by \r\n\r\n> I realized I can attach the data-related methods ([train|val]_dataloader, prepare_data) directly to LightningModule\r\n\r\nDo you mean dynamically bind the dataloader method of datamodule to lightning module? \r\nI would not personally recommend that because it can be harder to read and debug, especially if your LightningModule also defines dataloader methods that you may expect to run in a different context.\r\n\r\nI recommend passing the datamodule/dataloader into the fit function like so:\r\n```python\r\ntrainer.fit(model, datamodule=datamodule)\r\n# or\r\ntrainer.fit(model, train_dataloader=...)\r\n```\r\nThen it will be clear to any reader that the datamodule loaders are used for training. ",
    "meta": {
      "name": "Is the use of data methods on LightningModule standard?"
    },
    "answer": "Hi, could you explain what you mean by \r\n\r\n> I realized I can attach the data-related methods ([train|val]_dataloader, prepare_data) directly to LightningModule\r\n\r\nDo you mean dynamically bind the dataloader method of datamodule to lightning module? \r\nI would not personally recommend that because it can be harder to read and debug, especially if your LightningModule also defines dataloader methods that you may expect to run in a different context.\r\n\r\nI recommend passing the datamodule/dataloader into the fit function like so:\r\n```python\r\ntrainer.fit(model, datamodule=datamodule)\r\n# or\r\ntrainer.fit(model, train_dataloader=...)\r\n```\r\nThen it will be clear to any reader that the datamodule loaders are used for training. "
  },
  {
    "content": "If I have 100 training examples and 100 validation examples, and I run on a single gpu with a batch size of 10, the tqdm bar will show 20 epoch iterations. If I run on 2 gpus with ddp and the same batch size, the tqdm bar will still show 20 epoch iterations, but isnt the effective batch size now 20 instead of 10 because theres 2 gpus? Shouldnt the total number of iterations be half?\r\n\r\nThanks for any clarification.Hi @jipson7 ,\r\nFirst of all: You're right, that's how it should be.\r\n\r\nWe tried to reproduce this, but for us this produced the following (correct) output. Do you have a minimal reproduction example?\r\n\r\n```\r\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 17.23it/s, loss=-43.6, v_num=272]\r\nseen train: 5\r\nseen train: 5\r\n```Hi again. I figured it out. I was using a prefetch dataloader adopted from NVIDIA Apex. It wrapped the data loader in a custom generator to pipeline some of the data loading and GPU transfer. It was breaking DDP and causing the above mentioned error.",
    "meta": {
      "name": "Should the total epoch size be less when using multi-gpu DDP?"
    },
    "answer": "Hi @jipson7 ,\r\nFirst of all: You're right, that's how it should be.\r\n\r\nWe tried to reproduce this, but for us this produced the following (correct) output. Do you have a minimal reproduction example?\r\n\r\n```\r\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 17.23it/s, loss=-43.6, v_num=272]\r\nseen train: 5\r\nseen train: 5\r\n```"
  },
  {
    "content": "If I save scalar log like this,\r\n```\r\nself.logger.experiment.add_scalars(\u2018loss/nll\u2019, {\u2018train\u2019: trainloss, \u2018valid\u2019: validloss})\r\n```\r\nHow to monitor valid loss in Modelcheckpoint?\r\n\r\n\r\nyou need to call it with self.log instead (we automate the rest for you), so that we are aware of you logging it :)",
    "meta": {
      "name": "How to monitor tensorboard logged scalar in modelcheckpoint?"
    },
    "answer": "you need to call it with self.log instead (we automate the rest for you), so that we are aware of you logging it :)"
  },
  {
    "content": "My model has the property that I can prepare the test data in multiple different ways, which results in a set of equally plausible predictions for each data point (one prediction for each way of preparing the test data). By combining these predictions, it is possible to slightly boost overall performance on the test set. Right now, I do this in the following (abstract) way:\r\n\r\n------------------------------------------------------------------\r\n\r\n```\r\nfor iPrep in range(nPrep):\r\n   preppedData=prepare_data(testData,iPrep)\r\n   predictions[iPrep]=trainer.test(model,preppedData)\r\n\r\nfinal_predictions=combinePredictions(predictions)\r\n```\r\n\r\n------------------------------------------------------------------\r\n(obviously it is much longer in reality)\r\n\r\nis there a proper, 'lightning' way of hiding this loop inside the model, so I can still use the trainer for this, but only call it once? Maybe the prediction api can help you (currently beta, will be released in version 1.3).\r\n\r\nYou can have multiple predict dataloaders (your different test data). If you do\r\n\r\n```python\r\npredictions = trainer.predict(model, predict_dataloaders=[data1, data2, data3, ...])\r\n```\r\nand it returns the predictions grouped by the dataloader index. Then you can combine them with your own function.\r\n\r\n```python\r\nfinal_predictions=combinePredictions(predictions)\r\n```\r\n\r\nNot sure if this is 100% what you are looking for, but it can at least eliminate that one for loop you have.\r\nOptionally, you can also override `predict_step` in the LightningModule. \r\nIf you install the latest version, you can use this predict feature already. The documentation will be included in the 1.3 release.\r\n",
    "meta": { "name": "How to do this test in a lightning way?" },
    "answer": "Maybe the prediction api can help you (currently beta, will be released in version 1.3).\r\n\r\nYou can have multiple predict dataloaders (your different test data). If you do\r\n\r\n```python\r\npredictions = trainer.predict(model, predict_dataloaders=[data1, data2, data3, ...])\r\n```\r\nand it returns the predictions grouped by the dataloader index. Then you can combine them with your own function.\r\n\r\n```python\r\nfinal_predictions=combinePredictions(predictions)\r\n```\r\n\r\nNot sure if this is 100% what you are looking for, but it can at least eliminate that one for loop you have.\r\nOptionally, you can also override `predict_step` in the LightningModule. \r\nIf you install the latest version, you can use this predict feature already. The documentation will be included in the 1.3 release.\r\n"
  },
  {
    "content": "## \ud83d\udc1b Bug\r\n\r\nHello, I'm trying to use Pytorch Lightning in order to speed up my ESR GAN renders on Windows 10. \r\n\r\nHowever, when I ran the installation code and attempt to run Cupscale (which I use as a GUI for ESR GAN), I get an error saying \"Pytorch compiled without CUDA\".\r\n\r\nIs there a way to choose to install specifically the CUDA version of Pytorch with Lightning install, or are the two incompatible? If the latter is the case, that's not good to hear. If the former, can I have such a code?@TrocelengStudios ho this seems like you do not have CUDA installed, can you run `nvidia-smi`?\r\ncc: @awaelchli ",
    "meta": { "name": "Pytorch Lightning doesn't have CUDA?" },
    "answer": "@TrocelengStudios ho this seems like you do not have CUDA installed, can you run `nvidia-smi`?\r\ncc: @awaelchli "
  },
  {
    "content": "\"monitor (Optional[str]) \u2013 quantity to monitor. By default it is None which saves a checkpoint only for the last epoch.\"\r\nwhen i trainning a model, i set the 'monitor' to None, it should save the last epoch as the doc says. but it still save depend on the val_loss, it always save the model with lowest val_loss.\r\n\r\ni also try another way, set the 'save_last' to True. while this needs to set a monitor. And if i set save_top_k to 0, it will save nothing; if set to 1, it will save 2 models, the best one and the last one. But i just want to save the last one.\r\n\r\nis this a bug or i made sth wrong?  is there a way to save model with epoch asigned myself? such as the last 3 epochs?\r\nHey! Have a look at this example: \r\n\r\n\r\n```python\r\nimport os\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss, logger=False)\r\n        return {\"x\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2, num_workers=0)\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2, num_workers=0)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=5,  # this will save a checkpoint at epoch index 4 (last epoch)\r\n        weights_summary=None,\r\n        logger=False,\r\n        callbacks=[ModelCheckpoint(dirpath=\"./checkpoints\", monitor=None)]\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\nI'm choosing:\r\n`ModelCheckpoint(dirpath=\"./checkpoints\", monitor=None)`\r\n\r\nThat's all, it saves only one checkpoint, named **epoch=4-step=4.ckpt**, it corresponds to the last epoch being run. \r\nNote: for backward compatibility, when monitor=None, we choose \"val_loss\" as the monitor when it is available. You should be able to avoid that by just renaming your validation loss to \"valid_loss\" or something else :) \r\n",
    "meta": { "name": "how to save the last epoch only?" },
    "answer": "Hey! Have a look at this example: \r\n\r\n\r\n```python\r\nimport os\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss, logger=False)\r\n        return {\"x\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2, num_workers=0)\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2, num_workers=0)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=5,  # this will save a checkpoint at epoch index 4 (last epoch)\r\n        weights_summary=None,\r\n        logger=False,\r\n        callbacks=[ModelCheckpoint(dirpath=\"./checkpoints\", monitor=None)]\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\nI'm choosing:\r\n`ModelCheckpoint(dirpath=\"./checkpoints\", monitor=None)`\r\n\r\nThat's all, it saves only one checkpoint, named **epoch=4-step=4.ckpt**, it corresponds to the last epoch being run. \r\nNote: for backward compatibility, when monitor=None, we choose \"val_loss\" as the monitor when it is available. You should be able to avoid that by just renaming your validation loss to \"valid_loss\" or something else :) \r\n"
  },
  {
    "content": "I use one node and 4 gpus for training. And I use dali dataloader, I don't know why my gpu util is low, and training is also slow. About 1:30 per epoch, I train for 200 epoches, which will cost 5 hours. It's slower than the project mmclassification, which only cost 3.5 hours. Compared to mmclassification project which can only support torch.utils.data.dataloader, I think if I use dali_dataloader, it will accelerate my training. But as you can see, it's the opposite. I don't know why. Could anyone give me some advice? I use cifar10 dataset. And I train on slurm.\r\n![image](https://user-images.githubusercontent.com/23625282/114308721-666e7480-9b17-11eb-8842-7e1eb7334ff9.png)\r\nHere is my code.\r\n\r\nmain.py\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom net import ResNet18\r\nif __name__ == '__main__':\r\n    model = ResNet18()\r\n    trainer = pl.Trainer( max_epochs=200,log_every_n_steps=1,\r\n        log_gpu_memory='min_max',gpus=4,num_nodes=1,accelerator='ddp',\r\n        fast_dev_run=False,callbacks=[ModelCheckpoint(monitor='val_accuracy',mode='max')],\r\n        progress_bar_refresh_rate=1,replace_sampler_ddp=False)\r\n    trainer.fit(model)\r\n```\r\nnet.py\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport pytorch_lightning as pl\r\nfrom dataloader import dali_DataLoader,HybridPipe,dali_CIFAR10\r\nclass BasicBlock(nn.Module):\r\n    expansion = 1\r\n\r\n    def __init__(self, in_planes, planes, stride=1):\r\n        super(BasicBlock, self).__init__()\r\n        self.conv1 = nn.Conv2d(\r\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(planes)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\r\n                               stride=1, padding=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(planes)\r\n\r\n        self.shortcut = nn.Sequential()\r\n        if stride != 1 or in_planes != self.expansion*planes:\r\n            self.shortcut = nn.Sequential(\r\n                nn.Conv2d(in_planes, self.expansion*planes,\r\n                          kernel_size=1, stride=stride, bias=False),\r\n                nn.BatchNorm2d(self.expansion*planes)\r\n            )\r\n\r\n    def forward(self, x):\r\n        out = F.relu(self.bn1(self.conv1(x)))\r\n        out = self.bn2(self.conv2(out))\r\n        out += self.shortcut(x)\r\n        out = F.relu(out)\r\n        return out\r\n\r\n\r\nclass Bottleneck(nn.Module):\r\n    expansion = 4\r\n\r\n    def __init__(self, in_planes, planes, stride=1):\r\n        super(Bottleneck, self).__init__()\r\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(planes)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\r\n                               stride=stride, padding=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(planes)\r\n        self.conv3 = nn.Conv2d(planes, self.expansion *\r\n                               planes, kernel_size=1, bias=False)\r\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\r\n\r\n        self.shortcut = nn.Sequential()\r\n        if stride != 1 or in_planes != self.expansion*planes:\r\n            self.shortcut = nn.Sequential(\r\n                nn.Conv2d(in_planes, self.expansion*planes,\r\n                          kernel_size=1, stride=stride, bias=False),\r\n                nn.BatchNorm2d(self.expansion*planes)\r\n            )\r\n\r\n    def forward(self, x):\r\n        out = F.relu(self.bn1(self.conv1(x)))\r\n        out = F.relu(self.bn2(self.conv2(out)))\r\n        out = self.bn3(self.conv3(out))\r\n        out += self.shortcut(x)\r\n        out = F.relu(out)\r\n        return out\r\n\r\n\r\nclass ResNet(pl.LightningModule):\r\n    def __init__(self, block, num_blocks, num_classes=10):\r\n        super(ResNet, self).__init__()\r\n        self.in_planes = 64\r\n\r\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\r\n                               stride=1, padding=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(64)\r\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\r\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\r\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\r\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\r\n        self.linear = nn.Linear(512*block.expansion, num_classes)\r\n        self.correct = 0\r\n        self.total_size = 0\r\n    def _make_layer(self, block, planes, num_blocks, stride):\r\n        strides = [stride] + [1]*(num_blocks-1)\r\n        layers = []\r\n        for stride in strides:\r\n            layers.append(block(self.in_planes, planes, stride))\r\n            self.in_planes = planes * block.expansion\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        out = F.relu(self.bn1(self.conv1(x)))\r\n        out = self.layer1(out)\r\n        out = self.layer2(out)\r\n        out = self.layer3(out)\r\n        out = self.layer4(out)\r\n        out = F.avg_pool2d(out, 4)\r\n        out = out.view(out.size(0), -1)\r\n        out = self.linear(out)\r\n        return out\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        x = self(x)\r\n        loss_fn = nn.CrossEntropyLoss()\r\n        loss = loss_fn(x,y)\r\n        predicted = torch.argmax(x, dim=1, keepdim=False)\r\n        self.correct += (predicted == y).sum().item()\r\n        self.total_size += y.size(0)\r\n        self.log('train_loss', loss,prog_bar=True, logger=True)\r\n        self.log('train_accuracy', self.correct/self.total_size,prog_bar=True, logger=True)\r\n        return loss\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        x = self(x)\r\n        loss_fn = nn.CrossEntropyLoss()\r\n        loss = loss_fn(x,y)\r\n        predicted = torch.argmax(x, dim=1, keepdim=False)\r\n        self.correct += (predicted == y).sum().item()\r\n        self.total_size += y.size(0)\r\n        self.log('val_loss', loss,on_step=False, on_epoch=True,prog_bar=True, logger=True)\r\n        self.log('val_accuracy', self.correct/self.total_size,prog_bar=True, logger=True)\r\n        return loss\r\n    def validation_epoch_end(self,out):\r\n        self.log('val_accuracy', self.correct/self.total_size,prog_bar=True, logger=True)\r\n        self.correct=0\r\n        self.total_size=0\r\n    def train_epoch_end(self,out):\r\n        self.log('train_accuracy', self.correct/self.total_size,prog_bar=True, logger=True)\r\n        self.correct=0\r\n        self.total_size=0\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.1)\r\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [100,150], gamma=0.1, last_epoch=-1, verbose=False)\r\n        return [optimizer],[scheduler]\r\n    def train_dataloader(self):\r\n        loader = dali_DataLoader(pipelines=HybridPipe(dali_CIFAR10(root='./data'), batch_size=32, pad_ratio=1.25,num_threads=4,\r\n             is_distribute=True, crop_size=32,ramdom_flip=True,\r\n             normalize=dict(mean=[125.307, 122.961, 113.8575],std=[51.5865, 50.847, 51.255])))\r\n        return loader\r\n\r\n    def val_dataloader(self):\r\n        loader = dali_DataLoader(pipelines=HybridPipe(dali_CIFAR10(root='./data',test_mode=True), batch_size=100,\r\n             normalize=dict(mean=[125.307, 122.961, 113.8575],std=[51.5865, 50.847, 51.255])))\r\n        return loader\r\ndef ResNet18():\r\n    return ResNet(BasicBlock, [2, 2, 2, 2])\r\n```\r\ndataloader.py\r\n```python\r\nimport os,sys,math,random,pickle\r\nimport torch\r\nimport numpy as np\r\nimport torch.distributed as dist\r\ntry:\r\n    from nvidia import dali\r\n    from nvidia.dali.pipeline import Pipeline\r\n    import nvidia.dali.types as types\r\n    import nvidia.dali.fn as fn\r\n    import nvidia.dali.ops as ops\r\n    from nvidia.dali.plugin.pytorch import DALIClassificationIterator\r\nexcept:\r\n    print('Could not import DALI')\r\nclass dali_DataLoader():\r\n    def __init__(self, pipelines, **kwargs):\r\n        pipelines.build()\r\n        try:\r\n            self._dali_iterator = DALIClassificationIterator(pipelines=pipelines, size=len(pipelines.iterator.indices))\r\n            self.sampler = pipelines.iterator\r\n        except:\r\n            self._dali_iterator = DALIClassificationIterator(pipelines=pipelines, reader_name='Reader')\r\n            self.sampler = self\r\n    def set_epoch(self,epoch):\r\n        pass\r\n    def __iter__(self):\r\n        return self\r\n\r\n    def __len__(self):\r\n        return int(math.ceil(self._dali_iterator._size / self._dali_iterator.batch_size))\r\n    def __next__(self):\r\n        \r\n        try:\r\n            data = next(self._dali_iterator)\r\n        except StopIteration:\r\n            self._dali_iterator.reset()\r\n            raise StopIteration\r\n        # Decode the data output\r\n        input = data[0]['data']\r\n        target = data[0]['label'].squeeze().long()\r\n\r\n        return input,target\r\nclass identity():\r\n    def __call__(self,x,*tmp,**kargs):\r\n        return x\r\nclass HybridPipe(Pipeline):\r\n    def __init__(self,dataset, batch_size, file_root=None,filelist_path=None,num_threads=1, pad_ratio=1,is_distribute=True, resize=None,crop_size=[0,0],ramdom_flip=False,normalize=None,random_rotate_degree=None):\r\n        device_id = torch.cuda.current_device()\r\n        print(\"device_id\",device_id)\r\n        super(HybridPipe, self).__init__(batch_size, num_threads, device_id, seed=12 + device_id)\r\n        \r\n        if is_distribute:\r\n            if filelist_path is not None:\r\n                if file_root is None:\r\n                    raise Exception(\"if provide filelist_path, then must provide file_root\")\r\n                else:\r\n                    self.input = ops.readers.File(file_root=file_root,file_list=filelist_path,num_shards=dist.get_world_size(),prefetch_queue_depth=num_threads,read_ahead=True,shard_id=dist.get_rank())\r\n                    self.decode = ops.decoders.Image(device=\"mixed\", output_type=types.RGB)\r\n                    self.use_file=True\r\n            else:\r\n                self.iterator = iter(Distribute_Input_Iter(dataset, batch_size))\r\n                #self.input = ops.ExternalSource(source=self.iterator, num_outputs=2)\r\n                self.input = ops.ExternalSource()\r\n                self.input_label = ops.ExternalSource()\r\n                self.use_file=False\r\n        else:\r\n            if filelist_path is not None:\r\n                if file_root is None:\r\n                    raise Exception(\"if provide filelist_path, then must provide file_root\")\r\n                else:\r\n                    self.input = ops.readers.File(file_root=file_root,file_list=filelist_path,num_shards=dist.get_world_size(),prefetch_queue_depth=num_threads,read_ahead=True,shard_id=dist.get_rank())\r\n                    self.decode = ops.decoders.Image(device=\"mixed\", output_type=types.RGB)\r\n                    self.use_file=True\r\n            else:\r\n                self.iterator = iter(Normal_Input_Iter(dataset, batch_size))\r\n                self.input = ops.ExternalSource()\r\n                self.input_label = ops.ExternalSource()\r\n                self.use_file=False\r\n        dali_device = \"gpu\"\r\n        \r\n        if isinstance(resize,(tuple,list)) and len(resize)==2:\r\n            self.resize = ops.Resize(size=tuple(resize))\r\n        elif isinstance(resize,(int, float)):\r\n            self.resize = ops.Resize(size=tuple(resize,resize))\r\n        else:\r\n            self.resize = identity()\r\n        if normalize is not None and isinstance(normalize,dict):\r\n            self.mean = normalize.get('mean',0)\r\n            self.std = normalize.get('std',1)\r\n        else:\r\n            self.mean = 0\r\n            self.std = 1\r\n        if isinstance(crop_size, (int, float)):\r\n            crop_size = [crop_size,crop_size]\r\n        if (len(crop_size)==2 and (crop_size[0]==0 or crop_size[1]==0)):\r\n            self.crop = identity()\r\n        else:\r\n            self.crop = ops.Crop(device=dali_device, crop_h=crop_size[0], crop_w=crop_size[1])\r\n        if pad_ratio>1:\r\n            self.pad = ops.Paste(device=dali_device, ratio=pad_ratio, fill_value=0)\r\n        else:\r\n            self.pad = identity()\r\n        self.cmnp = ops.CropMirrorNormalize(device=\"gpu\",\r\n                                            dtype=types.FLOAT,\r\n                                            output_layout=types.NCHW,\r\n                                            mean=self.mean,\r\n                                            std=self.std\r\n                                            )\r\n        if ramdom_flip:\r\n            self.coin = ops.random.CoinFlip(probability=0.5)\r\n        else:\r\n            self.coin = lambda :0\r\n        if random_rotate_degree is not None:\r\n            try:\r\n                tmp = math.abs(int(random_rotate_degree))\r\n                self.degree = ops.random.Uniform(range=(-tmp, tmp))\r\n                self.rotate = ops.Rotate()\r\n            except:\r\n                self.degree = lambda :0\r\n                self.rotate = identity()\r\n        else:\r\n            self.degree = lambda :0\r\n            self.rotate = identity()\r\n        \r\n    def iter_setup(self):\r\n        if not self.use_file:\r\n            (images, labels) = self.iterator.__next__()\r\n            self.feed_input(self.jpegs, images, layout=\"HWC\")\r\n            self.feed_input(self.labels, labels)\r\n\r\n    def define_graph(self):\r\n        rng = self.coin()\r\n        print()\r\n        if self.use_file:\r\n            self.jpegs,self.labels = self.input(name=\"Reader\")\r\n            self.jpegs = self.decode(self.jpegs)\r\n        else:\r\n            self.jpegs= self.input()\r\n            self.labels = self.input_label()\r\n        output = self.jpegs\r\n        output = self.resize(output)\r\n        output = self.rotate(output, angle=self.degree())\r\n        output = self.pad(output.gpu())\r\n        output = self.crop(output)\r\n        output = self.cmnp(output, mirror=rng)\r\n        return [output, self.labels]\r\nclass Distribute_Input_Iter():\r\n    def __init__(self,dataset, batch_size, num_replicas=None,rank=None,shuffle=True,seed=0,drop_last=False):\r\n        if num_replicas is None:\r\n            if not dist.is_available():\r\n                raise RuntimeError(\"Requires distributed package to be available\")\r\n            num_replicas = dist.get_world_size()\r\n            #num_replicas = 1\r\n        if rank is None:\r\n            if not dist.is_available():\r\n                raise RuntimeError(\"Requires distributed package to be available\")\r\n            rank = dist.get_rank()\r\n            #rank = 0\r\n        if rank >= num_replicas or rank < 0:\r\n            raise ValueError(\r\n                \"Invalid rank {}, rank should be in the interval\"\r\n                \" [0, {}]\".format(rank, num_replicas - 1))\r\n        self.dataset = dataset\r\n        self.batch_size = batch_size\r\n        self.num_replicas = num_replicas\r\n        self.rank = rank\r\n        self.epoch = 0\r\n        self.drop_last = drop_last\r\n        \r\n        # If the dataset length is evenly divisible by # of replicas, then there\r\n        # is no need to drop any data, since the dataset will be split equally.\r\n        if self.drop_last and len(self.dataset) % self.num_replicas != 0:  # type: ignore\r\n            # Split to nearest available length that is evenly divisible.\r\n            # This is to ensure each rank receives the same amount of data when\r\n            # using this Sampler.\r\n            self.num_samples = math.ceil(\r\n                # `type:ignore` is required because Dataset cannot provide a default __len__\r\n                # see NOTE in pytorch/torch/utils/data/sampler.py\r\n                (len(self.dataset) - self.num_replicas) / self.num_replicas  # type: ignore\r\n            )\r\n        else:\r\n            self.num_samples = math.ceil(len(self.dataset) / self.num_replicas)  # type: ignore\r\n        self.total_size = self.num_samples * self.num_replicas\r\n        self.shuffle = shuffle\r\n        self.seed = seed\r\n        self.epoch=0\r\n        indices = list(range(len(self.dataset)))  # type: ignore\r\n\r\n        if not self.drop_last:\r\n            # add extra samples to make it evenly divisible\r\n            padding_size = self.total_size - len(indices)\r\n            if padding_size <= len(indices):\r\n                indices += indices[:padding_size]\r\n            else:\r\n                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]\r\n        else:\r\n            # remove tail of data to make it evenly divisible.\r\n            indices = indices[:self.total_size]\r\n        assert len(indices) == self.total_size,'len(indices) != self.total_size'\r\n\r\n        # subsample\r\n        indices = indices[self.rank:self.total_size:self.num_replicas]\r\n        assert len(indices) == self.num_samples,'len(indices) != self.num_samples'\r\n        self.indices = indices\r\n    def set_epoch(self,epoch):\r\n        self.epoch = epoch\r\n    def __iter__(self):\r\n        self.i = 0\r\n        self.n = len(self.indices)\r\n        return self\r\n    def __next__(self):\r\n        batch = []\r\n        labels = []\r\n        should_shuffle = False\r\n        \r\n        for _ in range(self.batch_size):\r\n            if self.i % self.n == self.n-1:\r\n                should_shuffle = True\r\n            img, label = self.dataset.__getitem__(self.indices[self.i])\r\n            batch.append(img)\r\n            labels.append(label)\r\n            self.i = (self.i + 1) % self.n\r\n        if should_shuffle:\r\n            random.shuffle(self.indices)\r\n        return (batch, labels)\r\nclass Normal_Input_Iter():\r\n    def __init__(self,dataset, batch_size):\r\n        self.dataset = dataset\r\n        self.batch_size = batch_size\r\n        self.indices = list(range(len(self.dataset)))\r\n    def __iter__(self):\r\n        self.i = 0\r\n        self.n = len(self.dataset)\r\n        return self\r\n    def __next__(self):\r\n        batch = []\r\n        labels = []\r\n        should_shuffle = False\r\n        \r\n        for _ in range(self.batch_size):\r\n            if self.i % self.n == self.n-1:\r\n                should_shuffle = True\r\n            img, label = self.dataset.__getitem__(self.indices[self.i])\r\n            batch.append(img)\r\n            labels.append(label)\r\n            self.i = (self.i + 1) % self.n\r\n        if should_shuffle:\r\n            random.shuffle(self.indices)\r\n        return (batch, labels)\r\n```When you compare the two implementations, make sure to leave out as many changing variables as possible. For example, since you train with DDP, run it only on 2 GPUs so that you can be sure it's not bottlenecked by CPU. I don't know the Dali data loader very well, but I doubt that they can guarantee a throughput increase for all use cases. ",
    "meta": { "name": "Why is my gpu-util low?" },
    "answer": "When you compare the two implementations, make sure to leave out as many changing variables as possible. For example, since you train with DDP, run it only on 2 GPUs so that you can be sure it's not bottlenecked by CPU. I don't know the Dali data loader very well, but I doubt that they can guarantee a throughput increase for all use cases. "
  },
  {
    "content": "Hello, I am trying to get pruning to work within my lightning model. I have tried multiple methods, but have not been able to get the ModelPruning module to work. \r\n\r\nWhen I try this:\r\n\r\n````\r\ndef compute_amount(epoch):\r\n    # the sum of all returned values need to be smaller than 1\r\n    if epoch == 10:\r\n        return 0.5\r\n\r\n    elif epoch == 50:\r\n        return 0.25\r\n\r\n    elif 75 < epoch < 99 :\r\n        return 0.01\r\n    else:\r\n      return 0\r\n\r\nprune = ModelPruning(\r\n            pruning_fn='l1_unstructured',\r\n            parameters_to_prune=[(model.model.conv1, 'weight'),\r\n                            (model.model.conv2, 'weight'),\r\n                            (model.model.lin1, 'weight'),\r\n                            (model.model.lin2, 'weight'),\r\n                            (model.model.upconv1, 'weight'),\r\n                            (model.model.upconv2, 'weight')],\r\n            amount=compute_amount,\r\n            use_global_unstructured=True,\r\n        )\r\n\r\ntrainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\r\ntrainer.fit(model, dm)\r\n````\r\n\r\nI get this output:\r\n\r\n> MisconfigurationException                 Traceback (most recent call last)\r\n> <ipython-input-27-327a57ef597c> in <module>()\r\n>      11 \r\n>      12 trainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\r\n> ---> 13 trainer.fit(model, dm)\r\n> \r\n> 4 frames\r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n>     457         # ----------------------------\r\n>     458         self.call_setup_hook(model)\r\n> --> 459         self.call_hook(\"on_before_accelerator_backend_setup\", model)\r\n>     460         self.accelerator.setup(self, model)  # note: this sets up self.lightning_module\r\n>     461 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in call_hook(self, hook_name, *args, **kwargs)\r\n>    1089             if hasattr(self, hook_name):\r\n>    1090                 trainer_hook = getattr(self, hook_name)\r\n> -> 1091                 trainer_hook(*args, **kwargs)\r\n>    1092 \r\n>    1093             # next call hook in lightningModule\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_before_accelerator_backend_setup(self, model)\r\n>      33         \"\"\"Called in the beginning of fit and test\"\"\"\r\n>      34         for callback in self.callbacks:\r\n> ---> 35             callback.on_before_accelerator_backend_setup(self, model)\r\n>      36 \r\n>      37     def setup(self, model, stage: str):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in on_before_accelerator_backend_setup(self, trainer, pl_module)\r\n>     360     def on_before_accelerator_backend_setup(self, trainer, pl_module: LightningModule):\r\n>     361         parameters_to_prune = self.sanitize_parameters_to_prune(\r\n> --> 362             pl_module, self._parameters_to_prune, parameter_names=self._parameter_names\r\n>     363         )\r\n>     364 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in sanitize_parameters_to_prune(pl_module, parameters_to_prune, parameter_names)\r\n>     441             if missing_modules or missing_parameters:\r\n>     442                 raise MisconfigurationException(\r\n> --> 443                     \"Some provided `parameters_to_tune` don't exist in the model.\"\r\n>     444                     f\" Found missing modules: {missing_modules} and missing parameters: {missing_parameters}\"\r\n>     445                 )\r\n> \r\n> MisconfigurationException: Some provided `parameters_to_tune` don't exist in the model. Found missing modules: [Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), Linear(in_features=784, out_features=32, bias=True), Linear(in_features=32, out_features=784, bias=True), ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)), ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))] and missing parameters: []\r\n\r\nWhich makes it seem like ModelPruning is not finding my model's weights, though I do not understand why. If I put into ModelPruning weights that do not exist in the model. It catches that problem during ModelPruning's init. Which is not happening here. So that does not seem to be the problem.  \r\n\r\nI have also tried bypassing the parameters_to_prune parameter altogether. \r\n\r\n````\r\ndef compute_amount(epoch):\r\n    # the sum of all returned values need to be smaller than 1\r\n    if epoch == 10:\r\n        return 0.5\r\n\r\n    elif epoch == 50:\r\n        return 0.25\r\n\r\n    elif 75 < epoch < 99 :\r\n        return 0.01\r\n    else:\r\n      return 0\r\n\r\nprune = ModelPruning(\r\n            pruning_fn='l1_unstructured',\r\n            parameter_names=['weight', 'bias'],\r\n            amount=compute_amount,\r\n            use_global_unstructured=True,\r\n        )\r\n\r\ntrainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\r\ntrainer.fit(model, dm)\r\n````\r\n\r\n\r\nThis works right until it does not. One epoch runs and then it snags on a callback within the validation step. \r\n\r\n\r\n> ---------------------------------------------------------------------------\r\n> TypeError                                 Traceback (most recent call last)\r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n>     636                     # run train epoch\r\n> --> 637                     self.train_loop.run_training_epoch()\r\n>     638 \r\n> \r\n> 56 frames\r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n>     576         if should_check_val:\r\n> --> 577             self.trainer.run_evaluation(on_epoch=True)\r\n>     578 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_evaluation(self, max_batches, on_epoch)\r\n>     750         # hook\r\n> --> 751         self.evaluation_loop.on_evaluation_end()\r\n>     752 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in on_evaluation_end(self, *args, **kwargs)\r\n>      99         else:\r\n> --> 100             self.trainer.call_hook('on_validation_end', *args, **kwargs)\r\n>     101 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in call_hook(self, hook_name, *args, **kwargs)\r\n>    1090                 trainer_hook = getattr(self, hook_name)\r\n> -> 1091                 trainer_hook(*args, **kwargs)\r\n>    1092 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_validation_end(self)\r\n>     184         for callback in self.callbacks:\r\n> --> 185             callback.on_validation_end(self, self.lightning_module)\r\n>     186 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in on_validation_end(self, trainer, pl_module)\r\n>     211         \"\"\"\r\n> --> 212         self.save_checkpoint(trainer, pl_module)\r\n>     213 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in save_checkpoint(self, trainer, pl_module)\r\n>     261         # Mode 2: save the last checkpoint\r\n> --> 262         self._save_last_checkpoint(trainer, pl_module, monitor_candidates)\r\n>     263 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_last_checkpoint(self, trainer, pl_module, ckpt_name_metrics)\r\n>     545         else:\r\n> --> 546             self._save_model(last_filepath, trainer, pl_module)\r\n>     547         if (\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_model(self, filepath, trainer, pl_module)\r\n>     334         if self.save_function is not None:\r\n> --> 335             self.save_function(filepath, self.save_weights_only)\r\n>     336         else:\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/properties.py in save_checkpoint(self, filepath, weights_only)\r\n>     326     def save_checkpoint(self, filepath, weights_only: bool = False) -> None:\r\n> --> 327         self.checkpoint_connector.save_checkpoint(filepath, weights_only)\r\n>     328 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in save_checkpoint(self, filepath, weights_only)\r\n>     396         # dump states as a checkpoint dictionary object\r\n> --> 397         checkpoint = self.dump_checkpoint(weights_only)\r\n>     398         if self.trainer.is_global_zero:\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in dump_checkpoint(self, weights_only)\r\n>     283             # dump callbacks\r\n> --> 284             checkpoint['callbacks'] = self.trainer.on_save_checkpoint(checkpoint)\r\n>     285 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_save_checkpoint(self, checkpoint)\r\n>     221             else:\r\n> --> 222                 state = callback.on_save_checkpoint(self, self.lightning_module, checkpoint)\r\n>     223             if state:\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in on_save_checkpoint(self, trainer, pl_module, checkpoint)\r\n>     399             # prune a copy so training can continue with the same buffers\r\n> --> 400             copy = deepcopy(pl_module.to(\"cpu\"))\r\n>     401             self.make_pruning_permanent(copy)\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     168                     if reductor:\r\n> --> 169                         rv = reductor(4)\r\n>     170                     else:\r\n> \r\n> TypeError: cannot serialize '_io.TextIOWrapper' object\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-29-327a57ef597c> in <module>()\r\n>      11 \r\n>      12 trainer = pl.Trainer(gpus=-1, max_epochs = 15, logger=wandb_logger, profiler=SimpleProfiler(\"/content/bla\"), progress_bar_refresh_rate=20, callbacks=[prune])\r\n> ---> 13 trainer.fit(model, dm)\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n>     497 \r\n>     498         # dispath `start_training` or `start_testing` or `start_predicting`\r\n> --> 499         self.dispatch()\r\n>     500 \r\n>     501         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n>     544 \r\n>     545         else:\r\n> --> 546             self.accelerator.start_training(self)\r\n>     547 \r\n>     548     def train_or_test_or_predict(self):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n>      71 \r\n>      72     def start_training(self, trainer):\r\n> ---> 73         self.training_type_plugin.start_training(trainer)\r\n>      74 \r\n>      75     def start_testing(self, trainer):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\r\n>     112     def start_training(self, trainer: 'Trainer') -> None:\r\n>     113         # double dispatch to initiate the training loop\r\n> --> 114         self._results = trainer.run_train()\r\n>     115 \r\n>     116     def start_testing(self, trainer: 'Trainer') -> None:\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n>     667         finally:\r\n>     668             # hook\r\n> --> 669             self.train_loop.on_train_end()\r\n>     670 \r\n>     671     def run_evaluation(self, max_batches=None, on_epoch=False):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in on_train_end(self)\r\n>     132         # when a checkpoint was saved at the last step\r\n>     133         self.trainer.global_step -= 1\r\n> --> 134         self.check_checkpoint_callback(should_update=True, is_last=True)\r\n>     135         self.trainer.global_step += 1\r\n>     136 \r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in check_checkpoint_callback(self, should_update, is_last)\r\n>     162 \r\n>     163             for cb in callbacks:\r\n> --> 164                 cb.on_validation_end(self.trainer, model)\r\n>     165 \r\n>     166     def check_early_stopping_callback(self, should_update):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in on_validation_end(self, trainer, pl_module)\r\n>     210         checkpoints can be saved at the end of the val loop\r\n>     211         \"\"\"\r\n> --> 212         self.save_checkpoint(trainer, pl_module)\r\n>     213 \r\n>     214     def on_save_checkpoint(self, trainer, pl_module, checkpoint: Dict[str, Any]) -> Dict[str, Any]:\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in save_checkpoint(self, trainer, pl_module)\r\n>     260 \r\n>     261         # Mode 2: save the last checkpoint\r\n> --> 262         self._save_last_checkpoint(trainer, pl_module, monitor_candidates)\r\n>     263 \r\n>     264     def __validate_init_configuration(self):\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_last_checkpoint(self, trainer, pl_module, ckpt_name_metrics)\r\n>     544             trainer.training_type_plugin.rpc_save_model(self._save_model, last_filepath, trainer, pl_module)\r\n>     545         else:\r\n> --> 546             self._save_model(last_filepath, trainer, pl_module)\r\n>     547         if (\r\n>     548             self.last_model_path and self.last_model_path != last_filepath\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py in _save_model(self, filepath, trainer, pl_module)\r\n>     333         # delegate the saving to the trainer\r\n>     334         if self.save_function is not None:\r\n> --> 335             self.save_function(filepath, self.save_weights_only)\r\n>     336         else:\r\n>     337             raise ValueError(\".save_function() not set\")\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/properties.py in save_checkpoint(self, filepath, weights_only)\r\n>     325 \r\n>     326     def save_checkpoint(self, filepath, weights_only: bool = False) -> None:\r\n> --> 327         self.checkpoint_connector.save_checkpoint(filepath, weights_only)\r\n>     328 \r\n>     329     @property\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in save_checkpoint(self, filepath, weights_only)\r\n>     395         \"\"\"\r\n>     396         # dump states as a checkpoint dictionary object\r\n> --> 397         checkpoint = self.dump_checkpoint(weights_only)\r\n>     398         if self.trainer.is_global_zero:\r\n>     399             # write the checkpoint dictionary on the file\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py in dump_checkpoint(self, weights_only)\r\n>     282         if not weights_only:\r\n>     283             # dump callbacks\r\n> --> 284             checkpoint['callbacks'] = self.trainer.on_save_checkpoint(checkpoint)\r\n>     285 \r\n>     286             optimizer_states = []\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py in on_save_checkpoint(self, checkpoint)\r\n>     220                 state = callback.on_save_checkpoint(self, self.lightning_module)  # noqa: parameter-unfilled\r\n>     221             else:\r\n> --> 222                 state = callback.on_save_checkpoint(self, self.lightning_module, checkpoint)\r\n>     223             if state:\r\n>     224                 callback_states[type(callback)] = state\r\n> \r\n> /usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/pruning.py in on_save_checkpoint(self, trainer, pl_module, checkpoint)\r\n>     398             prev_device = pl_module.device\r\n>     399             # prune a copy so training can continue with the same buffers\r\n> --> 400             copy = deepcopy(pl_module.to(\"cpu\"))\r\n>     401             self.make_pruning_permanent(copy)\r\n>     402             checkpoint[\"state_dict\"] = copy.state_dict()\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     178                     y = x\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n>     182     # If is its own copy, don't memoize.\r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     279     if state is not None:\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n>     283             y.__setstate__(state)\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     148     copier = _deepcopy_dispatch.get(cls)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n>     152         try:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     239     memo[id(x)] = y\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n>     243 d[dict] = _deepcopy_dict\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     178                     y = x\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n>     182     # If is its own copy, don't memoize.\r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     279     if state is not None:\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n>     283             y.__setstate__(state)\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     148     copier = _deepcopy_dispatch.get(cls)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n>     152         try:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     239     memo[id(x)] = y\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n>     243 d[dict] = _deepcopy_dict\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     178                     y = x\r\n>     179                 else:\r\n> --> 180                     y = _reconstruct(x, memo, *rv)\r\n>     181 \r\n>     182     # If is its own copy, don't memoize.\r\n> \r\n> /usr/lib/python3.7/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n>     279     if state is not None:\r\n>     280         if deep:\r\n> --> 281             state = deepcopy(state, memo)\r\n>     282         if hasattr(y, '__setstate__'):\r\n>     283             y.__setstate__(state)\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     148     copier = _deepcopy_dispatch.get(cls)\r\n>     149     if copier:\r\n> --> 150         y = copier(x, memo)\r\n>     151     else:\r\n>     152         try:\r\n> \r\n> /usr/lib/python3.7/copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n>     239     memo[id(x)] = y\r\n>     240     for key, value in x.items():\r\n> --> 241         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n>     242     return y\r\n>     243 d[dict] = _deepcopy_dict\r\n> \r\n> /usr/lib/python3.7/copy.py in deepcopy(x, memo, _nil)\r\n>     167                     reductor = getattr(x, \"__reduce_ex__\", None)\r\n>     168                     if reductor:\r\n> --> 169                         rv = reductor(4)\r\n>     170                     else:\r\n>     171                         reductor = getattr(x, \"__reduce__\", None)\r\n> \r\n> TypeError: cannot serialize '_io.TextIOWrapper' object\r\n\r\n\r\nI am a bit overwhelmed by that error. Does anyone know what I am doing wrong? Or have I found a bug?For the first error, what does `pl_module.modules()` return?\r\n\r\nYou get the error because the following code can't find the parameters you passed in your model:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/4c07ab5e99dd20c1f309d9e73cdaacc1ebad9499/pytorch_lightning/callbacks/pruning.py#L422-L444\r\n\r\nThe second error is definitely a bug. It happens because you are also using a profiler. Should be fixed in master, the fix will be released in 1.3. Feel free to try it on master in the meantime.",
    "meta": { "name": "Problems in Pruning" },
    "answer": "For the first error, what does `pl_module.modules()` return?\r\n\r\nYou get the error because the following code can't find the parameters you passed in your model:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/4c07ab5e99dd20c1f309d9e73cdaacc1ebad9499/pytorch_lightning/callbacks/pruning.py#L422-L444\r\n\r\nThe second error is definitely a bug. It happens because you are also using a profiler. Should be fixed in master, the fix will be released in 1.3. Feel free to try it on master in the meantime."
  },
  {
    "content": "Hi,\r\n\r\nI'm using ModelCheckpoint Callback to save my model checkpoint. pytorch lightning automatically attaches `-v0`, `-v1` to the filename I specified if it finds checkpoint models exist in `dirpath`.  Instead of saving all the models from different runs, is there a way to make the ModelCheckpoint Callback only save one model in the checkpoint folder and just override the model from previous runs?\r\n\r\nFor example, my `ModelCheckpoint` is as follow: \r\n```\r\nModelCheckpoint(monitor='valid_score',\r\n                              dirpath=\"./checkpoint/\",\r\n                              filename=\"model\",\r\n                              mode='max', save_top_k=1))\r\n```\r\nIf I run the code for 3 three times, my `checkpoints` folder will have the following: \r\n```\r\n- checkpoint:\r\n    - model.ckpt\r\n    - model-v0.ckpt\r\n    - model-v1.ckpt\r\n```\r\nWould it be possible to just have `model.ckpt` in my `checkpoint` folder no matter how many times I run the code? We don't support this, but you could always remove the file manually between runs with `os.remove()`\r\n",
    "meta": { "name": "Override .ckpt in ModelCheckpoint Callback" },
    "answer": "We don't support this, but you could always remove the file manually between runs with `os.remove()`\r\n"
  },
  {
    "content": "Hi!\r\nI currently use AWS SageMaker to train my PL models. I recently found out this link : \r\nhttps://aws.amazon.com/fr/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/\r\nSageMaker provides its own implementation of DDP and I think that would be nice to be able to use it with PL :)\r\nI looked into PL code and I think I could add  this feature by extending pytorch_lightning.accelerators.accelerator.Accelerator. Does it seems like a good way to implement it? Are there some general advices/guidance you could give me about this?\r\nIf the you are interested in this feature, I can make a PR once I'm done with it.\r\nThank you!\r\n\r\nR\u00e9miIn progress in https://github.com/PyTorchLightning/pytorch-lightning/pull/6271",
    "meta": { "name": "Add AWS DataParellism" },
    "answer": "In progress in https://github.com/PyTorchLightning/pytorch-lightning/pull/6271"
  },
  {
    "content": "I am using Latest PT-Lightning and your upgrade to [ModelCheckpoint](https://github.com/PyTorchLightning/pytorch-lightning/blob/f0c5479de99e9a1524c95a9dffe2fa0cc76ab1ea/pytorch_lightning/callbacks/model_checkpoint.py) broke my functionality to my codebase. \r\n\r\nV1.2.6 specifically coz I started facing these issues yesterday.  Downgraded to V1.2.5. Please check once. The dict based metrics returned from the `train_step` or `validation_step` are not captured in the ModelCheckpointCallback \r\n\r\n```python \r\n monitor_candidates = self._monitor_candidates(trainer)\r\n```\r\n`monitor_candidates` doesn't have any values returned from the trainer. And I am not using the `Result` class. I am returning a pure dictionary. \r\n\r\nDidn't have time to create replicate the bug exactly in a notebook because I am on a tight deadline. But please do check. \r\n\r\nAs I have more time, I will update more on the issue. \r\nYou need to use `self.log(\"your_metric\", metric)`\r\nAnd then set the monitor argument in `ModelCheckpoint(monitor=\"your_metric\")`\r\n\r\nLet me know if that helps.",
    "meta": {
      "name": "New ModelCheckpoint Code Update Breaking Model Saving Functionality : v1.2.5->v.1.2.6"
    },
    "answer": "You need to use `self.log(\"your_metric\", metric)`\r\nAnd then set the monitor argument in `ModelCheckpoint(monitor=\"your_metric\")`\r\n\r\nLet me know if that helps."
  },
  {
    "content": "Even with `checkpoint_callback=False`, Trainer appears to be using CheckpointConnector for some reason. Since very occasionally (once every ~30 runs) the checkpoint is either deleted too early or is never created in the first place (no idea which one), the whole experiment fails, as shown in the log below. Since CheckpointConnector does not appear to be doing anything important when running locally, is it possible to eliminate it without breaking the training process?\r\n```\r\nTraceback (most recent call last):\r\n  File \".\\my_code\\run_automated.py\", line 95, in <module>\r\n    main(experiment, config, dataset)\r\n  File \"D:\\GIT\\my_code\\processing.py\", line 117, in main\r\n    trainer.fit(model, dataloader_train, dataloader_val)\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 513, in fit\r\n    self.dispatch()\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 553, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\", line 74, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\", line 111, in start_training\r\n    self._results = trainer.run_train()\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 609, in run_train\r\n    self._pre_training_routine()\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 600, in _pre_training_routine\r\n    self.checkpoint_connector.restore_weights()\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py\", line 65, in restore_weights\r\n    max_suffix = self.max_ckpt_in_folder(dir_path_hpc, \"hpc_ckpt_\")\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py\", line 372, in max_ckpt_in_folder\r\n    files = [os.path.basename(f[\"name\"]) for f in fs.listdir(dir_path)]\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\fsspec\\spec.py\", line 1122, in listdir\r\n    return self.ls(path, detail=detail, **kwargs)\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 51, in ls\r\n    return [self.info(f) for f in paths]\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 51, in <listcomp>\r\n    return [self.info(f) for f in paths]\r\n  File \"c:\\users\\pluczak\\.conda\\envs\\pytorch\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 61, in info\r\n    out = os.stat(path, follow_symlinks=False)\r\nFileNotFoundError: [WinError 2] Nie mo\u017cna odnale\u017a\u0107 okre\u015blonego pliku: 'D:/GIT/my_code/04d2a63662b34828946b5545646c063f.pt'\r\n```So are you saying the file returned by `fs.listdir(dir_path)` gets removed?\r\nIf you can reproduce this, please open an Issue about it. Seems like a bug.\r\n\r\nIn the meantime, you can patch the problematic function like this:\r\n\r\n```python\r\ntrainer.checkpoint_connector.max_ckpt_in_folder = lambda *args, **kwargs: None\r\n```",
    "meta": { "name": "Is it possible to disable CheckpointConnector?" },
    "answer": "So are you saying the file returned by `fs.listdir(dir_path)` gets removed?\r\nIf you can reproduce this, please open an Issue about it. Seems like a bug.\r\n\r\nIn the meantime, you can patch the problematic function like this:\r\n\r\n```python\r\ntrainer.checkpoint_connector.max_ckpt_in_folder = lambda *args, **kwargs: None\r\n```"
  },
  {
    "content": "I would like to create an embedding that does not fit in the GPU memory\r\nbut can fit in the CPU memory. \r\n\r\nSelect the subset for a batch, send it to the GPU at the start of mini-batch.\r\n\r\nGPU_tensor = embedding(idx)\r\n\r\nThen at the end of training update the CPU embedding from the GPU embedding.\r\n\r\nI am using \r\npl.Trainer( gpus=[0,1], distributed_backend='ddp')\r\n\r\nand probably will need accumulate_grad_batches\r\n\r\nAny idea for how to do this ?\r\n\r\n\r\nDuplicate of https://github.com/PyTorchLightning/pytorch-lightning/discussions/6725",
    "meta": { "name": "embedding manual control location CPU vs GPU" },
    "answer": "Duplicate of https://github.com/PyTorchLightning/pytorch-lightning/discussions/6725"
  },
  {
    "content": "In my usage, LightningDatamodule is currently encapsulating batch collation, moving to device, and batch transformations (via `on_after_batch_transfer`). \r\n\r\nHowever, when I want to do inference on a bunch of inputs, I want the same steps to happen. What is the recommended way to achieve this? The problem is that Trainer drives the device transfers and hooks around it, and I don't have a Trainer during inference.Why would you not want to use the Trainer?\r\n\r\nYou can now use `trainer.predict` for inference (will be in beta after the 1.3 release)",
    "meta": { "name": "Datamodule without Trainer (for inference)" },
    "answer": "Why would you not want to use the Trainer?\r\n\r\nYou can now use `trainer.predict` for inference (will be in beta after the 1.3 release)"
  },
  {
    "content": "Hi, \r\n\r\nI was wondering if PL can use mkldnn/dnnl/onednn ? I may be mistaken, but it seems pytorch does not default to it and there is no explicit mention of it inside PL's code. If it's not, is there a way to enable it ?\r\n\r\nAlexandre No, unfortunately, MKL-DNN is currently not supported\r\n\r\nDuplicate of https://github.com/PyTorchLightning/pytorch-lightning/issues/6020",
    "meta": {
      "name": "Is pytorch lightning using MKLDNN when no GPU is available ?"
    },
    "answer": "No, unfortunately, MKL-DNN is currently not supported\r\n\r\nDuplicate of https://github.com/PyTorchLightning/pytorch-lightning/issues/6020"
  },
  {
    "content": "## Problem\r\nHi, Everyone. I have some question about ddp. Because I want to write `predict result` to file.\r\nAnd I use `trainer.test(model=model, test_dataloaders=test_dataloader)` to process it. But I just get `1/ngpus` dataset.\r\nFor example: \r\n```\r\nGPUS: 2\r\ntotal data: 10000\r\npredict data: 5000(total data / GPUS)\r\n```\r\nHope someone can help me to solve it. Because I have to write predict result to file.\r\n\r\n## Environment\r\n```\r\npytorch: 1.4.0\r\npytorch-lightning: 1.1.1\r\nGPUS: Tesla P100-PCIE-16GB * 2\r\n```\r\n## Sample Code\r\n```python\r\nclass ExampleModel(pl.LightningModule):\r\n   def __init__(self):\r\n       self.original_file = open('/path/to/file', 'a')\r\n   def train_step(...):\r\n       .....\r\n   def test_step(self, batch, batch_idx):\r\n        init_ids = batch['init_ids']\r\n        attention_mask = batch['attention_mask']\r\n        token_type_ids = batch['token_type_ids']\r\n        predictions = self.model(init_ids, attention_mask, token_type_ids)\r\n        ..........\r\n        self.original_file.write(convert_ids_to_str(init_ids.cpu().numpy()) + '\\t' + str(seg.cpu().numpy()[0]) + '\\n')\r\n\r\n\r\n```\r\n* trainer\r\n```python\r\ntrainer = pl.Trainer(max_epochs=EPOCH,\r\n                    gpus=[0,1], \r\n                    num_nodes=1,\r\n                    auto_select_gpus=True,\r\n                    num_sanity_val_steps=0,\r\n                    accelerator='ddp',\r\n                    callbacks=[modelcheckpoint_callback, earlystopping_callback])\r\n```\r\n```python\r\ntrainer.test(model=model, test_dataloaders=test_dataloader)\r\n```\r\n\r\nHope someone can help or answer how to do it.Please, check out `trainer.predict()` after 1.3 is released!",
    "meta": { "name": "Trainer.test() on ddp can not get entire dataset." },
    "answer": "Please, check out `trainer.predict()` after 1.3 is released!"
  },
  {
    "content": "I'm trying to use [Layer-wise relevance propagation](https://github.com/fhvilshoj/TorchLRP) as part of the training and validation loop of training my model, but it requires gradients to be present in order to calculate the relevance. I'm trying to figure out the best way of approaching this in a Lightning-friendly way. I can already do it for the training loop, but the validation loop is what's giving me problems. The only way I can think to do it at the moment is once the validation loop finishes, re-run the validation dataset with gradients enabled without calling the optimiser's `.step()` function. I'd really appreciate some guidance here before I dive in. Thanks! You can call `torch.set_grad_enabled(True)` in your validation loop or in any hook you want. Doesn't that work?\r\nAnd perhaps `model.train()` if you have normalization layers dropout or that kind of layers.",
    "meta": { "name": "Re-enabling gradients in Validation loop?" },
    "answer": "You can call `torch.set_grad_enabled(True)` in your validation loop or in any hook you want. Doesn't that work?\r\nAnd perhaps `model.train()` if you have normalization layers dropout or that kind of layers."
  },
  {
    "content": "I found that in the code below sigmoid activation applied before binary_cross_entropy_with_logits loss:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/71b4611c64059d7589e4d80115209fd2c89e8bdb/pl_examples/domain_templates/computer_vision_fine_tuning.py#L196-L231\r\nFrom the documentation https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss:\r\n```This loss combines a Sigmoid layer and the BCELoss in one single class. ```\r\nIs that performed intentionally? Or it's just a bug?Yes, the sigmoid should be removed from the forward, because BCEWithLogits already contains the sigmoid. \r\nDo you want to send a PR to update the example?",
    "meta": {
      "name": "Example in domain_templates: computer_vision_fine_tuning"
    },
    "answer": "Yes, the sigmoid should be removed from the forward, because BCEWithLogits already contains the sigmoid. \r\nDo you want to send a PR to update the example?"
  },
  {
    "content": "Hello, \r\nI'm trying to train my model with multi-nodes (2 nodes, 8 gpus per each, using ddp accelator & trying without using slurm)\r\nBut I got problem with GLOBAL_RANK\r\n\r\nin node 1, \r\n```\r\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/16\r\n...\r\ninitializing ddp: GLOBAL_RANK: 7, MEMBER: 8/16\r\n```\r\nsame as in node 2,\r\n```\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/16\r\n...\r\ninitializing ddp: GLOBAL_RANK: 7, MEMBER: 8/16\r\n```\r\n\r\nAnd got stuck with repeated message like below\r\n```\r\nWaiting in store based barrier to initialize process group for rank: 3, key: store_based_barrier_key:1 (world_size=16, worker_count=13, timeout=0:30:00)\r\nWaiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=16, worker_count=13, timeout=0:30:00)\r\n```\r\n\r\nI'm trying to setup like [this document](https://pytorch-lightning.readthedocs.io/en/latest/advanced/cluster.html#cluster-setup) but also got problem, like below\r\n```python\r\n os.environ[\"MASTER_ADDR\"] = master_addr\r\n os.environ[\"MASTER_PORT\"] = master_port\r\n os.environ[\"WORLD_SIZE\"] = \"16\"\r\n os.environ[\"NODE_RANK\"] = rank\r\n```\r\n```\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 474, in fit\r\n    self.accelerator.setup(self, model)\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu.py\", line 19, in setup\r\n    return super().setup(trainer, model)\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 69, in setup\r\n    self.connect_training_type_plugin(self.training_type_plugin, model)\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 328, in connect_training_type_plugin\r\n    plugin.connect(model)\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/parallel.py\", line 68, in connect\r\n    self.setup(model)\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 95, in setup\r\n    self.task_idx = self.cluster_environment.local_rank()\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pytorch_lightning/plugins/environments/torchelastic_environment.py\", line 48, in local_rank\r\n    return int(os.environ['LOCAL_RANK'])\r\n  File \"/home/user/.pyenv/versions/3.7.9/lib/python3.7/os.py\", line 681, in __getitem__\r\n    raise KeyError(key) from None\r\n```\r\n\r\nI'd appreciate any help. thanks in advanceI believe the docs might be outdated, have you tried changing `NODE_RANK` to `LOCAL_RANK`?\r\n\r\ncc: @awaelchli. We should provide better error messages for thisHi, @carmocca \r\nThanks for answering.\r\n\r\nI changed and try it but do not work.\r\nIs there any document or code with lightning multi-nodes without using slurm or shell script?\r\nor is there proper way to setting manually like using pytorch distributed (`dist.init_process_group` )?What have you set for `MASTER_ADDR` and `MASTER_PORT`? These have to reference to one of the two machines you are using. For example if I have two nodes like this:\r\n\r\nIP: `512.124.134.4`\r\nIP: `512.124.136.8`\r\n\r\nAnd I want `512.124.134.4` to be my master node.\r\n\r\nFor both my machines I'd need to run something like `MASTER_ADDR=512.124.134.4 MASTER_PORT=4500 python train.py`.\r\n\r\nLet me know if this helps! On top of this, we should update the doc if this does work :)@SeanNaren @awaelchli \r\nHi, I'm trying to use custom `ClusterEnvironment` setup with ddp accelerators, \r\nbut I think [docs1](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#plugins) and [docs2](https://pytorch-lightning.readthedocs.io/en/1.1.1/accelerators.html#implement-a-custom-accelerator) is outdated.\r\n\r\n```python\r\nfrom pytorch_lightning.plugins.environments.cluster_environment import ClusterEnvironment\r\nclass MyCluster(ClusterEnvironment):\r\n    def master_address(self):\r\n        return MY_ADDR\r\n    def master_port(self):\r\n        return MY_PORT\r\n    def world_size(self):\r\n        return MY_WORLD_SIZE\r\n```\r\n\r\nin main,\r\n```python\r\n...\r\n# case1\r\ncluster = [MyCluster()]\r\ntrainer = Trainer(args,  cluster_environment ='ddp')\r\n\r\n# case2\r\ncluster = [MyCluster()]\r\ntrainer = Trainer(args,  accelators='ddp', plugins=[cluster])\r\n```\r\nbut got error like this\r\n```\r\n# case1\r\nNo cluster_environment in trainer init\r\n\r\n# case2\r\nFound invalid type for plugin [<ppl.nsml_cluster.MyCluster object at 0x14e0fd8ed650>]. Expected a precision or training type plugin.\r\n```\r\n\r\nHow can I setup my own cluster and using with ddp? Thanks in advance",
    "meta": { "name": "wrong global rank when trying multi-nodes" },
    "answer": "What have you set for `MASTER_ADDR` and `MASTER_PORT`? These have to reference to one of the two machines you are using. For example if I have two nodes like this:\r\n\r\nIP: `512.124.134.4`\r\nIP: `512.124.136.8`\r\n\r\nAnd I want `512.124.134.4` to be my master node.\r\n\r\nFor both my machines I'd need to run something like `MASTER_ADDR=512.124.134.4 MASTER_PORT=4500 python train.py`.\r\n\r\nLet me know if this helps! On top of this, we should update the doc if this does work :)"
  },
  {
    "content": "Hi @awaelchli and thanks for your time, as you asked in pull requests, i am pinging you here\r\n\r\nFor other who see this, it's a discussion about Trainer.predict method where it is running BatchNorm Layers, code is below:\r\n\r\n\r\nhttps://colab.research.google.com/drive/1jujP4F_prSmbRz-F_wGfWPTKGOmY5DPE?usp=sharing\r\n\r\nWhat is the problem with my approach?Predict takes a dataloader, not a tensor. It still \"works\" because the trainer just iterates through the batch dimension, but then you get an error later because the input lost the batch dimension, and batch norm doesn't work with batch size 1. ",
    "meta": { "name": "Error with predict()" },
    "answer": "Predict takes a dataloader, not a tensor. It still \"works\" because the trainer just iterates through the batch dimension, but then you get an error later because the input lost the batch dimension, and batch norm doesn't work with batch size 1. "
  },
  {
    "content": "Hello everyone,\r\n\r\nI have upgraded pytorch-lightning to 1.2.6 recently, the behavior of dp seems different from 1.2.0. To be specific, the returned values of validation_step() are automatically reduced before sent to validation_epoch_end(). However, the metrics I use need the original predictions of each sample instead of the reduced values. Is there a way to disable the automatic reduce and pass the whole predictions to validation_epoch_end()? Note that the validation_step_end() is not implemented in my model.\r\n\r\ncc: @tchaton I notice that the validation_step_end() and test_step_end() functions in dp.py script are:\r\n\r\n```\r\ndef validation_step_end(output):\r\n       return self.reduce(output)\r\n\r\ndef test_step_end(output):\r\n       return self.reduce(output)\r\n```\r\n\r\nThus, overwrite these two methods as follows will disable the automatic reduce in evaluation and test:\r\n\r\n```\r\ndef validation_step_end(output):\r\n       return output\r\n\r\ndef test_step_end(output):\r\n       return output\r\n```",
    "meta": {
      "name": "How to disable the automatic reduce/mean while using dp?"
    },
    "answer": "I notice that the validation_step_end() and test_step_end() functions in dp.py script are:\r\n\r\n```\r\ndef validation_step_end(output):\r\n       return self.reduce(output)\r\n\r\ndef test_step_end(output):\r\n       return self.reduce(output)\r\n```\r\n\r\nThus, overwrite these two methods as follows will disable the automatic reduce in evaluation and test:\r\n\r\n```\r\ndef validation_step_end(output):\r\n       return output\r\n\r\ndef test_step_end(output):\r\n       return output\r\n```"
  },
  {
    "content": "Can a pl.LightningModule be used from native pytorch?\r\n\r\nWe are writing a library, and the use of pl.LightningModule for our modules is convenient, particularly because each module knows its device.\r\n\r\nHowever, our clients might be using native pytorch, and want to include our LightningModule as an nn.Module in their code.\r\n\r\nFWIW, our LightningModule currently is used *purely* for forward inference and currently passes no gradients, nor is it trainable.\r\n\r\n1) Are there any interoperability pitfalls in having a LightningModule be an nn.Module in a pure pytorch codebase?\r\n2) Are the benefits gained by using pl.LightningModules in our codebase no longer relevant when called from a pure pytorch codebase, particularly given that we pass back no gradients?1.  It should work as a native `nn.Module`, it actually subclasses it. If you find any, you can assume it's a bug so feel free to open issues about any pitfalls found.\r\n2. Depends on which features of the `LightningModule` you value the most. The benefits left would be mainly organization and such.",
    "meta": { "name": "Can a pl.LightningModule be used from native pytorch?" },
    "answer": "1.  It should work as a native `nn.Module`, it actually subclasses it. If you find any, you can assume it's a bug so feel free to open issues about any pitfalls found.\r\n2. Depends on which features of the `LightningModule` you value the most. The benefits left would be mainly organization and such."
  },
  {
    "content": "I want to train my discriminator ones per 10 iterations but couldn't figure out how to implement it with lightning. Do you have any advice on this?here is an example:\r\ndef optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_idx, closure,\r\n\u02d3\u2192on_tpu=False, using_native_amp=False, using_lbfgs=False):\r\n# update generator opt every 2 steps\r\nif optimizer_i == 0:\r\noptimizer.step(closure=closure, make_optimizer_step=(batch_nb % 2) == 0)\r\n# update discriminator opt every 4 steps\r\nif optimizer_i == 1:\r\noptimizer.step(closure=closure, make_optimizer_step=(batch_nb % 4) == 0)Check out the optimization docs. There are a few examples that may help you. https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#step-optimizers-at-arbitrary-intervals",
    "meta": { "name": "Train Discriminator less than Generator" },
    "answer": "Check out the optimization docs. There are a few examples that may help you. https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#step-optimizers-at-arbitrary-intervals"
  },
  {
    "content": "Hi all, \r\n\r\nI am trying to train my LightningModule but I seem to keep getting the error `TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'` on the line `closure_loss = closure_loss / self.accumulate_grad_batches`, in the function `training_step()` in the file training_loop.py. \r\n\r\nI think it might be something to do with how I format my LightningModule, so here is what my LightningModule looks like\r\n\r\n```python\r\nclass HPAModelV1(pl.LightningModule):\r\n  def __init__(self):\r\n    super().__init__()\r\n\r\n    #self.lossfunc = F.cross_entropy\r\n    self.lossfunc = F.nll_loss\r\n\r\n    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=3, padding=7)\r\n    self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\r\n    self.conv3 = nn.Conv2d(16, 16, kernel_size=5, stride=1, padding=1)\r\n    self.dense = nn.Linear(16, 19)\r\n\r\n  def forward(self, x): #input size is (256, 3, 256, 256)\r\n\r\n    x = x.float()\r\n    \r\n    out = self.conv1(x)\r\n    out = F.relu(out)\r\n    out = F.max_pool2d(out, 3) # output is (bs, 16, 30, 30)\r\n    \r\n    out = self.conv2(out)\r\n    out = F.relu(out)\r\n    out = F.max_pool2d(out, 3) # output is (bs, 16, 10, 10)\r\n\r\n    out = self.conv3(out)\r\n    out = F.relu(out)\r\n    out = F.max_pool2d(out, 8) # output is (bs, 16, 1, 1)\r\n\r\n    # dense layer\r\n    out = out.reshape(out.size()[0], 16)\r\n    out = self.dense(out)\r\n\r\n    return out\r\n\r\n  def configure_optimizers(self):\r\n    optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\r\n    return optimizer \r\n\r\n\r\n  def training_step(self, batch, batchidx):\r\n    # set labels and data\r\n    x = batch[0]\r\n    y = batch[1]\r\n    \r\n\r\n    # compute loop\r\n    preds = self(x)\r\n  \r\n  \r\n    probs = F.softmax(preds, dim=1)\r\n \r\n\r\n    # compute the loss function\r\n    J = self.lossfunc(probs, y)\r\n \r\n   \r\n\r\n    # compute accuracy \r\n    acc = accuracy(probs, y)\r\n\r\n    \r\n    #log for weights and biases\r\n    self.log('training loss (step)', J)\r\n    self.log('training accuracy (step)', acc)\r\n    self.log('mean training loss (epoch)', J, on_step=False, on_epoch=True)\r\n    self.log('mean training accuracy (epoch)', acc, on_step=False, on_epoch=True)\r\n\r\n\r\n\r\n    # add information to the progress bar\r\n    pbar =  {'train_acc': acc, 'train_loss' : J}\r\n\r\n    return J, acc\r\n\r\n  def validation_step(self, valbatch, valbatchidx):\r\n    # use the same training step on the val set\r\n\r\n    valJ, valAcc = self.training_step(valbatch, valbatchidx)\r\n\r\n    # log for wb\r\n    self.log('validation loss (step)', valJ)\r\n    self.log('validation accuracy (step)', valAcc)\r\n    self.log('mean validation loss (epoch)', valJ, on_step=False, on_epoch=True)\r\n    self.log('mean validation accuracy (epoch)', valAcc, on_step=False, on_epoch=True)\r\n\r\n    return valJ, valAcc\r\n\r\n  def validation_epoch_end(self, valStepOutputs):\r\n    pass\r\n   ```\r\n\r\nAnd if it may help in diagnosing the cause of the issue, here is the stack trace and output of of the Trainer:\r\n\r\n```\r\nGPU available: False, used: False\r\nTPU available: True, using: 1 TPU cores\r\nGlobal seed set to 0\r\n\r\n  | Name  | Type   | Params\r\n---------------------------------\r\n0 | conv1 | Conv2d | 448   \r\n1 | conv2 | Conv2d | 2.3 K \r\n2 | conv3 | Conv2d | 6.4 K \r\n3 | dense | Linear | 323   \r\n---------------------------------\r\n9.5 K     Trainable params\r\n0         Non-trainable params\r\n9.5 K     Total params\r\n0.038     Total estimated model params size (MB)\r\nEpoch 0: 0%\r\n0/7759 [00:02<?, ?it/s]\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-29-caf15077ca9b> in <module>()\r\n      2 os.environ['WANDB_CONSOLE'] = 'on'\r\n      3 trainer = Trainer(logger=wbLogger, tpu_cores=1, deterministic=True, max_epochs=epochNum, replace_sampler_ddp=False, num_sanity_val_steps=0)\r\n----> 4 trainer.fit(HPAModelV1(), trainDL, valDL)\r\n      5 \r\n      6 print(time.time() - t0)\r\n\r\n23 frames\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    497 \r\n    498         # dispath `start_training` or `start_testing` or `start_predicting`\r\n--> 499         self.dispatch()\r\n    500 \r\n    501         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n    544 \r\n    545         else:\r\n--> 546             self.accelerator.start_training(self)\r\n    547 \r\n    548     def train_or_test_or_predict(self):\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n     71 \r\n     72     def start_training(self, trainer):\r\n---> 73         self.training_type_plugin.start_training(trainer)\r\n     74 \r\n     75     def start_testing(self, trainer):\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/tpu_spawn.py in start_training(self, trainer)\r\n    264             del os.environ[\"XLA_USE_BF16\"]\r\n    265         self._close_logger(trainer)\r\n--> 266         xmp.spawn(self.new_process, **self.xmp_spawn_kwargs)\r\n    267 \r\n    268     def start_testing(self, trainer) -> None:\r\n\r\n/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py in spawn(fn, args, nprocs, join, daemon, start_method)\r\n    384   pf_cfg = _pre_fork_setup(nprocs)\r\n    385   if pf_cfg.num_devices == 1:\r\n--> 386     _start_fn(0, pf_cfg, fn, args)\r\n    387   else:\r\n    388     return torch.multiprocessing.start_processes(\r\n\r\n/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py in _start_fn(index, pf_cfg, fn, args)\r\n    321   # environment must be fully setup before doing so.\r\n    322   _setup_replication()\r\n--> 323   fn(gindex, *args)\r\n    324 \r\n    325 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/tpu_spawn.py in new_process(self, process_idx, trainer, mp_queue)\r\n     98         self.barrier(\"pre-run-stage\")\r\n     99 \r\n--> 100         results = trainer.train_or_test_or_predict()\r\n    101 \r\n    102         self.__save_end_of_training_weights(self.lightning_module)\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in train_or_test_or_predict(self)\r\n    554 \r\n    555         else:\r\n--> 556             results = self.run_train()\r\n    557 \r\n    558         return results\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n    635                 with self.profiler.profile(\"run_training_epoch\"):\r\n    636                     # run train epoch\r\n--> 637                     self.train_loop.run_training_epoch()\r\n    638 \r\n    639                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n    495             # ------------------------------------\r\n    496             with self.trainer.profiler.profile(\"run_training_batch\"):\r\n--> 497                 batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    498 \r\n    499             # when returning -1 from train_step, we end epoch early\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    657 \r\n    658                         # optimizer step\r\n--> 659                         self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n    660 \r\n    661                     else:\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in optimizer_step(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n    436             on_tpu=self.trainer._device_type == DeviceType.TPU and _TPU_AVAILABLE,\r\n    437             using_native_amp=using_native_amp,\r\n--> 438             using_lbfgs=is_lbfgs,\r\n    439         )\r\n    440 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py in optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\r\n   1388             # wraps into LightingOptimizer only for running step\r\n   1389             optimizer = LightningOptimizer._to_lightning_optimizer(optimizer, self.trainer, optimizer_idx)\r\n-> 1390         optimizer.step(closure=optimizer_closure)\r\n   1391 \r\n   1392     def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer, optimizer_idx: int):\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py in step(self, closure, *args, **kwargs)\r\n    212             profiler_name = f\"optimizer_step_and_closure_{self._optimizer_idx}\"\r\n    213 \r\n--> 214         self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\r\n    215         self._total_optimizer_step_calls += 1\r\n    216 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py in __optimizer_step(self, closure, profiler_name, **kwargs)\r\n    132 \r\n    133         with trainer.profiler.profile(profiler_name):\r\n--> 134             trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\r\n    135 \r\n    136     def step(self, *args, closure: Optional[Callable] = None, **kwargs):\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py in optimizer_step(self, optimizer, opt_idx, lambda_closure, **kwargs)\r\n    275         )\r\n    276         if make_optimizer_step:\r\n--> 277             self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\r\n    278         self.precision_plugin.post_optimizer_step(optimizer, opt_idx)\r\n    279         self.training_type_plugin.post_optimizer_step(optimizer, opt_idx, **kwargs)\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/tpu.py in run_optimizer_step(self, optimizer, optimizer_idx, lambda_closure, **kwargs)\r\n     32 \r\n     33     def run_optimizer_step(self, optimizer: Optimizer, optimizer_idx: int, lambda_closure: Callable, **kwargs):\r\n---> 34         xm.optimizer_step(optimizer, barrier=False, optimizer_args={'closure': lambda_closure, **kwargs})\r\n     35 \r\n     36     def all_gather(self, tensor: Union[torch.Tensor], group: Optional[Any] = None, sync_grads: bool = False):\r\n\r\n/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py in optimizer_step(optimizer, barrier, optimizer_args, groups)\r\n    779   \"\"\"\r\n    780   reduce_gradients(optimizer, groups=groups)\r\n--> 781   loss = optimizer.step(**optimizer_args)\r\n    782   if barrier:\r\n    783     mark_step()\r\n\r\n/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py in wrapper(*args, **kwargs)\r\n     86                 profile_name = \"Optimizer.step#{}.step\".format(obj.__class__.__name__)\r\n     87                 with torch.autograd.profiler.record_function(profile_name):\r\n---> 88                     return func(*args, **kwargs)\r\n     89             return wrapper\r\n     90 \r\n\r\n/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)\r\n     25         def decorate_context(*args, **kwargs):\r\n     26             with self.__class__():\r\n---> 27                 return func(*args, **kwargs)\r\n     28         return cast(F, decorate_context)\r\n     29 \r\n\r\n/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py in step(self, closure)\r\n     64         if closure is not None:\r\n     65             with torch.enable_grad():\r\n---> 66                 loss = closure()\r\n     67 \r\n     68         for group in self.param_groups:\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in train_step_and_backward_closure()\r\n    652                         def train_step_and_backward_closure():\r\n    653                             result = self.training_step_and_backward(\r\n--> 654                                 split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\r\n    655                             )\r\n    656                             return None if result is None else result.loss\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    745         with self.trainer.profiler.profile(\"training_step_and_backward\"):\r\n    746             # lightning module hook\r\n--> 747             result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    748             self._curr_step_result = result\r\n    749 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    325 \r\n    326 \r\n--> 327             closure_loss = closure_loss / self.trainer.accumulate_grad_batches\r\n    328 \r\n    329             # the loss will get scaled for amp. avoid any modifications to it\r\n\r\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\r\n```\r\n\r\nThank you, and sorry for all the text\r\nAHi @adamDhalla, `training_step` needs to return one of:\r\n- `Tensor` - The loss tensor\r\n- `dict` - A dictionary. Can include any keys, but must include the key `'loss'`\r\n- `None` - Training will skip to the next batch\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step",
    "meta": { "name": "Error when training: \"closure_loss\" is NoneType" },
    "answer": "Hi @adamDhalla, `training_step` needs to return one of:\r\n- `Tensor` - The loss tensor\r\n- `dict` - A dictionary. Can include any keys, but must include the key `'loss'`\r\n- `None` - Training will skip to the next batch\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step"
  },
  {
    "content": "I'm responsible for implementing `pytorch_lightning.loggers.neptune.NeptuneLogger`.\r\n\r\nRight now we're going through massive api update for neptune client, which requires updating `NeptuneLogger` in your repository (`PyTorchLightning/pytorch-lightning`).\r\n\r\nTheoretically we could implement our `class NeptuneLogger(LightningLoggerBase)` in our repo, and import this class as module in `PyTorchLightning/pytorch-lightning`.\r\nIn such configuration we could easily update logger in our repo without having to create PR in `PyTorchLightning/pytorch-lightning`, when updating our api again in the future.\r\n\r\nAre there any reasons against such solution?Hi \r\n\r\nThanks for your interest and notification about the upcoming changes.\r\nIt would be fine with us if the NeptuneLogger moves to your repo and we would certainly appreciate if the Neptune developers can keep it updated. I think it's a great idea, also because if the user updates Neptune they get the new Logger updates automatically and don't have to wait for a Lightning update :)\r\n\r\nGo for it! This is the best way to make sure users get the best experience. ",
    "meta": {
      "name": "Can custom logger extending `LightningLoggerBase` be imported from separate library?"
    },
    "answer": "Go for it! This is the best way to make sure users get the best experience. "
  },
  {
    "content": "Please see same question on[ Stack Overflow](https://stackoverflow.com/questions/67002099/how-to-use-pytorch-lightning-accuracy-with-ignore-class).\r\n\r\nWhen using `Accuracy` with a class that should be ignored, meaning it has labels but can never be predicted, the scoring is wrong, because it is calculated with the never predicted labels that should be ignored.\r\n\r\nHow to use Accuracy while ignoring some class?\r\n\r\n\r\nThanks :)It is currently not supported in the accuracy metric, but we have an open PR for implementing that exact feature https://github.com/PyTorchLightning/metrics/pull/155\r\n\r\nCurrently what you can is instead calculate the confusion matrix and then ignore some classes based on that (remember that the true positive/correctly classified are found on the diagonal of the confusion matrix):\r\n```python\r\nignore_index = 3\r\nmetric = ConfusionMatrix(num_classes=3)\r\nconfmat = metric(preds, target)\r\nconfmat = confmat[:2,:2] # remove last column and row corresponding to class 3\r\nacc = confmat.trace() / confmat.sum()\r\n```\r\n\r\n    \r\n\r\n\r\n",
    "meta": { "name": "How to use Accuracy with ignore class?" },
    "answer": "It is currently not supported in the accuracy metric, but we have an open PR for implementing that exact feature https://github.com/PyTorchLightning/metrics/pull/155\r\n\r\nCurrently what you can is instead calculate the confusion matrix and then ignore some classes based on that (remember that the true positive/correctly classified are found on the diagonal of the confusion matrix):\r\n```python\r\nignore_index = 3\r\nmetric = ConfusionMatrix(num_classes=3)\r\nconfmat = metric(preds, target)\r\nconfmat = confmat[:2,:2] # remove last column and row corresponding to class 3\r\nacc = confmat.trace() / confmat.sum()\r\n```\r\n\r\n    \r\n\r\n\r\n"
  },
  {
    "content": "I am using Pytorch Lightning 1.2.6 to train my models using DDP and TensorBoard is the default logger used by Lightning.\r\n\r\nMy code is setup to log the training and validation loss on each training and validation step respectively.\r\n\r\n```py\r\nclass MyLightningModel(pl.LightningModule):\r\n\r\n    def training_step(self, batch):\r\n        x, labels = batch\r\n        out = self(x)\r\n        loss = F.mse_loss(out, labels)\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def validation_step(self, batch):\r\n        x, labels = batch\r\n        out = self(x)\r\n        loss = F.mse_loss(out, labels)\r\n        self.log(\"val_loss\", loss)\r\n        return loss\r\n```\r\n\r\nTensorBoard correctly plots both the `train_loss` and `val_loss` charts in the `SCALERS` tab. However, in the `HPARAMS` tab, on the left side bar, only `hp_metric` is visible under `Metrics`. \r\n\r\n[![enter image description here][1]][1]\r\n\r\nHowever, in the `HPARAMS` tab, on the left side bar, only `hp_metric` is visible under `Metrics`. \r\n\r\n[![enter image description here][2]][2]\r\n\r\nHow can we add `train_loss` and `val_loss` to the `Metrics` section? This way, we will be able to use `val_loss` in the `PARALLEL COORDINATES VIEW` instead of `hp_metric`.\r\n\r\n**Image showing `hp_metric` and no `val_loss`:**\r\n[![enter image description here][3]][3]\r\n\r\n*Using Pytorch 1.8.1, Pytorch Lightning 1.2.6, TensorBoard 2.4.1*\r\n\r\n  [1]: https://i.stack.imgur.com/x9L17.png\r\n  [2]: https://i.stack.imgur.com/4lECK.png\r\n  [3]: https://i.stack.imgur.com/05U6a.pngI think it is explained very well in this section of the documentation:\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#logging-hyperparameters\r\nBasically, you just need to overwrite the `hp_metric` tag with whatever value you want to show up in the `HPARAMS` tab in tensorboard.\r\n\r\n",
    "meta": {
      "name": "How to Log Metrics (eg. Validation Loss, Accuracy) To TensorBoard Hparams?"
    },
    "answer": "I think it is explained very well in this section of the documentation:\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#logging-hyperparameters\r\nBasically, you just need to overwrite the `hp_metric` tag with whatever value you want to show up in the `HPARAMS` tab in tensorboard.\r\n\r\n"
  },
  {
    "content": "I have read the GAN [Demo](https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=ArrPXFM371jR), it is for two losses.\r\n\r\nSuppose I have two modules A and B. The training_step is:\r\n```\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        x, y = batch\r\n        a1, a2 = self.A(x[0])\r\n        preds = self.B(x[1], a1, a2)\r\n        loss = loss_fn(preds, y)\r\n        return loss\r\n```\r\nHow to train A and B with different optimizer or learning rate?The best is to use manual optimization: \r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization\r\n\r\nThen you have control over each optimizer step. \r\n\r\nYou can get the optimzers with self.optimizers()\r\nand for backward you simply have to replace loss.backward() with self.manual_backward(loss)\r\nMake sure you use the latest version of Lightning for that for optimal support.\r\n\r\nHope this helps",
    "meta": { "name": "How to train two optimizer with one loss?" },
    "answer": "The best is to use manual optimization: \r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization\r\n\r\nThen you have control over each optimizer step. \r\n\r\nYou can get the optimzers with self.optimizers()\r\nand for backward you simply have to replace loss.backward() with self.manual_backward(loss)\r\nMake sure you use the latest version of Lightning for that for optimal support.\r\n\r\nHope this helps"
  },
  {
    "content": "Is it possible to use automatic optimization with accumulate_grad_batches and performance trick zero_grad(set_to_none=True)?Yes, you can override the zero_grad hook in LightningModule\r\n\r\n```python\r\n    def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer, optimizer_idx: int):\r\n        optimizer.zero_grad(set_to_None=True)\r\n```",
    "meta": { "name": "set_to_none=True and accumulate_grad_batches" },
    "answer": "Yes, you can override the zero_grad hook in LightningModule\r\n\r\n```python\r\n    def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer, optimizer_idx: int):\r\n        optimizer.zero_grad(set_to_None=True)\r\n```"
  },
  {
    "content": "Hi,\r\n\r\nI try to implement an autoencoder where the latent space is not euclidean but has hyperbolic and spherical geometry. I use geoopt for this. They recommend to use double precision for numerical stability. When I run my LightningModule with `precision=32` I encounter NaNs in the training. When I change to `precision=64` I get this pickling error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 494, in run_training_epoch\r\n    self.check_checkpoint_callback(True)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 149, in check_checkpoint_callback\r\n    cb.on_validation_end(self.trainer, model)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 238, in on_validation_end\r\n    self.save_checkpoint(trainer)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 281, in save_checkpoint\r\n    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 660, in _save_none_monitor_checkpoint\r\n    self._save_model(trainer, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 410, in _save_model\r\n    self._do_save(trainer, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 422, in _do_save\r\n    self.save_function(filepath, self.save_weights_only)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py\", line 336, in save_checkpoint\r\n    self.checkpoint_connector.save_checkpoint(filepath, weights_only)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 393, in save_checkpoint\r\n    self.trainer.accelerator.save_checkpoint(_checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 490, in save_checkpoint\r\n    self.training_type_plugin.save_checkpoint(checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 254, in save_checkpoint\r\n    atomic_save(checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 63, in atomic_save\r\n    torch.save(checkpoint, bytesbuffer)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 372, in save\r\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 476, in _save\r\n    pickler.dump(obj)\r\n_pickle.PicklingError: Can't pickle <function LinkPredictionModule.training_step at 0x7f08220f0ee0>: it's not the same object as stereographic_link_prediction.Models.Modules.LinkPredictionModule.training_step\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 40, in <module>\r\n    trainer.fit(module, datamodule=datamodule)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 480, in fit\r\n    self.dispatch()\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 523, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 95, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 142, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 535, in run_stage\r\n    self.run_train()\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 634, in run_train\r\n    self.train_loop.on_train_end()\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 117, in on_train_end\r\n    self.check_checkpoint_callback(should_update=True, is_last=True)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 149, in check_checkpoint_callback\r\n    cb.on_validation_end(self.trainer, model)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 238, in on_validation_end\r\n    self.save_checkpoint(trainer)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 281, in save_checkpoint\r\n    self._save_none_monitor_checkpoint(trainer, monitor_candidates)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 660, in _save_none_monitor_checkpoint\r\n    self._save_model(trainer, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 410, in _save_model\r\n    self._do_save(trainer, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 422, in _do_save\r\n    self.save_function(filepath, self.save_weights_only)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/properties.py\", line 336, in save_checkpoint\r\n    self.checkpoint_connector.save_checkpoint(filepath, weights_only)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 393, in save_checkpoint\r\n    self.trainer.accelerator.save_checkpoint(_checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 490, in save_checkpoint\r\n    self.training_type_plugin.save_checkpoint(checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 254, in save_checkpoint\r\n    atomic_save(checkpoint, filepath)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 63, in atomic_save\r\n    torch.save(checkpoint, bytesbuffer)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 372, in save\r\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/torch/serialization.py\", line 476, in _save\r\n    pickler.dump(obj)\r\n_pickle.PicklingError: Can't pickle <function LinkPredictionModule.training_step at 0x7f08220f0ee0>: it's not the same object as stereographic_link_prediction.Models.Modules.LinkPredictionModule.training_step\r\nException ignored in: <function tqdm.__del__ at 0x7f086346a430>\r\nTraceback (most recent call last):\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1145, in __del__\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1299, in close\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1492, in display\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1148, in __str__\r\n  File \"/home/netter/.cache/pypoetry/virtualenvs/stereographic-link-prediction-ra14Y8Aq-py3.8/lib/python3.8/site-packages/tqdm/std.py\", line 1450, in format_dict\r\nTypeError: cannot unpack non-iterable NoneType object\r\n```\r\n\r\nI think this error comes from inconsistencies in my Module. But I do not know how to find them. Do you have any tips to track down the inconsistencies that disturb pickling?\r\n\r\nHowever, when I run with `precision=16` I do not run into NaNs. As this precision should be more numerically unstable than `precision=32` my only explanation for this is that there is automatic gradient clipping going on or so in 16-bit precision. Is this correct? And where do I find these values? Then I could apply them to 32-bit precision training.Hi @gatoniel, not sure about the NaNs but the pickle error definitely looks like a bug on our end. The double precision plugin patches the `training_step` which would explain that error. Mind opening an issue for it?",
    "meta": { "name": "Questions and problems with different precisions" },
    "answer": "Hi @gatoniel, not sure about the NaNs but the pickle error definitely looks like a bug on our end. The double precision plugin patches the `training_step` which would explain that error. Mind opening an issue for it?"
  },
  {
    "content": "I would like my wandb logger to just place their data under `wandb` dir, and checkpointcallback to save ckpts under `dir_path` I specified.\r\nAnd I don't want pl to create `lightning_logs` and files under it, but I can't set `logger=False` b/c I use a logger. Is there any suggestion ?You can set the `save_dir` in WandbLogger, something like\r\n\r\n```python\r\nlogger = WandbLogger(save_dir=\"wandb\", ...)\r\nTrainer(logger=logger, ...)\r\n```\r\nThis should work (haven't tested it).\r\nThen your logs and checkpoints will save to two different locations.",
    "meta": {
      "name": "How to not create lightning_logs when using a external logger like wandb ?"
    },
    "answer": "You can set the `save_dir` in WandbLogger, something like\r\n\r\n```python\r\nlogger = WandbLogger(save_dir=\"wandb\", ...)\r\nTrainer(logger=logger, ...)\r\n```\r\nThis should work (haven't tested it).\r\nThen your logs and checkpoints will save to two different locations."
  },
  {
    "content": "I want build a Super-Resolution Network with multiple optimizer. \r\n\r\nThe code is below, \r\n\r\n```python\r\n    def configure_optimizers(self):\r\n        d_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\r\n        g_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\r\n        id_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\r\n        recon_optimizer = torch.optim.Adam([{'params': self.parameters()}], lr=self.lr, betas=(0.5, 0.9))\r\n        # use multi optimizer\r\n        return [g_optimizer, d_optimizer, id_optimizer, recon_optimizer]\r\n\r\n```\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        print('optimizer_idx', optimizer_idx)\r\n        # print('criterionG', next(self.criterionG.parameters()).requires_grad)\r\n        # print('generator', next(self.generator.parameters()).requires_grad)\r\n\r\n        lr_img, id_label, hr_img = batch\r\n\r\n        fake_img = self(lr_img)\r\n        d_fake = self.discriminator(fake_img)\r\n        d_real = self.discriminator(hr_img)\r\n\r\n        # train generator\r\n        if optimizer_idx == 0:\r\n            # log sampled images\r\n            grid = torchvision.utils.make_grid(fake_img)\r\n            self.logger.experiment.add_image('generated_images', grid, 0)\r\n            g_loss = self.g_loss_function(d_fake, fake_img, hr_img)\r\n            g_loss.requires_grad_(True)\r\n\r\n            return {'loss': g_loss}\r\n\r\n        # train discriminator\r\n        elif optimizer_idx == 1:\r\n            d_fake_loss = torch.mean(d_fake)\r\n            d_real_loss = torch.mean(d_real)\r\n            d_loss = (d_fake_loss + d_real_loss)/2\r\n            tqdm_dict ={'d_loss': d_loss}\r\n            self.log('d_loss', d_loss)\r\n\r\n            return {'d_loss': d_loss}\r\n\r\n        # fine-tuning arcface model\r\n        elif optimizer_idx == 2:\r\n            fake_img = self.conv1(fake_img)\r\n            pred = self.recognition(fake_img)\r\n            loss = self.loss_function(pred, id_label)\r\n            self.log('id_loss', loss)\r\n            tqdm_dict = {'id_loss': loss}\r\n            output = OrderedDict({\r\n                'id_loss': loss,\r\n                'progress_bar': tqdm_dict,\r\n                'log': tqdm_dict\r\n            })\r\n            return output\r\n\r\n        # training reconstruction model\r\n        elif optimizer_idx == 3:\r\n            fake_lr = self.reconstruction(fake_img)\r\n            loss = self.recon_loss_function(hr_img, fake_lr)\r\n            self.log('recon_loss', loss)\r\n            tqdm_dict = {'recon_loss': loss}\r\n            output = OrderedDict({\r\n                'recon_loss': loss,\r\n                'pregress_bar': tqdm_dict,\r\n                'log': tqdm_dict\r\n            })\r\n            return output\r\n```\r\n\r\nBut, i got this error in 'if optimizer_idx == 0:'\r\n\r\n```\r\n    closure_loss = closure_loss / self.trainer.accumulate_grad_batches\r\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\r\n```\r\n\r\nCan you give me a advice? \r\nThank you.\r\nHi @choieq, `training_step` needs to return one of:\r\n- `Tensor` - The loss tensor\r\n- `dict` - A dictionary. Can include any keys, but must include the key `'loss'`\r\n- `None` - Training will skip to the next batch\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step",
    "meta": {
      "name": "When use multiple optimizer, TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'"
    },
    "answer": "Hi @choieq, `training_step` needs to return one of:\r\n- `Tensor` - The loss tensor\r\n- `dict` - A dictionary. Can include any keys, but must include the key `'loss'`\r\n- `None` - Training will skip to the next batch\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training-step"
  },
  {
    "content": "Hi all. How can one make ModelCheckpoint work with multiple val dataloaders? Currently, it receives a single monitor parameter. I would like to make the checkpointing happen only when both dataloaders show improvement. Should I subclass ModelCheckpoint to receive a monitor per dataloader?You can compute the min or max (depending if your improvement is a decreasing or increasing metric) of both metrics and use self.log to log that combined value. Then, you only have one monitor for both and save_top_k can work on that.",
    "meta": { "name": "ModelCheckpoint with multiple validation dataloaders" },
    "answer": "You can compute the min or max (depending if your improvement is a decreasing or increasing metric) of both metrics and use self.log to log that combined value. Then, you only have one monitor for both and save_top_k can work on that."
  },
  {
    "content": "I need to use `srun` run `python, so how does set Trainer of `pl` correctly?Not sure what you are asking. Is the question how to use PL with SLURM?\r\nI can point you to the SLURM tutorial in our docs:\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/clouds/slurm.html",
    "meta": {
      "name": "How does a gpu cluster system like SLRUM use ddp training ?"
    },
    "answer": "Not sure what you are asking. Is the question how to use PL with SLURM?\r\nI can point you to the SLURM tutorial in our docs:\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/clouds/slurm.html"
  },
  {
    "content": "Hello Lightning gods!\r\n\r\nWhen using a single logger (without specifying a logger when creating the `Trainer`) and also using a `ModelCheckpoint` callback to automatically save good checkpoints, everything works as expected: The directory created by TensorBoard at `<path_to_project>/lightning_logs/<experiment_name>/version_<N>` contains `hparams.yaml` and a directory `checkpoints` containing several `.ckpt` files.\r\n\r\n```py\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor=\"val_loss\",\r\n    save_top_k=5,\r\n)\r\ntrainer = pl.Trainer(\r\n    gpus=-1,\r\n    accelerator=\"ddp\",\r\n    callbacks=[checkpoint_callback],\r\n)\r\n```\r\n\r\nHowever, when I added a second logger, the `checkpoints` directory is no longer find inside the `lightning_logs` subdirectories. These subdirectories created by TensorBoard only contains `hparams.yaml` and a `events.out.tfevents.*` file.\r\n\r\n```py\r\nfrom aim.pytorch_lightning import AimLogger\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\n\r\ncheckpoint_callback = ModelCheckpoint(\r\n    monitor=\"val_loss\",\r\n    save_top_k=5,\r\n)\r\ntb_logger = TensorBoardLogger(\"lightning_logs\", name=\"my_experiment\")\r\naim_logger = AimLogger(experiment=\"my_experiment\")\r\ntrainer = pl.Trainer(\r\n    gpus=-1,\r\n    accelerator=\"ddp\",\r\n    callbacks=[checkpoint_callback],\r\n    logger=[aim_logger, tb_logger],\r\n)\r\n```\r\nHow can we configure Lightning to also include the checkpoint files in the TensorBoard log directories, just like in the original case?Hi\r\nWhen using multiple loggers, there is no canonical way to store the checkpoints in subdirs. What Lightning currently does is put the checkpoints one level above in a directory with the names of the loggers concatenated:\r\n\r\n![image](https://user-images.githubusercontent.com/5495193/113522729-5567b580-95a3-11eb-816b-b73e456bb448.png)\r\n\r\nThis is very reasonable, since both loggers were used to produce the same checkpoints. \r\nThere are alternatives, for example saving the checkpoints in the experiment dir of the first logger in the list, or copy the checkpoints to both subdirs, but this is not implemented. \r\n",
    "meta": { "name": "Missing Saved Checkpoints when using Multiple Loggers" },
    "answer": "Hi\r\nWhen using multiple loggers, there is no canonical way to store the checkpoints in subdirs. What Lightning currently does is put the checkpoints one level above in a directory with the names of the loggers concatenated:\r\n\r\n![image](https://user-images.githubusercontent.com/5495193/113522729-5567b580-95a3-11eb-816b-b73e456bb448.png)\r\n\r\nThis is very reasonable, since both loggers were used to produce the same checkpoints. \r\nThere are alternatives, for example saving the checkpoints in the experiment dir of the first logger in the list, or copy the checkpoints to both subdirs, but this is not implemented. \r\n"
  },
  {
    "content": "I implemented the validation loop like this (using `validation_epoch_end` because the number of labels in every batch is variable):\r\n\r\n```\r\nclass MyModel(pl.LightningModule):\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        return {'target': ... ,'pred': ...}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        targets = torch.cat([o['target'] for o in outputs])\r\n        preds = torch.cat([o['pred'] for o in outputs])\r\n        accuracy = (preds == targets).sum().item() / len(preds)\r\n        self.log(\"val_acc\", accuracy * 100)\r\n```\r\n\r\nI verified that `accuracy` is computed correctly (as a float). However, I can't see it in the console. The console output looks very odd, because the progress bar is repeated during validation (it looks okay during training), and the \"Validating\" prefix changed back to \"Epoch 0\" (the number of epochs also changed from 74 to 79).\r\n\r\n```\r\nEpoch 0:   6%|\u258b         | 5/79 [00:04<01:03,  1.16it/s, loss=6.29]\r\nValidating: 0it [00:00, ?it/s]\r\nValidating:   0%|          | 0/74 [00:00<?, ?it/s]\r\nEpoch 0:  10%|\u2588         | 8/79 [00:04<00:39,  1.80it/s, loss=6.29]\r\nEpoch 0:  14%|\u2588\u258d        | 11/79 [00:04<00:28,  2.41it/s, loss=6.29]\r\nEpoch 0:  18%|\u2588\u258a        | 14/79 [00:04<00:21,  2.98it/s, loss=6.29]\r\nEpoch 0:  22%|\u2588\u2588\u258f       | 17/79 [00:04<00:17,  3.51it/s, loss=6.29]\r\nEpoch 0:  25%|\u2588\u2588\u258c       | 20/79 [00:04<00:14,  4.04it/s, loss=6.29]\r\nEpoch 0:  29%|\u2588\u2588\u2589       | 23/79 [00:05<00:12,  4.52it/s, loss=6.29]\r\nEpoch 0:  33%|\u2588\u2588\u2588\u258e      | 26/79 [00:05<00:10,  5.00it/s, loss=6.29]\r\nEpoch 0:  37%|\u2588\u2588\u2588\u258b      | 29/79 [00:05<00:09,  5.45it/s, loss=6.29]\r\nEpoch 0:  41%|\u2588\u2588\u2588\u2588      | 32/79 [00:05<00:07,  5.88it/s, loss=6.29]\r\nEpoch 0:  44%|\u2588\u2588\u2588\u2588\u258d     | 35/79 [00:05<00:07,  6.27it/s, loss=6.29]\r\nEpoch 0:  48%|\u2588\u2588\u2588\u2588\u258a     | 38/79 [00:05<00:06,  6.65it/s, loss=6.29]\r\nEpoch 0:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 41/79 [00:05<00:05,  6.99it/s, loss=6.29]\r\nEpoch 0:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 44/79 [00:05<00:04,  7.34it/s, loss=6.29]\r\nEpoch 0:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 47/79 [00:06<00:04,  7.67it/s, loss=6.29]\r\nEpoch 0:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 50/79 [00:06<00:03,  7.99it/s, loss=6.29]\r\nEpoch 0:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 53/79 [00:06<00:03,  8.29it/s, loss=6.29]\r\nEpoch 0:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 56/79 [00:06<00:02,  8.58it/s, loss=6.29]\r\nEpoch 0:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 59/79 [00:06<00:02,  8.88it/s, loss=6.29]\r\nEpoch 0:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 62/79 [00:06<00:01,  9.15it/s, loss=6.29]\r\nEpoch 0:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 65/79 [00:06<00:01,  9.40it/s, loss=6.29]\r\nEpoch 0:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 68/79 [00:07<00:01,  9.63it/s, loss=6.29]\r\nEpoch 0:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 71/79 [00:07<00:00,  9.85it/s, loss=6.29]\r\nEpoch 0:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 74/79 [00:07<00:00, 10.10it/s, loss=6.29]\r\nEpoch 0:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 77/79 [00:07<00:00, 10.31it/s, loss=6.29]\r\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79/79 [00:07<00:00, 10.41it/s, loss=6.29]\r\nEpoch 1:   8%|\u258a         | 6/79 [00:03<00:46,  1.56it/s, loss=5.08]\r\nValidating: 0it [00:00, ?it/s]\r\nValidating:   0%|          | 0/74 [00:00<?, ?it/s]\r\nEpoch 1:  11%|\u2588\u258f        | 9/79 [00:04<00:31,  2.25it/s, loss=5.08]\r\nEpoch 1:  15%|\u2588\u258c        | 12/79 [00:04<00:23,  2.90it/s, loss=5.08]\r\n```\r\n\r\nWhat is the right way to implement validation accuracy? I'm new to lightning, but I looked at the documentation and couldn't find the answer. Thanks for your help.\r\n\r\nA few related posts:\r\n* https://github.com/PyTorchLightning/pytorch-lightning/issues/5774 (proposed solution is too complex)\r\n* https://github.com/PyTorchLightning/pytorch-lightning/issues/1906 (out of date, as validation_epoch_end shouldn't return anything now)\r\n* https://forums.pytorchlightning.ai/t/computing-validation-accuracy-at-the-end-of-each-epoch/188/2 (out of date, as EvalResult has been removed)\r\n\r\nUpdate:\r\nI added `self.log(\"val_loss\", ...)` to `validation_step`, but even this does not log anything in the progress bar (or anywhere in the console). It seems that `self.log` is not working at all.\r\n\r\nUpdate:\r\nI finally made it work by explicitly specifying all these parameters: `self.log(\"val_acc\", raw_accuracy * 100, prog_bar=True, on_step=False, on_epoch=True)`. (However, it still doesn't work for `val_loss`, which has on_step=True, on_epoch=False.)  The repeated progress bar problem is not fixed.@justusschock Set `prog_bar=True` in self.log()\r\nOr you can just open tensorboard to see the graphical logs\r\n\r\ntensorboard --logdir lightning_logs\r\n\r\ncheers",
    "meta": {
      "name": "LightningModule.log does not work for validation metrics"
    },
    "answer": "Set `prog_bar=True` in self.log()\r\nOr you can just open tensorboard to see the graphical logs\r\n\r\ntensorboard --logdir lightning_logs\r\n\r\ncheers"
  },
  {
    "content": "If the TensorBoard logger is set up as shown\r\n\r\n```py\r\nlogger = TensorBoardLogger(name=\"MyModel\")\r\ncheckpoint_callback = ModelCheckpoint(\r\n    filename=\"{epoch}-{step}-{val_loss:.2f}\",\r\n    monitor=\"val_loss\",\r\n    save_top_k=5,\r\n)\r\ntrainer = pl.Trainer(\r\n    default_root_dir=ROOT_DIR,\r\n    callbacks=[checkpoint_callback],\r\n    logger=[logger],\r\n)\r\n```\r\n\r\nhow do we configure the checkpoints to be written to directories that are automatically named `version_0`, `version_1`, the way it is if you do not pass a logger to `Trainer`?\r\n\r\n```py\r\ntrainer = pl.Trainer(\r\n    default_root_dir=ROOT_DIR,\r\n    callbacks=[checkpoint_callback],\r\n)\r\n```\r\nIf we pass in a logger to `Trainer`, the checkpoints are written to\r\n\r\n    <root_path>/<experiment_name>/<integer>/checkpoints\r\n\r\nwhile the tensorboard logs and `hparams.yaml` are written to\r\n\r\n    <root_path>/<experiment_name>/version_<integer>/\r\n\r\nIf we do not pass in a logger to `Trainer`, then checkpoint files, Tensorboard files and `hparams.yaml` are all written to the same directory\r\n\r\n    <root_path>/<experiment_name>/version_<integer>/\r\n\r\nHow can both checkpoints and tensorboard files we written to the same `version_<integer>` directory?Hi\r\nFor this you need to set the \"default_root_dir\" in the Trainer, and set the save_dir of the Logger to the same.\r\n\r\nThis works for me (latest PL version):\r\n\r\n```python \r\n\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\n\r\nimport pytorch_lightning as pl\r\nfrom pl_examples.basic_examples.mnist_datamodule import MNISTDataModule\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\n\r\n\r\nclass LitClassifier(pl.LightningModule):\r\n\r\n    def __init__(self, hidden_dim=128, learning_rate=1e-3):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\r\n        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        x = torch.relu(self.l1(x))\r\n        x = torch.relu(self.l2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('valid_loss', loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = parent_parser.add_argument_group(\"LitClassifier\")\r\n        parser.add_argument('--hidden_dim', type=int, default=128)\r\n        parser.add_argument('--learning_rate', type=float, default=0.0001)\r\n        return parent_parser\r\n\r\n\r\ndef cli_main():\r\n    pl.seed_everything(1234)\r\n\r\n    parser = ArgumentParser()\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    parser = LitClassifier.add_model_specific_args(parser)\r\n    parser = MNISTDataModule.add_argparse_args(parser)\r\n    args = parser.parse_args()\r\n\r\n    dm = MNISTDataModule.from_argparse_args(args, num_workers=2)\r\n    model = LitClassifier(args.hidden_dim, args.learning_rate)\r\n\r\n    ROOT_DIR = \"here\"\r\n    mylogger = TensorBoardLogger(name=\"MyModel\", save_dir=ROOT_DIR)\r\n    ckpt_callback = ModelCheckpoint(monitor=\"valid_loss\", filename=\"{epoch}-{step}-{valid_loss:.2f}\")\r\n    trainer = pl.Trainer.from_argparse_args(args, default_root_dir=ROOT_DIR, logger=mylogger, callbacks=[ckpt_callback], limit_train_batches=2, limit_val_batches=2)\r\n    trainer.fit(model, datamodule=dm)\r\n\r\n\r\nif __name__ == '__main__':\r\n    cli_main()\r\n\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/5495193/113522562-ce660d80-95a1-11eb-8352-2701158a172f.png)\r\n",
    "meta": {
      "name": "How to set Checkpoints to be used in the automatically generated `version_N` directories?"
    },
    "answer": "Hi\r\nFor this you need to set the \"default_root_dir\" in the Trainer, and set the save_dir of the Logger to the same.\r\n\r\nThis works for me (latest PL version):\r\n\r\n```python \r\n\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\n\r\nimport pytorch_lightning as pl\r\nfrom pl_examples.basic_examples.mnist_datamodule import MNISTDataModule\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\n\r\n\r\nclass LitClassifier(pl.LightningModule):\r\n\r\n    def __init__(self, hidden_dim=128, learning_rate=1e-3):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\r\n        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        x = torch.relu(self.l1(x))\r\n        x = torch.relu(self.l2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('valid_loss', loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = parent_parser.add_argument_group(\"LitClassifier\")\r\n        parser.add_argument('--hidden_dim', type=int, default=128)\r\n        parser.add_argument('--learning_rate', type=float, default=0.0001)\r\n        return parent_parser\r\n\r\n\r\ndef cli_main():\r\n    pl.seed_everything(1234)\r\n\r\n    parser = ArgumentParser()\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    parser = LitClassifier.add_model_specific_args(parser)\r\n    parser = MNISTDataModule.add_argparse_args(parser)\r\n    args = parser.parse_args()\r\n\r\n    dm = MNISTDataModule.from_argparse_args(args, num_workers=2)\r\n    model = LitClassifier(args.hidden_dim, args.learning_rate)\r\n\r\n    ROOT_DIR = \"here\"\r\n    mylogger = TensorBoardLogger(name=\"MyModel\", save_dir=ROOT_DIR)\r\n    ckpt_callback = ModelCheckpoint(monitor=\"valid_loss\", filename=\"{epoch}-{step}-{valid_loss:.2f}\")\r\n    trainer = pl.Trainer.from_argparse_args(args, default_root_dir=ROOT_DIR, logger=mylogger, callbacks=[ckpt_callback], limit_train_batches=2, limit_val_batches=2)\r\n    trainer.fit(model, datamodule=dm)\r\n\r\n\r\nif __name__ == '__main__':\r\n    cli_main()\r\n\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/5495193/113522562-ce660d80-95a1-11eb-8352-2701158a172f.png)\r\n"
  },
  {
    "content": "Hi,\r\nI was wondering how could i access trainer parameters from a LightningModule.\r\nI want indeed to do setup that depend on max_epochs. Configure a linear decay lr scheduler and also i want to take some special action on the validation step on the last epoch.\r\n\r\nThanks for the help!@nicolas-dufour You can access it with `self.trainer.max_epochs`.",
    "meta": { "name": "Access trainer parameters from LightningModule" },
    "answer": "@nicolas-dufour You can access it with `self.trainer.max_epochs`."
  },
  {
    "content": "Thanks for great framework.  \r\nI tried to train with tpu (Google Cloud Platform Environment). I encounter error like this:\r\n```\r\nkaki_ai@kaki-ins:~/kopite-bot$ python3 train_blender.py\r\n16:14:31 | Overriding opt[\"no_cuda\"] to True (previously: False)\r\n16:14:31 | Loading model with `--beam-block-full-context false`\r\n16:14:31 | loading dictionary from /home/kaki_ai/ParlAI/data/models/blender/blender_90M/model.dict\r\n16:14:31 | num words = 54944\r\n16:14:32 | DEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\r\n16:14:33 | Total parameters: 87,508,992 (87,508,992 trainable)\r\n16:14:33 | Loading existing model params from /home/kaki_ai/ParlAI/data/models/blender/blender_90M/model\r\nTraceback (most recent call last):\r\n  File \"train_blender.py\", line 47, in <module>\r\n    val_dataloader=test_loader,\r\n  File \"/home/kaki_ai/kopite-bot/training/lightning_base.py\", line 135, in fit\r\n    accumulate_grad_batches=self.accumulate_grad_batches,\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 39, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 321, in __init__\r\n    replace_sampler_ddp, deterministic, precision, amp_backend, amp_level, plugins\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 91, in __init__\r\n    self.tpu_cores = device_parser.parse_tpu_cores(tpu_cores)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/device_parser.py\", line 113, in parse_tpu_cores\r\n    raise MisconfigurationException('No TPU devices were found.')\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: No TPU devices were found.\r\n```\r\n\r\nIf you have any doubts, please help me. Thank you!See #6778. (just for the record)",
    "meta": { "name": "TPU Training: No TPU devices were found." },
    "answer": "See #6778. (just for the record)"
  },
  {
    "content": "How can I access the best validation loss in `validation_epoch_end`?  I am assuming the loss is stored somewhere as it is used to save the best model based on it, so I was wondering if I can somehow directly access it instead of tracking it my self.I think if you include a [ModelCheckpoint](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint) in your trainer, you should be able to retrieve the [best model score](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/callbacks/model_checkpoint.py#L201). But you should set your validation loss as the monitored variable in your ModelCheckpoint callback. From your `validation_epoch_end`, you should be able to retrieve your best score by reaching the callback with something similar to `self.trainer.callback.best_model_score`.",
    "meta": {
      "name": "Accessing the best validation loss so far in validation_epoch_ends"
    },
    "answer": "I think if you include a [ModelCheckpoint](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint) in your trainer, you should be able to retrieve the [best model score](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/callbacks/model_checkpoint.py#L201). But you should set your validation loss as the monitored variable in your ModelCheckpoint callback. From your `validation_epoch_end`, you should be able to retrieve your best score by reaching the callback with something similar to `self.trainer.callback.best_model_score`."
  },
  {
    "content": "How to manually call `model.eval()` or `model.train()` inside the lightning module? I happen to have several models and not all of them need to be updated during each forward pass. Thanks!You can use [`self.freeze()`](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1479) and [`self.unfreeze()`](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1494).",
    "meta": {
      "name": "Manually call model.eval() and model.train() inside the training loop"
    },
    "answer": "You can use [`self.freeze()`](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1479) and [`self.unfreeze()`](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1494)."
  },
  {
    "content": "Because I have a very small network which is quick at training and I want to compare many different runs, I want to disable Lightning's default output to both console and file system. I already tried setting `weights_summary=False, checkpoint_callback=False, logger=False` and `logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)`, but I still get the progress bar and the GPU availability report. How can I disable these?Set\r\n``` python\r\n# disable progress bar\r\ntrainer = Trainer(progress_bar_refresh_rate=0)\r\n```\r\nwill remove the progressbar. There was a bug related to the user setting the logging level, so if you try to update to master hopefully doing\r\n```  python\r\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\r\n```\r\nshould remove the GPU availability report.\r\n",
    "meta": { "name": "How to make PyTorch Lightning quiet?" },
    "answer": "Set\r\n``` python\r\n# disable progress bar\r\ntrainer = Trainer(progress_bar_refresh_rate=0)\r\n```\r\nwill remove the progressbar. There was a bug related to the user setting the logging level, so if you try to update to master hopefully doing\r\n```  python\r\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\r\n```\r\nshould remove the GPU availability report.\r\n"
  },
  {
    "content": "nvm\r\n\r\ncheersCheers! \ud83c\udf7b ",
    "meta": { "name": "Bug in SLURMConnector?" },
    "answer": "Cheers! \ud83c\udf7b "
  },
  {
    "content": "Hi pytorch-lightning friends! :) I'd like to start a discussion about `trainer.check` api. \r\nBasically, this API should check that all the user-define classes (models, data, callbacks, ...) are programmatically sound. I propose to use `inspect` to check for function correctness, here's my PR proposal at https://github.com/PyTorchLightning/pytorch-lightning/issues/3244\r\nLocking in favor of https://github.com/PyTorchLightning/pytorch-lightning/discussions/6029",
    "meta": { "name": "trainer.check" },
    "answer": "Locking in favor of https://github.com/PyTorchLightning/pytorch-lightning/discussions/6029"
  },
  {
    "content": "My code is like this:\r\n\r\n```python\r\nmodel = MyLitModel()\r\ntrainer = Trainer(gpus=1)\r\ntrainer.validate(model, dataloader)\r\n```\r\n\r\nHowever, I got an `AttributeError`:\r\n\r\n```\r\nAttributeError: type object 'Trainer' has no attribute 'validate'\r\n```It's available in master!\r\n\r\nNote that your first call is using `Trainer.validate` not `trainer.validate` and that's why you get that error\r\n\r\nfixed:\r\n```python\r\nmodel = MyLitModel()\r\ntrainer = Trainer(gpus=1)\r\ntrainer.validate(model, dataloader)\r\n```",
    "meta": {
      "name": "Is Trainer.validate not available now in the latest version?"
    },
    "answer": "It's available in master!\r\n\r\nNote that your first call is using `Trainer.validate` not `trainer.validate` and that's why you get that error\r\n\r\nfixed:\r\n```python\r\nmodel = MyLitModel()\r\ntrainer = Trainer(gpus=1)\r\ntrainer.validate(model, dataloader)\r\n```"
  },
  {
    "content": "**What I want it to look like:**\r\n![image](https://user-images.githubusercontent.com/17963619/110324632-3253ef80-8051-11eb-8ec0-abe20b9100e0.png)\r\n\r\n**What I got now:**\r\n![image](https://user-images.githubusercontent.com/17963619/110324679-4566bf80-8051-11eb-9518-cf4b317edc5f.png)\r\n\r\n**I think I do have logged metrics, how to make it printed beautifully ?**\r\n```\r\nclass SortNumberModel(pl.LightningModule):\r\n    def __init__(self, hf_config, lr):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.model = AutoModelForTokenClassification.from_config(hf_config)\r\n        self.val_acc = pl.metrics.Accuracy()\r\n\r\n    def forward(self, batch):\r\n        result = self.model(input_ids=batch[\"x\"], labels=batch[\"y\"], return_dict=True)\r\n        return result.logits.argmax(dim=-1), result.loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self.forward(batch)[-1]\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        preds, loss = self.forward(batch)\r\n        self.val_acc(preds.view(-1), batch[\"y\"].view(-1))\r\n        self.log(\"valid_acc\", self.val_acc, on_epoch=True)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, eps=1e-5)\r\n```In pytorch-lightning bolts we have a few extra callbacks, with one of them being the `PrintTableMetricsCallback` callback that should output exactly what you want. Simply install bolts as \r\n``` python\r\npip install pytorch-lightning-bolts\r\n```\r\nand initialize your trainer as:\r\n```python\r\ntrainer = Trainer(callbacks=[PrintTableMetricsCallback()])\r\n```\r\n@SkafteNicki,  Thank you for you answer. \r\n\r\nHowever, I found the solution prints different metric values accross different ddp process. \r\n\r\nI'm struggling to correctly print the metrics on the end of each epoch, but I meet a lot of problem discribed in this issue:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/issues/6122#issuecomment-797863438 \r\n\r\nWill you please look through my problems and give some suggestions? \r\n\r\nThank you for your help!\r\n\r\n",
    "meta": { "name": "How to print metric value every epoch ?" },
    "answer": "In pytorch-lightning bolts we have a few extra callbacks, with one of them being the `PrintTableMetricsCallback` callback that should output exactly what you want. Simply install bolts as \r\n``` python\r\npip install pytorch-lightning-bolts\r\n```\r\nand initialize your trainer as:\r\n```python\r\ntrainer = Trainer(callbacks=[PrintTableMetricsCallback()])\r\n```\r\n"
  },
  {
    "content": "Hello,\r\n\r\nI was wondering if it is possible to control the trainloop behavior of a module (beyond overriding `training_step()`).  I want to manually override the `.grad` value of each parameter by myself.\r\n\r\nFor example, let's say I have this routine:\r\n\r\n```Python\r\nm_0 = MyModel()\r\nloader_1 = getTrainLoader(1)\r\nloader_2 = getTrainLoader(2)\r\nloader_3 = getTrainLoader(3)\r\n\r\n# train the first two models\r\nm_1 = train_model_for_one_epoch(m_0, loader_1)\r\nm_2 = train_model_for_one_epoch(m_1, loader_2)\r\n\r\n# train the third model based on the previous models\r\nm_3 = MyModel()\r\ncriteriton = nn.CrossEntropyLoss()\r\noptimizer = optim.SGD(m_3.parameters(), lr)\r\n\r\n# main trainloop\r\nfor data, target in loader_3:\r\n    loss_1 = criteriton(m_1(data), target)\r\n    loss_1.backward()\r\n    grad_1 = get_gradient_vector(m_1)\r\n    loss_2 = criterion(m_2(data), target)\r\n    loss_2.backward()\r\n    grad_2 = get_gradient_vector(m_2)\r\n    \r\n    # manually calculate & set gradient\r\n    grad_3 = (grad_1 + grad_2) / 2.0\r\n    set_model_gradient(m_3, grad_3)\r\n    optimizer.step()\r\n    \r\n```\r\n\r\nHow can I implement the final loop in the above code in PL?\r\n\r\nThanks!Currently there's no easy way for users to manage the dataloaders themselves, but you can perform the optimization (and manipulate the gradients) by setting `automatic_optimization=False`\r\n\r\nsee: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization",
    "meta": { "name": "Custom training loop for LightningModule" },
    "answer": "Currently there's no easy way for users to manage the dataloaders themselves, but you can perform the optimization (and manipulate the gradients) by setting `automatic_optimization=False`\r\n\r\nsee: https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#manual-optimization"
  },
  {
    "content": "Hello--\r\n\r\nI am saving checkpoints inside my module using `self.trainer.save_checkpoint(path)`. I am able to load these checkpoints into the model using `MyModel.load_from_checkpoint(path)` and trainer using `Trainer(resume_from_checkpoint=path)`. However, both the resulting model and trainer have `global_step=0` regardless of the step when saving. From the documentation I was of the impression that checkpoints saved the global step, which is important for my use case. How can I attain the global step from a checkpoint?Discussion continued in https://github.com/PyTorchLightning/pytorch-lightning/issues/6470",
    "meta": { "name": "How to get global step from checkpoint?" },
    "answer": "Discussion continued in https://github.com/PyTorchLightning/pytorch-lightning/issues/6470"
  },
  {
    "content": "I need to pretrain the encode and decode section of an autoencoder first, then later attach a transformer in the middle of the encode and decode section. When I load the weights of the encode and decode section when pretraining it first, while initializing the weights of the transformer section, will I get an error about missing layers?You can use the `strict` flag of `load_from_checkpoint` to avoid the missing layer failure:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/079fe9bc0908fffdd55a08f7321a199bceaef6f8/pytorch_lightning/core/saving.py#L94-L95",
    "meta": {
      "name": "Pretrain some sections of a model, then initialize those parts when training a full model"
    },
    "answer": "You can use the `strict` flag of `load_from_checkpoint` to avoid the missing layer failure:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/079fe9bc0908fffdd55a08f7321a199bceaef6f8/pytorch_lightning/core/saving.py#L94-L95"
  },
  {
    "content": "**What I did:**\r\n```python\r\ndef configure_optimizers(self):\r\n\r\n    optimizer = torch.optim.AdamW(\r\n        self.parameters(),\r\n        lr=self.hparams.learning_rate,\r\n        eps=1e-5,\r\n    )\r\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\r\n        optimizer,\r\n        max_lr=self.hparams.learning_rate,\r\n        epochs=self.trainer.max_epochs,\r\n        steps_per_epoch=len(self.datamodule.train_dataloader()),\r\n    )\r\n    set_trace()\r\n    return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\r\n```\r\nI put an breakpoint on `scheduler.step` to see when it will be called.\r\n\r\n**What I got:**\r\n![image](https://user-images.githubusercontent.com/17963619/110569166-1b1b1c00-818f-11eb-804e-f6c9f645cd06.png)\r\nAnd I found \r\n- I ran into breakpoint (`scheduler.step` is called) only on the end of validation.\r\n- `epochs=self.trainer.max_epochs=1` and `steps_per_epoch=len(self.datamodule.train_dataloader())=144`, which are correct.\r\n\r\n**Code for your reference if needed**\r\n```python\r\nimport random\r\nfrom IPython.core.debugger import set_trace\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom transformers import AutoConfig, ElectraForTokenClassification\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\r\nfrom pl_bolts.callbacks import PrintTableMetricsCallback\r\nfrom pytorch_lightning.loggers import WandbLogger\r\n\r\nclass SortNumberDataset(torch.utils.data.Dataset):\r\n    def __init__(self, dataset_size, vocab_size, sequence_length):\r\n        super().__init__()\r\n        self.dataset_size = dataset_size\r\n        self.vocab_size = vocab_size\r\n        self.sequence_length = sequence_length\r\n\r\n    def __getitem__(self, i):\r\n        x = [\r\n            random.randint(0, self.vocab_size - 1) for _ in range(self.sequence_length)\r\n        ]\r\n        y = sorted(x)\r\n        return {\"x\": torch.tensor(x), \"y\": torch.tensor(y)}\r\n\r\n    def __len__(self):\r\n        return self.dataset_size\r\n\r\n\r\nclass NumberSorting(pl.LightningModule):\r\n    def __init__(\r\n        self,\r\n        hf_config,\r\n        learning_rate,\r\n        trainset_size,\r\n        valset_size,\r\n        vocab_size,\r\n        sequence_length,\r\n        batch_size=128,\r\n        num_workers=4,\r\n    ):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.datamodule = pl.LightningDataModule.from_datasets(\r\n            SortNumberDataset(trainset_size, vocab_size, sequence_length),\r\n            SortNumberDataset(valset_size, vocab_size, sequence_length),\r\n            batch_size=batch_size,\r\n            num_workers=num_workers,\r\n        )\r\n        # self.model = ElectraForTokenClassification(hf_config)\r\n         # tie input/output embeddings\r\n         delattr(self.model, \"classifier\")\r\n         self.model.classifier = (\r\n             lambda x: x @ self.model.electra.embeddings.word_embeddings.weight.t()\r\n        )\r\n        self.val_acc = pl.metrics.Accuracy()\r\n\r\n    def forward(self, batch):\r\n        result = self.model(input_ids=batch[\"x\"], labels=batch[\"y\"], return_dict=True)\r\n        return result.logits.argmax(dim=-1), result.loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # self.log(\"lr\", self.trainer.optimizers[0].param_groups[0][\"lr\"])\r\n        return self(batch)[-1]\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        preds, loss = self(batch)\r\n        self.val_acc(preds.view(-1), batch[\"y\"].view(-1))\r\n        self.log(\"valid_acc\", self.val_acc, on_epoch=True)\r\n\r\n    def configure_optimizers(self):\r\n        timizer = torch.optim.AdamW(optimizer_grouped_parameters, eps=1e-5)\r\n        optimizer = torch.optim.AdamW(\r\n            self.parameters(),\r\n            lr=self.hparams.learning_rate,\r\n        )\r\n        # return optimizer\r\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\r\n            optimizer,\r\n            max_lr=self.hparams.learning_rate,\r\n            epochs=self.trainer.max_epochs,\r\n            steps_per_epoch=len(self.datamodule.train_dataloader()),\r\n        )\r\n        set_trace()\r\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\r\n\r\n\r\nconfig = AutoConfig.from_pretrained(\r\n    \"google/electra-small-generator\",\r\n    pad_token_id=-1,\r\n    max_position_embeddings=7,\r\n    vocab_size=7,\r\n    num_labels=7,\r\n    embedding_size=10,\r\n    hidden_size=10,\r\n    intermediate_size=8,\r\n    num_hidden_layers=2,\r\n    num_attention_heads=2,\r\n)\r\nplmodule = NumberSorting(\r\n    config,\r\n    learning_rate=0.05,\r\n    trainset_size=18333,\r\n    valset_size=20000,\r\n    vocab_size=7,\r\n    sequence_length=7,\r\n)\r\n\r\n\r\ntrainer = pl.Trainer(\r\n    max_epochs=1,\r\n    gpus=\"0\",\r\n    callbacks=[\r\n        PrintTableMetricsCallback(),\r\n        LearningRateMonitor(logging_interval=\"step\", log_momentum=True),\r\n    ],\r\n    logger=WandbLogger(),\r\n)\r\ntrainer.fit(plmodule)\r\n\r\n```That is the default behaviour of learning rate schedulers, that they step at the end of the training epoch. \r\nCan I ask you what you are trying to achieve?\r\nIf you want the learning rate scheduler to step after each batch, you can read more about what the output of `configure_optimizers` should look like here: https://pytorch-lightning.readthedocs.io/en/0.9.0/optimizers.html#learning-rate-scheduling",
    "meta": { "name": "Scheduler.step() only called on the end of validation" },
    "answer": "That is the default behaviour of learning rate schedulers, that they step at the end of the training epoch. \r\nCan I ask you what you are trying to achieve?\r\nIf you want the learning rate scheduler to step after each batch, you can read more about what the output of `configure_optimizers` should look like here: https://pytorch-lightning.readthedocs.io/en/0.9.0/optimizers.html#learning-rate-scheduling"
  },
  {
    "content": "I can install pl in colab by\r\n`!pip install pytorch-lightning==1.2.2 --quiet`\r\nbut I cannot import it by\r\n`import pytorch_lightning as pl`\r\nI am thankful if you help me with this issue.Solution is here: \r\nhttps://github.com/PyTorchLightning/pytorch-lightning/issues/6426\r\nPlease upgrade to version 1.2.3 (released yesterday) where this issue was solved.",
    "meta": { "name": "import pytorch_lightning as pl does not work on colab" },
    "answer": "Please upgrade to version 1.2.3 (released yesterday) where this issue was solved."
  },
  {
    "content": "How do I set the number of epochs to train?\r\n\r\n#### What have you tried?    \r\nLooking for documentation. \r\nLooking for examples.\r\n\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/Trainer/Training%20Loop/#force-training-for-min-or-max-epochsUp-to-date link: \r\nhttps://pytorch-lightning.readthedocs.io/en/latest/pytorch_lightning.trainer.training_loop.html#force-training-for-min-or-max-epochsUp-to-date link : \r\nhttps://pytorch-lightning.readthedocs.io/en/latest/trainer.html#max-epochsUp-to-date link:\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#max-epochs",
    "meta": { "name": "How set number of epochs" },
    "answer": "Up-to-date link:\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#max-epochs"
  },
  {
    "content": "Changing val_check_interval from 0.01 to 0.005 changed the number of steps in an epoch from 2k to 3k for one of my experiments. Wanted to know if that is expected behavior.The number of steps in a epoch that is shown in the progressbar is both the number of training steps AND the number of validation steps. Since `val_check_interval` changes the number of validation steps, it makes sense that you are seeing the number changing :]",
    "meta": {
      "name": "changing val_check_interval from 0.01 to 0.005 changes number of steps in an epoch"
    },
    "answer": "The number of steps in a epoch that is shown in the progressbar is both the number of training steps AND the number of validation steps. Since `val_check_interval` changes the number of validation steps, it makes sense that you are seeing the number changing :]"
  },
  {
    "content": "I want to create a custom `Callback` class where I can access certain attributes from my `DataModule` and log/save them before the start of the train step. \r\n\r\nI am little confused on how to do this. Can anyone help me out with a quick snippet? Thanks!Something like this should work :]\r\n```python\r\nfrom pytorch_lightning.callbacks import Callback\r\nclass MyCallback(Callback):\r\n    def __init__(self, ...):\r\n        ...\r\n\r\n    # hook for doing something with your datamodule before training step\r\n    def on_train_batch_start(self, trainer, *args, **kwargs):\r\n        dm = trainer.datamodule # this is a reference to your datamodule during training\r\n        # do something here with your datamodule\r\n```",
    "meta": { "name": "Access datamodule in custom callbacks" },
    "answer": "Something like this should work :]\r\n```python\r\nfrom pytorch_lightning.callbacks import Callback\r\nclass MyCallback(Callback):\r\n    def __init__(self, ...):\r\n        ...\r\n\r\n    # hook for doing something with your datamodule before training step\r\n    def on_train_batch_start(self, trainer, *args, **kwargs):\r\n        dm = trainer.datamodule # this is a reference to your datamodule during training\r\n        # do something here with your datamodule\r\n```"
  },
  {
    "content": "How would one correctly apply the Precision metric from v1.2.0 on, with the revised metrics api?\r\n\r\nI am currently doing something like this:\r\n\r\n```python\r\nimport torch\r\nfrom pytorch_lightning import metrics\r\n\r\n# example data\r\npreds = [0] * 200 + [1] * 30 + [0] * 10 + [1] * 20\r\ntargets = [0] * 200 + [1] * 30 + [1] * 10 + [0] * 20\r\n\r\npreds = torch.tensor(preds)\r\ntargets = torch.tensor(targets)\r\n\r\n# define method for printing metrics\r\ndef _print_some_metrics(preds, targets, num_classes):\r\n    precision = metrics.classification.Precision(\r\n        num_classes=None, is_multiclass=False)\r\n    recall = metrics.classification.Recall(\r\n        num_classes=None, is_multiclass=False)\r\n    f1 = metrics.classification.F1(num_classes=num_classes)\r\n    f1beta = metrics.classification.FBeta(\r\n        num_classes=num_classes,\r\n        beta=2\r\n    )\r\n\r\n    accuracy = metrics.classification.Accuracy()\r\n    avg_precision = metrics.classification.AveragePrecision(\r\n        num_classes=None)\r\n    confusion_matrix = metrics.ConfusionMatrix(num_classes=2)\r\n\r\n    # print results\r\n    print(\"Precision:\\n{}\\n\".format(precision(preds, targets)))\r\n    print(\"Recall:\\n{}\\n\".format(recall(preds, targets)))\r\n    print(\"F1:\\n{}\\n\".format(f1(preds, targets)))\r\n    print(\"F1-Beta:\\n{}\\n\".format(f1beta(preds, targets)))\r\n\r\n    print(\"AVG Precision:\\n{}\\n\".format(avg_precision(preds, targets)))\r\n    print(\"Accuracy:\\n{}\\n\".format(accuracy(preds, targets)))\r\n    print(\"ConfMat:\\n{}\\n\".format(confusion_matrix(preds, targets)))\r\n\r\n_print_some_metrics(preds, targets, num_classes=2)\r\n```\r\n\r\nWhich gives me these results:\r\n> Precision:\r\n0.6000000238418579\r\n\r\n> Recall:\r\n0.75\r\n\r\n> F1:\r\n0.8846153616905212\r\n\r\n> F1-Beta:\r\n0.8846153616905212\r\n\r\n> AVG Precision:\r\n0.48846155405044556\r\n\r\n> Accuracy:\r\n0.8846153616905212\r\n\r\n> ConfMat:\r\ntensor([[200.,  20.],\r\n        [ 10.,  30.]])\r\n\r\nHowever, when calculating precision by hand [(TP / TP + FN)](https://en.wikipedia.org/wiki/Precision_and_recall) with the numbers from the contingency table, I get 30 / 50 = 0.6\r\n\r\nWhy does applying the precision class result in this (small) deviation?\r\n\r\nFurther, when logging the metrics on epoch_end steps inside my model, I am not able to reproduce the logged precision, recall or accuraccy numbers on the validation set with the output from the contingency table, logged on the same steps (I haven't validated the other metrics yet by hand).\r\n\r\nIt would be great to get some help on how to correctly apply the new metrics API for a binary use case.\r\nfor binary classification, where you are only interested in the positive class you should pass in `num_classes=1`. Here is your corrected code:\r\n``` python\r\ndef _print_some_metrics(preds, targets, num_classes):\r\n    precision = metrics.classification.Precision(\r\n        num_classes=num_classes, is_multiclass=False)\r\n    recall = metrics.classification.Recall(\r\n        num_classes=num_classes, is_multiclass=False)\r\n    f1 = metrics.classification.F1(num_classes=num_classes)\r\n    f1beta = metrics.classification.FBeta(\r\n        num_classes=num_classes,\r\n        beta=2\r\n    )\r\n\r\n    accuracy = metrics.classification.Accuracy()\r\n    avg_precision = metrics.classification.AveragePrecision(\r\n        num_classes=num_classes)\r\n    confusion_matrix = metrics.ConfusionMatrix(num_classes=2)\r\n\r\n    # print results\r\n    print(\"Precision:\\n{}\\n\".format(precision(preds, targets)))\r\n    print(\"Recall:\\n{}\\n\".format(recall(preds, targets)))\r\n    print(\"F1:\\n{}\\n\".format(f1(preds, targets)))\r\n    print(\"F1-Beta:\\n{}\\n\".format(f1beta(preds, targets)))\r\n\r\n    print(\"AVG Precision:\\n{}\\n\".format(avg_precision(preds, targets)))\r\n    print(\"Accuracy:\\n{}\\n\".format(accuracy(preds, targets)))\r\n    print(\"ConfMat:\\n{}\\n\".format(confusion_matrix(preds, targets)))\r\n\r\n_print_some_metrics(preds, targets, num_classes=1)\r\n```\r\nonly `confusion_matrix` need to be set `num_classes=2` because you want the statistics for both classes.\r\n",
    "meta": { "name": "How to correctly apply metrics API in binary use case" },
    "answer": "for binary classification, where you are only interested in the positive class you should pass in `num_classes=1`. Here is your corrected code:\r\n``` python\r\ndef _print_some_metrics(preds, targets, num_classes):\r\n    precision = metrics.classification.Precision(\r\n        num_classes=num_classes, is_multiclass=False)\r\n    recall = metrics.classification.Recall(\r\n        num_classes=num_classes, is_multiclass=False)\r\n    f1 = metrics.classification.F1(num_classes=num_classes)\r\n    f1beta = metrics.classification.FBeta(\r\n        num_classes=num_classes,\r\n        beta=2\r\n    )\r\n\r\n    accuracy = metrics.classification.Accuracy()\r\n    avg_precision = metrics.classification.AveragePrecision(\r\n        num_classes=num_classes)\r\n    confusion_matrix = metrics.ConfusionMatrix(num_classes=2)\r\n\r\n    # print results\r\n    print(\"Precision:\\n{}\\n\".format(precision(preds, targets)))\r\n    print(\"Recall:\\n{}\\n\".format(recall(preds, targets)))\r\n    print(\"F1:\\n{}\\n\".format(f1(preds, targets)))\r\n    print(\"F1-Beta:\\n{}\\n\".format(f1beta(preds, targets)))\r\n\r\n    print(\"AVG Precision:\\n{}\\n\".format(avg_precision(preds, targets)))\r\n    print(\"Accuracy:\\n{}\\n\".format(accuracy(preds, targets)))\r\n    print(\"ConfMat:\\n{}\\n\".format(confusion_matrix(preds, targets)))\r\n\r\n_print_some_metrics(preds, targets, num_classes=1)\r\n```\r\nonly `confusion_matrix` need to be set `num_classes=2` because you want the statistics for both classes.\r\n"
  },
  {
    "content": "I wrote a Python script that loops over sets of hyperparameters and for each set calls `trainer.fit()`. Subsequently `trainer.test()` is currently called 2 times, each for the best checkpoint that was logged for two metrics.\r\n\r\nThis script is executable via commandline and works well on CPU an 1 GPU for any number of hyperparameter sets.\r\n\r\nI want to run this code on multiple GPUs (2-4) and wonder if I could use 'ddp'?\r\n\r\nThe [latest documentation](https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html) says that if `your script needs to invoke both .fit and .test, or one of them multiple times` 'ddp' mode isn't possible.\r\n\r\nHowever, @awaelchli indicated [here](https://forums.pytorchlightning.ai/t/how-to-run-trainer-fit-and-trainer-test-in-ddp-distributed-mode/192/6), that running `trainer.fit()` and `trainer.test()` in 'ddp' mode is now possible but the documentation is outdated.\r\n\r\nQuestions:\r\n* Is it now possible to run both  `trainer.fit()` and subsequently `trainer.test()` (and eventually `trainer.test()` mutiple times) in 'ddp' mode?\r\n* Would I need to reorganize my code so that the loop over hyperparameter sets invokes a `subprocess.call` to actually instantiate every new trainer for 'ddp' in another process?\r\n* If calling `.test()` after `.fit()` in 'ddp' is still not possible, would it be a workaround to instantiate `trainer` again after `.fit()` is finished and just load the model weights from the best checkpoint for testing (to avoid using the same trainer object)?\r\n\r\nI would be really happy for some help on that.No this should be working fine. \r\nWe have tests for this here: https://github.com/PyTorchLightning/pytorch-lightning/blob/a6c98c4e4944cfc4977d27a4ea1e0c7c621dfb4d/tests/accelerators/test_ddp.py#L56\r\nI will update the docs.",
    "meta": { "name": "How to sequentially call fit() and test() in DDP" },
    "answer": "No this should be working fine. \r\nWe have tests for this here: https://github.com/PyTorchLightning/pytorch-lightning/blob/a6c98c4e4944cfc4977d27a4ea1e0c7c621dfb4d/tests/accelerators/test_ddp.py#L56\r\nI will update the docs."
  },
  {
    "content": "I've used `seed_everything(7)` to initially set the seed then passed `deterministic=True, accelerator='ddp'` to Trainer to have it run on 4 GPUs.\r\nThen I load my map-style dataset using a plain DataLoader with `shuffle=True, num_workers=10` .\r\nNow what happens is that each of the forked DDP processes spin up N (here 10) worker processes to read the data. So total 4 x 10 DataLoader processes. I have tried setting up a `worker_init_fn` to see the seeds they each receive, and indeed the worker processes for each GPU get different seeds, but they are the same across worker processes of different GPUs. This causes each data item to be read 4 times (the number of GPU / DDP processes) which I checked in the dataset's `__getitem__`. So the indexes for example would look like [3,3,3,3,7,7,7,7,2,2,2,2,...].\r\n\r\nWhat is the way to fix this? Shouldn't the DistributedSampler for DDP automatically get a seed based on the subprocess that it is forked on? (Similar to DataLoader's `worker_init_fn`)Update: This seemed to be the issue if only settings seeds and setting Pytorch and Cuda for deterministic execution, but adding `deterministirc=True` to PL's Trainer object seems to have resolved the issue.",
    "meta": {
      "name": "Deterministic DataLoader on DDP reads same data on all subprocesses"
    },
    "answer": "Update: This seemed to be the issue if only settings seeds and setting Pytorch and Cuda for deterministic execution, but adding `deterministirc=True` to PL's Trainer object seems to have resolved the issue."
  },
  {
    "content": "How do I save checkpoints every, as well as deleting and/or overwriting the previously saved checkpoints?Just to be sure, do you mean every n step or every n epoch.\r\nFor doing it every n epoch, you can initialize model checkpoint with `period` parameter\r\n``` python\r\ncheckpoint = pl.callbacks.ModelCheckpoint(period=n)\r\n```",
    "meta": {
      "name": "How to save a checkpoint every n steps and overwrite the previous saved checkpoints?"
    },
    "answer": "Just to be sure, do you mean every n step or every n epoch.\r\nFor doing it every n epoch, you can initialize model checkpoint with `period` parameter\r\n``` python\r\ncheckpoint = pl.callbacks.ModelCheckpoint(period=n)\r\n```"
  },
  {
    "content": "Hey,\r\nwhat is the canonical way to access/change the optimizer of a pl.LightningModule after model.setup('fit') was called?\r\n\r\nE.g. if I follow https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html 4 (the last part where I explicitly plot the found lr and then change it) I don\u2019t see a way how to change the optimizers lr.\r\nIf I call model.configure_optimizers() it\u2019ll just return the optimizer and not set them.\r\nIs there no official way to access and change them after construction?\r\n\r\nCheersWhat you probably want to do, is to have an `learning_rate` attribute that you can change after inspecting the plot:\r\nSo your model should look something like this:\r\n``` python\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self, ...)\r\n        self.learning_rate = 1e-2\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\r\n```\r\nthen you should be able to do:\r\n``` python\r\nmodel = MyModel(...)\r\ntrainer = Trainer(...)\r\nlrfinder = trainer.tuner.lr_finder(...)\r\nlffinder.plot() # find the learning rate you want\r\nmodel.learning_rate = 1234 # set what you want\r\n```\r\n\r\n",
    "meta": { "name": "Access and change models optimizer after setup" },
    "answer": "What you probably want to do, is to have an `learning_rate` attribute that you can change after inspecting the plot:\r\nSo your model should look something like this:\r\n``` python\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self, ...)\r\n        self.learning_rate = 1e-2\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\r\n```\r\nthen you should be able to do:\r\n``` python\r\nmodel = MyModel(...)\r\ntrainer = Trainer(...)\r\nlrfinder = trainer.tuner.lr_finder(...)\r\nlffinder.plot() # find the learning rate you want\r\nmodel.learning_rate = 1234 # set what you want\r\n```\r\n\r\n"
  },
  {
    "content": "Trying to understand compute() for logging. Could someone please tell if the self.log(train_acc..., on_epoch=True) is same as def training_epoch_end() which uses compute - code below\r\n\r\n    def __init__():\r\n        # ...\r\n        self.train_acc = pl.metrics.Accuracy()\r\n        self.valid_acc = pl.metrics.Accuracy()\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # ...\r\n        self.train_acc(pred, target)\r\n        self.log(\"train_acc\", self.train_acc, on_step=False, on_epoch=True)\r\n        self.log('train_loss', loss, on_step=True, on_epoch=False)\r\n\r\n        return loss\r\n\r\n    def training_epoch_end(self, training_step_outputs):\r\n        self.log('train_acc_epoch', self.train_acc.compute())\r\n        \r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        # ...\r\n        self.valid_acc(pred, target)\r\n        self.log('val_loss', loss, prog_bar=True)\r\n        self.log('val_acc', self.valid_acc, on_step=False, on_epoch=True, prog_bar=True)\r\n\r\n        return loss\r\n    \r\n    \r\n    def validation_epoch_end(self, validation_step_outputs):\r\n        self.log('valid_acc_epoch', self.valid_acc.compute())Yes, its the same. Here is the relevant code:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/4a8422c2dc870d29d26748e6aa01406d2e74fa48/pytorch_lightning/core/step_result.py#L325-L330\r\nSetting `on_epoch=True` will essentially call `metric.compute` at the end of the epoch.",
    "meta": { "name": "Logging metrics with compute()" },
    "answer": "Yes, its the same. Here is the relevant code:\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/4a8422c2dc870d29d26748e6aa01406d2e74fa48/pytorch_lightning/core/step_result.py#L325-L330\r\nSetting `on_epoch=True` will essentially call `metric.compute` at the end of the epoch."
  },
  {
    "content": "Hi,\r\n\r\nI am trying to incorporate the scheduler provided [here](https://github.com/mpyrozhok/adamwr). \r\n\r\nI went through the documentation of PyTorch lightning and it allows only scheduler interval \"epoch\" or \"step\". But in the scheduler attached, it needs to call \"step\" every epoch and  \"batch step\" after each iteration. Could someone help me figure out a way to do this? Any help will be appreciated.\r\n\r\nThank you,\r\nBest,\r\nShreyas KamathIt is currently not supported that a scheduler can step on both batch and epoch. The scheduler you are linking to are also different from a standard scheduler since it both has an `step` method and a `batch_step` method, where lightning only supports the `step` method. \r\nTo get this working in lightning, you would need to split the scheduler into two new schedulers that both operate on the same optimizer: one that steps on batch and one the steps on epoch.",
    "meta": {
      "name": "How do I incorporate a scheduler with \"step\" and \"batch step\"?"
    },
    "answer": "It is currently not supported that a scheduler can step on both batch and epoch. The scheduler you are linking to are also different from a standard scheduler since it both has an `step` method and a `batch_step` method, where lightning only supports the `step` method. \r\nTo get this working in lightning, you would need to split the scheduler into two new schedulers that both operate on the same optimizer: one that steps on batch and one the steps on epoch."
  },
  {
    "content": "Hi, I'm trying to refactor the official NLP (sentiment analysis) [tutorial](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html), using Lightning in order to take advantage of things like early stopping etc.\r\n\r\nI'm moving first steps, and the main hurdle is the creation of a Lightning module, and in particular coding the `training_step`.\r\n\r\nWhat I came up so far is \r\n\r\n```python\r\nclass LitTextClassifier(pl.LightningModule):\r\n    def __init__(self, num_class, criterion = CrossEntropyLoss):\r\n        super().__init__()\r\n        self.embedding = nn.EmbeddingBag(VOCAB_SIZE, EMBED_DIM, sparse=False)\r\n        self.fc = nn.Linear(EMBED_DIM, num_class)\r\n        self.init_weights()\r\n        self.criterion = criterion\r\n\r\n    def init_weights(self):\r\n        initrange = 0.5\r\n        self.embedding.weight.data.uniform_(-initrange, initrange)\r\n        self.fc.weight.data.uniform_(-initrange, initrange)\r\n        self.fc.bias.data.zero_()\r\n\r\n    def forward(self, text, offsets):\r\n        embedded = self.embedding(text, offsets)\r\n        return self.fc(embedded)\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = optim.SGD(self.parameters(), lr=4.0)\r\n        return optimizer\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # I am messing up things here\r\n        text, offsets, cls = batch\r\n        output = self.forward(text, offsets)\r\n        loss = self.criterion(output, cls)\r\n\r\n        return loss\r\n```\r\n\r\nI think I am getting the `training_step` wrong. Can someone provide guidance here?\r\nA full gist to reproduce code + errors I get is here: https://gist.github.com/davidefiocco/3b6c6b1e09c4f664b3a73e5bf24d1668/5aa4c224f7772db835bbaa92d559837c7a40f4df@davidefiocco Hi, I think you're trying to instantiate the criterion class with `output` and `cls`. You need to instantiate it in advance:\r\n```diff\r\n-        self.criterion = criterion\r\n+        self.criterion = criterion()\r\n```",
    "meta": {
      "name": "How to \u201clightninfy\u201d the official PyTorch sentiment analysis tutorial?"
    },
    "answer": "@davidefiocco Hi, I think you're trying to instantiate the criterion class with `output` and `cls`. You need to instantiate it in advance:\r\n```diff\r\n-        self.criterion = criterion\r\n+        self.criterion = criterion()\r\n```"
  },
  {
    "content": "On a Colab TPU, I get the error:\r\n\r\n```\r\nFile \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 42, in overwrite_by_env_vars\r\n    return fn(self, **kwargs)\r\nTypeError: __init__() got an unexpected keyword argument 'num_tpu_cores'\r\n```\r\n\r\nI'm running a script that is in my Google Drive. I copy the data up onto Colab. When I do the exact same process for the GPU, it works (with Colab set to GPU, and the appropriate trainer parameter adjustment). \r\n\r\nThe trainer instantiation is simply\r\n\r\n```\r\n    trainer = pl.Trainer(logger=logger,\r\n                         num_tpu_cores=8,\r\n                         fast_dev_run=False,\r\n                         max_epochs=20)\r\n```\r\n@naraugialusru `num_tpu_cores` should be `tpu_cores`. `num_tpu_cores` was removed in #2760. See also: [the docs](https://pytorch-lightning.readthedocs.io/en/1.2.1/common/trainer.html#tpu-cores)",
    "meta": {
      "name": "TPU on Colab: unexpected keyword argument 'num_tpu_cores'"
    },
    "answer": "@naraugialusru `num_tpu_cores` should be `tpu_cores`. `num_tpu_cores` was removed in #2760. See also: [the docs](https://pytorch-lightning.readthedocs.io/en/1.2.1/common/trainer.html#tpu-cores)"
  },
  {
    "content": "Hi, \r\nI am getting this weird error; I was able to run my code before and today I got this: \r\nimport pytorch_lightning as pl                                              \r\n\"~/dir/miniconda3/envs/pytorchenv/lib/python3.7/site-packages/pytorch_lightning/__init__.py\", line 66, in <module>\r\nfrom pytorch_lightning import metrics                                       \r\nImportError: cannot import name 'metrics' from 'pytorch_lightning' \r\n\r\n@mshooter Hi, could you try reinstalling it and running it again? I didn't experience the issue with the following command on Google Colab:\r\n```python\r\n!pip install pytorch-lightning --upgrade\r\nfrom pytorch_lightning import metrics\r\n```\r\n If the problem persists, could you run the following commands and share the output?\r\n```console\r\n$ wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py\r\n$ python collect_env_details.py\r\n```",
    "meta": { "name": "Error importing pytorch lighting" },
    "answer": "@mshooter Hi, could you try reinstalling it and running it again? I didn't experience the issue with the following command on Google Colab:\r\n```python\r\n!pip install pytorch-lightning --upgrade\r\nfrom pytorch_lightning import metrics\r\n```\r\n If the problem persists, could you run the following commands and share the output?\r\n```console\r\n$ wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py\r\n$ python collect_env_details.py\r\n```"
  },
  {
    "content": "## \ud83d\udc1b Bug\r\n\r\n```\r\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your \r\nmodule has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the \r\nkeyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` \r\nfunction outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel \r\nmodule wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss \r\nfunction and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\r\n```\r\n\r\n\r\nwhen I use pt 1.0.8, my model is ok, but when I switch to 1.1.4, it throws this error.  It seems 1.0.8 enable unused parameters by default, but 1.1.4 not.  How to solve this problem.\r\n\r\nI think switch `find_unused_parameters=True` by default to `False` is a breaking change, but in docs, it doesn't mention, yet no clear instructions to set to `True` .i ran into the same issue. any ideas? ```\r\n    class MyDDP(DDPPlugin):\r\n\r\n        def configure_ddp(self, model, device_ids):\r\n            model = LightningDistributedDataParallel(model, device_ids, find_unused_parameters=True)\r\n            return model\r\n\r\n    my_ddp = MyDDP()\r\n\r\n    trainer = Trainer.from_argparse_args(hparams,\r\n                                         callbacks = [lr_logger, checkpoint_callback],\r\n                                         plugins = [my_ddp])\r\n```\r\nthis is my workaround. but hasn't tested whether it will influence my  model precision.\r\nxiadingZ, does your solution go into PyTorch lightning or as an application?No need to subclass `DDPPlugin`. This is enough:\r\n\r\n```python\r\ntrainer = pl.Trainer(plugins=[DDPPlugin(find_unused_parameters=True)])\r\n```\r\n\r\nWhich is used here:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/0a50bb406fa41dfa6a0e2be52f531a9c81c87d00/pytorch_lightning/plugins/ddp_plugin.py#L66-L68\r\n\r\nSorry for the inconvenience!",
    "meta": { "name": "how to set find_unused_parameters=True?" },
    "answer": "No need to subclass `DDPPlugin`. This is enough:\r\n\r\n```python\r\ntrainer = pl.Trainer(plugins=[DDPPlugin(find_unused_parameters=True)])\r\n```\r\n\r\nWhich is used here:\r\n\r\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/0a50bb406fa41dfa6a0e2be52f531a9c81c87d00/pytorch_lightning/plugins/ddp_plugin.py#L66-L68\r\n\r\nSorry for the inconvenience!"
  },
  {
    "content": "Hi, I\u2019m trying to apply CNN to each image in a video. Currently, my implementation uses a for loop and torch.cat where I take each image and apply the CNN module in the loop. But clearly, this is sequential and I don\u2019t see why it can\u2019t be parallelized in theory since all images are independent from each other.\r\n\r\nHowever, I\u2019m not sure how this can be accomplished. I couldn\u2019t find any built-in function for PyTorch. Is there a way to do this in parallel in PyTorch Lightning?\r\n\r\nMy video input shape looks like this: (batch_size, seq_len, channel, height, width) and CNN takes input shape of (batch_size, channel, height, width).\r\n\r\nThanks in advance for your help!You can simply convert your `(batch_size, seq_len, channel, height, width)` tensor into an `(batch_size*seq_len, channel, height, width)` tensor, run your model and then reshape your output back:\r\n``` python \r\nbatch_size, seq_len, channel, height, width = 5, 10, 3, 28, 28 # just random picked\r\ninput = torch.randn(batch_size, seq_len, channel, height, width)\r\ninput = input.reshape(batch_size * seq_len, channel, height, width)\r\noutput = model(input) \r\n# split the batch dimension back into the original batch size and sequence length\r\noutput = output.reshape(batch_size, seq_len, *output.shape[1:])\r\n```\r\n\r\n\r\n\r\n\r\n",
    "meta": {
      "name": "How to apply a nn.Module (i.e. CNN) across an axis (i.e. Video input) in a parallelizable way"
    },
    "answer": "You can simply convert your `(batch_size, seq_len, channel, height, width)` tensor into an `(batch_size*seq_len, channel, height, width)` tensor, run your model and then reshape your output back:\r\n``` python \r\nbatch_size, seq_len, channel, height, width = 5, 10, 3, 28, 28 # just random picked\r\ninput = torch.randn(batch_size, seq_len, channel, height, width)\r\ninput = input.reshape(batch_size * seq_len, channel, height, width)\r\noutput = model(input) \r\n# split the batch dimension back into the original batch size and sequence length\r\noutput = output.reshape(batch_size, seq_len, *output.shape[1:])\r\n```\r\n\r\n\r\n\r\n\r\n"
  },
  {
    "content": "I have a UNet model. I'm trying for a regression model since, in my output, I have different floating values for each pixel. In order to check the r2score, I tried to put the below code in the 'model class', training_step, validation_step, and test_step. \r\n\r\n`from pytorch_lightning.metrics.functional import r2score`\r\n`r2 = r2score(logits, y)`\r\n`self.log('r2:',r2)`\r\n\r\nBut it's giving the following error\r\n\r\n> ValueError: Expected both prediction and target to be 1D or 2D tensors, but recevied tensors with dimension torch.Size([50, 1, 32, 32])\r\n\r\nHow can I check my model fit?I assume that you just want to calculate the total score, and in that case you should simply flatten your input before calculating the score:\r\n``` python\r\nfrom pytorch_lightning.metrics.functional import r2score\r\nr2 = r2score(logits.flatten(), y.flatten())\r\nself.log('r2:',r2)\r\n```",
    "meta": { "name": "To find r2score of my model" },
    "answer": "I assume that you just want to calculate the total score, and in that case you should simply flatten your input before calculating the score:\r\n``` python\r\nfrom pytorch_lightning.metrics.functional import r2score\r\nr2 = r2score(logits.flatten(), y.flatten())\r\nself.log('r2:',r2)\r\n```"
  },
  {
    "content": "If you still can't find what you need:     \r\n#### What is your question?    \r\n\r\nI think it's unclear how the training data is split into a training and validation split in the minimal example.\r\n\r\n(https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#minimal-example)\r\n\r\nDoes this example use all training data for both training and validation? As far as I'm aware, this is bad practice. \r\n\r\nIs there some magic background process which compares the training and validation data loaders and does splitting? I skimmed through the code and couldn't find anything.\r\n> Is there some magic background process which compares the training and validation data loaders and does splitting? I skimmed through the code and couldn't find anything.\r\n\r\nNo, I don't think this is happening. The dataset in the minimal example is the MNIST dataset, which only has two splits (train and test). In this example, the validation set is the same as the training set and if we wanted to split it, we would have to use something like [torch.utils.random_split](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split).exactly! it\u2019s meant to show how lightning works, not how validation works :)Should a training/validation split be something pytorch-lightning automates for you? Has this been considered as a feature?The [data preparation section](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.html#data-preparation) (on the same page as the minimal example) describes how one would split the datasets into train/test/split.\r\n\r\nI don't think it's something that should be automated by pytorch-lightning since you can easily implement a split in `setup()`> \r\n> \r\n> The [data preparation section](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.html#data-preparation) (on the same page as the minimal example) describes how one would split the datasets into train/test/split.\r\n> \r\n> I don't think it's something that should be automated by pytorch-lightning since you can easily implement a split in `setup()`\r\n\r\nthe link does not work.\r\n\r\ndid you mean [this?](https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html)\r\n",
    "meta": { "name": "Training/Validation split in minimal example" },
    "answer": "> Is there some magic background process which compares the training and validation data loaders and does splitting? I skimmed through the code and couldn't find anything.\r\n\r\nNo, I don't think this is happening. The dataset in the minimal example is the MNIST dataset, which only has two splits (train and test). In this example, the validation set is the same as the training set and if we wanted to split it, we would have to use something like [torch.utils.random_split](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split)."
  },
  {
    "content": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI'm trying to run the LitAutoEncoder on TPUs, but the code runs for 1 epoch and gets stuck there.\r\n\r\n#### Code\r\n\r\n```\r\nclass LitAutoEncoder(pl.LightningModule):\r\n\r\n    def __init__(self, hparams):\r\n        super().__init__()\r\n        self.hparams = hparams\r\n        self.encoder = nn.Sequential(\r\n            nn.Linear(28*28, 64),\r\n            nn.ReLU(),\r\n            nn.Linear(64, 3)\r\n        )\r\n        self.decoder = nn.Sequential(\r\n            nn.Linear(3, 64),\r\n            nn.ReLU(),\r\n            nn.Linear(64, 28*28)\r\n        )\r\n\r\n    def forward(self, x):\r\n        # in lightning, forward defines the prediction/inference actions\r\n        embedding = self.encoder(x)\r\n        return embedding\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defined the train loop.\r\n        # It is independent of forward\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = F.mse_loss(x_hat, x)\r\n        # Logging to TensorBoard by default\r\n        self.log('train_loss', loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\ndataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\r\ntrain_loader = DataLoader(dataset, drop_last=True, batch_size=32)\r\n\r\nargs_dict = dict(\r\n    num_train_epochs=1,\r\n    seed=42,\r\n)\r\n\r\nargs = argparse.Namespace(**args_dict)\r\nautoencoder = LitAutoEncoder(args)\r\n\r\ntrain_params = dict(\r\n    tpu_cores=8,\r\n    progress_bar_refresh_rate=30,\r\n)\r\n\r\ntrainer = pl.Trainer(**train_params)\r\ntrainer.fit(autoencoder, train_loader)\r\n\r\n```\r\n#### Reproducible Colab Notebook\r\n\r\n[Notebook](https://colab.research.google.com/drive/1wDIqHsrl5hji4fWlBGznJ3lXqOlxqKfn?usp=sharing)\r\n\r\n#### What's your environment?\r\n\r\n - Colab\r\n - Packaging pip\r\n - pytorch-1.7\r\n - pytorch-lightning-1.1.5Could you check again? I just ran your colab and it finished both epochs. Maybe it's random? So far did not see random behaviourThanks Adrian,\n\nSorry I've been editing the notebook. It's running now, but not sure what\nthe issue was. My bad I ended up editing the notebook and error was not\nreproducible anymore. Will update it back to the old version.\n\nOn Sun, Jan 24, 2021 at 12:00 AM Adrian W\u00e4lchli <notifications@github.com>\nwrote:\n\n> Could you check again? I just ran your colab and it finished both epochs.\n> Maybe it's random? So far did not see random behaviour\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/PyTorchLightning/pytorch-lightning/issues/5625#issuecomment-766157230>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AGX3GHX6NN62CNNLKPFB6O3S3MIVXANCNFSM4WPSSWOA>\n> .\n>\n",
    "meta": { "name": "Code stuck after running 1 epoch on TPU" },
    "answer": "Thanks Adrian,\n\nSorry I've been editing the notebook. It's running now, but not sure what\nthe issue was. My bad I ended up editing the notebook and error was not\nreproducible anymore. Will update it back to the old version.\n\nOn Sun, Jan 24, 2021 at 12:00 AM Adrian W\u00e4lchli <notifications@github.com>\nwrote:\n\n> Could you check again? I just ran your colab and it finished both epochs.\n> Maybe it's random? So far did not see random behaviour\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/PyTorchLightning/pytorch-lightning/issues/5625#issuecomment-766157230>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AGX3GHX6NN62CNNLKPFB6O3S3MIVXANCNFSM4WPSSWOA>\n> .\n>\n"
  },
  {
    "content": "Weights summary gets printed when Trainer calls `fit()`, but the output is not persistent as I have everything wrapped up in ray tune, which overwrites the contents of the output cell in jupyter\r\n\r\nIs there something we can call to manually print the weights summary of a particular model without having to fit the model every time?just do \r\n```python\r\nmodel = YourLightningModule(...)\r\nmodel.summarize(mode=...)\r\n```\r\n`mode='top', 'full'`",
    "meta": { "name": "Print weights summary" },
    "answer": "just do \r\n```python\r\nmodel = YourLightningModule(...)\r\nmodel.summarize(mode=...)\r\n```\r\n`mode='top', 'full'`"
  },
  {
    "content": "When [Precision](https://pytorch-lightning.readthedocs.io/en/stable/metrics.html#precision) and [Recall](https://pytorch-lightning.readthedocs.io/en/stable/metrics.html#recall) are directly computed, I get the following result:\r\n\r\n```python\r\nimport torch\r\nfrom pytorch_lightning.metrics import Precision\r\nfrom pytorch_lightning.metrics import Recall\r\n\r\ny = torch.tensor([0, 0, 2, 2, 1, 1, 1, 2, 0, 0])\r\ny_hat = torch.tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1])\r\n\r\nprecision = Precision(num_classes=3)\r\nrecall = Recall(num_classes=3)\r\nprecision(y_hat, y)\r\n#>>>tensor(0.2917)\r\nrecall(y_hat, y)\r\n#>>>tensor(0.4444)\r\n```\r\n\r\nHowever, when the same metrics are computed over `validation_step`, I get the following stranger result:\r\n\r\n```python\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch[\"x\"], batch[\"y\"] # y = tensor([0, 0, 2, 2, 1, 1, 1, 2, 0, 0], device='cuda:0')\r\n        y_hat = self(x) # y_hat  = tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1], device='cuda:0')\r\n\r\n        precision = self.precision_score(y_hat, y) # precision =  tensor(0.4000, device='cuda:0')\r\n        recall = self.recall_score(y_hat, y) # recall = tensor(0.4000, device='cuda:0')\r\n\r\n```\r\n\r\nwhat am I missing?@Ceceu can you provide code to reproduce? seems weird to me@Ceceu after trying myself, I assume you have set the `average` argument in the first case to `macro` and in the second case to `micro` (default):\r\n\r\n``` python\r\ny = torch.tensor([0, 0, 2, 2, 1, 1, 1, 2, 0, 0])\r\ny_hat = torch.tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1])\r\n\r\nprecision = Precision(num_classes=3, average='macro')\r\nrecall = Recall(num_classes=3, average='macro')\r\nprint(precision(y_hat, y), recall(y_hat, y)) # tensor(0.2917), tensor(0.4444)\r\n\r\nprecision = Precision(num_classes=3, average='micro')\r\nrecall = Recall(num_classes=3, average='micro')\r\nprint(precision(y_hat, y), recall(y_hat, y)) # tensor(0.4000), tensor(0.4000)\r\n```> @Ceceu after trying myself, I assume you have set the `average` argument in the first case to `macro` and in the second case to `micro` (default):\r\n> \r\n> ```python\r\n> y = torch.tensor([0, 0, 2, 2, 1, 1, 1, 2, 0, 0])\r\n> y_hat = torch.tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1])\r\n> \r\n> precision = Precision(num_classes=3, average='macro')\r\n> recall = Recall(num_classes=3, average='macro')\r\n> print(precision(y_hat, y), recall(y_hat, y)) # tensor(0.2917), tensor(0.4444)\r\n> \r\n> precision = Precision(num_classes=3, average='micro')\r\n> recall = Recall(num_classes=3, average='micro')\r\n> print(precision(y_hat, y), recall(y_hat, y)) # tensor(0.4000), tensor(0.4000)\r\n> ```\r\n\r\nHello @SkafteNicki,\r\nSorry for being late.\r\nYou are completely right, this was the reason for that behavior.\r\n\r\nThanks!Nice, closing this issue. ",
    "meta": { "name": "Precision and Recall over validation step" },
    "answer": "@Ceceu after trying myself, I assume you have set the `average` argument in the first case to `macro` and in the second case to `micro` (default):\r\n\r\n``` python\r\ny = torch.tensor([0, 0, 2, 2, 1, 1, 1, 2, 0, 0])\r\ny_hat = torch.tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1])\r\n\r\nprecision = Precision(num_classes=3, average='macro')\r\nrecall = Recall(num_classes=3, average='macro')\r\nprint(precision(y_hat, y), recall(y_hat, y)) # tensor(0.2917), tensor(0.4444)\r\n\r\nprecision = Precision(num_classes=3, average='micro')\r\nrecall = Recall(num_classes=3, average='micro')\r\nprint(precision(y_hat, y), recall(y_hat, y)) # tensor(0.4000), tensor(0.4000)\r\n```"
  },
  {
    "content": "When using pytorch_lightning.tuner.lr_finder.lr_find, ddp have some error. So i change to dp using 4 gpus. Can the learning rate find by dp used by ddp? They have same gpu numbers.I think the answer is no. \r\n\r\nDP doesn't change your effective batch size while DDP does (in your case with one node and 4GPUs, the effective batch size is 4 times bigger with DDP). You can find more info about the effective batch size in the \"multi-GPU\" section of Lightning's documentation [here](https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html).\r\n\r\nAs a consequence of this, you should probably increase your learning rate. Rule of thumbs is to increase it linearly (so by 4) but there is more than just doing that. Have a look at that paper: [https://arxiv.org/pdf/1706.02677.pdf](https://arxiv.org/pdf/1706.02677.pdf)OK\uff0cthanks a lot.",
    "meta": {
      "name": "Can the learning rate find by dp using one more gpu be used in ddp?"
    },
    "answer": "I think the answer is no. \r\n\r\nDP doesn't change your effective batch size while DDP does (in your case with one node and 4GPUs, the effective batch size is 4 times bigger with DDP). You can find more info about the effective batch size in the \"multi-GPU\" section of Lightning's documentation [here](https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html).\r\n\r\nAs a consequence of this, you should probably increase your learning rate. Rule of thumbs is to increase it linearly (so by 4) but there is more than just doing that. Have a look at that paper: [https://arxiv.org/pdf/1706.02677.pdf](https://arxiv.org/pdf/1706.02677.pdf)"
  },
  {
    "content": "I am using Pytorch Lightning in an RL setting and want to save a model when it hits a new max average reward. I am using the Tensorboard logger where I return my neural network loss in the `training_step()` using: \r\n\r\n```\r\nlogs = {\"policy_loss\": pred_loss}\r\nreturn {'loss':pred_loss, 'log':logs}\r\n```\r\n\r\nAnd then I am saving my RL environment rewards using in `on_epoch_end()`: \r\n\r\n```\r\nself.logger.experiment.add_scalar(\"mean_reward\", np.mean(reward_losses), self.global_step)\r\nself.logger.experiment.add_scalars('rollout_stats', {\"std_reward\":np.std(reward_losses),\r\n                \"max_reward\":np.max(reward_losses), \"min_reward\":np.min(reward_losses)}, self.global_step)\r\n```\r\n\r\nAnd every 5 epochs I am also writing out another RL reward loss where I use the best actions rather than sampling from them: \r\n```\r\nif self.current_epoch % self.hparams['eval_every']==0 and self.logger:\r\n            output = self.collect_rollouts(greedy=True, num_episodes=self.hparams['eval_episodes'])\r\n            reward_losses = output[0]\r\n            self.logger.experiment.add_scalar(\"eval_mean\", np.mean(reward_losses), self.global_step)\r\n```\r\n\r\nMy question is, how can I set my ModelCheckpoint to monitor `eval_mean` (which is only written out every 5 epochs, this seems like it would be a problem)? I would also settle for monitoring `mean_reward` (written out every epoch)? Right now I can only successfully monitor `policy_loss` which does not always correspond to higher rewards obtained (setting monitor = to anything else throws an error).\r\n\r\nI know that in the new PL version `self.log()` should be used but after re-writing my code using this it still didn't solve my issue. \r\n\r\nI have spent a lot of time looking through the docs and for examples of this but I have found the logging docs on this to be quite sparse and difficult to even get everything to log in the first place. \r\n\r\nI am using Pytorch Lightning 1.0.5 and Pytorch 1.7.0.\r\n\r\nThank you for any help/guidance. \r\n\r\nHi! thanks for your contribution!, great first issue!I have multiple comments that I did not verify yet but they might help\r\n\r\n- If I'm not mistaken, `self.log` only works within a selection of hooks currently. I suggest you try to move the relevant code to `training_epoch_end` where `self.log` should work correctly.\r\n- set the monitor key in the `ModelCheckpoint(monitor=)` explicitly. \r\n- You have the problem that you can only update/log every n epochs: I see two solutions: 1) synchronize your ModelCheckpoint with the `period` parameter to only run on the epochs you update the monitor quantity. 2) Cache the last value and log it in the epochs between your regular interval, to make the ModelCheckpoint see it as unchanged. The second option may even be the default behavior by Lightning but need to verify.\r\n\r\nSo in summary, I imagine something like this:\r\n```python\r\n\r\n# Model\r\n\r\ndef training_epoch_end(self, outputs):\r\n    # ... compute reward losses\r\n    \r\n    if self.current_epoch % self.hparams['eval_every']==0:\r\n        self.last_eval_mean = # compute the new eval mean\r\n\r\n     self.log(\"eval_mean\", self.last_eval_mean)\r\n\r\n\r\n# Trainer\r\ntrainer = Trainer(callbacks=[ModelCheckpoint(monitor=\"eval_mean\")]\r\n\r\n# or maybe also try\r\ntrainer = Trainer(callbacks=[ModelCheckpoint(monitor=\"eval_mean\", period=self.hparams['eval_every'])]\r\n```Thanks for this all of this. It sounds like the fundamental problem may be that with my code I was not logging from `training_epoch_end()`? Because I was setting `monitor=` to explicitly track either `eval_mean` or `mean_reward` but they wouldn't be detected. \r\n\r\nI will try this and let you know if it works. I had a very similar issue: in my reinforcement learning framework I wanted to measure the validation performance of my agent. Of course I would do so without a `validation_dataloader`, hence I thought I could just set that dataloader to `None` and define a `validation_step` myself. Unfortunately, given a `validation_dataloader` that is `None`, `validation_step` and all validation methods are not called at all.\r\nI tried to solve this via a callback to `on_train_epoch_end` or `on_epoch_end`. This worked, but in those callbacks the `self.log()` call does not work at all - most importantly, there is no feedback from pytorch_lightning that the call was unsuccesfull. Luckily enough, I found this thread here and moved my validation code into `training_epoch_end`, which works.\r\n\r\nMaybe pytorch_lightning could at least give a warning once one tries to use `self.log()` in a place where it has no effect?Regarding the `self.log()` from callbacks, @tchaton was working on this in #3813 and it should now be working.",
    "meta": {
      "name": "Logging RL results and tracking them with ModelCheckpoint(monitor=...)"
    },
    "answer": "I have multiple comments that I did not verify yet but they might help\r\n\r\n- If I'm not mistaken, `self.log` only works within a selection of hooks currently. I suggest you try to move the relevant code to `training_epoch_end` where `self.log` should work correctly.\r\n- set the monitor key in the `ModelCheckpoint(monitor=)` explicitly. \r\n- You have the problem that you can only update/log every n epochs: I see two solutions: 1) synchronize your ModelCheckpoint with the `period` parameter to only run on the epochs you update the monitor quantity. 2) Cache the last value and log it in the epochs between your regular interval, to make the ModelCheckpoint see it as unchanged. The second option may even be the default behavior by Lightning but need to verify.\r\n\r\nSo in summary, I imagine something like this:\r\n```python\r\n\r\n# Model\r\n\r\ndef training_epoch_end(self, outputs):\r\n    # ... compute reward losses\r\n    \r\n    if self.current_epoch % self.hparams['eval_every']==0:\r\n        self.last_eval_mean = # compute the new eval mean\r\n\r\n     self.log(\"eval_mean\", self.last_eval_mean)\r\n\r\n\r\n# Trainer\r\ntrainer = Trainer(callbacks=[ModelCheckpoint(monitor=\"eval_mean\")]\r\n\r\n# or maybe also try\r\ntrainer = Trainer(callbacks=[ModelCheckpoint(monitor=\"eval_mean\", period=self.hparams['eval_every'])]\r\n```"
  },
  {
    "content": "Question regarding logging from lightning module in DDP model\r\n\r\nFor example, here is a validation step function that computes accuracy.\r\n\r\n```python\r\ndef validation_step(self, batch, batch_idx):\r\n       x, y = batch\r\n       logits = self(x)\r\n       acc = acc_fun(logits,y)     \r\n       self.log('val_acc', acc)\r\n```\r\n\r\nSo what happens in DDP, is the logged value averaged across GPUs? At the end of every epoch? \r\nIt is averaged if you set `sync_dist=True` (https://pytorch-lightning.readthedocs.io/en/stable/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.log.params.sync_dist)\r\n\r\nYou can change the op from average (mean) to something else with `sync_dist_op`.\r\n\r\nGiven that you are logging inside of `validation_step`, it will be done at the end of the epoch by default. You can change this behaviour via `on_step` and `on_epoch`.\r\n\r\nPlease have a look at the docs page I linked. Lots of useful info there :smile: Thanks. I missed the sync_dist flag. ",
    "meta": { "name": "logging question in DDP" },
    "answer": "It is averaged if you set `sync_dist=True` (https://pytorch-lightning.readthedocs.io/en/stable/lightning_module.html#pytorch_lightning.core.lightning.LightningModule.log.params.sync_dist)\r\n\r\nYou can change the op from average (mean) to something else with `sync_dist_op`.\r\n\r\nGiven that you are logging inside of `validation_step`, it will be done at the end of the epoch by default. You can change this behaviour via `on_step` and `on_epoch`.\r\n\r\nPlease have a look at the docs page I linked. Lots of useful info there :smile: "
  },
  {
    "content": "## \u2753 Questions and Help\r\n\r\n#### Problem\r\n\r\nJupyter terminal freezes, and connection to AWS node closes. \r\nThe problem is reproducible with any Lightning example.\r\n\r\n#### What have you tried?\r\n\r\n`python pl_examples/basic_examples/image_classifier.py --gpus 4 --accelerator ddp`\r\n\r\n#### What's your environment?\r\n\r\nLinux using docker image\r\nLightning 1.0.4\r\npytorch 1.6 \r\ncuda 10.2\r\nThe solution was to use \r\n\r\n`export NCCL_SOCKET_IFNAME=lo`",
    "meta": { "name": "DDP NCCL freezes in docker AWS Jupyter" },
    "answer": "The solution was to use \r\n\r\n`export NCCL_SOCKET_IFNAME=lo`"
  },
  {
    "content": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nThis is about DDP specifics and about the handling of functions in a script that we only want executed once (not for every GPU).\r\n\r\nI think they are missing from the [doc](https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-data-parallel) and I couldn't find answers elsewhere. I apologize if they have been covered already.\r\n\r\n**Questions:**\r\n1. Does the regular `torch.save(model.state_dict(), path)` call work normally or does DDP complicate things for multiple GPUS?\r\n2. Does DDP run **all** the functions of a script for every GPU? For example, in the following script will the `delete_and_remake_folder` function be executed multiple times (that would result in conflicts)? Is there a way to specify functions to be run only once?\r\n3. Am I correct that `sync_batchnorm=True` and `precision=16` work in DDP?\r\n4. Does the `trainer.test()` function automatically aggregate results accross devices or is it required to set `self.log(loss,  sync_dist=True)` in the model?\r\n5. Am I correct in assuming that, if we set `num_workers=X` in a Dataloader, the actual CPU core usage will be X*N for N GPUS?\r\n\r\n\r\nQuestions 1-4 are summarized in the following script and whether it works/can work.\r\n```\r\ndef main():\r\n   delete_and_remake_folder() # I only want to run once\r\n   model = Model()\r\n   trainer = Trainer(gpus = 8, backend='ddp, sync_batchnorm=True, precision=16)\r\n   trainer.fit()\r\n   trainer.test()\r\n   torch.save(model.pt_model.state_dict(), save_dir) # I probably only want to run once (?)\r\n```1. yes, but you might run into the issue in your 2nd question\r\n2. yes, yes `if self.global_rank == 0: do stuff`\r\n3. yes\r\n4. no, yes (think of it like another val loop)\r\n5. yes\r\n\r\nI'll close this for now, if you have more questions please use our forums! https://forums.pytorchlightning.ai/",
    "meta": {
      "name": "DDP specifics: Single node function execution, test loss sync, num_workers, sync_batchnorm, precision"
    },
    "answer": "1. yes, but you might run into the issue in your 2nd question\r\n2. yes, yes `if self.global_rank == 0: do stuff`\r\n3. yes\r\n4. no, yes (think of it like another val loop)\r\n5. yes\r\n\r\nI'll close this for now, if you have more questions please use our forums! https://forums.pytorchlightning.ai/"
  },
  {
    "content": "I have a need to use a custom `[DistributedDataParallel](https://pytorch.org/docs/stable/notes/ddp.html)` implementation. I'd like to do this with the existing `DDPBackend` today.\r\n\r\nWith Lightning, the API and docs are unclear as to whether I need to extend `LightningDistributedDataParallel`, or if I can directly extend torch `DistributedDataParallel`.\r\n\r\nThe LightningModule API docs suggest `configure_ddp` should work with torch `DistributedDataParallel`: https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html#configure-ddp\r\n\r\nHowever, there are spots in Lightning which rely on checking isinstance of the custom Lightning overrides: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/model_connector.py#L31-L34 1\r\n\r\nThe Lightning DDP also forwards calls to train/val/test step. Is this a requirement for custom DDP implementations when used with Lightning?\r\n\r\nTLDR: should I subclass `LightningDistributedDataParallel` or `DistributedDataParallel` when I implement the model hook for `configure_ddp`? \r\n\r\nAlso asked here: https://forums.pytorchlightning.ai/t/expectations-for-custom-data-parallel-implementations/162Almost certainly you need to inherit LightningDistributedDataParallel for the reasons you have already mentioned. As @williamFalcon  already explained, this is probably not what should be done today. I have come across this as well. There are libraries like torchgeometry that have a custom [DataParallel](https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/data_parallel.py) class and I believe it is currently not possible to just drop that into Lightning, because of the reasons you have already mentioned.@awaelchli it seems like the workaround for now is to extend `LightningDistributedDataParallel` while the accelerator backend refactors are happening. We should update the code and docs to reflect that: https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#configure-ddp . Right now they're misleading and will break because of those checks. Though I'm pretty sure you can check if `isinstance(model, DistributedDataParallel)` to grab the module reference and it'll work. What happens if a LightningModule doesn't implement `forward`? Would this raise an exception?@williamFalcon PTAL@ananthsub Not sure if you have seen it already, there is now a DDP Plugin in which you can override the DistributedDataParallel. It can be passed into the trainer via plugins list. \r\nhttps://pytorch-lightning.readthedocs.io/en/latest/plugins.html?highlight=DDPPlugin\r\nyup I saw. that resolves this issue \ud83d\udc4d ",
    "meta": { "name": "Expectations for custom data parallel implementations" },
    "answer": "@ananthsub Not sure if you have seen it already, there is now a DDP Plugin in which you can override the DistributedDataParallel. It can be passed into the trainer via plugins list. \r\nhttps://pytorch-lightning.readthedocs.io/en/latest/plugins.html?highlight=DDPPlugin\r\n"
  },
  {
    "content": "I wanted to ask how pytorch handles accuracy (and maybe even loss) logging when we have something like `pl.Trainer(accumulate_grad_batches=ACCUMULATIONS)`.\r\n\r\nMy training looks like this:\r\n```python\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y, weight=self.weight)\r\n        result = pl.TrainResult(loss)\r\n        result.log(\"train_loss\", loss, prog_bar=True)\r\n        result.log(\"train_accuracy\", self.accuracy(y_hat.argmax(dim=-1), y), prog_bar=True)\r\n\r\n        return result\r\n```\r\nwhere `self.accuracy = pl.metrics.classification.Accuracy()`. Is there a way to make sure that the loss and accuracy is averaged across the accumulated batches?\r\n\r\nIf this is not currently the case, I'm happy to do a PR if someone can show me where to look in the source code to make such a change.\r\n\r\nThanks in advanceHi @sachinruk \r\nNot sure about accuracy or some other metrics, but I can confirm that `loss` is averaged across `accumulate_grad_batches`looking at the progress bar, it seems like the loss and train_loss as seen above are two (slightly) different numbers. And yes loss seems to be working as expected, mainly metrics Im worried about.@sachinruk Class based metrics have been revamped! Please checkout the documentation for the new interface.\r\nWhile the metrics package does not directly integrate with the `accumulate_grad_batches` argument (yet), you should be able to do something like this now:\r\n``` python\r\ndef training_step(self, batch, batch_idx):\r\n    x, y = batch\r\n    y_hat = self(x)\r\n\r\n    self.accuracy.update(y_hat.argmax(dim=-1), y)\r\n    if self.trainer.accumulate_grad_batches % self.global_step == 0:\r\n        accumulated_val = self.accuracy.compute()\r\n        self.log('acc_accumulate', accumulated_val)\r\n    ...\r\n```\r\nClosing this for now.",
    "meta": { "name": "Logging accuracy with batch accumulation" },
    "answer": "@sachinruk Class based metrics have been revamped! Please checkout the documentation for the new interface.\r\nWhile the metrics package does not directly integrate with the `accumulate_grad_batches` argument (yet), you should be able to do something like this now:\r\n``` python\r\ndef training_step(self, batch, batch_idx):\r\n    x, y = batch\r\n    y_hat = self(x)\r\n\r\n    self.accuracy.update(y_hat.argmax(dim=-1), y)\r\n    if self.trainer.accumulate_grad_batches % self.global_step == 0:\r\n        accumulated_val = self.accuracy.compute()\r\n        self.log('acc_accumulate', accumulated_val)\r\n    ...\r\n```\r\nClosing this for now."
  },
  {
    "content": "Having refactored my code to avoid iterable datasets I've now got DDP training working (I also had to set ulimit to prevent another crash). However now it crashes at the test step.\r\n\r\nThe message implies DDP is not needed for testing - but I don't see any mention in the documentation of how to disable DDP once training has complete (plus I would assume that trainer.test() would do this if it were required).\r\n\r\nIs there something I should be doing different - this is my train / test code - the test dateloader has batchsize=1\r\n\r\n```\r\n    trainer.fit(transformer, datamodule=dm)\r\n    transformer.freeze()\r\n\r\n    # run tests\r\n    result = trainer.test(transformer, datamodule=dm)\r\n```\r\n\r\n> \r\n> -- Process 0 terminated with the following error:\r\n> Traceback (most recent call last):\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n>     fn(i, *args)\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py\", line 152, in ddp_train\r\n>     model = model.configure_ddp(model, device_ids)\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 837, in configure_ddp\r\n>     model = LightningDistributedDataParallel(model, device_ids=device_ids, find_unused_parameters=True)\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 269, in __init__\r\n>     assert any((p.requires_grad for p in module.parameters())), (\r\n> AssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.\r\n>  So looks like `transformer.freeze()` is the issue - commented that out and it's fine. Is that a bug?Does not look like a but to me. \r\n\r\nThe error seems to come from the underlying PyTorch DistributedDataParallel, not Lightning. \r\nIt seems we can't really do anything. But freezing the model before test should not be needed, test does not alter the weights and puts model into eval mode anyway. Makes sense?Yes that makes sense.",
    "meta": { "name": "Test step with DDP" },
    "answer": "Does not look like a but to me. \r\n\r\nThe error seems to come from the underlying PyTorch DistributedDataParallel, not Lightning. \r\nIt seems we can't really do anything. But freezing the model before test should not be needed, test does not alter the weights and puts model into eval mode anyway. Makes sense?"
  },
  {
    "content": "## \u2753 Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI don't know whether this is a bug...\r\nAs shown in the code below, I think the behavior of `dp` mode is unexpected? (The attribute is reset every batch)\r\nWhen using `ddp` mode, everything is fine. (The property will be initialized only once per GPU)\r\n\r\n#### Code\r\n\r\n```python\r\nimport os\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning import Trainer\r\n\r\nfrom argparse import Namespace\r\n\r\n\r\nclass LitModel(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n        self._dummy_property = None\r\n\r\n    @property\r\n    def dummy_propery(self):\r\n        if self._dummy_property is None:\r\n            self._dummy_property = '*' * 30\r\n            print('print only once per gpu')\r\n        return self._dummy_property\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        print(self._dummy_property)\r\n        # Access every batch\r\n        self.dummy_propery\r\n        print(self._dummy_property)\r\n\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return pl.TrainResult(loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.02)\r\n\r\n\r\ntrain_loader = DataLoader(\r\n    MNIST(\r\n        os.getcwd(),\r\n        download=True,\r\n        transform=transforms.ToTensor()\r\n    ),\r\n    batch_size=128\r\n)\r\ntrainer = pl.Trainer(gpus=2,\r\n                     distributed_backend='dp',\r\n                     max_epochs=2)\r\n\r\nmodel = LitModel()\r\ntrainer.fit(model, train_loader)\r\n```\r\n\r\n\r\n#### Output\r\n\r\n```\r\nNone\r\nprint only once per gpu\r\n******************************\r\nNone\r\nprint only once per gpu\r\n******************************\r\nNone\r\nprint only once per gpu\r\n******************************\r\nNone\r\nprint only once per gpu\r\n******************************\r\n...\r\n```\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What's your environment?\r\n\r\n - Version 0.9.0\r\n@yukw777 mind have a look? \ud83d\udc30 @Borda sure thing! A bit busy right now, but I\u2019ll try to get to it later this week or weekend!oh, this is actually a known problem and comes from DataParallel in PyTorch itself.\r\nSee #565 and #1649 for reference.\r\n@ananyahjha93 has been working on a workaround but it seems to be super non trivial #1895",
    "meta": { "name": "Attribute is reset per batch in `dp` mode" },
    "answer": "oh, this is actually a known problem and comes from DataParallel in PyTorch itself.\r\nSee #565 and #1649 for reference.\r\n@ananyahjha93 has been working on a workaround but it seems to be super non trivial #1895"
  },
  {
    "content": "I am training a  model with lightning where I am attempting to use all the GPUs on my system (4 in total).\r\n\r\nMy trainer is run as:\r\n\r\n```\r\nmodel = MyModel(hparams)\r\nif torch.cuda.is_available():\r\n    trainer = Trainer(gpus=-1)\r\nelse:\r\n    trainer = Trainer()\r\ntrainer.fit(model)\r\n```\r\n\r\nMy model is defined as follows:\r\n\r\n```\r\nclass SiameseNet(pl.LightningModule):\r\n    \"\"\"\r\n    Implement a siamese network as a feature extractor withh Lightning module\r\n    \"\"\"\r\n    def __init__(self,\r\n                 hparams):\r\n        \"\"\"\r\n        Build the network\r\n        \"\"\"\r\n        super(SiameseNet, self).__init__()\r\n        self.net = self._build_net()\r\n        self.hparams = hparams\r\n        self.train_data_path = hparams.get('train_data_path', None)\r\n        self.test_data_path = hparams.get('test_data_path', None)\r\n        self.val_data_path = hparams.get('val_data_path', None)\r\n        self.train_dataset = None\r\n        self.val_dataset = None\r\n        self.test_dataset = None\r\n\r\n        self.lossfn = TripletLoss(margin=1.0)\r\n\r\n    def forward_once(self, x):\r\n        output = self.net(x)\r\n        output = torch.squeeze(output)\r\n        return output\r\n\r\n    def forward(self, input1, input2, input3=None):\r\n        output1 = self.forward_once(input1)\r\n        output2 = self.forward_once(input2)\r\n\r\n        if input3 is not None:\r\n            output3 = self.forward_once(input3)\r\n            return output1, output2, output3\r\n\r\n        return output1, output2\r\n\r\n    @staticmethod\r\n    def _build_net():\r\n        net = nn.Sequential(\r\n            nn.Conv2d(3, 32,kernel_size=3,stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(32),\r\n\r\n            nn.Conv2d(32, 64, kernel_size=3, stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(64),\r\n\r\n            nn.Conv2d(64, 128, kernel_size=3, stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(128),\r\n\r\n            nn.Conv2d(128, 256, kernel_size=1, stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(256),\r\n\r\n            nn.Conv2d(256, 256, kernel_size=1, stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(256),\r\n\r\n            nn.Conv2d(256, 512, kernel_size=3, stride=2),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(512),\r\n\r\n            nn.Conv2d(512, 1024, kernel_size=1, stride=1),\r\n            nn.ReLU(inplace=True),\r\n            nn.BatchNorm2d(1024))\r\n\r\n        return net\r\n\r\n    def prepare_data(self):\r\n        transform = torchvision.transforms.Compose([\r\n            torchvision.transforms.Resize((128, 128)),\r\n            torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\r\n            torchvision.transforms.RandomHorizontalFlip(),\r\n            torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\r\n            torchvision.transforms.ToTensor()\r\n        ])\r\n\r\n        if self.train_data_path:\r\n            train_folder_dataset = dset.ImageFolder(root=self.train_data_path)\r\n            self.train_dataset = SiameseTriplet(image_folder_dataset=train_folder_dataset,\r\n                                                transform=transform)\r\n        if self.val_data_path:\r\n            val_folder_dataset = dset.ImageFolder(root=self.val_data_path)\r\n            self.val_dataset = SiameseTriplet(image_folder_dataset=val_folder_dataset)\r\n\r\n        if self.test_data_path:\r\n            test_folder_dataset = dset.ImageFolder(root=self.test_data_path)\r\n            self.test_dataset = SiameseTriplet(image_folder_dataset=test_folder_dataset)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        anchor, positive, negative = batch\r\n        anchor_out, positive_out, negative_out = self.forward(anchor, positive, negative)\r\n        loss_val = self.lossfn(anchor_out, positive_out, negative_out)\r\n        return {'loss': loss_val}\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = optim.Adam(self.parameters(), lr=self.hparams.get('learning_rate', 0.001))\r\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\r\n        return [optimizer], [scheduler]\r\n\r\n    @pl.data_loader\r\n    def train_dataloader(self):\r\n        if self.train_dataset:\r\n            return DataLoader(self.train_dataset,\r\n                              self.hparams.get('batch_size', 64),\r\n                              num_workers=12)\r\n        return None\r\n```\r\n\r\nWhen I try and run it, it seems the beginning of the epoch hangs for like 10 minutes to get data into the model and after that the progress is  very sluggish.\r\n\r\nI also get these messages in  the beginning. Not sure if it is of concern\r\n\r\n```\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nMASTER_ADDR environment variable is not defined. Set as localhost\r\ninitializing proc_rank 0 world 4\r\nMASTER_ADDR environment variable is not defined. Set as localhost\r\ninitializing proc_rank 1 world 4\r\nMASTER_ADDR environment variable is not defined. Set as localhost\r\ninitializing proc_rank 2 world 4\r\nMASTER_ADDR environment variable is not defined. Set as localhost\r\ninitializing proc_rank 3 world 4\r\n```\r\n\r\nIt basically hangs with this:\r\n```\r\nEpoch 1:   0%|                                                                                          | 0/172 [00:00<?, ?it/s]\r\n```\r\n\r\nDuring this time, looking at GPU  utilisation it seems:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:05:00.0 Off |                  N/A |\r\n| 48%   79C    P2    90W / 250W |   4527MiB / 11176MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\r\n| 45%   76C    P2    85W / 250W |   1636MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |\r\n| 45%   76C    P2    79W / 250W |   1626MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 108...  Off  | 00000000:0A:00.0 Off |                  N/A |\r\n| 32%   65C    P2    79W / 250W |   2689MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1584      C   /home/pd/.conda/envs/alchera37/bin/python    801MiB |\r\n|    0     10714      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\r\n|    0     28957      C   /home/pd/.conda/envs/alchera37/bin/python   1047MiB |\r\n|    0     30880      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n|    0     32266      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n|    1     10733      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\r\n|    1     28972      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n|    2     10789      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\r\n|    2     32297      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n|    3     10807      C   /home/pd/.conda/envs/alchera37/bin/python    543MiB |\r\n|    3     29006      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n|    3     30967      C   /home/pd/.conda/envs/alchera37/bin/python   1063MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nSo, it seems that getting the data into the GPU is quite slow even though everything looks maxed out.\r\n\r\nAnd when it does eventually start the epoch after ~30 minutes, it seems to give similar performance as my CPU on MacBook Pro. I am really not sure if I am doing somethingvery  wrong here in how I am using  PL.update to master. will reopen if not fixedUnfortunately, this seems to be even worst with the current master.\r\n\r\nFirst it gives the error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/pd/Altrack/altrack/apps/train_siamnet.py\", line 3, in <module>\r\n    from ..models.siamnet import SiameseNet\r\nImportError: attempted relative import with no known parent package\r\n```\r\nwhich is incorrect. This did not happen with the  previous version.\r\n\r\nAnd then it gets stuck after this line (at least no console output)\r\n\r\n```\r\ninitializing ddp: LOCAL_RANK: 0/3 WORLD_SIZE:4\r\n```@pamparana34 mind chek the recent 0.9 from master?> And then it gets stuck after this line (at least no console output)\r\n\r\nThis looks very familiar, and I am sure I fixed this problem in #2997, please try again with the latest version. Regarding the relative import error, you probably just launched the script in the wrong directory, but anyway I recommend to use absolute imports. Please let me know if the upgrade fixes your problem, thanks.",
    "meta": { "name": "extremely slow training with multiple GPUs" },
    "answer": "> And then it gets stuck after this line (at least no console output)\r\n\r\nThis looks very familiar, and I am sure I fixed this problem in #2997, please try again with the latest version. Regarding the relative import error, you probably just launched the script in the wrong directory, but anyway I recommend to use absolute imports. Please let me know if the upgrade fixes your problem, thanks."
  },
  {
    "content": "## \u2753 Multi-GPU Training GPU Usage\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### Hi, I'm using lightning and ddp as backend to do multi-gpu training, with Apex amp (amp_level = 'O1'). The gpu number is 8. I noticed that during training, most of time GPU0's utilization is 0%, while others are almost 100%. But their memory usage are the same. Is this normal? I use OpenPAI and have attached their utilization and memeory usage below. Thanks.\r\n\r\n\r\n![amp_O1_gpu_usage](https://user-images.githubusercontent.com/11988890/88447203-9c8b8d00-cdfe-11ea-8a5b-b1c2551fc4e5.png)\r\n\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - OS: [e.g. iOS, Linux, Win]\r\n - Packaging [e.g. pip, conda]\r\n - Version [e.g. 0.5.2.1]\r\ncheck the cpu usage, to make sure dataloading is not a bottleneck.> check the cpu usage, to make sure dataloading is not a bottleneck.\r\n\r\nHi, thanks for the reply. The total metric(including cpu, gpu) is as follows:\r\n\r\n![Screenshot from 2020-07-26 00-58-22](https://user-images.githubusercontent.com/11988890/88471811-5b1cdf80-cedb-11ea-8b04-d71b5b4ef9b5.png)\r\n\r\nFrom these images, I have no idea if dataloading is a bottleneck.\r\n\r\nI also did profilers, as mentioned [in this issue](https://github.com/PyTorchLightning/pytorch-lightning/issues/2699#issuecomment-663932393) . The dataloading time is constant. But there's some weird call to ''torch._C._TensorBase' objects' that takes a lot of time.\r\n\r\nThanks!\r\nYour cpu usage seems high. It could be the cpu is the bottleneck here. Try fewer gpus and observe then observe the gpu utilization. > Your cpu usage seems high. It could be the cpu is the bottleneck here. Try fewer gpus and observe then observe the gpu utilization.\r\n\r\nYes. I have found it out. Thank you so much!",
    "meta": { "name": "Multi-GPU Training GPU Usage" },
    "answer": "Your cpu usage seems high. It could be the cpu is the bottleneck here. Try fewer gpus and observe then observe the gpu utilization. "
  },
  {
    "content": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI have been googling around but can't seem to find if there is a `multiprocessing` module available in Pytorch-Lightning, just like how Pytorch has a `torch.multiprocessing` module.\r\n\r\nDoes anyone know if Pytorch-Lightning has this (or a `Joblib` similar) module? I am looking for a Pytorch-Lightning module which allows me to parallelize over multiple GPUs\r\n\r\nMany thanks in advance.\r\n\r\nPs. Sorry if this this the wrong place to post this question. I have posted the same question in Stackoverflow, but haven't received a reply. \r\n\r\n**Edit:** To be more specific, I am looking for a `multiprocessing` module in Pytorch-Lightning which allows me to parallelize over multiple GPUs on non-neural network computations, such as:\r\n```\r\nimport numpy as np\r\nimport torch\r\nfrom torch.multiprocessing import Pool\r\n\r\nX = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])\r\nX = torch.DoubleTensor(X)\r\n\r\ndef X_power_func(j):\r\n    X_power = X.cuda()**j\r\n    return X_power\r\n\r\nif __name__ == '__main__':\r\n  with Pool(processes = 2) as p:   # Parallelizing over 2 GPUs\r\n    results = p.map(X_power_func, range(4))\r\n\r\nresults\r\n```PL is just PyTorch under the hood, you can use `torch` on `joblib` directly...\r\nin case you want train distributed in CPU only you can use `ddp_cpu` backend \ud83d\udc30 ",
    "meta": {
      "name": "Does Pytorch-Lightning have a multiprocessing (or Joblib) module?"
    },
    "answer": "PL is just PyTorch under the hood, you can use `torch` on `joblib` directly...\r\nin case you want train distributed in CPU only you can use `ddp_cpu` backend \ud83d\udc30 "
  },
  {
    "content": "## \u2753 Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### I'm trying to use pl and 'ddp' to do single node multi-GPU training. My code is as below. I used batch_size 2 in Dataloader. After the code, does every gpu (in total 8 gpus) each get batch_size of 2, or only 2 gpus can access the data (each has bacth_size 1)?\r\n\r\n#### train_loader = DataLoader(dataset=train_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\r\ntrainer = pl.Trainer(gpus=8, num_nodes = 4 distributed_backend='ddp')\r\ntrainer.fit(model, train_dataloader=train_loader)\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - OS: [e.g. iOS, Linux, Win]\r\n - Packaging [e.g. pip, conda]\r\n - Version [e.g. 0.5.2.1]\r\nIs this what you need? https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#batch-size> Is this what you need? https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#batch-size\r\n\r\nYes. That's exactly what I want. Thank you so much!",
    "meta": {
      "name": "What's the Correct Batch Size in Distributed Data Parallel Training?"
    },
    "answer": "Is this what you need? https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#batch-size"
  },
  {
    "content": "In my experience, it never works. I looked to the trainer code and saw that the code managing this works only on DDP.\r\n``` python3\r\ndef configure_slurm_ddp(self, num_gpu_nodes):\r\n    self.is_slurm_managing_tasks = False\r\n\r\n    ### !!HERE!!\r\n    if self.use_ddp:\r\n        self.num_requested_gpus = self.num_gpus * num_gpu_nodes\r\n        self.num_slurm_tasks = 0\r\n        try:\r\n            self.num_slurm_tasks = int(os.environ['SLURM_NTASKS'])\r\n            self.is_slurm_managing_tasks = self.num_slurm_tasks == self.num_requested_gpus\r\n\r\n            # in interactive mode we don't manage tasks\r\n            job_name = os.environ['SLURM_JOB_NAME']\r\n            if job_name == 'bash':\r\n                self.is_slurm_managing_tasks = False\r\n\r\n        except Exception:\r\n            # likely not on slurm, so set the slurm managed flag to false\r\n            self.is_slurm_managing_tasks = False\r\n```\r\n\r\n\r\n However, sometimes we are not using the distributed computing on slurm (only DP). It would be nice to have the auto resubmit feature still working in this situation.\r\n\r\n\r\n - OS: Linux\r\n - Packaging conda\r\n - Version 16good point. i think auto resubmit should work no matter how training is happening so long as it detects slurm. \r\n\r\nmind submitting a PR?I'll try to work on it!actually, lightning supports slurm no matter what backend you use...\r\n\r\n```\r\n    def register_slurm_signal_handlers(self):\r\n        # see if we're using slurm (not interactive)\r\n        on_slurm = False\r\n        try:\r\n            job_name = os.environ['SLURM_JOB_NAME']\r\n            if job_name != 'bash':\r\n                on_slurm = True\r\n        except Exception as e:\r\n            pass\r\n\r\n        if on_slurm:\r\n            log.info('Set SLURM handle signals.')\r\n            signal.signal(signal.SIGUSR1, self.sig_handler)\r\n            signal.signal(signal.SIGTERM, self.term_handler)\r\n```",
    "meta": { "name": "Is it possible for SLURM auto submit to work on DP?" },
    "answer": "actually, lightning supports slurm no matter what backend you use...\r\n\r\n```\r\n    def register_slurm_signal_handlers(self):\r\n        # see if we're using slurm (not interactive)\r\n        on_slurm = False\r\n        try:\r\n            job_name = os.environ['SLURM_JOB_NAME']\r\n            if job_name != 'bash':\r\n                on_slurm = True\r\n        except Exception as e:\r\n            pass\r\n\r\n        if on_slurm:\r\n            log.info('Set SLURM handle signals.')\r\n            signal.signal(signal.SIGUSR1, self.sig_handler)\r\n            signal.signal(signal.SIGTERM, self.term_handler)\r\n```"
  },
  {
    "content": "## \u2753How to use pytorch-lightning distributed training without SLURM?\r\n\r\nCouldn't find anywhere a single note or tutorial on this.\r\n\r\nFor example I have just 2 node with 4 GPUs on each.\r\nOn each node environment variables required for Pytorch distributed communication are configured (see [pytorch documentation](https://pytorch.org/tutorials/intermediate/dist_tuto.html#initialization-methods)).\r\n\r\nIs this possible to train pytorch-lightning script in this setup and if so how?you can configure your own environment variables and do your own setup.\r\n\r\nJust override ```LightningModule.init_ddp_connection```\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html#lightningmodule-class\r\n\r\n\r\n(corrected)you also need to override `LightningModule.init_ddp_connection`",
    "meta": {
      "name": "How to use pytorch-lightning distributed training without SLURM?"
    },
    "answer": "you can configure your own environment variables and do your own setup.\r\n\r\nJust override ```LightningModule.init_ddp_connection```\r\nhttps://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html#lightningmodule-class\r\n\r\n\r\n(corrected)"
  },
  {
    "content": "**Background**\r\n\r\nHi, I try to track the prediction of each individual sample during training/validation-step. The main purpose is to do online hard-example mining/examining. \r\n\r\nI found out a way of doing this is to make the input variable of the functions **training/validation_step**  carrying the sample-id information, for example, the file-name. So I made the input to be a dictionary.\r\n\r\n**Example Code**  \r\n\r\n    class LightningModule():\r\n        def validation_step(self, batch, batch_idx):\r\n            y = batch['target'].float()\r\n            y_hat = self.forward(batch) \r\n            loss = self.get_loss(y_hat, y) \r\n\r\n            # append the individual result \r\n            for i in range(len(batch['sample_id'])):\r\n                self.validation_result['prediction_result'].append(y_hat[i])\r\n                self.validation_result['sample_id'].append(batch['sample_id'][i])\r\n                self.validation_result['target'].append(batch['target'][i])\r\n            return {'val_loss': loss}\r\n\r\n        def forward(self, batch):\r\n            x = batch['x']\r\n            y_hat = self.model( x)\r\n            return y_hat\r\n\r\n**Input-Dict works in Single GPU but fail under multi-GPUs-dp** \r\n\r\n    input_batch = {  \r\n        'x' : Tensor (1st dimension as batch), \r\n        'target':  Tensor (1st dimension as batch), \r\n        'sample-id': [a, b, c] (list-object) \r\n    }\r\n\r\n\r\nAND It takes me some time to realize that all value-objects inside the input-dictionary should be torch.Tensor, not list contains strings, otherwise while training under Multi-GPU ='dp' mode, the list-obj won't be separated properly.  \r\n \r\n**Input-Dict works in both Single/multi-GPUs-dp** \r\n\r\n    input_batch = {  \r\n        'x' : Tensor (1st dimension as batch), \r\n        'target':  Tensor (1st dimension as batch), \r\n        'sample-id': 1D-Tensor for sample-id  ex Tensor([1 , 3, 5]) \r\n    }\r\n\r\n\r\n\r\nCurrently, I still have some doubts on this approach... \r\nDoes anyone try to implement similar functions, online hard-example mining, with different approaches?  \r\nTks : ) \r\n\r\n@neggert @jeffling @jeremyjordan pls ^^have you considered using a library such as [`pytorch-metric-learning`](https://kevinmusgrave.github.io/pytorch-metric-learning/)?\r\n\r\nin general, it would look something like \r\n\r\n```\r\nclass MinerNetwork(pl.LightningModule):\r\n  def __init__(...):\r\n    self.network = # define network here\r\n    self.miner_function = miners.DistanceWeightedMiner()\r\n    self.objective = losses.TripletMarginLoss()\r\n\r\n  def forward(self, data, labels):\r\n    embeddings = self.network(data)\r\n    return embeddings\r\n\r\n  def training_step(self, batch, batch_idx):\r\n    data, labels = batch\r\n    embeddings = self(data)\r\n    pairs = self.miner_function(embeddings, labels)\r\n    loss = self.objective(embeddings, labels, pairs)\r\n    return loss\r\n```\r\n\r\nthis does mining within each batch that you pass in. i'm not sure where you're doing the mining currently but it seems suspicious to be appending data to a class attribute (`self.validation_result`). this will likely break if you try running on ddp because you send a copy of the model to each worker.Thanks for reply! \r\n\r\nMy original purpose is to pick-out and record the hard-samples during the training/validation after every epoch. Therefore I append the result into the lightning-model-instance. Thanks for pointing out that it would be a failure design on multi-gpus with ddp mode. \r\n\r\nI didn't know pytorch-metric-learning before. It seems to be one of right libraries that I should look at. Really appreciate! \r\n\r\n",
    "meta": {
      "name": "online hard-example mining/examining under Multi-GPU ='dp'"
    },
    "answer": "have you considered using a library such as [`pytorch-metric-learning`](https://kevinmusgrave.github.io/pytorch-metric-learning/)?\r\n\r\nin general, it would look something like \r\n\r\n```\r\nclass MinerNetwork(pl.LightningModule):\r\n  def __init__(...):\r\n    self.network = # define network here\r\n    self.miner_function = miners.DistanceWeightedMiner()\r\n    self.objective = losses.TripletMarginLoss()\r\n\r\n  def forward(self, data, labels):\r\n    embeddings = self.network(data)\r\n    return embeddings\r\n\r\n  def training_step(self, batch, batch_idx):\r\n    data, labels = batch\r\n    embeddings = self(data)\r\n    pairs = self.miner_function(embeddings, labels)\r\n    loss = self.objective(embeddings, labels, pairs)\r\n    return loss\r\n```\r\n\r\nthis does mining within each batch that you pass in. i'm not sure where you're doing the mining currently but it seems suspicious to be appending data to a class attribute (`self.validation_result`). this will likely break if you try running on ddp because you send a copy of the model to each worker."
  },
  {
    "content": "## \u2753 Questions and Help\r\n\r\n#### Having 2 gpus with DP seems to be slowers than using just 1. Is it normal?\r\nMy intuition is that if you are using 2 GPUs and the batch is being splitted into 2 batches, this should be faster. But when I tested the same code using 1 vs >1 my epoch time increased\r\n\r\n#### Code\r\n\r\n[Minimalist Implementation of a BERT Sentence Classifier](https://github.com/ricardorei/lightning-text-classification)\r\n\r\n#### What have you tried?\r\n\r\nI also tried to run ddp but my code seems to break with a `TypeError: cannot serialize '_io.TextIOWrapper' object` error. I searched online but I couldn't find the reason...\r\n\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\nyou should double your batch size.\r\ndp still has overhead in communication, so it won't be linear scaling.\r\n\r\nalso try ddp",
    "meta": { "name": "Why MultiGPU dp seems slower?" },
    "answer": "you should double your batch size.\r\ndp still has overhead in communication, so it won't be linear scaling.\r\n\r\nalso try ddp"
  },
  {
    "content": "Freezing layers at the beginning of training works, however unfreezing in ```on_epoch_start()``` during training causes the gradient to explode. Without the unfreezing part (or without freezing at all), the model trains fine with no gradient issues.\r\n\r\nI'm using DDP + Apex O2 and the loss scaling will keep going down to 0 where it would encounter 0 division and crash.\r\n\r\nIs unfreezing during training not possible in pytorch/lightning? or am I missing snippet?you can unfreeze whenever. if gradients explode it's for another reason",
    "meta": { "name": "Unfreezing layers during training?" },
    "answer": "you can unfreeze whenever. if gradients explode it's for another reason"
  },
  {
    "content": "#### Where is EarlyStopping search for metrics?\r\n\r\n#### Code\r\n\r\n```\r\n    def validation_end(self, outputs):\r\n        ...\r\n        metrics = {\r\n        'val_acc': val_acc,\r\n        'val_loss': val_loss\r\n        }\r\n        ...\r\n        output = OrderedDict({\r\n            'val_acc':  torch.tensor(metrics['val_acc']),\r\n            'val_loss': torch.tensor(metrics['val_loss']),\r\n            'progress_bar': metrics,\r\n            'log': metrics\r\n        })\r\n        return output\r\n```\r\n\r\nif I attempt to early stop according to `val_acc` I get the following error:\r\n```\r\nRuntimeWarning: Early stopping conditioned on metric 'val_acc' which is not available. Available metrics are: loss,train_loss\r\n```\r\nThe metrics mentioned (loss,train_loss) are from `training_step` from what [I could find](https://github.com/williamFalcon/pytorch-lightning/blob/12edc3099cd0d529b4ff0553f5c7cbe9f47dfdfb/pytorch_lightning/trainer/training_loop.py#L461-L467).\r\n\r\nI guess I'm doing something wrong, could anyone point me in the correct direction?\r\n\r\n - OS: Ubuntu\r\n - Packaging: pip\r\n - Version 0.5.3.2\r\n\r\n---\r\n\r\n**Update #1**: the same code works with version `0.5.1`. Bug in `0.5.3`?\r\n\r\n**Update #2**:\r\nI found that [this line](https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L539) in `trainer/training_loop.py`:\r\n```\r\nself.callback_metrics = {k: v for d in all_callback_metrics for k, v in d.items()}\r\n```\r\nFrom what I see, before this line is executed, `self.callback_metrics` contains `val_acc`. After this line values that were put in `callback_metrics` after validation are gone, therefore `EarlyStopping` can't find them. Can anyone confirm this is an issue?If I understand correctly it is a known issue. Please look at #490. #492 fixes this in master.Hi @kuynzereb, thanks! this was indeed the issue.\r\nKind of an embarrassing question, what is the best way to get these fixes from master?\r\nInstall like so?\r\n```\r\npip install git+https://github.com/williamFalcon/pytorch-lightning.git@master --upgrade\r\n```Well, I don't really know either, but it looks like you are right :)pip install git+https://github.com/williamFalcon/pytorch-lightning.git@master --upgrade\r\n`pip install https://github.com/PyTorchLightning/pytorch-lightning/archive/master.zip -U`I tried installing with both William's and Borda's methods but I still get an error that it can't find the `val_loss` metric. My `validation_step` is defined as\r\n\r\n    def validation_step(self, batch, batch_np):\r\n        x, y = batch\r\n        y_hat, _ = self.forward(x)\r\n\r\n        return {'val_loss': loss(y_hat, y)}\r\n\r\nand as I understand that should be enough. Any ideas what might be wrong?You should also define `validation_end()`, not only `validation_step()`.That worked, thank you!\r\n\r\nI must say that from the [documentation](https://pytorch-lightning.readthedocs.io/en/latest/LightningModule/RequiredTrainerInterface/#validation_end) it is not clear to me that it is necessary to define `validation_end()` as well\r\n\r\n> The outputs here are strictly for the progress bar. If you don't need to display anything, don't return anything. \r\n\r\nAs for as I know I am not doing anything progress-bar related and I should not have to define validation end. What is it that I have misunderstood?",
    "meta": { "name": "Where is EarlyStopping searching for metrics?" },
    "answer": "If I understand correctly it is a known issue. Please look at #490. #492 fixes this in master."
  },
  {
    "content": "Hi, thanks for the nice product again.\r\n\r\nFrom #525 and #599, I could guess that `hparams` is required to load a saved model (which I think should be mentioned somewhere in the doc btw). And from the examples, seems like `hparams` may be `argparse.Namespace`. Unfortunately though, it was not so easy to understand the concept. \r\n\r\nWhat is `hparams` exactly? What kind of information it should/can/should not include to work properly? Is it recommended to use [`hyperparameter argument parser`](https://williamfalcon.github.io/test-tube/hyperparameter_optimization/HyperOptArgumentParser/)? Say, if I'm not into hyperparameter search at the moment and just want to be able to load the checkpoint model, what is the requirement on the hparams?hparams is just a dict that contains your hyperparameters for easy logging and experiments. It is optional, but does make loading models more tedious.It should be an [`argparse.Namespace`](https://docs.python.org/3.7/library/argparse.html#argparse.Namespace). You can get this from `argparse`, `testtube`'s `HyperOptArgumentParser`, or create it manually from a dict like so: `argparse.Namespace(**my_dict)`.@s-rog Thanks, but it's not a dict :-) \r\n@neggert Thanks. Yes those are the info I could *assume* by reading code etc. But I think those and probably more details should be documented somewhere. I'd like to contribute, but I can't document something I don't know clearly. \r\nAlso, in my opinion, it should be very strongly recommended to use it so that they can load the model later, assuming the use cases of `pytorch-lightning` would be more serious than just training something on MNIST. Because, for those toy example, why bother :) oh yeah I keep forgetting, you're right!Yes, there's a complete overhaul of the docs in progress. We'll make sure this gets in there.thanks for the input! \r\nOnce we have the new docs, please let us know of any problems! :)",
    "meta": { "name": "What is hparams exactly?" },
    "answer": "It should be an [`argparse.Namespace`](https://docs.python.org/3.7/library/argparse.html#argparse.Namespace). You can get this from `argparse`, `testtube`'s `HyperOptArgumentParser`, or create it manually from a dict like so: `argparse.Namespace(**my_dict)`."
  },
  {
    "content": "Hi, thanks for the nice library. In the `readme`, the example uses  `model.forward(x)` not `model(x)`. But wouldn't it usually recommended to use `model(x)` so that other things (hooks etc) can be, well, hooked as well? What's the best practice? forward should implement what you want to use when calling model(x). \r\n\r\nyou may need to call that in training step (usually do), which means you have to do self.forward(...) because you are in the model when you make that call. ",
    "meta": { "name": "Call or Forward?" },
    "answer": "forward should implement what you want to use when calling model(x). \r\n\r\nyou may need to call that in training step (usually do), which means you have to do self.forward(...) because you are in the model when you make that call. "
  },
  {
    "content": "What would be the most lightning way to restore the best model? Either directly after training (in the same script) or for later use (in another script)? \r\n\r\nThanks in advance !You can use the checkpont callback to save the only the best model as described here: https://williamfalcon.github.io/pytorch-lightning/Trainer/Checkpointing/ (note that that doc needs to be updated. Use `save_top_k=1` instead of `save_best_only`)\r\n\r\nYou can then use the `load_from_checkpoint` method to restore your checkpoint: https://williamfalcon.github.io/pytorch-lightning/LightningModule/methods/#load_from_metricsin the last version, there is also `Trainer` initialization parameter `resume_from_checkpoint`> You can use the checkpont callback to save the only the best model as described here: https://williamfalcon.github.io/pytorch-lightning/Trainer/Checkpointing/ (note that that doc needs to be updated. Use `save_top_k=1` instead of `save_best_only`)\r\n\r\nI didn't pay attention, I was using `save_best_only`, thanks !\r\n\r\nThank you both for you answers, but what if I want to keep my checkpoints, and still restore the best one automatically, how would I do?\r\nRight now, I don't think the score is saved with the checkpoint. You'd need to take a look at your logs and manually select the best one.",
    "meta": { "name": "Restore the best model" },
    "answer": "You can use the checkpont callback to save the only the best model as described here: https://williamfalcon.github.io/pytorch-lightning/Trainer/Checkpointing/ (note that that doc needs to be updated. Use `save_top_k=1` instead of `save_best_only`)\r\n\r\nYou can then use the `load_from_checkpoint` method to restore your checkpoint: https://williamfalcon.github.io/pytorch-lightning/LightningModule/methods/#load_from_metrics"
  },
  {
    "content": "#### What is your question?    \r\nI want my tqdm logger to show me a history of my training on the terminal. Right now, when a epoch ends, all data for it is scrubbed from the command line and the new epoch data is shown.\r\n\r\nAlso I want to see the running accuracy of my network and a running average of my loss on the tqdm bar. How should I go on about doing that ?\r\n#### What have you tried?    \r\nI have looked at the docs and logging but am unable to figure out how to modify the tqdm logger, more so maintain a running average\r\n#### What's your environment?   \r\n\r\n- conda version: latest\r\n- PyTorch version   : 1.3\r\n- Lightning version : pip install pytorch-lightning at the date of this issue  \r\n- Test-tube version: came boot strapped with lightning.\r\n\r\nI installed everything on the date of this issue.\r\nUse tensorboard!\r\n\r\nFor running averages you have to implement the logic in training stepThank you, yes I've figured it out now !",
    "meta": { "name": "Running Average of my accuracy, losses etc." },
    "answer": "Use tensorboard!\r\n\r\nFor running averages you have to implement the logic in training step"
  },
  {
    "content": "### What is your question?    \r\nAfter a training, is there an easy way to get the best scores returned by the `validation_end` function? In order to use a hyperparameters optimizer like [Tune](https://ray.readthedocs.io/en/latest/tune.html).\r\n\r\n#### Code example\r\n```\r\nmodel = CoolSystem()\r\ntrainer = Trainer()    \r\ntrainer.fit(model)   \r\n\r\nbest_scores = ???\r\nprint(best_scores)\r\n```\r\n\r\n#### What's your environment?   \r\n- conda 4.7.10   \r\n- PyTorch 1.3.0   \r\n- Lightning 0.5.3.1\r\nSome options:\r\n- print the scores in training_end\r\n- log them to the logger\r\n- access trainer.tqdm_metrics",
    "meta": {
      "name": "Simple way to get the best scores at the end of a training?"
    },
    "answer": "Some options:\r\n- print the scores in training_end\r\n- log them to the logger\r\n- access trainer.tqdm_metrics"
  },
  {
    "content": "Hi,\r\n\r\nI am tring to use BERT for a project. The pretrained BERT model is part of my model. I am wondering how will PL initialize the model weights. Will it overwrite the pretrained BERT weights?\r\n\r\nThanks.lightning doesn\u2019t do any magic like this under the hood. you control all the weights and what gets initiated I see. So where should I do the weight initialization step if I want to follow the PL design idea? In the `__init__` of `pl.LightningModule`?This is up to you and you should follow standard PyTorch guidelines.\r\nNormally it's done in ```__init__```",
    "meta": { "name": "About the Weight Initialization in PL" },
    "answer": "lightning doesn\u2019t do any magic like this under the hood. you control all the weights and what gets initiated "
  },
  {
    "content": "In ddp mode, if I use 8 gpus, then it will creates 8 processes for each gpu, if I want to create new tensor at runtime, can I use `.cuda(self.trainer.root_gpu)`? will it use correct gpu corresponding to its process?yes",
    "meta": {
      "name": "How to get gpu id corresponding to each process in 'ddp'?"
    },
    "answer": "yes"
  },
  {
    "content": "#### What is your question?    \r\nHi, I'm trying to implement my project with your framework, however, I'd like to count the time each part costs to make full use of GPUs, but it's puzzling that the time count by myself is not the same as tqdm does. So could you give me some advice about what happened? From process bar, the time is 1.4s/it while data time is 0.003s, gpu time is 0.5~0.7s.\r\n#### Code    \r\n```python\r\n            # what I add to the trainer\r\n            # code added by me\r\n            batch_start_tic = time.time()\r\n            for batch_nb, data_batch in enumerate(self.tng_dataloader):\r\n                self.batch_nb = batch_nb\r\n                self.global_step += 1\r\n\r\n                model = self.__get_model()\r\n                model.global_step = self.global_step\r\n\r\n                # stop when the flag is changed or we've gone past the amount\r\n                #  requested in the batches\r\n                self.total_batch_nb += 1\r\n                met_batch_limit = batch_nb > self.nb_tng_batches\r\n                if met_batch_limit:\r\n                    break\r\n\r\n                # ---------------\r\n                # RUN TRAIN STEP\r\n                # ---------------\r\n                batch_fb = time.time()\r\n                batch_result = self.__run_tng_batch(data_batch, batch_nb)\r\n                early_stop_epoch = batch_result == -1\r\n                # code added by me\r\n                batch_fb_end = time.time()\r\n                self.__add_tqdm_metrics({'data time': batch_fb-batch_start_tic,'gpu time': batch_fb_end-batch_fb})\r\n                batch_start_tic = time.time()\r\n```\r\nBy the way, I find the gpu utils is about 80%, is there any tricks can make it up to 100%?\r\n#### What's your environment?   \r\n- PyTorch version   1.1.0\r\n- Lightning version   0.3.6.9\r\n- Test-tube version  0.6.7.6\r\n\r\nThanks a lot.tqdm time is a running average. you have to let it warm up for a bit before it converges to the correct time. ",
    "meta": { "name": "How to analysis the time cost of each part" },
    "answer": "tqdm time is a running average. you have to let it warm up for a bit before it converges to the correct time. "
  },
  {
    "content": "I am utilizing the MostSimilarDocuments Pipeline as well as teh DocumentSearchPipeline.  With the MostSimilarDocuments (MSD) we pass in the document id that we want to find similar documents for.  However, If I take that same text and pass it as the query in DocumentSearch pipeline I am getting different document embeddings and I end up with somewhat different results and when looking closer I realized that the embeddings that are created for the text do not match the embeddings that were calculated for the same text in the MSD. \r\n\r\nI am using the sentence-transformers/all-mpnet-base-v2 language model and I have uploaded the documents into an ElasticsearchDocumentStore.  Here is the code I use to retrieve the most similar documents to one document in the index.  The results are excellent and I get a score of 1.0 when the text is 100% the same (itself and a few duplicates):\r\n\r\n```\r\ndocument_store` = ElasticsearchDocumentStore(host=configs['url'],  port=configs['port'], \r\n                                             username=configs['user'], \r\n                                             password=configs['secret'],\r\n                                             index=\"haystack_all-mpnet\",\r\n                                             embedding_field=\"embedding\",\r\n                                             similarity='cosine',\r\n                                             ca_certs=configs['ca_certs'],\r\n                                             verify_certs=False,\r\n                                             scheme='https')\r\n\r\nmsd_pipeline = MostSimilarDocumentsPipeline(document_store=document_store)\r\nmpnet_result = msd_pipeline.run(document_ids=['b8446f55abd4ed7625ebb8b294236c64'], > top_k=50)'\r\n```\r\n\r\nFor the document search pipeline, I have implemented the following code run immediately after the above code:\r\n\r\n```\r\nretriever = EmbeddingRetriever(document_store=document_store, \r\n                                                    embedding_model=embedding_model,\r\n                                                    batch_size=256, \r\n                                                    max_seq_len=256, \r\n                                                    top_k=100, \r\n                                                    use_gpu=True)\r\n\r\ndoc_search_pipeline = DocumentSearchPipeline(retriever=retriever)\r\n\"\"\"Take the text from the very first document returned in the most_similar and use that as the query.'\"\"\"\r\nresults = doc_search_pipeline.run(query=mpnet_result[0][0].content, debug=True)\r\n\r\n```\r\n\r\nThe results are similar but not the same.  The document that the text comes from got a score of 0.96 instead of 1.0.  \r\nIs there a reason for this difference?  NOTE: The text itself about 80 characters so it shouldn't be the settings in the embedding retriever.  \r\n\r\nAny insight as to why these results are different would be appreciated.\r\n\r\nThanksHi @mwade-noetic One explanation for the difference that you see in the generated document embeddings is that a document's meta data and in particular its title (name) is taken into account when generating an embedding (and therefore also in your MostSimilarDocumentsPipeline): \r\nhttps://github.com/deepset-ai/haystack/blob/2298155a20e77c4b4c73569fbdede0ef1f663d44/haystack/nodes/retriever/_embedding_encoder.py#L188\r\n\r\nThis step is not used for your query text though when you run the following in your DocumentSearchPipeline:\r\n`doc_search_pipeline.run(query=mpnet_result[0][0].content, debug=True)`\r\n\r\nEven if there is no document name, what is used for creating the document's embedding is something like\r\n`[\"\", d.content]`Yes @julian-risch. I have tested similar use-case for my PR https://github.com/deepset-ai/haystack/pull/3368  \r\n\r\n@mwade-noetic I will update here once my PR is merged and maybe you can confirm if it solves your issue. We would be happy to look into it if it doesn't fix :)",
    "meta": {
      "name": "MostSimilarDocuments Pipeline embeddings do not match DocumentSearchPipline embeddings with same text"
    },
    "answer": "Hi @mwade-noetic One explanation for the difference that you see in the generated document embeddings is that a document's meta data and in particular its title (name) is taken into account when generating an embedding (and therefore also in your MostSimilarDocumentsPipeline): \r\nhttps://github.com/deepset-ai/haystack/blob/2298155a20e77c4b4c73569fbdede0ef1f663d44/haystack/nodes/retriever/_embedding_encoder.py#L188\r\n\r\nThis step is not used for your query text though when you run the following in your DocumentSearchPipeline:\r\n`doc_search_pipeline.run(query=mpnet_result[0][0].content, debug=True)`\r\n\r\nEven if there is no document name, what is used for creating the document's embedding is something like\r\n`[\"\", d.content]`"
  },
  {
    "content": "Hey Team,\r\nI am using Haystack with ElasticsearchDocumentStore and EmbeddingRetriever in my application, I have a question related to duplicate_documents = \"overwrite\". Based on my understanding, I came to the conclusion that records are classified as duplicates based on **id_hash_keys** of **Document** and the values for **id_hash_keys** can be a list containing content as well as meta. My metadata will have multiple fields say solution and weightage, is it possible to provide a particular field from meta to  **id_hash_keys**? If not, is there a way to classify records as duplicates based on content as well as some fields from the meta?Hi @JaisVJ , when you set `id_hash_keys=['content','meta']`, Haystack will create a hash of the content and everything that is stored in meta, which I believe works for use case, no? Does selecting a subset of the data stored in meta make a difference for you?\r\n\r\nIf you have two documents and they have the exact same content in meta (and content), then they are duplicates. If the two documents differ in some values stored in meta, then they are no duplicates. Or, in your use case, could it be that two documents differ in some of their metadata values but they should still be treated as duplicates?\r\n\r\nRight now, id_hash_keys can only be a subset of `[content, content_type, id, score, meta, embedding]` because of the way we extract the field's value from a document with `getattr` here:\r\nhttps://github.com/deepset-ai/haystack/blob/e2e6887ee8ae6ce425f579ade589b117871372ee/haystack/schema.py#L125\r\nTherefore, it is unfortunately not possible to provide a particular field from `meta` to `id_hash_keys` and I am not aware of any other way to classify records as duplicates. Here is the part of the code where we drop duplicates based on their id, if you are interested: https://github.com/deepset-ai/haystack/blob/b10e2c392e05e0baede3d2ea9673e045b14ab422/haystack/document_stores/base.py#L616\r\n\r\nAs a side note: there is an open issue on id_hash_keys not working as expected in some cases: https://github.com/deepset-ai/haystack/issues/3236",
    "meta": { "name": "Duplicate documents based on content and meta" },
    "answer": "Hi @JaisVJ , when you set `id_hash_keys=['content','meta']`, Haystack will create a hash of the content and everything that is stored in meta, which I believe works for use case, no? Does selecting a subset of the data stored in meta make a difference for you?\r\n\r\nIf you have two documents and they have the exact same content in meta (and content), then they are duplicates. If the two documents differ in some values stored in meta, then they are no duplicates. Or, in your use case, could it be that two documents differ in some of their metadata values but they should still be treated as duplicates?\r\n\r\nRight now, id_hash_keys can only be a subset of `[content, content_type, id, score, meta, embedding]` because of the way we extract the field's value from a document with `getattr` here:\r\nhttps://github.com/deepset-ai/haystack/blob/e2e6887ee8ae6ce425f579ade589b117871372ee/haystack/schema.py#L125\r\nTherefore, it is unfortunately not possible to provide a particular field from `meta` to `id_hash_keys` and I am not aware of any other way to classify records as duplicates. Here is the part of the code where we drop duplicates based on their id, if you are interested: https://github.com/deepset-ai/haystack/blob/b10e2c392e05e0baede3d2ea9673e045b14ab422/haystack/document_stores/base.py#L616\r\n\r\nAs a side note: there is an open issue on id_hash_keys not working as expected in some cases: https://github.com/deepset-ai/haystack/issues/3236"
  },
  {
    "content": "Hi Team,\r\nI am using Haystack with ElasticsearchDocumentStore and EmbeddingRetriever in my application for semantic search. My doubt is the following, the **content** field is created by using Issue and Solution columns from the dataset, \r\neg:  Issue: Error code #32\r\n       Solution: error occurred due to problem with xyz\r\n       content: \"{ \"Issue: Error code #32\", \"Solution: error occurred due to problem with xyz\"}\"\r\nBut when we are performing a search, with the query as \"Error code #32\" this particular result is coming at 8th position, I think this is because the model ignores #32 and brings results related to \"Error code\". Is there a way to increase the priority for #32 during the search?\r\n\r\nHi @JaisVJ if some of the queries are not full sentences but something like \"\"Error code #32\" then this looks to me more like a keyword search, with \"error\", \"code\" and \"32\" being the keywords. You could check whether the results for this query get better if you use the BM25Retriever instead of the EmbeddingRetriever. If that's the case, you could change your pipeline such that it contains two retrievers. Our [tutorial 11](https://github.com/deepset-ai/haystack-tutorials/blob/main/tutorials/11_Pipelines.ipynb) contains an example. Just search for \"CustomQueryClassifier\" in that tutorial.\r\nAnother idea would be to store \"issue\" and \"solution\" in two different fields of the document when you create it from your dataset. \"solution\" could be still stored in \"content\" but \"issue\" could be stored as metadata instead. When you perform a search, you could use the metadata field to filter for documents containing \"error code 32\".",
    "meta": { "name": "Similarity search accuracy" },
    "answer": "Hi @JaisVJ if some of the queries are not full sentences but something like \"\"Error code #32\" then this looks to me more like a keyword search, with \"error\", \"code\" and \"32\" being the keywords. You could check whether the results for this query get better if you use the BM25Retriever instead of the EmbeddingRetriever. If that's the case, you could change your pipeline such that it contains two retrievers. Our [tutorial 11](https://github.com/deepset-ai/haystack-tutorials/blob/main/tutorials/11_Pipelines.ipynb) contains an example. Just search for \"CustomQueryClassifier\" in that tutorial.\r\nAnother idea would be to store \"issue\" and \"solution\" in two different fields of the document when you create it from your dataset. \"solution\" could be still stored in \"content\" but \"issue\" could be stored as metadata instead. When you perform a search, you could use the metadata field to filter for documents containing \"error code 32\"."
  },
  {
    "content": "Hello, I just got started with Haystack not too long ago and I've been to run a simple question and answer python script and also been able to set up the UI and rest API using docker-compose. For elastic search in docker, I already set the image to \"elasticsearch:7.9.1\" and not the instance with countries and capitals, so that it will start an empty elasticsearch instance and I can add my documents myself.\r\n\r\nBut now, whenever I want to set up anything using Elastic search as my document store, I get a series of errors. If I leave the default value for embedding_dims as 768, I get an error saying \"Embedding dim. of model (128) doesn't match embedding dim. in DocumentStore (768).Specify the arg `embedding_dim` when initializing ElasticsearchDocumentStore()\" and whenever I specify the embedding_dim to 128, I get another error saying there is a conflict with the existing mapping \"'Mapper for [embedding] conflicts with existing mapping: [mapper [embedding] has different dims]'\". I want to believe I get this error because the first time I ran Elastic search, it's embedding dims where set to 768 which is also visible when I visit http://localhost:9200/document/_mapping . Now I want to know if there's any way to get around this error, the only way so far is using an entirely different document store like FAISS, but is there a way I can fix this issue while still using Elastic Search. Thanks for your timeHi @heyt0pe great to hear that you started trying out Haystack! The problem that you describe should be easy to fix, let me explain how.\r\nThe problem seems to be that your ElasticsearchDocumentStore already contains an index with 768 dimensions for embedding vectors. Now, if you want to store another document in that same DocumentStore but the document has only 128 dimensions in its embedding vector, you cannot store it in the same index. It's a mismatch. The number of dimensions is of the document's embedding vector depends on the model that is chosen to embed the documents. This model is set in the retriever node.\r\nWhat you need to do whenever you change that model and therefore in some cases implicitly change the number of dimensions of the embeddings is to either delete the existing index before or create a new index with a different name.\r\n\r\nWhen you initialize the document store, you can set `recreate_index` to `True` so that an existing index with the same name is overridden. Note that any indexed documents will be lost from that existing index.\r\n```\r\nimport os\r\nfrom haystack.document_stores import ElasticsearchDocumentStore\r\n\r\n# Get the host where Elasticsearch is running, default to localhost\r\nhost = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\r\ndocument_store = ElasticsearchDocumentStore(host=host, username=\"\", password=\"\", index=\"document\", recreate_index=True)\r\n```\r\n\r\n\r\nAs an alternative, when you initialize the document store, you can pick a new name for the index, e.g. `document-dim-128`\r\n```\r\nimport os\r\nfrom haystack.document_stores import ElasticsearchDocumentStore\r\n\r\n# Get the host where Elasticsearch is running, default to localhost\r\nhost = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\r\ndocument_store = ElasticsearchDocumentStore(host=host, username=\"\", password=\"\", index=\"document-dim-128\")\r\n```",
    "meta": {
      "name": "Start/Update elasticsearch instance with 128 embedding dims"
    },
    "answer": "Hi @heyt0pe great to hear that you started trying out Haystack! The problem that you describe should be easy to fix, let me explain how.\r\nThe problem seems to be that your ElasticsearchDocumentStore already contains an index with 768 dimensions for embedding vectors. Now, if you want to store another document in that same DocumentStore but the document has only 128 dimensions in its embedding vector, you cannot store it in the same index. It's a mismatch. The number of dimensions is of the document's embedding vector depends on the model that is chosen to embed the documents. This model is set in the retriever node.\r\nWhat you need to do whenever you change that model and therefore in some cases implicitly change the number of dimensions of the embeddings is to either delete the existing index before or create a new index with a different name.\r\n\r\nWhen you initialize the document store, you can set `recreate_index` to `True` so that an existing index with the same name is overridden. Note that any indexed documents will be lost from that existing index.\r\n```\r\nimport os\r\nfrom haystack.document_stores import ElasticsearchDocumentStore\r\n\r\n# Get the host where Elasticsearch is running, default to localhost\r\nhost = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\r\ndocument_store = ElasticsearchDocumentStore(host=host, username=\"\", password=\"\", index=\"document\", recreate_index=True)\r\n```\r\n\r\n\r\nAs an alternative, when you initialize the document store, you can pick a new name for the index, e.g. `document-dim-128`\r\n```\r\nimport os\r\nfrom haystack.document_stores import ElasticsearchDocumentStore\r\n\r\n# Get the host where Elasticsearch is running, default to localhost\r\nhost = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\r\ndocument_store = ElasticsearchDocumentStore(host=host, username=\"\", password=\"\", index=\"document-dim-128\")\r\n```"
  },
  {
    "content": "This site [https://www.deepset.ai/blog/build-smart-conversational-agents-with-chatbots-qa](https://www.deepset.ai/blog/build-smart-conversational-agents-with-chatbots-qa) says it is possible to create **different intents** for questions about **different domains** with giving more examples of new intents and retraining the Rasa model to become good at differentiating between the topics. Each topic should then have its own action, each of which should call a **different Haystack service**.\r\n\r\nMy question is: how can this be done via the Haystack REST API? Which files do I need to work on to make these differentiations?Hi @SoniaMola If you want to use different intents for questions about different domains, there are several ways to implement that in Haystack. One option would be to use metadata filtering. Imagine every document in your document store having a metadata field that contains the domain of the document. Then you can filter for one of the domains based on the question/intent. You can pass filters to the retriever node: https://github.com/deepset-ai/haystack/blob/b10e2c392e05e0baede3d2ea9673e045b14ab422/haystack/nodes/retriever/dense.py#L261\r\n\r\nHere is our documentation page about metadata filtering: https://docs.haystack.deepset.ai/docs/metadata-filtering\r\n\r\nI assume you already had a look at the code in the repository here: https://github.com/deepset-ai/rasa-haystack ? If not, it might also be helpful.\r\n\r\nAnother, more advanced but also more flexible way would be to have multiple indices in the document store or even multiple separate pipelines. The latter would not work out of the box though.",
    "meta": {
      "name": "Rasa integration with Haystack - how to call different Haystack services"
    },
    "answer": "Hi @SoniaMola If you want to use different intents for questions about different domains, there are several ways to implement that in Haystack. One option would be to use metadata filtering. Imagine every document in your document store having a metadata field that contains the domain of the document. Then you can filter for one of the domains based on the question/intent. You can pass filters to the retriever node: https://github.com/deepset-ai/haystack/blob/b10e2c392e05e0baede3d2ea9673e045b14ab422/haystack/nodes/retriever/dense.py#L261\r\n\r\nHere is our documentation page about metadata filtering: https://docs.haystack.deepset.ai/docs/metadata-filtering\r\n\r\nI assume you already had a look at the code in the repository here: https://github.com/deepset-ai/rasa-haystack ? If not, it might also be helpful.\r\n\r\nAnother, more advanced but also more flexible way would be to have multiple indices in the document store or even multiple separate pipelines. The latter would not work out of the box though."
  },
  {
    "content": "Hello, I just got started with Haystack not too long ago and I've been to run a simple question and answer python script and also been able to set up the UI and rest API using docker-compose. For elastic search in docker, I already set the image to \"elasticsearch:7.9.1\" and not the instance with countries and capitals, so that it will start an empty elasticsearch instance and I can add my documents myself.\r\n\r\nBut now, whenever I want to set up anything using Elastic search as my document store, I get a series of errors. If I leave the default value for embedding_dims as 768, I get an error saying \"Embedding dim. of model (128) doesn't match embedding dim. in DocumentStore (768).Specify the arg embedding_dim when initializing ElasticsearchDocumentStore()\" and whenever I specify the embedding_dim to 128, I get another error saying there is a conflict with the existing mapping \"'Mapper for [embedding] conflicts with existing mapping: [mapper [embedding] has different dims]'\". I want to believe I get this error because the first time I ran Elastic search, it's embedding dims where set to 768 which is also visible when I visit http://localhost:9200/document/_mapping . Now I want to know if there's any way to get around this error, the only way so far is using an entirely different document store like FAISS, but is there a way I can fix this issue while still using Elastic Search. Thanks for your timeHi @heyt0pe I just responded to your question in the over discussion you started here: https://github.com/deepset-ai/haystack/discussions/3329 \ud83d\ude42 ",
    "meta": {
      "name": "Create new haystack elasticsearch instance in docker / update embedding dims in existing elastic search instance"
    },
    "answer": "Hi @heyt0pe I just responded to your question in the over discussion you started here: https://github.com/deepset-ai/haystack/discussions/3329 \ud83d\ude42 "
  },
  {
    "content": "Is it possible to restrict passages that get passed to the reader by threshold?\r\n\r\nThanks\r\nHermanHello @hermans0101g currently, Haystack does not support filtering the output of retriever nodes based on the relevance scores of the passages. However, the output of reader nodes can be filtered based on a threshold: https://github.com/deepset-ai/haystack/blob/dcb132ba5940c869e9e45029918330e31a89b38e/haystack/nodes/reader/farm.py#L122\r\n\r\nI'd suggest that you have a look at how the implementation works for the reader. Maybe you can use it as a blueprint for the retriever?",
    "meta": { "name": "Minimum score for retrieved passage" },
    "answer": "Hello @hermans0101g currently, Haystack does not support filtering the output of retriever nodes based on the relevance scores of the passages. However, the output of reader nodes can be filtered based on a threshold: https://github.com/deepset-ai/haystack/blob/dcb132ba5940c869e9e45029918330e31a89b38e/haystack/nodes/reader/farm.py#L122\r\n\r\nI'd suggest that you have a look at how the implementation works for the reader. Maybe you can use it as a blueprint for the retriever?"
  },
  {
    "content": "I'm trying to set up the `ElasticsearchDocumentStore` in memory within an AWS EC2 instance. As a prerequisite, I'm running the below command (from a python script) but running into issues related to docker (error messages also mentioned below)\r\n\r\n```\r\nfrom haystack.utils import launch_es\r\nlaunch_es()\r\n```\r\n\r\nError messages from the log\r\n\r\n```\r\n/bin/sh: 1: docker: not found\r\nWARNING: haystack.utils.doc_store: Tried to start Elasticsearch through Docker but this failed. It is likely that there is already an existing Elasticsearch instance running. \r\n```\r\n\r\nAny help to resolve this error is appreciated!From `/bin/sh: 1: docker: not found`, it seems that Docker is not installed.\r\n\r\nDepending on your system/preferences you have two possible solutions:\r\n\r\n- [Install Docker in your system](https://docs.docker.com/get-docker/) and retry with the current approach\r\n- Manually download Elasticsearch and run it from source, as you see in [this Haystack tutorial](https://haystack.deepset.ai/tutorials/first-qa-system#document-store)\r\n",
    "meta": { "name": "Docker with ElasticSearch" },
    "answer": "From `/bin/sh: 1: docker: not found`, it seems that Docker is not installed.\r\n\r\nDepending on your system/preferences you have two possible solutions:\r\n\r\n- [Install Docker in your system](https://docs.docker.com/get-docker/) and retry with the current approach\r\n- Manually download Elasticsearch and run it from source, as you see in [this Haystack tutorial](https://haystack.deepset.ai/tutorials/first-qa-system#document-store)\r\n"
  },
  {
    "content": "I am using the haystack rest api integrated in a rasa chatbot. My doubt is the following: is it possible to use the haystack rest api with an empty document store, using only the pre-trained model (like roberta-base-squad2)? Can the chatbot answer the questions that the language model chosen in the pipeline has been trained on (defined in pipelines.haystack-pipeline.yml)? Or is the haystack rest api only able to answer questions relating to the documents contained in the document store?For generating answers I would recommend looking at the nodes `RAGenerator`, `Seq2SeqGenerator`, or the `OpenAIAnswerGenerator` in Haystack. You can find more info on our docs pages here for answer generation https://docs.haystack.deepset.ai/docs/answer_generator. By default, these pipelines still expect a retriever b/c the answer generator takes both the retrieved documents and question into account to answer the question. \r\n\r\nHowever, it should be possible to set up the `Seq2SeqGenerator` and `OpenAIAnswerGenerator` in such a way that they can work without even if they are given zero retrieved documents. For example, you could try something like\r\n```python\r\nresult = generator.predict(\r\n    query='What are the best party games for adults?',\r\n    documents=[],\r\n    top_k=top_k\r\n)\r\n```",
    "meta": { "name": "Doubts about rest api and pre-trained model" },
    "answer": "For generating answers I would recommend looking at the nodes `RAGenerator`, `Seq2SeqGenerator`, or the `OpenAIAnswerGenerator` in Haystack. You can find more info on our docs pages here for answer generation https://docs.haystack.deepset.ai/docs/answer_generator. By default, these pipelines still expect a retriever b/c the answer generator takes both the retrieved documents and question into account to answer the question. \r\n\r\nHowever, it should be possible to set up the `Seq2SeqGenerator` and `OpenAIAnswerGenerator` in such a way that they can work without even if they are given zero retrieved documents. For example, you could try something like\r\n```python\r\nresult = generator.predict(\r\n    query='What are the best party games for adults?',\r\n    documents=[],\r\n    top_k=top_k\r\n)\r\n```"
  },
  {
    "content": "Hi team,\r\n\r\nI am using Haystack with ElasticsearchDocumentStore in my application. My doubt is the following: I have a requirement where on user interactions, we need to update a field(say priority) in the metadata. So, is there an option in the retriever or any other way, to get the task done?Hi @JaisVJ you can do communicate document updates directly to your running Elasticsearch instance and don't need to use Haystack for that: https://www.elastic.co/guide/en/elasticsearch/reference/7.17/docs-update.html#_update_part_of_a_document\r\n\r\nFor some use cases one can use the custom_query parameter in Haystack here to interact with Elasticsearch but I don't think it works for updating a document's meta data: https://github.com/deepset-ai/haystack/blob/dcb132ba5940c869e9e45029918330e31a89b38e/haystack/nodes/retriever/sparse.py#L33\r\n\r\nLast but not least, what you can do is overwrite the existing document by using `write_documents` and using the id of the existing document. As the id already exisits, elasticsearch will overwrite the existing document with the new one.",
    "meta": {
      "name": "How do I update metadata of Elasticsearch Document Store?"
    },
    "answer": "Hi @JaisVJ you can do communicate document updates directly to your running Elasticsearch instance and don't need to use Haystack for that: https://www.elastic.co/guide/en/elasticsearch/reference/7.17/docs-update.html#_update_part_of_a_document\r\n\r\nFor some use cases one can use the custom_query parameter in Haystack here to interact with Elasticsearch but I don't think it works for updating a document's meta data: https://github.com/deepset-ai/haystack/blob/dcb132ba5940c869e9e45029918330e31a89b38e/haystack/nodes/retriever/sparse.py#L33\r\n\r\nLast but not least, what you can do is overwrite the existing document by using `write_documents` and using the id of the existing document. As the id already exisits, elasticsearch will overwrite the existing document with the new one."
  },
  {
    "content": "Hi -\r\n\r\nI am doing a project for content de-duplication. My document repository has large number of files (only English language) with varying formats (PDF, Excel, Word). The contents are semantically similar. \r\n\r\nIs it possible to use any of the pre-trained haystack models and its pipelines for de-duplication use case? If I have to train a new model for this use case,what approach would be best?\r\n\r\nThe question can be as simple as: \"Show all documents that are semantically similar?\"\r\n\r\nHope someone in this group can assist.\r\n\r\nThanks,\r\nSekhar H.\r\nHi @sekh77!\r\n\r\nWe have a `MostSimilarDocumentsPipeline` (see [here](https://github.com/deepset-ai/haystack/blob/d3fd888a766f4b197a4c26c9346f290cbc71048a/haystack/pipeline.py#L1368)) that allows you to find the most similar documents given one document. For creating document embeddings, you might want to use a sentence-transformers model (see [here](https://haystack.deepset.ai/components/retriever#embedding-retrieval) for details).\r\n\r\nI hope this answers your question :)Great stuff! Thank you @bogdankostic \r\n\r\nWhat are \"document_ids\"? Are these filenames? For the use case that I am developing, only the document repository is passed as input to the system. In other words, the repository is likely to contain thousands of documents in varying formats (PDF, word, excel, plain text). What is expected by the end-user is that the model should automatically take one document (file) at a time from this repository and compare this document against the remaining (n-1) documents. Repeat this for every document in the repository. And then eventually publish a report (in JSON format) showing the following information:\r\n1. \"document name\"\r\n2.  \"number of documents that are similar to this document\"\r\n3.  \"names of top 3 similar documents\"\r\n4.  \"similarity score with the original document for each of the top 3\"\r\n\r\nIs it possible to update your code snippet to achieve this?\r\n\r\nA feature like this will be an excellent addition to Haystack because this is a very common use case for most of the enterprises w.r. to their data migration activity. Such a report will help to migrate only the latest and greatest version of documents ignoring duplicates.\r\n\r\nTruly appreciate your wonderful support!\r\n\r\nCheers,\r\nSekhar H.Hi @bogdankostic - I managed to get this running for my document store. And could see duplicates being reported. Here's an example of the Document result object.\r\n\r\nmost_similar_docs = [{'text': '<<here the actual text appears from the document>>, 'score': 1.0, 'question': None, 'meta': {'_split_id': 0, 'name': 'file_0673.txt'}, 'embedding': None, 'id': 'e1eccfd26a6354b493a601bf966d2b2a'}, 'text': '<<here the actual text appears from the document>>, 'score': 0.93728964, 'question': None, 'meta': {'_split_id': 0, 'name': 'file_0781.txt'}, 'embedding': None, 'id': 'ea119020fb1dad657dbbef87e7419894'},}}]\r\n\r\nI have 10 different entries most_similar_docs[0] to most_similar_docs[10] in the result object. Each entry has top_k=4 - so most_similar_docs[0] has 4 entries.\r\n\r\nHow do I loop through most_similar_docs, and generate a CSV report as follows:\r\n\r\nFile name, Score, Duplicate Files\r\nfile_0673.txt, 93.7%, 0781.txt\r\n\r\nI tried in this way so far: print(list(map(lambda item: item.get('score', 'default value'), most_similar_docs)))\r\nBut I get the error: AttributeError: 'list' object has no attribute 'get'\r\n\r\nAny help would be greatly appreciated? \r\n\r\nThanks,\r\nSekhar H.\r\n\r\nYou can just follow the example in the link but you will need to pass in\nyour own annotated dataset for a custom evaluation or you can compare\nagainst how your model does with a public dataset. At the bottom of that\nlink, it describes the dataset should be in the SQuAD format and it\nmentions to checkout SquadDataobject inhaystack/squad_data.py.\n\nThat should have code examples. There aren\u2019t any other examples to my\nknowledge.\n\nOn Fri, Sep 30, 2022 at 12:59 PM Sankalp ***@***.***> wrote:\n\n> @JoeREISys <https://github.com/JoeREISys> thanks a lot, I got it working.\n> \ud83c\udf89\n>\n> A quick follow-up question - I also want to evaluate the retrieved\n> sentences (documents) using the mAP and Recall metrics mentioned here\n> <https://haystack.deepset.ai/guides/evaluation#metrics-retrieval>. How\n> can I achieve it? Is there an example? Thanks!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/deepset-ai/haystack/discussions/1496#discussioncomment-3774027>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AB77SQYMPGVKG6OFTHTRS2TWA4MAVANCNFSM5ETAS72Q>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n",
    "meta": { "name": "Content de-duplication" },
    "answer": "Hi @sekh77!\r\n\r\nWe have a `MostSimilarDocumentsPipeline` (see [here](https://github.com/deepset-ai/haystack/blob/d3fd888a766f4b197a4c26c9346f290cbc71048a/haystack/pipeline.py#L1368)) that allows you to find the most similar documents given one document. For creating document embeddings, you might want to use a sentence-transformers model (see [here](https://haystack.deepset.ai/components/retriever#embedding-retrieval) for details).\r\n\r\nI hope this answers your question :)"
  },
  {
    "content": "**Use Case**: retrieve `top_k` similar sentences given a given query sentence\r\n\r\nI'm thinking of using `ElasticsearchDocumentStore` to store all sentences and their embeddings using `EmbeddingRetriever`. \r\nCan I use the ready-made `MostSimilarDocumentsPipeline` to retrieve the `top_k` most similar sentences from the document store?\r\n\r\nFor my use case, I may have entirely new query sentences that are not part of the document store. And my understanding is that `MostSimilarDocumentsPipeline` expects a list of document IDs already in the document store. So how do I work around sentences that are not in the document store? Updating the document store with query sentences is not an option.\r\n\r\nAny help is appreciated, thanks! Hi,\r\n\r\n    The most similar document pipeline is just using the already calculated word embeddings of a document to find similar documents to that document vector.  YOu can achieve the same thing using the DocumentSearchPipeline and the EmbeddingRetriever.\r\n\r\nYou will simply pass the text from the sentence that you want to find all similar documents and it will create the embeddings that sentence and then just run the same query_by_embeddings method that the MSD uses.\r\n\r\nHere is a simple outline of the code:\r\n\r\ndocument_store = ElasticserachDocumentStore(similarity='cosine')\r\nretriever = EmbeddingRetriever(document_store=document_store, embedding_model='sentence-transformers/all-mpnet-base-v2')\r\nsearch_pipeline = DocumentSearchPipeline(retriever=retriever)\r\nsimilar_docs = search_pipline.run(query=text_of_sentence, params={\"Retriever\": { \"top_k\": 10 }})\r\n\r\nNote, you may need to use a PreProcessor() to clean/split the text unless it is fairly short in length.  I just kept this as a simple example.\r\nThis get's you good similarity estimates w/o having to index the documents. \r\n",
    "meta": {
      "name": "MostSimilarDocumentsPipeline for Retrieving Similar Sentences"
    },
    "answer": "Hi,\r\n\r\n    The most similar document pipeline is just using the already calculated word embeddings of a document to find similar documents to that document vector.  YOu can achieve the same thing using the DocumentSearchPipeline and the EmbeddingRetriever.\r\n\r\nYou will simply pass the text from the sentence that you want to find all similar documents and it will create the embeddings that sentence and then just run the same query_by_embeddings method that the MSD uses.\r\n\r\nHere is a simple outline of the code:\r\n\r\ndocument_store = ElasticserachDocumentStore(similarity='cosine')\r\nretriever = EmbeddingRetriever(document_store=document_store, embedding_model='sentence-transformers/all-mpnet-base-v2')\r\nsearch_pipeline = DocumentSearchPipeline(retriever=retriever)\r\nsimilar_docs = search_pipline.run(query=text_of_sentence, params={\"Retriever\": { \"top_k\": 10 }})\r\n\r\nNote, you may need to use a PreProcessor() to clean/split the text unless it is fairly short in length.  I just kept this as a simple example.\r\nThis get's you good similarity estimates w/o having to index the documents. \r\n"
  },
  {
    "content": "Hello,\r\n\r\nI am trying to use the annotation tool locally per instructions. After running `docker-compose up` on the given example config (vanilla) of the latest repo version, the log says:\r\n\r\n> WARN 88 [-/172.21.0.1/-/100ms GET /api/user/info] nodejs.ForbiddenError: User not found\r\n\r\n... with the same error displayed on the browser client in a pop-up. There is no other information or display.\r\n\r\nThere seems to be no other error or warning in the log.\r\n\r\nThe migrations are successful. I can inspect the PostgreSQL database via PGAdmin.\r\n\r\nThe \"users\" table contains a single user (from `DEFAULT_ADMIN_EMAIL`).\r\n\r\nI tried PostgresSQL 12 and 14.\r\n\r\nIt should be noted that I have to run `docker-compose up` twice before the database is properly set up. Some errors are thrown.\r\n\r\nDo you have an idea what's going on here? Thanks!\r\n\r\nSystem: MacOS 12.5, Docker 20.10.17Hi @kaumanns just to double check did you replace all instances of `somesafeuser` and `somesafepassword` in the [docker-compose.yml](https://github.com/deepset-ai/haystack/blob/master/annotation_tool/docker-compose.yml) file? There are a few more instances that need to be replaced than what is explained in the README. ",
    "meta": { "name": "Annotation Tool: User not found" },
    "answer": "Hi @kaumanns just to double check did you replace all instances of `somesafeuser` and `somesafepassword` in the [docker-compose.yml](https://github.com/deepset-ai/haystack/blob/master/annotation_tool/docker-compose.yml) file? There are a few more instances that need to be replaced than what is explained in the README. "
  },
  {
    "content": "My code is this\r\n\r\n```\r\nfrom haystack.nodes import TransformersSummarizer, TransformersTranslator\r\nfrom haystack import Document\r\n\r\ntranslator = TransformersTranslator(model_name_or_path=\"path/to/opus-mt-de-en\")\r\nsummarizer = TransformersSummarizer(model_name_or_path=\"google/pegasus-xsum\",progress_bar=True)\r\n```\r\n\r\nAs the models have already been downloaded, I switched off the internet connection(as data to be processed is sensitive) and tried the model with it.\r\n\r\nI got \r\n\r\n```\r\nERROR:posthog:error uploading: HTTPSConnectionPool(host='tm.hs.deepset.ai', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5B34A3BE0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\r\nERROR:posthog:error uploading: HTTPSConnectionPool(host='tm.hs.deepset.ai', port=443): Max retries exceeded with url: /batch/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5B34A3820>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\r\n```\r\n\r\nSo I got the error message not only for the relative path whose model has already been downloaded, but also for the absolute path. Also I noticed the `error uploading`, which confused me, as I don't understand what should be uploaded.\r\n\r\nI found solutions, like https://github.com/deepset-ai/haystack/issues/486#issuecomment-708931895 but the FARM framework does not accept `google/pegasus-xsum`, also it is for QA and not for summarization.\r\n\r\nHow do I use the models locally for privacy reasons?\r\n\r\nMaybe @bglearning , @vblagoje or @bogdankostic can help.Hi @ErfolgreichCharismatisch !\n\nThe upload error you are looking at is caused by telemetry. You can disable it by setting environment variable HAYSTACK_TELEMETRY_ENABLED=False or by calling disable_telemetry(). \n\nHaystack telemetry is totally anonymized.\n\nRegarding the Transformers models, you can use locally cached models or manually downloaded, the location will be set/get by env vars:\n- TRANSFORMERS_CACHE \n- HF_HOME + transformers/. \n- XDG_CACHE_HOME + /huggingface/transformers. \n\nBut I think that if you don't remove the virtual env or clean these directories, you can use it offline. Remember to turn off telemetry.\n",
    "meta": { "name": "How do I use models locally for privacy reasons?" },
    "answer": "Hi @ErfolgreichCharismatisch !\n\nThe upload error you are looking at is caused by telemetry. You can disable it by setting environment variable HAYSTACK_TELEMETRY_ENABLED=False or by calling disable_telemetry(). \n\nHaystack telemetry is totally anonymized.\n\nRegarding the Transformers models, you can use locally cached models or manually downloaded, the location will be set/get by env vars:\n- TRANSFORMERS_CACHE \n- HF_HOME + transformers/. \n- XDG_CACHE_HOME + /huggingface/transformers. \n\nBut I think that if you don't remove the virtual env or clean these directories, you can use it offline. Remember to turn off telemetry.\n"
  },
  {
    "content": "I'm not sure if I've missed something but assuming I have a single trivial doc in my elasticsearch store:\r\n\r\n```json\r\n\"_source\": {\r\n    \"content\": \"the queen of England is Elizabeth 2\",\r\n    \"content_type\": \"text\",\r\n    \"folder\": \"root\",\r\n    \"_split_id\": 0\r\n}\r\n```\r\n\r\nThis call takes around 2 seconds:\r\n\r\n```python\r\nfilters = {'folder': 'root'}\r\ndocument_store.delete_documents(filters=filters)\r\n```\r\n\r\nWhereas the corresponding elastic search query:\r\n\r\n`http://localhost:9200/documents/_delete_by_query`\r\n\r\n```json\r\n{\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"must\": [\r\n          { \"term\": {\"folder\": \"root\" } }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\ntakes only 6 ms\r\n\r\nAny ideas?I think I might have worked out what's going on.\r\n\r\nIt seems it's the `refresh_type='wait_for'` that's causing the issue\r\n\r\nNow I need to work out why wait_for is so slow...",
    "meta": { "name": "Why is delete_documents slow?" },
    "answer": "I think I might have worked out what's going on.\r\n\r\nIt seems it's the `refresh_type='wait_for'` that's causing the issue\r\n\r\nNow I need to work out why wait_for is so slow..."
  },
  {
    "content": "Hi. I want to use a generator without a retriever. I have short phrases (less than a complete sentence) that I need to convert to a standardized form. For this purpose, I want to fine tune a T5 model in generative QA form, wherein I ask the question, 'What is the standardized form of the given phrase?' and the answer would be the standardized form of the phrase.  I could not find a way to fine tune T5 on haystack for this purpose. I believe RAG and LFQA do not suit my use case. After going through some previous discussions here, I guess Seq2SeqGenerator may be suitable, but can't figure out how I can fine tune it. \r\nThanks.Hi @SigProcess , \r\n\r\nBased on your description, I think you're correct in that it's a Text2Text task. So if it's always about getting the standardized form, then perhaps the query part is not relevant. So a direct Text2Text task: `non-standard_phrase` -> `standardized-form`. More specifically, sounds like a paraphrasing task.\r\n\r\nAnd so Seq2SeqGenerator as you say is right. However, it doesn't support finetuning at the moment. It is a wrapper around HuggingFace [Text2Text models](https://huggingface.co/models?pipeline_tag=text2text-generation). So perhaps you could look around specifically for fine-tuning those models for paraphrasing. [One example](https://medium.com/@imjeffhi4/creating-a-paraphrase-generator-model-using-t5-and-deploying-on-ainize-7742bc83532a).\r\n\r\ncc: @vblagoje (in case there is something incorrect in the assessment above or you have something to add)",
    "meta": { "name": "How to Fine-tune a generator?" },
    "answer": "Hi @SigProcess , \r\n\r\nBased on your description, I think you're correct in that it's a Text2Text task. So if it's always about getting the standardized form, then perhaps the query part is not relevant. So a direct Text2Text task: `non-standard_phrase` -> `standardized-form`. More specifically, sounds like a paraphrasing task.\r\n\r\nAnd so Seq2SeqGenerator as you say is right. However, it doesn't support finetuning at the moment. It is a wrapper around HuggingFace [Text2Text models](https://huggingface.co/models?pipeline_tag=text2text-generation). So perhaps you could look around specifically for fine-tuning those models for paraphrasing. [One example](https://medium.com/@imjeffhi4/creating-a-paraphrase-generator-model-using-t5-and-deploying-on-ainize-7742bc83532a).\r\n\r\ncc: @vblagoje (in case there is something incorrect in the assessment above or you have something to add)"
  },
  {
    "content": "I am using QuestionAnswerGeneration pipeline to generate questions and answers. I would like to export it in a format that i can then load it in the annotator along with context, offset etc., Let me know if there is a way to do it.https://github.com/deepset-ai/haystack/issues/1811 answers the same question. As of now there is no way to do it and have to write our own util to help with it.",
    "meta": {
      "name": "Export generated questions and answers in SQUAD format"
    },
    "answer": "https://github.com/deepset-ai/haystack/issues/1811 answers the same question. As of now there is no way to do it and have to write our own util to help with it."
  },
  {
    "content": "Hi, I am currently playing around with GPL negative mining step. I am just getting one negative doc per positive doc even after setting top_k value to any number like 10 or 50. Am I setting the variable in some weird way? Any inputs would be appreciated. I have total of 278 positive passages.Hey @sinchanabhat, can you share a colab notebook? I'll take a look. ",
    "meta": { "name": "top_k in psuedo_label_generator" },
    "answer": "Hey @sinchanabhat, can you share a colab notebook? I'll take a look. "
  },
  {
    "content": "Hello, Please refer following link that describes answer generation - \r\n\r\n[https://haystack.deepset.ai/pipeline_nodes/answer-generator]\r\n\r\nIt suggests following code to implement stand alone answering system - \r\n\r\n`result = generator.predict(\r\n    query='What are the best party games for adults?',\r\n    documents=[doc1, doc2, doc3...],\r\n    top_k=top_k\r\n)` \r\n\r\nHere, the 'generator' object is initialized using a 'retriever' object and I guess retriever object requires some kind of data store (elastic search).  I wish to understand how I can simply pass a paragraph and a question to generate an answer.\r\nPlease suggest. Thanks!\r\n\r\n\r\n\r\nThe [available generators](https://haystack.deepset.ai/pipeline_nodes/answer-generator) are:\r\n- `RAGenerator` (Retrieval-Augmented Generator): it relies on a retriever or, anyway, it needs vector embeddings for the documents\r\n- `Seq2SeqGenerator`: generic sequence-to-sequence generator based on Hugging Face's transformers\r\n- `OpenAIAnswerGenerator`: you need an OpenAI account\r\n\r\nIf you want to quickly manually try a Generator node, I'll report a straightforward example for you using `Seq2SeqGenerator`.\r\n_Note: In any case, for better results, I suggest to read the available tutorials ([RAG](https://haystack.deepset.ai/tutorials/retrieval-augmented-generation), [LFQA](https://haystack.deepset.ai/tutorials/lfqa)) that use the generators in combination with a retriever._\r\n\r\n```python\r\nfrom haystack import Document\r\nfrom haystack.nodes import Seq2SeqGenerator\r\n\r\n# some texts about Queen Elizabeth\r\ntexts=[\r\n    \"Elizabeth II (Elizabeth Alexandra Mary; 21 April 1926 \u2013 8 September 2022) was Queen of the United Kingdom and the other Commonwealth realms from 6 February 1952 until her death in 2022.\",\r\n    \"She was queen regnant of 32 sovereign states during her life and served as monarch of 15 of them at the time of her death.\",\r\n    \"Elizabeth met her future husband, Prince Philip of Greece and Denmark, in 1934 and again in 1937.\",\r\n    \"Her reign of 70 years and 214 days is the longest of any British monarch and the longest recorded of any female head of state in history.\",\r\n    \"Elizabeth's four children, along with her daughters-in-law and grandsons Prince William and Prince Harry, travelled to Balmoral. Her death was confirmed that evening at 18:30 BST\",\r\n    \"From Elizabeth's birth onwards, the British Empire continued its transformation into the Commonwealth of Nations.\"\r\n]\r\n\r\ndocs = [Document(text) for text in texts]\r\ngenerator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\")\r\n\r\ngenerator.predict(\r\n        query=\"When did Queen Elizabeth die?\",\r\n        documents=docs,\r\n        top_k=1\r\n     )\r\n```\r\n\r\nYou get this output:\r\n\r\n>{'query': 'When did Queen Elizabeth die?',\r\n 'answers': [<Answer {'answer': '**Queen Elizabeth II was born on 21 April 1926. She was Queen of the United Kingdom and the other Commonwealth realms from 6 February 1952 until her death on 8 September 2022.**', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_id': None, 'meta': {'doc_ids': ['5c814df2d3848b1992b68ab35ce2dfe7', 'dc9a27075bd1924b0966227ca3fee4e7', '75525760a793bcbbf3d31c863b0e072b', '97866a6328a076c8daff28fea3fb228a', 'c5911420964e6e86b86cbb5d3cbe5b7', '6a9a9536cf715b5ef9b06d8e3fd88795'], 'doc_scores': [None, None, None, None, None, None], 'content': ['Elizabeth II (Elizabeth Alexandra Mary; 21 April 1926 \u2013 8 September 2022) was Queen of the United Kingdom and the other Commonwealth realms from 6 February 1952 until her death in 2022.', 'She was queen regnant of 32 sovereign states during her life and served as monarch of 15 of them at the time of her death.', 'Elizabeth met her future husband, Prince Philip of Greece and Denmark, in 1934 and again in 1937.', 'Her reign of 70 years and 214 days is the longest of any British monarch and the longest recorded of any female head of state in history.', \"Elizabeth's four children, along with her daughters-in-law and grandsons Prince William and Prince Harry, travelled to Balmoral. Her death was confirmed that evening at 18:30 BST\", \"From Elizabeth's birth onwards, the British Empire continued its transformation into the Commonwealth of Nations.\"], 'titles': ['', '', '', '', '', '']}}>]}",
    "meta": { "name": "Implementing simple stand alone question answering." },
    "answer": "The [available generators](https://haystack.deepset.ai/pipeline_nodes/answer-generator) are:\r\n- `RAGenerator` (Retrieval-Augmented Generator): it relies on a retriever or, anyway, it needs vector embeddings for the documents\r\n- `Seq2SeqGenerator`: generic sequence-to-sequence generator based on Hugging Face's transformers\r\n- `OpenAIAnswerGenerator`: you need an OpenAI account\r\n\r\nIf you want to quickly manually try a Generator node, I'll report a straightforward example for you using `Seq2SeqGenerator`.\r\n_Note: In any case, for better results, I suggest to read the available tutorials ([RAG](https://haystack.deepset.ai/tutorials/retrieval-augmented-generation), [LFQA](https://haystack.deepset.ai/tutorials/lfqa)) that use the generators in combination with a retriever._\r\n\r\n```python\r\nfrom haystack import Document\r\nfrom haystack.nodes import Seq2SeqGenerator\r\n\r\n# some texts about Queen Elizabeth\r\ntexts=[\r\n    \"Elizabeth II (Elizabeth Alexandra Mary; 21 April 1926 \u2013 8 September 2022) was Queen of the United Kingdom and the other Commonwealth realms from 6 February 1952 until her death in 2022.\",\r\n    \"She was queen regnant of 32 sovereign states during her life and served as monarch of 15 of them at the time of her death.\",\r\n    \"Elizabeth met her future husband, Prince Philip of Greece and Denmark, in 1934 and again in 1937.\",\r\n    \"Her reign of 70 years and 214 days is the longest of any British monarch and the longest recorded of any female head of state in history.\",\r\n    \"Elizabeth's four children, along with her daughters-in-law and grandsons Prince William and Prince Harry, travelled to Balmoral. Her death was confirmed that evening at 18:30 BST\",\r\n    \"From Elizabeth's birth onwards, the British Empire continued its transformation into the Commonwealth of Nations.\"\r\n]\r\n\r\ndocs = [Document(text) for text in texts]\r\ngenerator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\")\r\n\r\ngenerator.predict(\r\n        query=\"When did Queen Elizabeth die?\",\r\n        documents=docs,\r\n        top_k=1\r\n     )\r\n```\r\n\r\nYou get this output:\r\n\r\n>{'query': 'When did Queen Elizabeth die?',\r\n 'answers': [<Answer {'answer': '**Queen Elizabeth II was born on 21 April 1926. She was Queen of the United Kingdom and the other Commonwealth realms from 6 February 1952 until her death on 8 September 2022.**', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_id': None, 'meta': {'doc_ids': ['5c814df2d3848b1992b68ab35ce2dfe7', 'dc9a27075bd1924b0966227ca3fee4e7', '75525760a793bcbbf3d31c863b0e072b', '97866a6328a076c8daff28fea3fb228a', 'c5911420964e6e86b86cbb5d3cbe5b7', '6a9a9536cf715b5ef9b06d8e3fd88795'], 'doc_scores': [None, None, None, None, None, None], 'content': ['Elizabeth II (Elizabeth Alexandra Mary; 21 April 1926 \u2013 8 September 2022) was Queen of the United Kingdom and the other Commonwealth realms from 6 February 1952 until her death in 2022.', 'She was queen regnant of 32 sovereign states during her life and served as monarch of 15 of them at the time of her death.', 'Elizabeth met her future husband, Prince Philip of Greece and Denmark, in 1934 and again in 1937.', 'Her reign of 70 years and 214 days is the longest of any British monarch and the longest recorded of any female head of state in history.', \"Elizabeth's four children, along with her daughters-in-law and grandsons Prince William and Prince Harry, travelled to Balmoral. Her death was confirmed that evening at 18:30 BST\", \"From Elizabeth's birth onwards, the British Empire continued its transformation into the Commonwealth of Nations.\"], 'titles': ['', '', '', '', '', '']}}>]}"
  },
  {
    "content": "Hello! In this article https://www.deepset.ai/blog/generate-questions-automatically-for-faster-annotation in the section \"Human in The Loop: Making the Most Out of Automated Question Answering Annotation\" there is a quote \"You can then retrain your language model with the manually amended dataset, to make sure the model doesn't repeat the same mistakes.\"\r\n\r\nI want to first get a list of questions to my text using QuestionGenerationPipeline, after that, I correct this list of questions manually (1.leave valuable questions, 2.delete bad questions, 3.add other valuable questions) and add the resulting list of questions to the training dataset and then retrain the model for generating questions valhalla/t5-base-e2e-qg which is listed here https://haystack.deepset.ai/reference/question-generator\r\n\r\nWhat is the best way to train the model valhalla/t5-base-e2e-qg for the your Module question_generator using a list of questions received from the QuestionGenerationPipeline and corrected manually?\r\n\r\nPlease tell me if I understood correctly:\r\nvalhalla/t5-small-e2e-qg model trained on squad_multitask dataset?\r\n\r\nDoes the squad_multitask dataset contain question-answer pairs?\r\n\r\nI have text. The model generated a set of questions for me. I manually corrected the received set of questions and want to retrain the valhalla/t5-small-e2e-qg model.\r\n\r\nI have the first list of questions that the model produced and the new list of questions that I got by manually improving the first list. How can I combine these two lists of questions (without answers) and use them as a dataset to retrain the valhalla/t5-small-e2e-qg model?\r\n\r\nI didn't find such example in your tutorial https://haystack.deepset.ai/tutorials/question-generation\r\n\r\nPlease show an example code.\r\n\r\nThanks a lot in advance.Hey @SergiyBarskyy,\r\n\r\nIf I understood you correctly, you want to adapt/fine-tune the valhalla/t5-small-e2e-qg question generation model on your data? \r\nIf so, the author of the model provided the details at https://github.com/patil-suraj/question_generation#fine-tuning\r\nFine-tuning such a model is out of scope for Haystack. But after you train it, then use it on Haystack \ud83d\udc4d ",
    "meta": {
      "name": "Human in The Loop: Making the Most Out of Automated Question Answering Annotation"
    },
    "answer": "Hey @SergiyBarskyy,\r\n\r\nIf I understood you correctly, you want to adapt/fine-tune the valhalla/t5-small-e2e-qg question generation model on your data? \r\nIf so, the author of the model provided the details at https://github.com/patil-suraj/question_generation#fine-tuning\r\nFine-tuning such a model is out of scope for Haystack. But after you train it, then use it on Haystack \ud83d\udc4d "
  },
  {
    "content": "Hey everyone\r\n\r\ni got two questions about the results of my chatbot application that i have built with haystack. My haystack setup follows the tutorials and does not have any special customization to it (once i described it another [GitHub discussion)](https://github.com/deepset-ai/haystack/discussions/2889)).\r\n\r\nI have 40 plain text documents that are of type explanatory or descriptive. When i query haystack with questions that are definitely answerable and can be found in the text i have two problems:\r\n\r\n- in most cases the score of the answers are extremely low (in range 0.1 to 0.6) even if the answer is correct\r\n- the answers are in most cases too short\r\n\r\n```\r\nExample: \r\n   question: \"Does the brown fox jump over the fence and climb a tree?\"\r\n   answer: \"The fox jumps over the fence\"\r\n   score: 0.4\r\n   context: \"... (more text) ... The brown fox jumps over the fence and climbs a tree. ... (more text) ...\"\r\n\r\nExpectation: \"The brown fox jumps over the fence and climbs a tree.\"\r\n```\r\n\r\nHow do i get higher scores for the answers (without the annotation tool)? And how can i improve the length of the answers (without a GenerativeQA pipeline, or would that in general incorporate more information into the answer, aside from the boilerplate text?)?\r\n\r\n\r\nBest,\r\nFelixHi @felixriehm ,\r\n\r\nJust to confirm (based on the previous discussions), you now have two pipelines? one for general docs, one for FAQ. \r\nGuessing these questions are for the document pipeline. Which retriever (model) and reader (model) are you using currently?",
    "meta": { "name": "Need help improving the answer result of haystack" },
    "answer": "Hi @felixriehm ,\r\n\r\nJust to confirm (based on the previous discussions), you now have two pipelines? one for general docs, one for FAQ. \r\nGuessing these questions are for the document pipeline. Which retriever (model) and reader (model) are you using currently?"
  },
  {
    "content": "Hello,\r\n\r\nI am quite desperate trying to make this work but I'm not able. I am doing my master thesis and would like to train a DPR using a SQuAD dataset and have to submit it next week.\r\n\r\nI will put you a bit in context. My goal is to train a DPR for Catalan language using the SQuAD dataset translated for this language (https://huggingface.co/datasets/BSC-TeMU/viquiquad). I saw that you have a script that directly takes the hard negative passages using the BM25 as it is shown in the paper. I tried to use this script for converting the SQuAD dataset into DPR format but I cannot initialize the ElasticSearch server neither the Faiss DocumentStore I tried to install Faiss to support the DocumentStore but nothing, my computer does not support cuda because my GPU is quite old. I also tried to execute the code in Collab because I am quite desperate but I am not able to access the server, I manage to create it but apart from that I cannot write data on it.\r\n\r\nMy final option was to change the code manually for making InMemoryDocumentStore able for making it work, but I got really confused with the objects used in the code. As my dataset is composed only of 11k questions, I do not really think that it is necessary to use ElasticSearch or Faiss to retriever the most relevant document, maybe I am wrong and have no really idea of what I am doing.\r\n\r\nI also tried to execute such code in the computer of a friend but error of ElasticSearch arrise again.\r\n![WhatsApp Image 2022-08-29 at 2 24 57 PM](https://user-images.githubusercontent.com/80527775/187247011-a9ba37d8-4971-41dc-90e1-600e88362604.jpeg)\r\n\r\nIf needed I can share more screenshots, I just deleted all the environments in anaconda because I none of them worked.\r\n\r\nIs it possible to make the changes for making it work with InMemoryDocumentStore? I think I already tried everything to make it work but I am going to get crazy with this. I also thought about creating my own code for retrieving the hard negative contexts but I am  just really stupid and not able to do it well and will take me a lot of time for the time left I have. Just needed to train this to give more consistency to the project.\r\n\r\nI will be really grateful if someone could give me a hint of what's going on with the code.\r\n\r\nIf not I will just rule out the idea of training the model\r\n\r\nThanks for all the effort creating this library. It helped me a lot with my master thesis.\r\n\r\nI am back here again with good news \ud83d\ude06 \r\n\r\nI just review the code and saw that it was necessary create an ElasticSearchDocumentStore because by default BM25 uses ElasticsearchDocumentStore, OpenSearchDocumentStore or OpenDistroElasticsearchDocumentStore\r\n\r\nI didn't have any hope that this could work but I just execute the code from this tutorial https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial4_FAQ_style_QA.ipynb#scrollTo=cJ36e9nia7-l for creating the ElasticSearchServer.\r\n\r\nWith 0 hope I just ran the code and magically everything started to work smooth. I know SQuAD is no the more suitable dataset for training the DPR but there in no other dataset for catalan's language so I just choose this one.  The hard negatives look good and will just run the training process with a pretrained catalan model.\r\n\r\nThanks again for developing this amazing library\r\n\r\nGreetings \ud83d\ude04 \r\nHi @Myko-10 \r\n\r\nI created a starter notebook based on tutorial 09 (DPR training): [DPR-Catalan.ipynb](https://colab.research.google.com/drive/1CK76kxRmRka39KMLOE6KHvanHB-jkATU?usp=sharing). \r\n\r\nIt downloads the data from huggingface, starts the ES server, and then runs the squad_to_dpr script to prepare the data for training. \r\n\r\nBut ya, good if you managed to get it working as well :D \r\n\r\nNote: I think you might have to be vigilant about the details of tokenization. It might be English based though not sure if it's a problem for Catalan.",
    "meta": {
      "name": "Trying to convert SQuAD to DPR format, unable to initialize the ElasticSearch"
    },
    "answer": "I am back here again with good news \ud83d\ude06 \r\n\r\nI just review the code and saw that it was necessary create an ElasticSearchDocumentStore because by default BM25 uses ElasticsearchDocumentStore, OpenSearchDocumentStore or OpenDistroElasticsearchDocumentStore\r\n\r\nI didn't have any hope that this could work but I just execute the code from this tutorial https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial4_FAQ_style_QA.ipynb#scrollTo=cJ36e9nia7-l for creating the ElasticSearchServer.\r\n\r\nWith 0 hope I just ran the code and magically everything started to work smooth. I know SQuAD is no the more suitable dataset for training the DPR but there in no other dataset for catalan's language so I just choose this one.  The hard negatives look good and will just run the training process with a pretrained catalan model.\r\n\r\nThanks again for developing this amazing library\r\n\r\nGreetings \ud83d\ude04 \r\n"
  },
  {
    "content": "So I was trying to load a saved model and Got this error..I am adding my written code and snippet here. This error happened while I was trying things on my local machine\r\n\r\nnew_document_store = FAISSDocumentStore.load(\"./faiss.index\")\r\nnew_retriever = DensePassageRetriever.load(\"./retriever.pt/\", document_store=new_document_store)\r\n\r\n![Missing Dependency Error](https://user-images.githubusercontent.com/109495543/186258831-54c4648b-2c37-41c7-8990-2fde43560274.JPG)\r\n\r\nAnother Error I Got While Working with google colab.. Both of these error is regarding Loading saved files and index.\r\n\r\n![Error 2](https://user-images.githubusercontent.com/109495543/186259061-9643d1a7-9aa1-453a-b102-a6cdd0ceb3ac.JPG)\r\n\r\n\r\n\r\nHi, \r\n## For the first part\r\n\r\n`faiss` is part of the optional dependencies. So during installation you'll need to specify it also be installed.\r\n\r\n`pip install farm-haystack[faiss]`\r\n\r\nLet me know if you had already done it but still get this issue. My guess is you ran a notebook with `[faiss]` included but this notebook doesn't do it when installing haystack (?)\r\n\r\n## For the second part\r\n\r\nWhat is probably happening is that the `faiss_document_store.db` file is missing during your second run. \r\n**Solution**: You need to save that too into Drive. For this, you can pass the location as `FAISSDocumentStore(sql_url=\"sqlite:///content/drive/MyDrive/Dataset/faiss_document_store.db\", ...)` when first creating it.\r\n\r\nAnd do the same when creating it again for loading.\r\n\r\n**Background and more details**:\r\n\r\n- `faiss_document_store.db` (the default sqlite location) stores the actual documents\r\n- When we do `faiss_document_store.save('index.faiss')` it saves the embeddings as `index.faiss` (and `index.json` for metadata)\r\n\r\nSo for `FAISSDocumentStore.load(...)` to work properly (without mismatch in num-docs and num-embeddings) we need to have all three files. Actually, the two files + the db connection (which by default is saved on disk with sqlite). \r\n\r\nOne option is to directly initialize instead of calling `load` and skip check with `validate_index_sync=False`. So you'd do: \r\n```python\r\nFAISSDocumentStore(faiss_index_path=\"<your-path>.faiss\", \r\n                   faiss_config_path=\"<your-path>.json\", \r\n                   validate_index_sync=False)\r\n```\r\n\r\nbut I guess having index/embeddings without documents doesn't make sense.",
    "meta": {
      "name": "type object 'MissingDependency' has no attribute 'load'"
    },
    "answer": "Hi, \r\n## For the first part\r\n\r\n`faiss` is part of the optional dependencies. So during installation you'll need to specify it also be installed.\r\n\r\n`pip install farm-haystack[faiss]`\r\n\r\nLet me know if you had already done it but still get this issue. My guess is you ran a notebook with `[faiss]` included but this notebook doesn't do it when installing haystack (?)\r\n\r\n## For the second part\r\n\r\nWhat is probably happening is that the `faiss_document_store.db` file is missing during your second run. \r\n**Solution**: You need to save that too into Drive. For this, you can pass the location as `FAISSDocumentStore(sql_url=\"sqlite:///content/drive/MyDrive/Dataset/faiss_document_store.db\", ...)` when first creating it.\r\n\r\nAnd do the same when creating it again for loading.\r\n\r\n**Background and more details**:\r\n\r\n- `faiss_document_store.db` (the default sqlite location) stores the actual documents\r\n- When we do `faiss_document_store.save('index.faiss')` it saves the embeddings as `index.faiss` (and `index.json` for metadata)\r\n\r\nSo for `FAISSDocumentStore.load(...)` to work properly (without mismatch in num-docs and num-embeddings) we need to have all three files. Actually, the two files + the db connection (which by default is saved on disk with sqlite). \r\n\r\nOne option is to directly initialize instead of calling `load` and skip check with `validate_index_sync=False`. So you'd do: \r\n```python\r\nFAISSDocumentStore(faiss_index_path=\"<your-path>.faiss\", \r\n                   faiss_config_path=\"<your-path>.json\", \r\n                   validate_index_sync=False)\r\n```\r\n\r\nbut I guess having index/embeddings without documents doesn't make sense."
  },
  {
    "content": "Hello There,\r\nWe are working on SquaD formatted custom data, in the logs during training what I could see is the training loss. \r\nAs a conventional approach, if there is a way for noting down \"Validation loss\", then I can somehow investigate overfit. \r\n\r\nIn order to get those eval details, I have added the following parameters:\r\n**param evaluate_every:** Perform dev set evaluation after this many steps of training.\r\n:**param eval_report**: If evaluate_every is not 0, specifies if an eval report should be generated when evaluating https://github.com/deepset-ai/haystack/blob/bc6f71b5ba1c308f25cfe81a5cada68528c086c7/haystack/modeling/training/base.py#L158 \r\n\r\nHowever, I get the following error: \r\n**TypeError:** train() got an unexpected keyword argument 'eval_report'\r\n\r\nPlease let me know if there is a way to know the overfit case for Haystack models. \r\nThank you,\r\nRenuk\r\nHi @Renuk9390! I assume you are using the `FARMReader`, please correct me if I am wrong. You are getting the TypeError, as the `FARMReader`'s `train` method does not allow an argument `eval_report`. `eval_report` is a property of the `Trainer` that is used inside the `FARMReader`'s `train` method, and it is set to `True` by default, so the eval report should be produced. The problem here probably is that the eval report is logged at the logging level `INFO`, while the default logging level in python is `WARNING`. In order to see the logs at the `INFO` level produced by Haystack, you need to add the following code snippet:\r\n\r\n```python\r\nimport logging\r\nlogging.basicConfig(format=\"%(levelname)s - %(name)s - %(message)s\", level=logging.WARNING)\r\nlogging.getLogger(\"haystack\").setLevel(logging.INFO)\r\n```\r\n\r\nPlease let me know if this helps :)",
    "meta": {
      "name": "Is there a way to know \"Haystack model's overfitting/not\" on our data?"
    },
    "answer": "Hi @Renuk9390! I assume you are using the `FARMReader`, please correct me if I am wrong. You are getting the TypeError, as the `FARMReader`'s `train` method does not allow an argument `eval_report`. `eval_report` is a property of the `Trainer` that is used inside the `FARMReader`'s `train` method, and it is set to `True` by default, so the eval report should be produced. The problem here probably is that the eval report is logged at the logging level `INFO`, while the default logging level in python is `WARNING`. In order to see the logs at the `INFO` level produced by Haystack, you need to add the following code snippet:\r\n\r\n```python\r\nimport logging\r\nlogging.basicConfig(format=\"%(levelname)s - %(name)s - %(message)s\", level=logging.WARNING)\r\nlogging.getLogger(\"haystack\").setLevel(logging.INFO)\r\n```\r\n\r\nPlease let me know if this helps :)"
  },
  {
    "content": "Hi there,\r\nWe are finetuning Minilm and Roberta on some domain-specific data, which we have kept in sQUAD format: \r\nMinilm is finetuning on 100 thousand samples and Roberta on 200 thousand samples. \r\n\r\nIssue: However, during the finetuning, logging information shows it only 30 thousand  (for Minilm) and 29 thousand (for Roberta).\r\nReference figure is attached (a line with 35%... where it is showing). \r\n\r\nIs someone aware of this issue?\r\nThe issue might be something silly, but we are unaware of it. \r\nAnswers will be really appreciated. \r\nThank you. \r\n![issue](https://user-images.githubusercontent.com/34164541/183397734-8a40c657-14b1-4090-ab8e-f46e8e1fa644.png)\r\n\r\n\r\nRenuk\r\nIs that a number that shows a number of steps by chance?? if I am not wrong!Hey @Renuk9390 sorry for making you wait for an answer here. As you suggested, the progress bar shows the number of training steps (number of batches), not the number of samples. [Here you can see the relevant line of code](https://github.com/deepset-ai/haystack/blob/26c938a8e6ac4fa76a2ab96ae5eafa201172ce24/haystack/modeling/training/base.py#L269) \r\n\r\nHope that helps, let me know \ud83d\ude0aHi Tuana Celik,\nThank you very much for this information.\nSure, I will connect to you in future, we are also working on domain\nspecific QA datasets and we made it up to 84% F1 with the models from\nHaystack and the suggested way of fine tuning.\n\nWe would like to evaluate the complete QA pipeline including both Reader\nand Retriever. And we are aiming to compare it to the benchmarks what\nHaystack claimed, however one doubt is that both are different datasets and\ndon\u2019t how it could be valid comparison.\n\nBut our is to produce this dataset and release to the community in future\nthrough an article.\n\nThanks\nRenuk\n\nOn Fri 12. Aug 2022 at 3:28 PM, Tuana Celik ***@***.***>\nwrote:\n\n> Hey @Renuk9390 <https://github.com/Renuk9390> sorry for making you wait\n> for an answer here. As you suggested, the progress bar shows the number of\n> training steps (number of batches), not the number of samples. Here you\n> can see the relevant line of code\n> <https://github.com/deepset-ai/haystack/blob/26c938a8e6ac4fa76a2ab96ae5eafa201172ce24/haystack/modeling/training/base.py#L269>\n>\n> Hope that helps, let me know \ud83d\ude0a\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/deepset-ai/haystack/discussions/2987#discussioncomment-3384023>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIEU6POHPW5NZWHKQUL4ANTVYZGN7ANCNFSM554S7MHA>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n-- \nBest,\nRenuk\n",
    "meta": {
      "name": "logging shows lesser samples compare to the actual given training samples"
    },
    "answer": "Hey @Renuk9390 sorry for making you wait for an answer here. As you suggested, the progress bar shows the number of training steps (number of batches), not the number of samples. [Here you can see the relevant line of code](https://github.com/deepset-ai/haystack/blob/26c938a8e6ac4fa76a2ab96ae5eafa201172ce24/haystack/modeling/training/base.py#L269) \r\n\r\nHope that helps, let me know \ud83d\ude0a"
  },
  {
    "content": "Hello! I was reading the documentation/API/Q&A but still could not find a way so I can split a original Document (parent), save those new splitted Documents (with new Ids and own embbedings) into Milvus, run a query and return the splitted documents but also their parent Ids (parent Document). My basic idea is to save smaller fragments of a long text into milvus and query for the whole Document later. Thanks.Hi @wilsonlimaneto - sorry this has taken a couple of days to respond. It sounds to me like you might make use if the `PreProcessor`. You could 2 things: 1. Either use the `split_length` argument when you're first preprocessing the files to create `Documents` - or 2. Since you already seem to have `Documents`, you could use the `split` function. This one works on a single document. \r\n\r\nAs for their ids, you could make use of the `meta` field to store the parent ids, or, you could make use of the `id_hash_keys` to construct one with the parent id included also.\r\n\r\nHere is the API reference for that: https://haystack.deepset.ai/reference/preprocessor\r\n\r\nHope this helps. Let me know \ud83d\ude0a",
    "meta": { "name": "How to get the id of the original splitted Document" },
    "answer": "Hi @wilsonlimaneto - sorry this has taken a couple of days to respond. It sounds to me like you might make use if the `PreProcessor`. You could 2 things: 1. Either use the `split_length` argument when you're first preprocessing the files to create `Documents` - or 2. Since you already seem to have `Documents`, you could use the `split` function. This one works on a single document. \r\n\r\nAs for their ids, you could make use of the `meta` field to store the parent ids, or, you could make use of the `id_hash_keys` to construct one with the parent id included also.\r\n\r\nHere is the API reference for that: https://haystack.deepset.ai/reference/preprocessor\r\n\r\nHope this helps. Let me know \ud83d\ude0a"
  },
  {
    "content": "Hi, I am building Haystack indexing pipeline, and as I understood from documentation PrePorcessors are language specific, while they work good with english, they are also pretty decent with different languages, but to improve perfomance I would need to specify language.\r\n\r\nQuestion: So I would want to build CustomLanguageDetector, which would route documents into three different Preprocessors somehting like this? \r\n\r\n````\r\nfrom langdetect import detect\r\n    \r\nclass CustomLanguageClassifier(BaseComponent):\r\n        outgoing_edges = 3\r\n\r\n        def run(self, document: Document):\r\n            \r\n            language = detect(document.content)\r\n            if language == 'en':\r\n                return {\"documents\":document}, \"output_1\"\r\n            elif language == 'gr':\r\n                return {\"documents\":document}, \"output_2\"               \r\n            elif language == 'cz':\r\n                return {\"documents\":document}, \"output_3\"\r\n            else:\r\n                raise ValueError('You are using docuemnt with unsupported language!')\r\n\r\n        def run_batch(self, documents:List[Document]):\r\n            pass\r\n````\r\n\r\nBut I am getting an error lie this one:\r\n\r\n````\r\nTypeError: run() missing 1 required positional argument: 'document'\r\n````\r\n\r\nDid someone make the same thing and could help me solve the issue?\r\nThanks a lot inadvance.Hello! Your approach is 90% correct! There are a couple of things to know.\r\n\r\n1. `run` takes an argument called `documents`. Mind that it contains a list of documents, so in the body of the function you have to iterate over them.\r\n2. `run` can produce output only on a single edge at a time. So if your `documents` list contains docs in different languages, you must discard some or throw an exception. In your case, this might be an issue...\r\n\r\nThere are other ways to approach this problem, so don't worry yet :smile: First of all I need to know, how does your pipeline look like, and how did you write your documents into the document store? What I have in mind is that you should rather add a `language` metadata tag to the documents _before_ writing them into the document store. So when you need to retrieve them, you can filter them by language. I don't know if this approach is viable in your setup though, so let me know! I will help you along.",
    "meta": { "name": "Custom language detector node" },
    "answer": "Hello! Your approach is 90% correct! There are a couple of things to know.\r\n\r\n1. `run` takes an argument called `documents`. Mind that it contains a list of documents, so in the body of the function you have to iterate over them.\r\n2. `run` can produce output only on a single edge at a time. So if your `documents` list contains docs in different languages, you must discard some or throw an exception. In your case, this might be an issue...\r\n\r\nThere are other ways to approach this problem, so don't worry yet :smile: First of all I need to know, how does your pipeline look like, and how did you write your documents into the document store? What I have in mind is that you should rather add a `language` metadata tag to the documents _before_ writing them into the document store. So when you need to retrieve them, you can filter them by language. I don't know if this approach is viable in your setup though, so let me know! I will help you along."
  },
  {
    "content": "Thear all, \r\nThank you for the amazing work done to come out with the paper and the 2 datasets GermanDPR and GermanQuAD.\r\nI have come across your valuable paper GermanQuAD and GermanDPR: Improving Non-English Question Answering and Passage Retrieval and I would like to do a replica study on it and I have a few questions, namely how to evalute a model trained on GermanDPR for retrieval:\r\n\r\n(1) How can I obtain the corpus you used? I.e. what specific Wikipedia dump was used in combination with GenSim's WikiCorpus(it will be nice if i can get the specific version of the corpus maybe as link)?\r\n(2) How can I use the GermanDPR test set to evaluate retrieval? As far as I can see, there are no passage IDs or similar. Once I have retrieved passages from the corpus, how can I use your labels to compute retrieval metrics (similar to your experiment in the paper, Figure 4). if there is an evaluation script available i will be happy to see it.\r\n\r\nI look forward to your feedback and thank you in advance.Hi @Zaker237 \r\nDownload links for GermanQuAD and GermanDPR are at the bottom of this page, including the test set: [https://deepset.ai/germanquad](https://t.co/NF4bZUgGsR) The Wikipedia dump is from June 2019. The latest dump can be found in xml format here: https://dumps.wikimedia.org/dewiki/latest/ Older dumps you would need to find from the source yourself but I think they are also available. Maybe somebody at Wikimedia can help you.Thank for the answer and the script. it really help me to understand the evaluation Process.  but still now i'm facing some other problem.\r\nI fund the wiki dump and wanted to use it bot for the evaluation and also the training but still yet i was not able to genere the IDs of the article please do you habe any advice for me??\r\nthank you",
    "meta": { "name": "Evaluation of Retrieval with GermanDPR" },
    "answer": "Hi @Zaker237 \r\nDownload links for GermanQuAD and GermanDPR are at the bottom of this page, including the test set: [https://deepset.ai/germanquad](https://t.co/NF4bZUgGsR) The Wikipedia dump is from June 2019. The latest dump can be found in xml format here: https://dumps.wikimedia.org/dewiki/latest/ Older dumps you would need to find from the source yourself but I think they are also available. Maybe somebody at Wikimedia can help you."
  },
  {
    "content": "hi im trying to install haystack on docker with a Dockerfile like that\r\n\r\n```\r\n# Python version on the development machine\r\nFROM python:3.9.12\r\n\r\n# Create an app directory\r\nRUN mkdir /app\r\n\r\n# Copy the app to the app directory\r\nCOPY . /app\r\n\r\n# set the working directory to the app directory\r\nWORKDIR /app\r\n\r\n# Install requirements\r\nRUN pip install -r requirements.txt\r\nRUN pip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\r\n\r\n# Run the app\r\nCMD [\"python\", \"run.py\"]\r\n```\r\n\r\nand build it with that command \r\n`docker build -t project:1.0 .`\r\n\r\nbut when i try to run the container this error appear\r\n\r\n```\r\nERROR\u00a0-\u00a0root\u00a0-\u00a0\u00a0`libsndfile`\u00a0not\u00a0found,\u00a0it's\u00a0probably\u00a0not\u00a0installed.\u00a0The\u00a0node\u00a0will\u00a0most\u00a0likely\u00a0crash.\u00a0Please\u00a0install\u00a0soundfile's\u00a0dependencies\u00a0(https://python-soundfile.readthedocs.io/en/latest/)\r\nTraceback\u00a0(most\u00a0recent\u00a0call\u00a0last):\r\n\u00a0\u00a0File\u00a0\"/usr/local/lib/python3.9/site-packages/haystack/nodes/audio/_text_to_speech.py\",\u00a0line\u00a011,\u00a0in\u00a0<module>\r\n\u00a0\u00a0\u00a0\u00a0import\u00a0soundfile\u00a0as\u00a0sf\r\n\u00a0\u00a0File\u00a0\"/usr/local/lib/python3.9/site-packages/soundfile.py\",\u00a0line\u00a0142,\u00a0in\u00a0<module>\r\n\u00a0\u00a0\u00a0\u00a0raise\u00a0OSError('sndfile\u00a0library\u00a0not\u00a0found')\r\nOSError:\u00a0sndfile\u00a0library\u00a0not\u00a0found\r\n/usr/local/lib/python3.9/site-packages/pydub/utils.py:170:\u00a0RuntimeWarning:\u00a0Couldn't\u00a0find\u00a0ffmpeg\u00a0or\u00a0avconv\u00a0-\u00a0defaulting\u00a0to\u00a0ffmpeg,\u00a0but\u00a0may\u00a0not\u00a0work\r\n\u00a0\u00a0warn(\"Couldn't\u00a0find\u00a0ffmpeg\u00a0or\u00a0avconv\u00a0-\u00a0defaulting\u00a0to\u00a0ffmpeg,\u00a0but\u00a0may\u00a0not\u00a0work\",\u00a0RuntimeWarning)\r\nTraceback\u00a0(most\u00a0recent\u00a0call\u00a0last):\r\n\u00a0\u00a0File\u00a0\"/app/run.py\",\u00a0line\u00a062,\u00a0in\u00a0<module>\r\n\u00a0\u00a0\u00a0\u00a0main()\r\n\u00a0\u00a0File\u00a0\"/app/run.py\",\u00a0line\u00a011,\u00a0in\u00a0main\r\n\u00a0\u00a0\u00a0\u00a0document_store\u00a0=\u00a0FAISSDocumentStore.load(index_path=\"data/\"\u00a0+\u00a0language_data\u00a0+\u00a0\"_data.faiss\",\r\nAttributeError:\u00a0type\u00a0object\u00a0'MissingDependency'\u00a0has\u00a0no\u00a0attribute\u00a0'load'\r\n```\r\nbut the soundfile lib is in my requirment.txt which i run in the docker file `SoundFile~=0.10.3.post1` i even tried to install it directly \r\n```\r\n# Install requirements\r\nRUN pip install -r requirements.txt\r\nRUN pip install SoundFile\r\nRUN pip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\r\n```\r\nbut the error is still the same i have haystack installed on my computer (windows) and it work perfectly.Hi @azetoy!\r\n\r\nYou need to install the Linux library, not just the Python wrapper to soundfile.\r\n\r\nPlease add this to your Dockerfile.\r\n`RUN apt-get install libsndfile1 -yqq`\r\n\r\nThere are other system dependencies that you may need to install, it depends on which part of haystack you are making usage of.Hi @danielbichuetti !\r\n\r\nThanks for your reply as you told me i installed the package successfully but now i have this strange erro code just telling me that `MissingDependency has no attribute load`\r\n\r\nhere the full error \r\n\r\n```\r\nTraceback\u00a0(most\u00a0recent\u00a0call\u00a0last):\r\n\u00a0\u00a0File\u00a0\"/app/run.py\",\u00a0line\u00a062,\u00a0in\u00a0<module>\r\n\u00a0\u00a0\u00a0\u00a0main()\r\n\u00a0\u00a0File\u00a0\"/app/run.py\",\u00a0line\u00a011,\u00a0in\u00a0main\r\n\u00a0\u00a0\u00a0\u00a0document_store\u00a0=\u00a0FAISSDocumentStore.load(index_path=\"data/\"\u00a0+\u00a0language_data\u00a0+\u00a0\"_data.faiss\",\r\nAttributeError:\u00a0type\u00a0object\u00a0'MissingDependency'\u00a0has\u00a0no\u00a0attribute\u00a0'load'\r\n```\r\n\r\nand my dockerfile\r\n```\r\n# Python version on the development machine\r\nFROM python:3.9.12\r\nFROM ubuntu:latest\r\n# Create an app directory\r\nRUN mkdir /app\r\n\r\n# Copy the app to the app directory\r\nCOPY . /app\r\n\r\n# set the working directory to the app directory\r\nWORKDIR /app\r\n\r\n# Install requirements\r\nRUN apt-get update\r\n# install python 3.9.12\r\n#RUN apt-get install -y python3.9\r\n\r\n# install pip version 22.0.4\r\nRUN apt-get install -y python3-pip && pip3 install --upgrade pip==22.0.4\r\nRUN apt-get install libsndfile1 -yqq\r\nRUN pip install -r requirements.txt\r\nRUN pip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\r\n\r\n# Run the app\r\nCMD [\"python3\", \"run.py\"]\r\n```\r\n\r\ni may need other Linux lib to install but the error dont tell me which one.",
    "meta": { "name": "Install Haystack With a Dockerfile" },
    "answer": "Hi @danielbichuetti !\r\n\r\nThanks for your reply as you told me i installed the package successfully but now i have this strange erro code just telling me that `MissingDependency has no attribute load`\r\n\r\nhere the full error \r\n\r\n```\r\nTraceback\u00a0(most\u00a0recent\u00a0call\u00a0last):\r\n\u00a0\u00a0File\u00a0\"/app/run.py\",\u00a0line\u00a062,\u00a0in\u00a0<module>\r\n\u00a0\u00a0\u00a0\u00a0main()\r\n\u00a0\u00a0File\u00a0\"/app/run.py\",\u00a0line\u00a011,\u00a0in\u00a0main\r\n\u00a0\u00a0\u00a0\u00a0document_store\u00a0=\u00a0FAISSDocumentStore.load(index_path=\"data/\"\u00a0+\u00a0language_data\u00a0+\u00a0\"_data.faiss\",\r\nAttributeError:\u00a0type\u00a0object\u00a0'MissingDependency'\u00a0has\u00a0no\u00a0attribute\u00a0'load'\r\n```\r\n\r\nand my dockerfile\r\n```\r\n# Python version on the development machine\r\nFROM python:3.9.12\r\nFROM ubuntu:latest\r\n# Create an app directory\r\nRUN mkdir /app\r\n\r\n# Copy the app to the app directory\r\nCOPY . /app\r\n\r\n# set the working directory to the app directory\r\nWORKDIR /app\r\n\r\n# Install requirements\r\nRUN apt-get update\r\n# install python 3.9.12\r\n#RUN apt-get install -y python3.9\r\n\r\n# install pip version 22.0.4\r\nRUN apt-get install -y python3-pip && pip3 install --upgrade pip==22.0.4\r\nRUN apt-get install libsndfile1 -yqq\r\nRUN pip install -r requirements.txt\r\nRUN pip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\r\n\r\n# Run the app\r\nCMD [\"python3\", \"run.py\"]\r\n```\r\n\r\ni may need other Linux lib to install but the error dont tell me which one."
  },
  {
    "content": "I have found in your [tutorial ](https://haystack.deepset.ai/tutorials/retrieval-augmented-generation) how to use Generative QA but can I just take a model that work with french (BERT variant maybe ?) here ?\r\n \r\n`generator = RAGenerator(\r\n    model_name_or_path=\"facebook/rag-token-nq\")`\r\n\r\nDo you have any advice on wich one ?Unfortunately, to the best of my knowledge, there is no french RAG model available. As an alternative, you could use an English model together with our `TranslationWrapper` node. It's described in the end of the following tutorial: https://haystack.deepset.ai/tutorials/question-generation\r\n@vblagoje are you aware of any LFQA models working with french texts? Probably not?@sjrl after editing my script to be that (it seem with RAG i cant load the data of the DPR i need to init a new one)\r\n\r\n```\r\ndocument_store.load(\"data/Fr_data.faiss\", config_path=\"data/Fr_data_config\")\r\n\r\nretriever = DensePassageRetriever(\r\n    document_store=document_store,\r\n    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\r\n    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\")\r\n\r\ndoc_dir = \"data/Fr\"\r\n\r\ndocs = convert_files_to_docs(dir_path=doc_dir, split_paragraphs=True)\r\n\r\ngenerator = RAGenerator(\r\n    model_name_or_path=\"facebook/rag-token-nq\",\r\n    top_k=5,\r\n    max_length=200,\r\n    min_length=2,\r\n    embed_title=False,\r\n    num_beams=2,\r\n)\r\n\r\npipe = GenerativeQAPipeline(generator, retriever)\r\n\r\nquestion = translator_model.translate(query=\"What are the most beautiful place ?\")\r\n\r\nres = pipe.run(query=question, params={\"Generator\": {\"top_k\": 1}, \"Retriever\": {\"top_k\": 1}})\r\nprint_answers(res, details=\"minimum\")\r\n\r\n```\r\n\r\nand the warning now is from an assert error\r\n```\r\nC:\\Users\\zined\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:179: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\r\n  warnings.warn(\r\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \r\nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \r\nThe class this function is called from is 'DPRQuestionEncoderTokenizer'.\r\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \r\nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \r\nThe class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\r\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \r\nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \r\nThe class this function is called from is 'BartTokenizer'.\r\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \r\nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \r\nThe class this function is called from is 'BartTokenizerFast'.\r\nSome weights of the model checkpoint at facebook/rag-token-nq were not used when initializing RagTokenForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.weight', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.bias']\r\n- This IS expected if you are initializing RagTokenForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing RagTokenForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nSome weights of RagTokenForGeneration were not initialized from the model checkpoint at facebook/rag-token-nq and are newly initialized: ['rag.generator.lm_head.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nINFO - haystack.modeling.utils -  Using devices: CPU\r\nINFO - haystack.modeling.utils -  Number of GPUs: 0\r\nINFO - haystack.modeling.utils -  Using devices: CPU\r\nINFO - haystack.modeling.utils -  Number of GPUs: 0\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\pipelines\\base.py\", line 509, in run\r\n    node_output, stream_id = self.graph.nodes[node_id][\"component\"]._dispatch_run(**node_input)\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\base.py\", line 172, in _dispatch_run\r\n    return self._dispatch_run_general(self.run, **kwargs)\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\base.py\", line 216, in _dispatch_run_general\r\n    output, stream = run_method(**run_inputs, **run_params)\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\retriever\\base.py\", line 278, in run\r\n    output, stream = run_query_timed(\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\retriever\\base.py\", line 109, in wrapper\r\n    ret = fn(*args, **kwargs)\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\retriever\\base.py\", line 327, in run_query\r\n    documents = self.retrieve(\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\retriever\\dense.py\", line 312, in retrieve\r\n    documents = self.document_store.query_by_embedding(\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\document_stores\\faiss.py\", line 607, in query_by_embedding\r\n    score_matrix, vector_id_matrix = self.faiss_indexes[index].search(query_emb, top_k)\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\faiss\\__init__.py\", line 341, in replacement_search\r\n    assert d == self.d\r\nAssertionError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\run.py\", line 44, in <module>\r\n    res = pipe.run(query=\"What are the most beautiful place ?\", params={\"Generator\": {\"top_k\": 1}, \"Retriever\": {\"top_k\": 1}})\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\pipelines\\standard_pipelines.py\", line 377, in run\r\n    output = self.pipeline.run(query=query, params=params, debug=debug)\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\pipelines\\base.py\", line 512, in run\r\n    raise Exception(\r\nException: Exception while running node `Retriever` with input `{'root_node': 'Query', 'params': {'Generator': {'top_k': 1}, 'Retriever': {'top_k': 1}}, 'query': 'What are the most beautiful place ?', 'node_id': 'Retriever'}`: , full stack trace: Traceback (most recent call last):\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\pipelines\\base.py\", line 509, in run\r\n    node_output, stream_id = self.graph.nodes[node_id][\"component\"]._dispatch_run(**node_input)\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\base.py\", line 172, in _dispatch_run\r\n    return self._dispatch_run_general(self.run, **kwargs)\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\base.py\", line 216, in _dispatch_run_general\r\n    output, stream = run_method(**run_inputs, **run_params)\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\retriever\\base.py\", line 278, in run\r\n    output, stream = run_query_timed(\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\retriever\\base.py\", line 109, in wrapper\r\n    ret = fn(*args, **kwargs)\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\retriever\\base.py\", line 327, in run_query\r\n    documents = self.retrieve(\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\nodes\\retriever\\dense.py\", line 312, in retrieve\r\n    documents = self.document_store.query_by_embedding(\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\haystack\\haystack\\document_stores\\faiss.py\", line 607, in query_by_embedding\r\n    score_matrix, vector_id_matrix = self.faiss_indexes[index].search(query_emb, top_k)\r\n  File \"C:\\Users\\zined\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\faiss\\__init__.py\", line 341, in replacement_search\r\n    assert d == self.d\r\nAssertionError\r\n\r\n```\r\n\r\nSome warning of unexpected tokenizer and bart configuration but still cant find out why this is not working :(Hi @sjrl sure here is the full code to train (generate the SQL DB and translate the doc from french to eng)\r\n\r\n```\r\nfrom haystack.document_stores import FAISSDocumentStore\r\nfrom haystack.nodes import TransformersTranslator\r\nfrom haystack.nodes import DensePassageRetriever\r\nfrom haystack.utils import convert_files_to_docs\r\n\r\n\r\ndef train_save():\r\n    document_store = FAISSDocumentStore(sql_url=\"sqlite:///Fr_data.db\", faiss_index_factory_str=\"Flat\")\r\n\r\n    # Let's first get some files that we want to use\r\n    doc_dir = \"data/Fr\"\r\n\r\n    # Convert files to dicts\r\n    docs = convert_files_to_docs(dir_path=doc_dir, split_paragraphs=True)\r\n\r\n    # Now, let's write the dicts containing documents to our DB.\r\n\r\n    retriever = DensePassageRetriever(\r\n        document_store=document_store,\r\n        query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\r\n        passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\"\r\n    )\r\n    document_store.delete_documents()\r\n    document_store.write_documents(translate(docs))\r\n    document_store.update_embeddings(retriever=retriever)\r\n\r\n    document_store.save(\"data/Fr_data.faiss\", config_path=\"data/Fr_data_config\")\r\n    retriever.save(\"data/Fr_data_retriever.pt\")\r\n\r\n\r\n# translate function\r\ndef translate(doc):\r\n    translator = TransformersTranslator(model_name_or_path=\"Helsinki-NLP/opus-mt-fr-en\")\r\n    return translator.translate_batch(documents=doc)\r\n\r\n\r\ndef main():\r\n    train_save()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nand here the full code to ask the question \r\n```\r\nfrom haystack.document_stores import FAISSDocumentStore\r\nfrom haystack.nodes import DensePassageRetriever\r\nfrom haystack.nodes import RAGenerator\r\nfrom haystack.nodes import TransformersTranslator\r\nfrom haystack.pipelines import GenerativeQAPipeline\r\nfrom haystack.utils import print_answers, convert_files_to_docs\r\n\r\nlanguage_data = \"Fr\"\r\n\r\ndocument_store = FAISSDocumentStore.load(index_path=\"data/Fr_data.faiss\",\r\n                                         config_path=\"data/Fr_data_config\")\r\n\r\ndocument_store.load(\"data/Fr_data.faiss\",\r\n                    config_path=\"data/Fr_data_config\")\r\n\r\nretriever = DensePassageRetriever(\r\n    document_store=document_store,\r\n    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\r\n    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\")\r\n\r\ndoc_dir = \"data/Fr\"\r\n\r\ndocs = convert_files_to_docs(dir_path=doc_dir, split_paragraphs=True)\r\n\r\ngenerator = RAGenerator(\r\n    model_name_or_path=\"facebook/rag-token-nq\",\r\n    top_k=5,\r\n    max_length=200,\r\n    min_length=2,\r\n    num_beams=2\r\n)\r\n\r\npipe = GenerativeQAPipeline(generator, retriever)\r\n\r\ninput_t = TransformersTranslator(model_name_or_path=\"Helsinki-NLP/opus-mt-fr-en\")\r\n\r\nquestion = input_t.translate(query=\"Quel est le plus beau lieux ?\")\r\n\r\nres = pipe.run(query=question, params={\"Generator\": {\"top_k\": 1}, \"Retriever\": {\"top_k\": 1}})\r\nprint_answers(res, details=\"minimum\")\r\n\r\n```\r\n\r\nas for the doc here a sample of the french doc i use https://we.tl/t-HcknFdE5lW hope it help and thank you for your help\r\n",
    "meta": {
      "name": "How to use Generative QA with other language (like french)."
    },
    "answer": "Unfortunately, to the best of my knowledge, there is no french RAG model available. As an alternative, you could use an English model together with our `TranslationWrapper` node. It's described in the end of the following tutorial: https://haystack.deepset.ai/tutorials/question-generation\r\n@vblagoje are you aware of any LFQA models working with french texts? Probably not?"
  },
  {
    "content": "So, I'm building a LFQA pipeline, and when the load the models inside DPR it gives some tokenizer warning(mismatch tokenizer). I wanted to ask if it's something that might cause problem in the retrieving process?\r\nIf yes, then what's the solution to it? Also curious what's actually causing this problem?, as I'm totally following the tutorial in your docs. \r\n\r\nCode:\r\n\r\n```\r\nretriever = DensePassageRetriever(\r\n    document_store=document_store,\r\n    query_embedding_model=\"vblagoje/dpr-question_encoder-single-lfqa-wiki\",\r\n    passage_embedding_model=\"vblagoje/dpr-ctx_encoder-single-lfqa-wiki\",\r\n    use_gpu=True,\r\n    \r\n)\r\n\r\ndocument_store.update_embeddings(retriever)\r\n```\r\n\r\n\r\nWarnings:\r\n\r\n> haystack.modeling.model.language_model -  Could not find vblagoje/dpr-question_encoder-single-lfqa-wiki locally.\r\nINFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\r\nINFO - haystack.modeling.model.language_model -  Loaded vblagoje/dpr-question_encoder-single-lfqa-wiki\r\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \r\nThe tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \r\nThe class this function is called from is 'DPRContextEncoderTokenizerFast'.\r\n\r\n> Hi, @Zeni69 I'm not entirely sure what is causing this error to be thrown. It is an error that comes from the HuggingFace library and it's unclear why there is a mismatch. You can check that the Tokenizer classes for both the `query_embeddingmodel` and the `passage_embedding_model` are correct by running the following:\r\n\r\n```python\r\ntype(retriever.query_tokenizer)\r\n# transformers.models.dpr.tokenization_dpr_fast.DPRQuestionEncoderTokenizerFast\r\n```\r\n\r\ngives the expected `DPRQuestionEncoderTokenizerFast` tokenizer for the `query_embedding_model` and \r\n\r\n```python\r\ntype(retriever.passage_tokenizer)\r\n# transformers.models.dpr.tokenization_dpr_fast.DPRContextEncoderTokenizerFast\r\n``` \r\ngives the expected `DPRContextEncoderTokenizerFast` tokenizer for the `passage_embedding_model`. \r\n\r\nSo to answer your question you are doing everything correctly and everything is being loaded properly despite the warning being thrown by HuggingFace. We will have to investigate further why this might be occurring. You've probably hit this known issue in HF Transformers: https://github.com/huggingface/transformers/issues/12926. It's just a misleading log message, nothing wrong is actually happening behind the scenes.",
    "meta": { "name": "Tokenizer Warning when Loading DPR" },
    "answer": "You've probably hit this known issue in HF Transformers: https://github.com/huggingface/transformers/issues/12926. It's just a misleading log message, nothing wrong is actually happening behind the scenes."
  },
  {
    "content": "Hey, I'm trying to test out the crawler from haystack. \r\nI'm running the command `!pip install farm-haystack[colab,crawler]` but it just goes on forever in trying to resolve dependencies issues etc. I have had it running or like 3 hours and its still going. How to resolve this?\r\n\r\n> Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\r\nRequirement already satisfied: farm-haystack[Crawler,colab] in /usr/local/lib/python3.7/dist-packages (1.6.0)\r\nRequirement already satisfied: posthog in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.4.9)\r\nRequirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.9.1)\r\nRequirement already satisfied: elastic-apm in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (6.10.1)\r\nRequirement already satisfied: rapidfuzz<3,>=2.0.15 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (2.3.0)\r\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (4.3.3)\r\nRequirement already satisfied: elasticsearch<7.11,>=7.7 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (7.10.1)\r\nRequirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.0.2)\r\nRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (3.7)\r\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.7.3)\r\nRequirement already satisfied: torch<1.13,>1.9 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.12.0+cu113)\r\nRequirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (2.2.2)\r\nRequirement already satisfied: azure-ai-formrecognizer==3.2.0b2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (3.2.0b2)\r\nRequirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.0.9)\r\nRequirement already satisfied: mmh3 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (3.0.0)\r\nRequirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.2.2)\r\nRequirement already satisfied: tika in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.24)\r\nRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (4.64.0)\r\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (8.13.0)\r\nRequirement already satisfied: azure-core<1.23 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.22.1)\r\nRequirement already satisfied: transformers==4.20.1 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (4.20.1)\r\nRequirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (2.6.3)\r\nRequirement already satisfied: mlflow in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.27.0)\r\nRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.3.5)\r\nRequirement already satisfied: quantulum3 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (0.7.10)\r\nRequirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (0.8.11)\r\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (4.12.0)\r\nRequirement already satisfied: huggingface-hub<0.8.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (0.7.0)\r\nRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (0.3.5.1)\r\nRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (2.23.0)\r\nRequirement already satisfied: webdriver-manager in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (3.8.2)\r\nRequirement already satisfied: selenium!=4.1.4 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (4.3.0)\r\nRequirement already satisfied: grpcio==1.43.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[Crawler,colab]) (1.43.0)\r\nRequirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack[Crawler,colab]) (1.15.0)\r\nRequirement already satisfied: msrest>=0.6.21 in /usr/local/lib/python3.7/dist-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack[Crawler,colab]) (0.6.21)\r\nRequirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.7/dist-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack[Crawler,colab]) (1.1.28)\r\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.1->farm-haystack[Crawler,colab]) (1.21.6)\r\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.1->farm-haystack[Crawler,colab]) (0.12.1)\r\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.1->farm-haystack[Crawler,colab]) (21.3)\r\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.1->farm-haystack[Crawler,colab]) (2022.6.2)\r\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.1->farm-haystack[Crawler,colab]) (6.0)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.1->farm-haystack[Crawler,colab]) (3.7.1)\r\nRequirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch<7.11,>=7.7->farm-haystack[Crawler,colab]) (1.24.3)\r\nRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch<7.11,>=7.7->farm-haystack[Crawler,colab]) (2022.6.15)\r\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.8.0,>=0.5.0->farm-haystack[Crawler,colab]) (4.1.1)\r\nRequirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b2->farm-haystack[Crawler,colab]) (0.6.1)\r\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b2->farm-haystack[Crawler,colab]) (1.3.1)\r\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.20.1->farm-haystack[Crawler,colab]) (3.0.9)\r\nRequirement already satisfied: jarowinkler<2.0.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from rapidfuzz<3,>=2.0.15->farm-haystack[Crawler,colab]) (1.2.0)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[Crawler,colab]) (3.0.4)\r\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[Crawler,colab]) (2.10)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b2->farm-haystack[Crawler,colab]) (3.2.0)\r\nRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[Crawler,colab]) (1.1.0)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[Crawler,colab]) (3.1.0)\r\nRequirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.7/dist-packages (from selenium!=4.1.4->farm-haystack[Crawler,colab]) (0.9.2)\r\nCollecting urllib3[secure,socks]~=1.26\r\n  Using cached urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\r\nRequirement already satisfied: trio~=0.17 in /usr/local/lib/python3.7/dist-packages (from selenium!=4.1.4->farm-haystack[Crawler,colab]) (0.21.0)\r\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=2.2.0->farm-haystack[Crawler,colab]) (0.1.96)\r\nRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=2.2.0->farm-haystack[Crawler,colab]) (0.13.0+cu113)\r\nRequirement already satisfied: outcome in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium!=4.1.4->farm-haystack[Crawler,colab]) (1.2.0)\r\nRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium!=4.1.4->farm-haystack[Crawler,colab]) (21.4.0)\r\nRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium!=4.1.4->farm-haystack[Crawler,colab]) (2.4.0)\r\nRequirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium!=4.1.4->farm-haystack[Crawler,colab]) (1.2.0)\r\nRequirement already satisfied: async-generator>=1.9 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium!=4.1.4->farm-haystack[Crawler,colab]) (1.10)\r\nRequirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.7/dist-packages (from trio-websocket~=0.9->selenium!=4.1.4->farm-haystack[Crawler,colab]) (1.1.0)\r\n  Using cached urllib3-1.26.10-py2.py3-none-any.whl (139 kB)\r\n  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\r\n  Using cached urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\r\n  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\r\n  Using cached urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\r\n  Using cached urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\r\n  Using cached urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\r\n  Using cached urllib3-1.26.3-py2.py3-none-any.whl (137 kB)\r\n  Using cached urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\r\n  Using cached urllib3-1.26.1-py2.py3-none-any.whl (136 kB)\r\n  Using cached urllib3-1.26.0-py2.py3-none-any.whl (136 kB)\r\nINFO: pip is looking at multiple versions of urllib3 to determine which version is compatible with other requirements. This could take a while.\r\nCollecting urllib3<2,>=1.21.1\r\n  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\r\nINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\r\n  Using cached urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\r\n  Using cached urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\r\n  Using cached urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\r\n  Using cached urllib3-1.25.7-py2.py3-none-any.whl (125 kB)\r\n  Using cached urllib3-1.25.6-py2.py3-none-any.whl (125 kB)\r\n  Using cached urllib3-1.25.5-py2.py3-none-any.whl (125 kB)\r\nINFO: pip is looking at multiple versions of urllib3 to determine which version is compatible with other requirements. This could take a while.\r\n  Using cached urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\r\nINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\r\n  Using cached urllib3-1.25.3-py2.py3-none-any.whl (150 kB)\r\n  Using cached urllib3-1.25.2-py2.py3-none-any.whl (150 kB)\r\n  Using cached urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\r\n  Using cached urllib3-1.24.2-py2.py3-none-any.whl (131 kB)\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\r\n  Using cached urllib3-1.24.1-py2.py3-none-any.whl (118 kB)\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\r\n  Using cached urllib3-1.24-py2.py3-none-any.whl (117 kB)\r\n  Using cached urllib3-1.23-py2.py3-none-any.whl (133 kB)\r\n  Using cached urllib3-1.22-py2.py3-none-any.whl (132 kB)\r\n  Using cached urllib3-1.21.1-py2.py3-none-any.whl (131 kB)\r\nINFO: pip is looking at multiple versions of typing-extensions to determine which version is compatible with other requirements. This could take a while.\r\nCollecting typing-extensions>=3.7.4.3\r\n  Using cached typing_extensions-4.3.0-py3-none-any.whl (25 kB)\r\n  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)\r\n  Using cached typing_extensions-4.1.1-py3-none-any.whl (26 kB)\r\n  Using cached typing_extensions-4.1.0-py3-none-any.whl (26 kB)\r\n  Using cached typing_extensions-4.0.1-py3-none-any.whl (22 kB)\r\n  Using cached typing_extensions-4.0.0-py3-none-any.whl (22 kB)\r\n  Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\r\nINFO: pip is looking at multiple versions of typing-extensions to determine which version is compatible with other requirements. This could take a while.\r\n  Using cached typing_extensions-3.10.0.1-py3-none-any.whl (26 kB)\r\n  Using cached typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\r\n  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\r\nINFO: pip is looking at multiple versions of trio-websocket to determine which version is compatible with other requirements. This could take a while.\r\nCollecting trio-websocket~=0.9\r\n  Using cached trio_websocket-0.9.2-py3-none-any.whl (16 kB)\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\r\n  Using cached trio_websocket-0.9.1-py3-none-any.whl (16 kB)\r\n  Using cached trio_websocket-0.9.0-py3-none-any.whl (16 kB)\r\nINFO: pip is looking at multiple versions of attrs to determine which version is compatible with other requirements. This could take a while.\r\nCollecting attrs>=19.2.0\r\n  Using cached attrs-22.1.0-py2.py3-none-any.whl (58 kB)\r\nINFO: pip is looking at multiple versions of trio-websocket to determine which version is compatible with other requirements. This could take a while.\r\n  Using cached attrs-21.4.0-py2.py3-none-any.whl (60 kB)\r\n  Downloading attrs-21.3.0-py2.py3-none-any.whl (61 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61 kB 557 kB/s \r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\r\n  Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 53 kB 3.2 MB/s \r\n  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49 kB 7.5 MB/s \r\n  Downloading attrs-20.2.0-py2.py3-none-any.whl (48 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48 kB 5.8 MB/s \r\n  Downloading attrs-20.1.0-py2.py3-none-any.whl (49 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49 kB 7.1 MB/s \r\nINFO: pip is looking at multiple versions of attrs to determine which version is compatible with other requirements. This could take a while.\r\n  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\r\n  Downloading attrs-19.2.0-py2.py3-none-any.whl (40 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40 kB 6.3 MB/s \r\nINFO: pip is looking at multiple versions of async-generator to determine which version is compatible with other requirements. This could take a while.\r\nCollecting async-generator>=1.9\r\n  Using cached async_generator-1.10-py3-none-any.whl (18 kB)\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\r\n\r\n\r\n> I believe there is a dependency conflict with some of the packages that crawler uses.\r\nWhen tried to use pip's previous version of dependency resolver, here the error:\r\n\r\n> ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nrequests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.11 which is incompatible.\r\nmsrest 0.7.1 requires azure-core>=1.24.0, but you have azure-core 1.22.1 which is incompatible.\r\ndatascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\r\nSuccessfully installed cryptography-37.0.4 pyOpenSSL-22.0.0 urllib3-1.26.11\r\nWARNING: The following packages were previously imported in this runtime:\r\n  [urllib3]\r\nYou must restart the runtime in order to use newly installed versions.\r\n\r\n> Hi @Zeni69 this should work now in Google Colab. We just merged the fix in this PR https://github.com/deepset-ai/haystack/pull/2921 \r\n```python\r\n!pip install --upgrade pip\r\n!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab,crawler]\r\n```",
    "meta": {
      "name": "Downloading and Installing Crawler on Colab is taking forever"
    },
    "answer": "Hi @Zeni69 this should work now in Google Colab. We just merged the fix in this PR https://github.com/deepset-ai/haystack/pull/2921 \r\n```python\r\n!pip install --upgrade pip\r\n!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab,crawler]\r\n```"
  },
  {
    "content": "**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\n\r\nMy data has a metadata that divides my data, so for each request I can get the data for one of them.\r\nFor example, if the value for \"x\" in my request is \"a\", I will do the request on data where the \"x\" in the metadata is \"a\".\r\nSo to make that, I use:\r\n``\r\nparams={\"Retriever\": {\"filters\": {\"x\": request[\"x\"]}}\r\n``\r\n\r\nThe issue is that with a big amount of data, I would like that \"x\" value to be my index to make performance better.\r\n\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\nCurrently index is a parameter for data store, so if I create a different data store for each request, I'll have to probably create a different retriever which means load the model for each request which won't be ideal.\r\nIs there any solution I miss here, and if not, can this be a feature where I specify the index in the pipe.run function instead?\r\n\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\nI considered creating a different data store and retriever for each request but I assume it'll be very slow (correct me if I'm wrong).\r\nI also considered to change the data store's index in my predict request with ``data_store.index = new_index`` but I'm afraid to do it in a multi-threaded program.\r\nHi, @naor-pragma thanks for asking your question. I believe you can achieve what you want without needing to create a new retriever for each document_store. \r\n\r\n1. You can add multiple indices to the same document store by specifying the `index` parameter in the `DocumentStore.write_documents()` method. The API docs explaining the parameters can be found here: https://haystack.deepset.ai/reference/document-store\r\n2. It's possible to specify the `index` parameter to the `retrieve()` method of the retriever. Here are the API docs on all parameters for the `Retriever.retrieve()` method https://haystack.deepset.ai/reference/retriever.\r\n\r\nSo I believe you can pass this on by specifying \r\n```python\r\nparams = {\"Retriever\": {\"index\": YOUR_INDEX}}\r\n```\r\n\r\nWith these two steps you should only need one document store to contain all of your different types of data and then only need one retriever which can search through your different indices by passing the `index` parameter. \r\n\r\nLet me know if this helps! ",
    "meta": { "name": "index per request" },
    "answer": "Hi, @naor-pragma thanks for asking your question. I believe you can achieve what you want without needing to create a new retriever for each document_store. \r\n\r\n1. You can add multiple indices to the same document store by specifying the `index` parameter in the `DocumentStore.write_documents()` method. The API docs explaining the parameters can be found here: https://haystack.deepset.ai/reference/document-store\r\n2. It's possible to specify the `index` parameter to the `retrieve()` method of the retriever. Here are the API docs on all parameters for the `Retriever.retrieve()` method https://haystack.deepset.ai/reference/retriever.\r\n\r\nSo I believe you can pass this on by specifying \r\n```python\r\nparams = {\"Retriever\": {\"index\": YOUR_INDEX}}\r\n```\r\n\r\nWith these two steps you should only need one document store to contain all of your different types of data and then only need one retriever which can search through your different indices by passing the `index` parameter. \r\n\r\nLet me know if this helps! "
  },
  {
    "content": "Hi Team,\r\n\r\nWe are exploring the haystack semantic search capability on Knowledge graphs.\r\n\r\nCould you please help us by providing the process to train the model for our own dataset ?\r\n\r\nThe section \"https://haystack.deepset.ai/tutorials/fine-tuning-a-model\" has provided a way to train a natural language text with annotation tool. But is there any reference docs or steps to train a model for a custom knowledge graph data ?\r\n\r\nRequesting an advice on this , that helps a lot!.\r\n\r\nThanks,\r\nMuthuHi @Muthusamy-J sure, happy to help. We got this question earlier and you can find the answer here: https://github.com/deepset-ai/haystack/discussions/1146 Good luck with your project! Let us know how it goes! \ud83d\ude42 ",
    "meta": {
      "name": "Customizing the model for the own knowledge graph data"
    },
    "answer": "Hi @Muthusamy-J sure, happy to help. We got this question earlier and you can find the answer here: https://github.com/deepset-ai/haystack/discussions/1146 Good luck with your project! Let us know how it goes! \ud83d\ude42 "
  },
  {
    "content": "Hi All,\r\n\r\nCould someone, please let me know what is the required **python, pip and centos operating system version** that's required to be compatible to run with the latest version of haystack to execute knowledge graph db-based question and answering ?Hi @Muthusamy-J,\r\n\r\nyou need python 3.7 or higher for Haystack.\r\nFor pip we recommend to use the latest version, but that's not a strict requirement. If you need to stick to an earlier version, I'd suggest to try to install Haystack with it. Most of the time this should work as well.\r\nFor CentOS we do not have any specific requirements. All you need is a working pytorch environment. If the standard pip way to install it does not work, try out pytorch.org's install section (e.g. via conda envs). Appart from that you can use an in memory graph (InMemoryKnowledgeGraph) which does not require any additional software or a graphDB database (GraphDBKnowledgeGraph). \r\nA good way to start and also to assess compatibility is to run Tutorial 10.",
    "meta": { "name": "Pre-requisites (environment) for running haystack" },
    "answer": "Hi @Muthusamy-J,\r\n\r\nyou need python 3.7 or higher for Haystack.\r\nFor pip we recommend to use the latest version, but that's not a strict requirement. If you need to stick to an earlier version, I'd suggest to try to install Haystack with it. Most of the time this should work as well.\r\nFor CentOS we do not have any specific requirements. All you need is a working pytorch environment. If the standard pip way to install it does not work, try out pytorch.org's install section (e.g. via conda envs). Appart from that you can use an in memory graph (InMemoryKnowledgeGraph) which does not require any additional software or a graphDB database (GraphDBKnowledgeGraph). \r\nA good way to start and also to assess compatibility is to run Tutorial 10."
  },
  {
    "content": "Hey everyone\r\n\r\ni have built a chatbot with haystack by following the tutorials on the website and i now got some question about the correctness of my setup.\r\n\r\nI'm using the rest api with a pipeline yml file. There, I defined a retriever of type `BM25Retriever` and a reader of type `FARMReader` which uses `deepset/roberta-base-squad2` as model. The pipeline is simple: query -> retriever -> reader. Now i wonder if this configuration works fine with the way i have written data to elasticsearch?\r\n\r\nI have written a python script to upload documents to elasticsearch. This script is divided in two sections. The first section writes general documents to elasticsearch with \r\n\r\n    document_store = ElasticsearchDocumentStore(host=\"localhost\", port=9200, username=\"\", password=\"\", index=\"document\", similarity=\"dot_product\")\r\n    document_store.write_documents(docs)\r\n\r\n\r\nAfter that i update the embedding with `document_store.update_embeddings(retriever)`. As a retriever is use `DensePassageRetriever` with\r\n\r\n    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\r\n    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\r\n\r\nIn the second section i write FAQ data to elasticsearch which follows [this tutorial](https://haystack.deepset.ai/tutorials/existing-faqs).\r\n\r\n\r\n    # Upload FAQ docs to elastic search\r\n    document_store_FAQ = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\",\r\n                                                index=\"faq\",\r\n                                                similarity=\"cosine\",\r\n                                                embedding_field=\"question_emb\",\r\n                                                embedding_dim=384,\r\n                                                excluded_meta_data=[\"question_emb\"])\r\n    print(document_store_FAQ.get_all_documents())\r\n    document_store_FAQ.delete_documents()\r\n    \r\n    retriever = EmbeddingRetriever(document_store=document_store_FAQ, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\", use_gpu=False)\r\n    \r\n    df = pd.read_csv(str(sys.argv[2]))\r\n    # Minimal cleaning\r\n    df.fillna(value=\"\", inplace=True)\r\n    df[\"question\"] = df[\"question\"].apply(lambda x: x.strip())\r\n    print(df.head())\r\n\r\n    # Get embeddings for our questions from the FAQs\r\n    questions = list(df[\"question\"].values)\r\n    df[\"question_emb\"] = retriever.embed_queries(texts=questions)\r\n    df = df.rename(columns={\"question\": \"content\"})\r\n    \r\n    # Convert Dataframe to list of dicts and index them in our DocumentStore\r\n    docs_to_index = df.to_dict(orient=\"records\")\r\n    document_store_FAQ.write_documents(docs_to_index)\r\n    #document_store_FAQ.update_embeddings(retriever)\r\n    print(document_store_FAQ.get_all_documents())\r\n\r\nThe reasons i am confused with this setup:\r\n- When I'm writing the general documents to elasticsearch I'm using `DensePassageRetriever` to embed the data but when i do a query i use `BM25Retriever` (which is [sparse](https://haystack.deepset.ai/usage/v0.4.0/retriever)). Is this intended to work correctly or are the embedding values completely different and don't work together?\r\n- My intention with separating general documents and FAQ data is to improve the results. However, I'm not sure if this takes effect. I'm using `sentence-transformers/all-MiniLM-L6-v2` embedded retriever when i write FAQ data to elasticsearch but when i do a query it will take the `deepset/roberta-base-squad2` `FARMReader` (is this reader suited for FAQ data?) and in addition to that i retrieve the data with `BM25Retriever` which is, like in my first question, not the same as the `sentence-transformers/all-MiniLM-L6-v2` embedded retriever that i used initially to write to elasticsearch. So there can't be a positive effect? Do i have to use two retrievers and two readers (one for general documents and one for FAQ data) and then merge the results? And what is the purpose of using two different indices for writing to elasticsearch (in my case 'faq' and 'document'). Can't i just write all documents to elasticsearch with the same index?\r\n\r\n\r\nThen there are other general question that i have:\r\n- When I do a query there is a 'answers' and a 'documents' attribute. How can i control that output so it only shows me 'answers' or 'documents'?\r\n- When I do a query usually the scores of 'documents' are higher than the 'answers' scores. Is that normal? If yes, why? Are the scores of 'documents' and 'answers' comparable?\r\n\r\n\r\nBest,\r\nFelixTo answer your first question\r\n\r\n> * When I'm writing the general documents to elasticsearch I'm using DensePassageRetriever to embed the data but when i do a query i use BM25Retriever (which is [sparse](https://haystack.deepset.ai/usage/v0.4.0/retriever)). Is this intended to work correctly or are the embedding values completely different and don't work together?\r\n\r\nThe `BM25Retriever` actually does not use vector embeddings when retrieving documents. It's based off of TF-IDF (more details can be found [here](https://haystack.deepset.ai/usage/v0.4.0/retriever)). So in short this combination is intended to work, but you are currently not using the embeddings for document retrieval. If you never plan on using a `DensePassageRetriever` in your query pipeline then you do not need to run `document_store.update_embeddings()` after writing your documents to the document store. If you would like to use `DensePassageRetrieval` in your query pipeline then you should replace the `BM25Retriever` with the `DensePassageRetrieval`. > My intention with separating general documents and FAQ data is to improve the results. \r\n\r\nI'm not entirely sure what you mean by improving the results in this case. A reason I could see for having two separate indices for general data and FAQ data is if you have two separate query pipelines for general data and FAQ data. \r\n\r\n>  Do i have to use two retrievers and two readers (one for general documents and one for FAQ data) and then merge the results?And what is the purpose of using two different indices for writing to elasticsearch (in my case 'faq' and 'document'). Can't i just write all documents to elasticsearch with the same index?\r\n\r\nNo, you do not have to use two separate retrievers and readers. Yes, you could write both the general documents and FAQ data to the same document store using the same index. If you do this then your query pipeline will return answers that could either be from the general data or the FAQ data. \r\n\r\n",
    "meta": { "name": "Need help reviewing my configuration" },
    "answer": "To answer your first question\r\n\r\n> * When I'm writing the general documents to elasticsearch I'm using DensePassageRetriever to embed the data but when i do a query i use BM25Retriever (which is [sparse](https://haystack.deepset.ai/usage/v0.4.0/retriever)). Is this intended to work correctly or are the embedding values completely different and don't work together?\r\n\r\nThe `BM25Retriever` actually does not use vector embeddings when retrieving documents. It's based off of TF-IDF (more details can be found [here](https://haystack.deepset.ai/usage/v0.4.0/retriever)). So in short this combination is intended to work, but you are currently not using the embeddings for document retrieval. If you never plan on using a `DensePassageRetriever` in your query pipeline then you do not need to run `document_store.update_embeddings()` after writing your documents to the document store. If you would like to use `DensePassageRetrieval` in your query pipeline then you should replace the `BM25Retriever` with the `DensePassageRetrieval`. "
  },
  {
    "content": "Hello haystack, \r\n\r\nI am interested to learn more about how the Text2Sparql model in the Tutorial10_Knowledge_Graph tutorial was trained. If you can direct me to the right place, I will appreciate it. \r\n\r\nEliranHi @eboraks the training of such a model is not supported in Haystack but we give some hints how to do it here in the last paragraph at the very bottom of the page: https://haystack.deepset.ai/docs/latest/knowledgegraphmd You would need to train a BART model with transformers to translate text questions to queries in SPARQL format. There are only a few training datasets available to train such a model. One is the LC-QuAD dataset. Hope this helps! :slightly_smiling_face:Let me briefly summarize the main steps that we took to train a Text2SPARQL model on the Harry Potter Fandom data.\r\n\r\n1. We started with the LC-QuAD dataset, which includes text questions and corresponding SPARQL queries, for example:\r\n\r\n> \"sparql_wikidata\": \" select distinct ?obj where { wd:Q127998 wdt:P27 ?obj . ?obj wdt:P31 wd:Q6256 } \",\r\n> \"paraphrased_question\": \"Which country is Mahmoud Abbas from?\"\r\n\r\n2. To better make use of the LC-QuAD dataset, we replaced all numeric identifiers, e.g., wd:Q127998, wdt:P27, wdt:P31, wd:Q6256, with something the model could come up with based on the question text without any other context. To this end, we looked up the name that refers to each identifier and created a dictionary in the form `{\"P35\": \"head of state\", \"Q127998\": \"Mahmoud Abbas\", \"P31\": \"instance of\", \"Q6256\": \"country\", \"Q1045\": \"Somalia\", \"P1082\": \"population\", ...`.  Using this dictionary and the LC-QuAD dataset leads to the transformed training sample:\r\n\r\n> \"sparql_wikidata\": \"select distinct ?a  { hp:Mahmoud_abbas hp:nationality ?a . ?a hp:instance_of hp:Country }\"\r\n> \"paraphrased_question\": \"Which country is Mahmoud Abbas from?\"\r\n\r\nWe also mapped some of the names so that they better fit what we use in the Fandom dataset and our graph, e.g., \"country_of_citizenship\" -> \"nationality\" or \"characters\" -> \"harrypotterrole\".\r\n4. We pre-trained the model on this transformed LC-QuAD dataset.\r\n5. Finally, we fine-tuned the model on a small set of training samples from the Harry Potter Fandom data. These samples were manually created. This step is more important for learning to map relations correctly. The mapping of the name \"Harry Potter\" in the text question to \"hp:Harry_potter\" in SPARQL should already work quite well even before this step.",
    "meta": {
      "name": "How the Text2Sparql Model in Tutorial10_Knowledge_Graph was trained"
    },
    "answer": "Let me briefly summarize the main steps that we took to train a Text2SPARQL model on the Harry Potter Fandom data.\r\n\r\n1. We started with the LC-QuAD dataset, which includes text questions and corresponding SPARQL queries, for example:\r\n\r\n> \"sparql_wikidata\": \" select distinct ?obj where { wd:Q127998 wdt:P27 ?obj . ?obj wdt:P31 wd:Q6256 } \",\r\n> \"paraphrased_question\": \"Which country is Mahmoud Abbas from?\"\r\n\r\n2. To better make use of the LC-QuAD dataset, we replaced all numeric identifiers, e.g., wd:Q127998, wdt:P27, wdt:P31, wd:Q6256, with something the model could come up with based on the question text without any other context. To this end, we looked up the name that refers to each identifier and created a dictionary in the form `{\"P35\": \"head of state\", \"Q127998\": \"Mahmoud Abbas\", \"P31\": \"instance of\", \"Q6256\": \"country\", \"Q1045\": \"Somalia\", \"P1082\": \"population\", ...`.  Using this dictionary and the LC-QuAD dataset leads to the transformed training sample:\r\n\r\n> \"sparql_wikidata\": \"select distinct ?a  { hp:Mahmoud_abbas hp:nationality ?a . ?a hp:instance_of hp:Country }\"\r\n> \"paraphrased_question\": \"Which country is Mahmoud Abbas from?\"\r\n\r\nWe also mapped some of the names so that they better fit what we use in the Fandom dataset and our graph, e.g., \"country_of_citizenship\" -> \"nationality\" or \"characters\" -> \"harrypotterrole\".\r\n4. We pre-trained the model on this transformed LC-QuAD dataset.\r\n5. Finally, we fine-tuned the model on a small set of training samples from the Harry Potter Fandom data. These samples were manually created. This step is more important for learning to map relations correctly. The mapping of the name \"Harry Potter\" in the text question to \"hp:Harry_potter\" in SPARQL should already work quite well even before this step."
  },
  {
    "content": "jetson orin need torch 1.11 at least, but can not install OpenPCDet,. someone know how to fix it? or somebody success in using OpenPCDet in jetson Orin@XGL-github is this question related to Haystack? Otherwise i'd suggest to ask your question in [OpenPCDet's repo](https://github.com/open-mmlab/OpenPCDet).",
    "meta": {
      "name": "install OpenPCDet with torch 1.11 cuda 11.4,  problem : THC/THC.h: No such file or directory . how to fix it"
    },
    "answer": "@XGL-github is this question related to Haystack? Otherwise i'd suggest to ask your question in [OpenPCDet's repo](https://github.com/open-mmlab/OpenPCDet)."
  },
  {
    "content": "Whenever I try to execute pipe.run (for example, prediction = pipe.run(query=Questions)),  me elasticsearch gives this  **ERROR: Elasticsearch exited unexpectedly**\r\n\r\nAnd when I restart my elastic search and my Jupyter notebook, this works one time and again fails. Don't know why this is happening. What is causing this issue?\r\n\r\nIf anyone can help, it would be great.\r\nAs there is an issue about this, let's handle it there: #2821 ",
    "meta": {
      "name": "Elastic search exit uexpectedly on this prediction = pipe.run(query=Questions)"
    },
    "answer": "As there is an issue about this, let's handle it there: #2821 "
  },
  {
    "content": "Hi there!\r\n\r\nI'm rying to run haystack annotation tool loally. After running docker-compose up in the annotation_tool subfolder, I will be presented th login screen, just like in the [handbook of the hosted service](https://drive.google.com/file/d/1Wv3OIC0Z7ibHIzOm9Xw_r0gjTFmpl-33/view). I can't log into the service, can't register.\r\n\r\nThe console log looks like this:\r\n\r\n> annotation_tool-db-1       | 2022-07-15 15:25:37.051 UTC [433] FATAL:  database \"somesafeuser\" does not exist\r\n> annotation_tool-db-1       | 2022-07-15 15:25:37.094 UTC [442] FATAL:  role \"somesafepassword\" does not exist\r\n\r\nA failed login attempt looks like this:\r\n\r\n> annotation_tool-backend-1  | parameters: undefined\r\n> annotation_tool-backend-1  | headers: {\"Access-Control-Allow-Origin\":\"http://localhost:7001\",\"vary\":\"Origin\"}   \r\n> annotation_tool-backend-1  | pid: 83\r\n> annotation_tool-backend-1  | hostname: af3a1f9fa996\r\n\r\nI'm trying with:\r\n\r\n> DEFAULT_ADMIN_EMAIL: \"**example@example.com**\"\r\n> DEFAULT_ADMIN_PASSWORD: \"**DEMO_PASSWORD**\"\r\n\r\n_running docker ps_ gives me:\r\n> CONTAINER ID   IMAGE                                          COMMAND                  CREATED          STATUS                      PORTS    NAMES\r\n> af3a1f9fa996   deepset/haystack-annotation:latest             \"./entrypoint.sh\"        23 minutes ago   Up 23 minutes               0.0.0.0:7001->7001/tcp, 8080/tcp   annotation_tool-backend-1\r\n> **fecfb23f922d**   postgres:12                                    \"docker-entrypoint.s\u2026\"   24 minutes ago   Up 23 minutes (unhealthy)   0.0.0.0:5432->5432/tcp    annotation_tool-db-1\r\n> 8654dda874c6   deepset/haystack-cpu:latest                    \"/bin/bash -c 'sleep\u2026\"   28 minutes ago   Up 28 minutes               0.0.0.0:8000->8000/tcp    haystack-haystack-api-1\r\n> ea1af708d222   deepset/elasticsearch-countries-and-capitals   \"/tini -- /usr/local\u2026\"   28 minutes ago   Up 28 minutes               0.0.0.0:9200->9200/tcp, 9300/tcp   haystack-elasticsearch-1\r\n> 4e08bdd061ae   deepset/haystack-streamlit-ui:latest           \"/bin/bash -c 'sleep\u2026\"   28 minutes ago   Up 28 minutes               0.0.0.0:8501->8501/tcp    haystack-ui-1\r\n \r\nand then _docker exec -ti **fecfb23f922d** bash_ and _psql -h localhost_ returns:\r\n\r\n> connection to server at \"localhost\" (127.0.0.1), port 5432 failed: FATAL:  role \"root\" does not exist\r\n\r\nWhat am I doing wrong? It looks as if the postgresql is not properly configured.@zebrassimo \r\nthx for reporting. There's a bug within the docker-compose file. \r\nAs quickfix just correct the username of the following line to `somesafeuser` as this:\r\n`test: \"pg_isready --username=somesafeuser && psql --username=somesafeuser --list\"`\r\n\r\nThen it should work. Official fix is underway, too.",
    "meta": { "name": "relation \"users\" does not exist" },
    "answer": "@zebrassimo \r\nthx for reporting. There's a bug within the docker-compose file. \r\nAs quickfix just correct the username of the following line to `somesafeuser` as this:\r\n`test: \"pg_isready --username=somesafeuser && psql --username=somesafeuser --list\"`\r\n\r\nThen it should work. Official fix is underway, too."
  },
  {
    "content": "Hello guys! First of all, I would like to thank you for all the work you have put into this library.\r\n\r\nI am having a problem importing the tokenizer from the context encoder.\r\n\r\nI am using the following model: https://huggingface.co/IIC/dpr-spanish-question_encoder-allqa-base\r\n\r\nEvery time I try to create the DensePassageRetriever, I get the Tokenizer class error: \"Tokenizer class DPRContextEncoderTokenizer does not exist or is not currently imported.\"\r\n\r\nI would like to know if the problem is due to the model itself or to the library. I have tried downloading all the model files and uploading the model locally instead of from the model hub but I can't get it to work at all. The DPRQuestionEncoderTokenizer seems to work correctly.\r\n\r\nI am quite desperate because I would like it to work properly. Any idea of what's happening?\r\n\r\nThanks for the help in advanceHello! \r\n\r\nI've been reading the library for some time now and I have a question related with the parameter \"infer_tokenizer_classes\" when initializing the DPR.\r\n\r\nWhen I set it True, looks like the model can be loaded without problem, but I've seen that this parameter is set to False and I do not really understand what it does in general.\r\n\r\nIn the code I see that when the parameter is False by default the tokenizer_default_classes takes as question and context tokenizers the AutoTokenizer class. In the other hand when it is True, this variable the question and context tokenizers are None and I would like to know from where they are imported if they are None.\r\n\r\nThanks for your help!Hi @Myko-10, thanks for bringing our attention to this issue. This was a bug and should be fixed once #2755 is merged.\r\n\r\nThe following was happening:\r\nWhen initializing a `DensePassageRetriever`, we were using `AutoTokenizer`s in order to load the tokenizers for the query embedding model and the passage embedding model. In the config of the model you tried load, the tokenizer_clas is set explicitly to `DPRContextEncoderTokenizer`. However, it seems that the `AutoTokenizer` doesn't support loading this tokenizer, that's why you got the error message. It only worked when setting `infer_tokenizer_classes` to `True` because with this setting, we didn't use `AutoTokenizer` but try to infer the correct tokenizer using the model's config and name.",
    "meta": { "name": "Unable to import DPRContextEncoderTokenizer" },
    "answer": "Hi @Myko-10, thanks for bringing our attention to this issue. This was a bug and should be fixed once #2755 is merged.\r\n\r\nThe following was happening:\r\nWhen initializing a `DensePassageRetriever`, we were using `AutoTokenizer`s in order to load the tokenizers for the query embedding model and the passage embedding model. In the config of the model you tried load, the tokenizer_clas is set explicitly to `DPRContextEncoderTokenizer`. However, it seems that the `AutoTokenizer` doesn't support loading this tokenizer, that's why you got the error message. It only worked when setting `infer_tokenizer_classes` to `True` because with this setting, we didn't use `AutoTokenizer` but try to infer the correct tokenizer using the model's config and name."
  },
  {
    "content": "Hi, I want to perform a semantic search on +80 million news articles for which I have a publishing date. Before conducting the semantic search, I want to narrow down the news articles and only search within the given date range.\r\n\r\nI managed to use Elasticsearch to do the semantic search of over 7 million news articles just by using an input text as a query and getting pretty solid results. However,  for the whole dataset written in the document store,  it doesn't make sense for me to do the search over the whole 80 million news articles when I know the date range of what I'm looking for!\r\n\r\nFor each row of my dataset, I have these features: Topic, URL, Date, Context\r\n\r\nWhich I turned the dataframe into Haystack's [DocumentStore format](https://haystack.deepset.ai/docs/latest/get_startedmd), the result is a dictionary as below for each news article:\r\n\r\n` dict = {'content': news_df['context'], 'meta': {'datetime': news_df['Date'], 'url': news_df['URL'], 'topic': news_df['Topic']}}`\r\n\r\n\r\nI'm using \r\n1. `document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")`\r\n6. `retriever = BM25Retriever(document_store=document_store)`\r\n7. ready-made pipeline `DocumentSearchPipeline` \r\n\r\n\r\nTo be clear, I have a sentence with a date. I want to find the most relevant news articles given that sentence. Since I have the sentence and the news articles publishing date, I want to narrow down the news articles for the given date (a few days before and after maybe) and then conduct the semantic search.\r\n\r\n[Here's](https://haystack.deepset.ai/reference/document-store) the example of using filters for `BaseElasticsearchDocumentStore`\r\nBut I can't find such a thing for `ElasticsearchDocumentStore`, also don't know how to put it together with the `DocumentSearchPipeline.run()` since these are the inputs for the method `def run(self, query: str, params: Optional[dict] = None, debug: Optional[bool] = None`\r\n\r\nI can use `search_fields ` in `ElasticsearchDocumentStore` and add the Date feature as one of the search fields to Retrieve that day's news articles but still don't know how to make it work for the semantic search in `DocumentSearchPipeline`. How to tell the pipeline to filter the news first based on the date range and then do the semantic search on the previous query results.\r\n\r\nThanks in advance for the time you will kindly dedicate to help me out with this issue.\r\n\r\n P.S. I wanted to use FAISS but building the indexing for this size took forever. I came up with the below index based on this article [guide](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index)\r\n`index = faiss.index_factory(768, \"PCA64,IVF16384_HNSW32,Flat\")`\r\n\r\nHi @Squishy-33 thank you for explaining your setup in such detail. Your `BM25Retriever` can handle any filters via the `filters` parameter in its `retrieve` method: https://github.com/deepset-ai/haystack/blob/a2905d05f798ea3335596247b98ec711eb6cd542/haystack/nodes/retriever/sparse.py#L114\r\n\r\nHowever, note that the search won't really be a semantic search but only a keyword-based search if you use `ElasticsearchDocumentStore` with a `BM25Retriever`. For semantic search you would need to use an EmbeddingRetriever and with your number of documents a DocumentStore other than `ElasticsearchDocumentStore`makes more sense (faiss or milvus). The indexing takes much longer than, that's true even with a GPU. However, the indexing is needed only once and it is required for semantic search to calculate a vector representation for every document in your DocumentStore. \r\n",
    "meta": {
      "name": "Semantic document search using dates to narrow down the search"
    },
    "answer": "Hi @Squishy-33 thank you for explaining your setup in such detail. Your `BM25Retriever` can handle any filters via the `filters` parameter in its `retrieve` method: https://github.com/deepset-ai/haystack/blob/a2905d05f798ea3335596247b98ec711eb6cd542/haystack/nodes/retriever/sparse.py#L114\r\n\r\nHowever, note that the search won't really be a semantic search but only a keyword-based search if you use `ElasticsearchDocumentStore` with a `BM25Retriever`. For semantic search you would need to use an EmbeddingRetriever and with your number of documents a DocumentStore other than `ElasticsearchDocumentStore`makes more sense (faiss or milvus). The indexing takes much longer than, that's true even with a GPU. However, the indexing is needed only once and it is required for semantic search to calculate a vector representation for every document in your DocumentStore. \r\n"
  },
  {
    "content": "In my pipeline, I defined a reader node which sends all requests to a remote server, waits for answers and return them upon receiving them. so I need to define predict function of reader class async. However, defining async predict function rises error:\r\n\r\nFile \"/usr/local/lib/python3.7/site-packages/haystack/nodes/reader/base.py\", line 96, in run\r\n          BaseReader.add_doc_meta_data_to_answer(documents=documents, answer=answer) for answer in results[\"answers\"]\r\nTypeError: 'coroutine' object is not subscriptable\r\n\r\nany idea is appreciated. @nasrin-taghizadeh the predict function is not intended to be async. The whole pipeline can only proceed when the prediction was made. So we wouldn't gain too much by making it async-compatible. You can make a blocking call to the remote server however. \r\nIf you want to make multiple predictions at once, check out the `predict_batch` function. Also this function is not async. But you could fire multiple requests from it, collect them and return the collected batch results.\r\nIf you're interested in the latter approach, checkout how `asyncio.run` is being used in https://blog.devgenius.io/how-to-send-concurrent-http-requests-in-python-d9cda284c86aThank you for your consideration.",
    "meta": { "name": "Aysnc reader node" },
    "answer": "@nasrin-taghizadeh the predict function is not intended to be async. The whole pipeline can only proceed when the prediction was made. So we wouldn't gain too much by making it async-compatible. You can make a blocking call to the remote server however. \r\nIf you want to make multiple predictions at once, check out the `predict_batch` function. Also this function is not async. But you could fire multiple requests from it, collect them and return the collected batch results.\r\nIf you're interested in the latter approach, checkout how `asyncio.run` is being used in https://blog.devgenius.io/how-to-send-concurrent-http-requests-in-python-d9cda284c86a"
  },
  {
    "content": "**Question**\r\nI need to utilize multiple retrievers in a chain, i.e. the first one retrieves some documents from data store and pass them to the second one. Haystack provided JoinDocuments node which concatenates or merges output of multiple retrievers but it doesn't fit to my need. Any idea is appreciated.\r\nHi @nasrin-taghizadeh \r\n\r\nretrievers only support Documentstores as their input and we don't have plans to change this, but depending on your use case, you might find helpful using a ranker, specifically the `SentenceTransformersRanker`. This node can take the input from (e.g.) a BM25 retriever and use a transformer model to sort its results. We have an example [in our docs](https://github.com/deepset-ai/haystack/blob/master/haystack/nodes/ranker/sentence_transformers.py#L30) (I'm linking the code because the documentation website has a problem rendering this specific snippet):\r\n\r\n```python\r\n    retriever = BM25Retriever(document_store=document_store)\r\n    ranker = SentenceTransformersRanker(model_name_or_path=\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\r\n    p = Pipeline()\r\n    p.add_node(component=retriever, name=\"ESRetriever\", inputs=[\"Query\"])\r\n    p.add_node(component=ranker, name=\"Ranker\", inputs=[\"ESRetriever\"])\r\n```\r\n\r\n",
    "meta": { "name": "How to implement a chain of multiple retrievers?" },
    "answer": "Hi @nasrin-taghizadeh \r\n\r\nretrievers only support Documentstores as their input and we don't have plans to change this, but depending on your use case, you might find helpful using a ranker, specifically the `SentenceTransformersRanker`. This node can take the input from (e.g.) a BM25 retriever and use a transformer model to sort its results. We have an example [in our docs](https://github.com/deepset-ai/haystack/blob/master/haystack/nodes/ranker/sentence_transformers.py#L30) (I'm linking the code because the documentation website has a problem rendering this specific snippet):\r\n\r\n```python\r\n    retriever = BM25Retriever(document_store=document_store)\r\n    ranker = SentenceTransformersRanker(model_name_or_path=\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\r\n    p = Pipeline()\r\n    p.add_node(component=retriever, name=\"ESRetriever\", inputs=[\"Query\"])\r\n    p.add_node(component=ranker, name=\"Ranker\", inputs=[\"ESRetriever\"])\r\n```\r\n\r\n"
  },
  {
    "content": "Hi im trying to learn how to use Faiss and Generative QA\r\n\r\nim saving the data like that based on the tutorial12\r\n\r\n```\r\nfrom haystack.document_stores import FAISSDocumentStore\r\nfrom haystack.nodes import DensePassageRetriever\r\nfrom haystack.utils import convert_files_to_docs, fetch_archive_from_http, clean_wiki_text\r\n\r\ndocument_store = FAISSDocumentStore(embedding_dim=128, faiss_index_factory_str=\"Flat\")\r\n\r\n# Let's first get some files that we want to use\r\ndoc_dir = \"data/tutorial12\"\r\ns3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt12.zip\"\r\nfetch_archive_from_http(url=s3_url, output_dir=doc_dir)\r\n\r\n# Convert files to dicts\r\ndocs = convert_files_to_docs(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\r\n\r\n# Now, let's write the dicts containing documents to our DB.\r\ndocument_store.write_documents(docs)\r\n\r\nretriever = DensePassageRetriever(\r\n    document_store=document_store,\r\n    query_embedding_model=\"vblagoje/dpr-question_encoder-single-lfqa-wiki\",\r\n    passage_embedding_model=\"vblagoje/dpr-ctx_encoder-single-lfqa-wiki\",\r\n)\r\n\r\ndocument_store.update_embeddings(retriever=retriever)\r\n\r\ndocument_store.save(\"data/faiss_data.faiss\", config_path=\"data/faiss_config\")\r\nretriever.save(\"data/retriever.pt\")\r\n\r\n```\r\n\r\nBut if i run it twice it give me this error\r\n\r\n```\r\nValueError: The number of documents present in the SQL database (2358) does not match the number of embeddings in FAISS (0). Make sure your FAISS configuration file correctly points to the same database that was used when creating the original index\r\n```\r\n\r\n\r\ni dont know how to resolve it and if i add new doc in the future i will need start from scratch and it will take too much time.\r\nThanks for your reply.Hi @azetoy did you have a look at this related and resolved issue already? https://github.com/deepset-ai/haystack/issues/1903",
    "meta": { "name": "Faiss dont update the number of embeddings" },
    "answer": "Hi @azetoy did you have a look at this related and resolved issue already? https://github.com/deepset-ai/haystack/issues/1903"
  },
  {
    "content": "Hey all, \r\n\r\nI have a simple question that bugs me a lot... Hope to get some help from the community of Haystack...\r\n\r\n### Context:\r\nBenefit from the Haystack library, we are able to use `Reader` to infer multiple documents retrieved by `Retriever` in order to get the final answers. And when the document is too long we chunkify them into chunks that fit the `max_seq_len` limitation. However, the final goal is to provide the best-sorted answers for the end-users... And for doing that, we will need to aggregate the answers from different chunks of the same document (possibly, only when the document is longer than `max_seq_len`), and also aggregate the answers from different documents...\r\n## My question will be:\r\n* How do we aggregate the answers? \r\n    - based on the `start_logit + end_logit` for each answer?\r\n* How do we softmax the logits to get probabilities?\r\n    - softmax the chunks from the same document?\r\n    - softmax all the `start_logit + end_logit` from all answers?\r\n\r\nHaystack `FARMReader` currently outputs answers with reasonable probabilities from different documents, I would like to ask under the hood what's the mechanism?  \r\n\r\nThanks in advance for all the advice : )Hi @gabinguo in this context I would like to quote the famous saying \"A line of code is worth a thousand words.\" \ud83d\ude09 So I would recommend that you go through the lines of code in the `logits_to_preds` method of the `QuestionAnsweringHead` here: https://github.com/deepset-ai/haystack/blob/f7d00476f92ef77df115dafef8eecb7115c276d1/haystack/modeling/model/prediction_head.py#L444\r\n\r\nThe other method you want to look at is `aggregate_preds`: https://github.com/deepset-ai/haystack/blob/f7d00476f92ef77df115dafef8eecb7115c276d1/haystack/modeling/model/prediction_head.py#L666\r\n\r\nIn short, softmax is calculated only for all answers in the same passage and then there is only one more complex trick we do to compare answers from different passages and no_answers in `reduce_preds` here: https://github.com/deepset-ai/haystack/blob/f7d00476f92ef77df115dafef8eecb7115c276d1/haystack/modeling/model/prediction_head.py#L790",
    "meta": {
      "name": "[Question] How the answers are aggregated (from different documents) ?"
    },
    "answer": "Hi @gabinguo in this context I would like to quote the famous saying \"A line of code is worth a thousand words.\" \ud83d\ude09 So I would recommend that you go through the lines of code in the `logits_to_preds` method of the `QuestionAnsweringHead` here: https://github.com/deepset-ai/haystack/blob/f7d00476f92ef77df115dafef8eecb7115c276d1/haystack/modeling/model/prediction_head.py#L444\r\n\r\nThe other method you want to look at is `aggregate_preds`: https://github.com/deepset-ai/haystack/blob/f7d00476f92ef77df115dafef8eecb7115c276d1/haystack/modeling/model/prediction_head.py#L666\r\n\r\nIn short, softmax is calculated only for all answers in the same passage and then there is only one more complex trick we do to compare answers from different passages and no_answers in `reduce_preds` here: https://github.com/deepset-ai/haystack/blob/f7d00476f92ef77df115dafef8eecb7115c276d1/haystack/modeling/model/prediction_head.py#L790"
  },
  {
    "content": "Hello, I am trying to implement document type classification (e.g. is a text document a novel, journal, blog, encyclopedia entry, textbook etc.) in my pipeline. It seems to me that the most suitable node for this is the Document Classifier, but I am getting poor results with the models I've been finding on the Huggingface Hub. \r\n\r\nI would like to fine-tune a model using training data, but I am unsure how to do so in this case. The tutorial for fine-tuning a model explains how to use the annotation tool to label a QA dataset, but this tool does not seem to meet my needs. From the looks of it, the Haystack annotation tool requires me to assign questions to portions of text, but I would like to assign a single label (e.g. \"textbook\") to an entire document.\r\n\r\nDoes Haystack support what I am trying to do?  If not, is anyone aware of any tools I could use to fine-tune my model in a way that Haystack can use in the Document Classifier?\r\n\r\nThank you in advance!Hi @Nosajsom you're right that the Document Classifier node is the node you need. Training of this node is not directly supported in Haystack yet at the moment but you can do the training easily with Hugging Face transformers. Here is an example/tutorial: https://huggingface.co/docs/transformers/training",
    "meta": { "name": "Fine-Tuning a Document Classification Model" },
    "answer": "Hi @Nosajsom you're right that the Document Classifier node is the node you need. Training of this node is not directly supported in Haystack yet at the moment but you can do the training easily with Hugging Face transformers. Here is an example/tutorial: https://huggingface.co/docs/transformers/training"
  },
  {
    "content": " I am building a search engine over documents containing german and english text but our priority is the gemran text , so I would like to know what's the best choice of a Retriever would be to use in that case ,also As now I am using BM25 with the german gelectra-large-germanquad model as reader but think since my case is search only I don't need the reader part anymore.If you would like to use a dense retrieval model, I would recommend Haystack's EmbeddingRetriever  class together with a sentencetransformers model trained on the MSMARCO dataset. There is a list of such models here: https://www.sbert.net/docs/pretrained-models/msmarco-v3.html You can also find them on the HuggingFace model hub, for example, you can load sentence-transformers/msmarco-distilbert-dot-v5 from there.\r\n\r\nRegarding smaller models that can also work well on CPUs, I would recommend to first try out sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 It's truly multilingual, includes German and is relatively small. Then you could also try the very small models sentence-transformers/all-MiniLM-L6-v2 and sentence-transformers/msmarco-MiniLM-L6-cos-v5 and then another truly multilingual model: sentence-transformers/distiluse-base-multilingual-cased-v1 :crossed_fingers:",
    "meta": { "name": "Best way to build search engine with haystack" },
    "answer": "If you would like to use a dense retrieval model, I would recommend Haystack's EmbeddingRetriever  class together with a sentencetransformers model trained on the MSMARCO dataset. There is a list of such models here: https://www.sbert.net/docs/pretrained-models/msmarco-v3.html You can also find them on the HuggingFace model hub, for example, you can load sentence-transformers/msmarco-distilbert-dot-v5 from there.\r\n\r\nRegarding smaller models that can also work well on CPUs, I would recommend to first try out sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 It's truly multilingual, includes German and is relatively small. Then you could also try the very small models sentence-transformers/all-MiniLM-L6-v2 and sentence-transformers/msmarco-MiniLM-L6-cos-v5 and then another truly multilingual model: sentence-transformers/distiluse-base-multilingual-cased-v1 :crossed_fingers:"
  },
  {
    "content": "For better results on domain specific data, how to re-train the roberta-base-squad2 QA model using MLM technique on domain data and then do fine tune on QA data for domain.?Hi @ashar1202!\r\n\r\nFor the first step, I will point you towards [huggingface's Github account](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling). They have great examples on how to do language model training.\r\n\r\nFor fine-tuning on your own QA data, there is a [great tutorial](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb) on Haystack's end.\r\n\r\nHope this was helpful! Have fun!",
    "meta": { "name": "Re-train roberta-base-squad2 QA model on domain data" },
    "answer": "Hi @ashar1202!\r\n\r\nFor the first step, I will point you towards [huggingface's Github account](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling). They have great examples on how to do language model training.\r\n\r\nFor fine-tuning on your own QA data, there is a [great tutorial](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb) on Haystack's end.\r\n\r\nHope this was helpful! Have fun!"
  },
  {
    "content": "I've run [Tutorial 4, Utilising Existing FAQs](https://haystack.deepset.ai/tutorials/existing-faqs), and noted that all of the returned answers have almost equal scores, between 0.500 and 0.502. Is this expected? This happened when I ran the colab without any changes. Additionally, it happens whether the question exactly matches an existing question in the database, e.g. \"What is a novel coronavirus?\", or whether the question is completely made up. This seems odd, since the Extractive Q&A systems provide a score where it meaningfully ranges between 0 and 1.\r\n\r\nMy use-case is merging a set of results from an extractive Q&A pipeline and results from a FAQ-retrieval pipeline. I'm not sure how to do this if FAQ answers are always 0.5, regardless of how well they actually matched the question.\r\n\r\nIn short - can I make the FAQ answers have a more useful score?Hi @stevenhaley thanks for raising the issue. In the tutorial we're using the wrong similarity algorithm, indeed you should see a warning when running the example:\r\n```\r\nWARNING - haystack.nodes.retriever._embedding_encoder -  You are using a Sentence Transformer with the dot_product function. We recommend using cosine instead. This can be set when initializing the DocumentStore\r\n```\r\nWe [opened a PR](https://github.com/deepset-ai/haystack/pull/2673) to fix the tutorial.\r\n",
    "meta": {
      "name": "Tutorial 4, FAQ retrieval: all results have near-identical score"
    },
    "answer": "Hi @stevenhaley thanks for raising the issue. In the tutorial we're using the wrong similarity algorithm, indeed you should see a warning when running the example:\r\n```\r\nWARNING - haystack.nodes.retriever._embedding_encoder -  You are using a Sentence Transformer with the dot_product function. We recommend using cosine instead. This can be set when initializing the DocumentStore\r\n```\r\nWe [opened a PR](https://github.com/deepset-ai/haystack/pull/2673) to fix the tutorial.\r\n"
  },
  {
    "content": "Hi, following the instructions given in the Haystack API documentation ([link](https://haystack.deepset.ai/components/rest-api)) creates 3 docker containers. The Elasticsearch instance that is created is either empty/ filled using deepset's data on countries and capitals. \r\n\r\nI would like to know how I can connect Haystack API to my own Elasticsearch datastore. Should I edit the docker-compose.yml part on elasticsearch? If so, how should I do it? \r\n\r\nOr is it possible to follow tutorial 1, where I can create a new document store and initialise the object with my own parameters? And if I do this, where should I then write this code? \r\n\r\nThank you for your help. Hi @nniiggeell \r\n\r\nthe `docker-compose.yml` file is a good starting point but can be customises in multiple ways that totally depend on your use case. \r\n\r\n> I would like to know how I can connect Haystack API to my own Elasticsearch datastore.\r\n\r\nLet's say you already have an ES cluster running somewhere - you don't need to spin up a container at all, you can comment out this whole section:\r\n```yml\r\n  # elasticsearch:\r\n  #   # This will start an empty elasticsearch instance (so you have to add your documents yourself)\r\n  #   #image: \"elasticsearch:7.9.2\"\r\n  #   # If you want a demo image instead that is \"ready-to-query\" with some indexed articles\r\n  #   # about countries and capital cities from Wikipedia:\r\n  #   image: \"deepset/elasticsearch-countries-and-capitals\"\r\n  #   ports:\r\n  #     - 9200:9200\r\n  #   restart: on-failure\r\n  #   environment:\r\n  #     - discovery.type=single-node\r\n```\r\nNow you have to tweak the `haystack-api` container so that it points to your existing ES instance:\r\n```yml\r\n  haystack-api:\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile\r\n    image: \"deepset/haystack-cpu:latest\"\r\n    # Mount custom Pipeline YAML and custom Components.\r\n    # volumes:\r\n    #   - ./rest_api/pipeline:/home/user/rest_api/pipeline\r\n    ports:\r\n      - 8000:8000\r\n    restart: on-failure\r\n    environment:\r\n      # See rest_api/pipeline/pipelines.haystack-pipeline.yml for configurations of Search & Indexing Pipeline.\r\n      # The ES host might be a cloud instance now\r\n      - DOCUMENTSTORE_PARAMS_HOST=https://your/cloud/hosted/url/\r\n      # We would probably need to pass some sort of authentication\r\n      - DOCUMENTSTORE_PARAMS_USERNAME=elasticsearch\r\n      - DOCUMENTSTORE_PARAMS_PASSWORD=secret!\r\n      - PIPELINE_YAML_PATH=/home/user/rest_api/pipeline/pipelines.haystack-pipeline.yml\r\n      - CONCURRENT_REQUEST_PER_WORKER\r\n    # we comment the `depends_on` section as we don't depend anymore on the ES container\r\n    # depends_on:\r\n    #   - elasticsearch\r\n    # Starts REST API with only 2 workers so that it can be run on systems with just 4GB of memory\r\n    # If you need to handle large loads of incoming requests and have memory to spare, consider increasing the number of workers\r\n    command: \"/bin/bash -c 'sleep 10 && gunicorn rest_api.application:app -b 0.0.0.0 -k uvicorn.workers.UvicornWorker --workers 2 --timeout 180'\"\r\n```\r\n\r\n> Or is it possible to follow tutorial 1, where I can create a new document store and initialise the object with my own parameters?\r\n\r\nIf you followed pedantically the tutorial, you should have an ES container running locally in your computer. The process would be same as above, just you manage the ES container manually and not through Docker Compose and you point `DOCUMENTSTORE_PARAMS_HOST` to localhost.\r\n\r\nHope this helps, there's a lot to unpack here but let me know if you want to dig further into details.\r\n",
    "meta": { "name": "Haystack API with my own Elasticsearch datastore" },
    "answer": "Hi @nniiggeell \r\n\r\nthe `docker-compose.yml` file is a good starting point but can be customises in multiple ways that totally depend on your use case. \r\n\r\n> I would like to know how I can connect Haystack API to my own Elasticsearch datastore.\r\n\r\nLet's say you already have an ES cluster running somewhere - you don't need to spin up a container at all, you can comment out this whole section:\r\n```yml\r\n  # elasticsearch:\r\n  #   # This will start an empty elasticsearch instance (so you have to add your documents yourself)\r\n  #   #image: \"elasticsearch:7.9.2\"\r\n  #   # If you want a demo image instead that is \"ready-to-query\" with some indexed articles\r\n  #   # about countries and capital cities from Wikipedia:\r\n  #   image: \"deepset/elasticsearch-countries-and-capitals\"\r\n  #   ports:\r\n  #     - 9200:9200\r\n  #   restart: on-failure\r\n  #   environment:\r\n  #     - discovery.type=single-node\r\n```\r\nNow you have to tweak the `haystack-api` container so that it points to your existing ES instance:\r\n```yml\r\n  haystack-api:\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile\r\n    image: \"deepset/haystack-cpu:latest\"\r\n    # Mount custom Pipeline YAML and custom Components.\r\n    # volumes:\r\n    #   - ./rest_api/pipeline:/home/user/rest_api/pipeline\r\n    ports:\r\n      - 8000:8000\r\n    restart: on-failure\r\n    environment:\r\n      # See rest_api/pipeline/pipelines.haystack-pipeline.yml for configurations of Search & Indexing Pipeline.\r\n      # The ES host might be a cloud instance now\r\n      - DOCUMENTSTORE_PARAMS_HOST=https://your/cloud/hosted/url/\r\n      # We would probably need to pass some sort of authentication\r\n      - DOCUMENTSTORE_PARAMS_USERNAME=elasticsearch\r\n      - DOCUMENTSTORE_PARAMS_PASSWORD=secret!\r\n      - PIPELINE_YAML_PATH=/home/user/rest_api/pipeline/pipelines.haystack-pipeline.yml\r\n      - CONCURRENT_REQUEST_PER_WORKER\r\n    # we comment the `depends_on` section as we don't depend anymore on the ES container\r\n    # depends_on:\r\n    #   - elasticsearch\r\n    # Starts REST API with only 2 workers so that it can be run on systems with just 4GB of memory\r\n    # If you need to handle large loads of incoming requests and have memory to spare, consider increasing the number of workers\r\n    command: \"/bin/bash -c 'sleep 10 && gunicorn rest_api.application:app -b 0.0.0.0 -k uvicorn.workers.UvicornWorker --workers 2 --timeout 180'\"\r\n```\r\n\r\n> Or is it possible to follow tutorial 1, where I can create a new document store and initialise the object with my own parameters?\r\n\r\nIf you followed pedantically the tutorial, you should have an ES container running locally in your computer. The process would be same as above, just you manage the ES container manually and not through Docker Compose and you point `DOCUMENTSTORE_PARAMS_HOST` to localhost.\r\n\r\nHope this helps, there's a lot to unpack here but let me know if you want to dig further into details.\r\n"
  },
  {
    "content": "I'm trying to include the full version of Haystack as a dependency for a python project.\r\nMy current best guess is to use this line:\r\nfarm-haystack[all] @ git+https://github.com/deepset-ai/haystack.git\r\nhowever this emits a bunch of warnings of the form\r\n...\r\nWARNING: farm-haystack 0.2.0.post1 does not provide the extra 'docstores'\r\nWARNING: farm-haystack 0.2.0.post1 does not provide the extra 'ocr'\r\nWARNING: farm-haystack 0.2.0.post1 does not provide the extra 'onnx'\r\nWARNING: farm-haystack 0.2.0.post1 does not provide the extra 'preprocessing'\r\nWARNING: farm-haystack 0.2.0.post1 does not provide the extra 'ray'\r\nWARNING: farm-haystack 0.1.0.post2 does not provide the extra 'beir'\r\nWARNING: farm-haystack 0.1.0.post2 does not provide the extra 'crawler'\r\nWARNING: farm-haystack 0.1.0.post2 does not provide the extra 'dev'\r\n...\r\nbefore eventually failing.\r\n\r\nERROR: Requested dill from https://files.pythonhosted.org/packages/3e/ad/31932a4e2804897e6fd2f946d53df51dd9b4aa55e152b5404395d00354d1/dill-0.3.1.tar.gz#sha256=d3ddddf2806a7bc9858b20c02dc174396795545e9d62f243b34481fd26eb3e2c (from farm-haystack[all]@ git+https://github.com/deepset-ai/haystack.git->-r /dss_data/tmp/pip-requirements-install/req3361828774079305889.txt (line 1)) has different version in metadata: '0.3.1.dev0'\r\n\r\n I'm sure that I just don't have the syntax quite right in the import statement, but the internet has been amazingly unhelpful. What is the correct way to go about this?\r\n\r\nHi @rdubester, are you working in Colab by any chance? I was able to replicate your problem there, but when I ran it locally everything went smoothly. If you are working in Colab, then `farm-haystack[all]` unfortunately won't work, and you should use `farm-haystack[colab]`.Hey @rdubester, I'm pasting here my response from SO.\r\n\r\nThe format for listing Haystack in your `requirements.txt` should be the following (I pin the version here but it's not mandatory):\r\n```\r\nfarm-haystack[all]==1.5.0\r\n```\r\n\r\nIf you want to pin a specific git commit instead, the line in your `requirements.txt` should just be:\r\n```\r\ngit+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[all]\r\n``` \r\n",
    "meta": { "name": "Installing Haystack+Extras via a requirements.txt" },
    "answer": "Hey @rdubester, I'm pasting here my response from SO.\r\n\r\nThe format for listing Haystack in your `requirements.txt` should be the following (I pin the version here but it's not mandatory):\r\n```\r\nfarm-haystack[all]==1.5.0\r\n```\r\n\r\nIf you want to pin a specific git commit instead, the line in your `requirements.txt` should just be:\r\n```\r\ngit+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[all]\r\n``` \r\n"
  },
  {
    "content": "Hello there. I have a couple of questions, especially, regarding the TableQA.\r\n\r\n1. You use the model 'deepset/all-mpnet-base-v2-table' to embed the tables in the document store. Will it be possible to tell us a bit more about how you trained the model?\r\n2. Is there a particular reason why you guys used a Sentence Transformer model to embed the flattened tables?\r\n3. Is there a way to train/finetune the Embedding Retriever to adapt to domain-specific tables?\r\n4. Is there a way to train the 'TableReader' or is it better to train a version of TAPAS separately and use it?\r\n\r\nThank you very much, for making QA at scale so accessible. Cheers.\r\n\r\n\r\nHi @sbhttchryy, thanks for your interest in TableQA.\r\n\r\n1. For training the model, we've used the dataset that we have introduced in [this paper](https://arxiv.org/abs/2108.04049). We trained for one epoch and used the `MultipleNegativesRankingLoss` implemented in the sentence transformers library.\r\n2. The paper mentioned above describes that specific table encodings do not seem to result in better model performance. We also tried more recently newer table retrieval models and their table-specific encodings also didn't seem to result in any improvements. These findings were also confirmed by [a paper](https://arxiv.org/pdf/2205.09843.pdf) that was released a few weeks ago.\r\nFor this reason, we decided just to fine-tune an existing sentence embedding model on tables in csv format and we found all-mpnet-base-v2 to work best in that case. The reason we used csv was just that it uses just very few tokens. In general, most ways of encoding the table seemed not to perform significantly different.\r\n3. Currently, haystack does not support training an EmbeddingRetriever for tables. You could, however, use a library like sentence-transformers and do the table to csv conversion yourself.\r\n4. I am afraid there's also no way to train a TableReader in haystack. I would recommend using the transformers library for this. They have quite an [extensive tutorial](https://huggingface.co/docs/transformers/model_doc/tapas#usage-finetuning) on it and afterwards you should be able to easily load it into haystack.",
    "meta": { "name": "Queries regarding TableQA" },
    "answer": "Hi @sbhttchryy, thanks for your interest in TableQA.\r\n\r\n1. For training the model, we've used the dataset that we have introduced in [this paper](https://arxiv.org/abs/2108.04049). We trained for one epoch and used the `MultipleNegativesRankingLoss` implemented in the sentence transformers library.\r\n2. The paper mentioned above describes that specific table encodings do not seem to result in better model performance. We also tried more recently newer table retrieval models and their table-specific encodings also didn't seem to result in any improvements. These findings were also confirmed by [a paper](https://arxiv.org/pdf/2205.09843.pdf) that was released a few weeks ago.\r\nFor this reason, we decided just to fine-tune an existing sentence embedding model on tables in csv format and we found all-mpnet-base-v2 to work best in that case. The reason we used csv was just that it uses just very few tokens. In general, most ways of encoding the table seemed not to perform significantly different.\r\n3. Currently, haystack does not support training an EmbeddingRetriever for tables. You could, however, use a library like sentence-transformers and do the table to csv conversion yourself.\r\n4. I am afraid there's also no way to train a TableReader in haystack. I would recommend using the transformers library for this. They have quite an [extensive tutorial](https://huggingface.co/docs/transformers/model_doc/tapas#usage-finetuning) on it and afterwards you should be able to easily load it into haystack."
  },
  {
    "content": "For some secure reason, I need to deploy Haystack completely offline. I want to download all possible packages and move them to the offline PC.  If someone had a expereice. Please give me a hint. Here is what I have done.\r\n- I used the Anaconda to create a haystack environment. Then I  used the [official installation document(https://haystack.deepset.ai/overview/installation) to install everything. the command is `pip install -e .[all]`. Then I use `conda pack` to pack the entire environment. The terminal said can't pack editable file. I know I install the github repo using `pip -e`. So I add `--ignore-editable-packages` to ignore the installation of haystack. So in this way, I can't move the environments to the offline PC.\r\n\r\n- Then I had a second trial. I used `pip freeze > requirements.txt` to export every packages of haystack environment. I download them with pip. But the `requirements.txt` have two lines of code which the pip command had issue:\r\n```\r\n# Editable install with no version control (farm-haystack==1.4.1rc0)\r\n-e e:\\workspace\\python\\haystack-master\r\n```\r\nCan I just install hayStack offline?  I saw [farm-haystack](https://pypi.org/project/farm-haystack/) on PyPi but I needed to downgrade the transformers and tokenizer to some version.(pip will rasie the package dependency error.)\r\n\r\nWith great respect for everyone. Thanks for your suggestion in advance.:)Good news. I have just successfully deployed the Haystack to an offline windows10 PC.\r\n1. First I create a new anaconda environment on the PC which have access to Internet. Let's call the environment `my_env`. Activate the new environment use command `conda activate my_env`.\r\n2. Then I use the command provided in README to install every packages :\r\n`pip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html` (note it will support GPU compute if you have a GPU)\r\n3. Use conda-pack to pack the new environment. `conda pack -n my_env`\r\n4. Use disk to copy the `my_env.tar.gz` to the offline PC.\r\n5. Unpack the `my_env.tar.gz` to `anaconda/envs/my_env` (You can use command line or do it manually.)\r\n6. `cd` to the `my_env` directory and run `conda activate myenv` in terminal.\r\n7. Then run `conda-unpack`(That is important. Don't foget this step. Or the `my_env` won't work correctly.)\r\n\r\nI will open a new discussion or write a post if anyone need a more detailed instruction. But now I have to have a sweet dream. I have to say: with great respect to Haystack!",
    "meta": { "name": "How to install Haystack offline?" },
    "answer": "Good news. I have just successfully deployed the Haystack to an offline windows10 PC.\r\n1. First I create a new anaconda environment on the PC which have access to Internet. Let's call the environment `my_env`. Activate the new environment use command `conda activate my_env`.\r\n2. Then I use the command provided in README to install every packages :\r\n`pip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html` (note it will support GPU compute if you have a GPU)\r\n3. Use conda-pack to pack the new environment. `conda pack -n my_env`\r\n4. Use disk to copy the `my_env.tar.gz` to the offline PC.\r\n5. Unpack the `my_env.tar.gz` to `anaconda/envs/my_env` (You can use command line or do it manually.)\r\n6. `cd` to the `my_env` directory and run `conda activate myenv` in terminal.\r\n7. Then run `conda-unpack`(That is important. Don't foget this step. Or the `my_env` won't work correctly.)\r\n\r\nI will open a new discussion or write a post if anyone need a more detailed instruction. But now I have to have a sweet dream. I have to say: with great respect to Haystack!"
  },
  {
    "content": "Thank you for all the Haystack contributors. It is a really great job!\r\nI want to build a QA system that searches questions based on the user's query. In other words, the QA system prepares a question and an answer as a key-value pair. The retriever module only needs to find the top-K match question and return the question-answer pair to the user. Is there any practical pipeline I can refer to?\r\nI have read the [discussion2328](https://github.com/deepset-ai/haystack/discussions/2328).  It uses a ranker to score two sentences.    And [Document Search Method](https://haystack.deepset.ai/components/ready-made-pipelines#documentsearchpipeline) but  I still haven't dived into them. Any ideas or suggestions are appreciated!Hi @JimReno what you describe sounds like our `FAQPipeline`. Here is a tutorial about it: https://haystack.deepset.ai/tutorials/existing-faqs Please have a look and let me know if it fits your use case. We also have a [blogpost about FAQ Search](https://www.deepset.ai/blog/semantic-faq-search-with-haystack) but because of the fast development of Haystack be aware that some code examples from the blogpost are outdated by now so I'd suggest you focus more on the tutorial than the blogpost.Thank you very much @julian-risch . I have used the tutorial4 for FAQ task. Sorry for that I didn't read the document carefully. Also, I have some questions to bother you.\r\n1\u3001I have installed cuda11.3 to support GPU.  But the running outputs seems not use gpu when it is inferencing.  (Please see the picture below)\r\n![image](https://user-images.githubusercontent.com/28293145/167382434-b89a7683-3fca-44d9-b15b-3d6d35745cb1.png)\r\n2\u3001I have tested the covid datasets and it worked well. So I tried to build my own FAQ system using Chinese QA pairs. I used the default `all-MiniLM-L6-v2` model and load the `csv` dataset locally. The Dataset has only 80 QA pairs but  it worked very bad. Any idead about fixing this issue?  I have tried `paraphrase-multilingual-MiniLM-L12-v2` model but it seems not working correctly.\r\nWith great respect to you! Thanks for the great work you have done for haystack.Thank you @julian-risch . Your advice saved me! Now I use `sentence-transformers/distiluse-base-multilingual-cased` for question embedding. It seems to have a nice result. The dataset I used is very small. So I decide to finetune the model later.\r\n\r\nA little more help please. Now I am going to buid a REST API for the FAQ system. It seems Haystack has already privde a tutorial on [REST](https://www.deepset.ai/blog/building-a-rest-api-for-question-answering-with-haystack). But I want to build the FAQ system with out docker. I checked the `.yaml` config on [issue1361](https://github.com/deepset-ai/haystack/pull/1361) but it won't work.\r\nMaybe the option1 in [REST API](https://haystack.deepset.ai/components/rest-api) is a alternative way. I still can't figure out how to write this code. Sorry that I am a newbie in QA domain.\r\nThank you anyway. You have helped me so much!\r\n\r\n",
    "meta": { "name": "Best pipeline for similarity question search." },
    "answer": "Hi @JimReno what you describe sounds like our `FAQPipeline`. Here is a tutorial about it: https://haystack.deepset.ai/tutorials/existing-faqs Please have a look and let me know if it fits your use case. We also have a [blogpost about FAQ Search](https://www.deepset.ai/blog/semantic-faq-search-with-haystack) but because of the fast development of Haystack be aware that some code examples from the blogpost are outdated by now so I'd suggest you focus more on the tutorial than the blogpost."
  },
  {
    "content": "Hi, \r\nWe want to host our models on Hugging Face. Anyone can think of a reason that it's a bad idea to call the HF inference api inside a custom node and put that in the pipeline as reader?\r\n\r\nI did see this [Outsourcing Reader](https://github.com/deepset-ai/haystack/discussions/1157) but it didn't answer my question.\r\n\r\nI would appreciate some insight Hi @rezatakhshid that's a great question! Your idea should in general work. You could implement a custom node that communicates with an API, yes. Our [documentation page about custom nodes](https://haystack.deepset.ai/pipeline_nodes/custom-nodes) will be helpful for that.\r\nThe downsides that I can think of are first of all latency and also some limitations on the responses you get back from the API compared to the answers you get from `FARMReader`. For example, with a `FARMReader` you can have calibrated confidence scores and you can use the evaluation functionalities of Haystack. That won't be possible otherwise. And while a custom node that communicates with an external API gives you the advantage that you would not need to host a reader model yourself, you would still need to host all other parts of the pipeline. In case you are considering to host and manage not only nodes but entire pipelines instead, you could maybe have a look at [deepset Cloud](https://www.deepset.ai/deepset-cloud) as another option.",
    "meta": { "name": "Wise to call outside API on custom node?" },
    "answer": "Hi @rezatakhshid that's a great question! Your idea should in general work. You could implement a custom node that communicates with an API, yes. Our [documentation page about custom nodes](https://haystack.deepset.ai/pipeline_nodes/custom-nodes) will be helpful for that.\r\nThe downsides that I can think of are first of all latency and also some limitations on the responses you get back from the API compared to the answers you get from `FARMReader`. For example, with a `FARMReader` you can have calibrated confidence scores and you can use the evaluation functionalities of Haystack. That won't be possible otherwise. And while a custom node that communicates with an external API gives you the advantage that you would not need to host a reader model yourself, you would still need to host all other parts of the pipeline. In case you are considering to host and manage not only nodes but entire pipelines instead, you could maybe have a look at [deepset Cloud](https://www.deepset.ai/deepset-cloud) as another option."
  },
  {
    "content": "Hi, I've been working my way through the tutorials and have set up an elastic search docker container instance (with a persistent volume) and a document store using some FAQ data (Tutorial 4).\r\n\r\nAre there any guides on adapting the streamlit UI app to make use of this instance instead of the default one? It would be useful to demonstrate some use-cases to my colleagues using our data.Hi @danielduckworth, a Streamlit app for FAQ would be really cool! As I see it, this should be possible. I just want to set the scene here a bit first. So a Haystack pipeline can be deployed by wrapping it in a REST API application. The Streamlit UI communicates with the REST API in order to fire requests at the Haystack pipeline. By calling `docker-compose up` you can start up an ES instance, the Haystack REST API app and also the UI. I would highly recommend looking through our [REST API documentation](https://haystack.deepset.ai/guides/rest-api) which gives a detailed guide on how to interact with these components. But I can give a high level overview of the steps that I think we need to take:\r\n\r\n1) Ensure that your ES instance connects to Haystack\r\n2) Ensure that your FAQ pipeline is being used by the Haystack REST API\r\n3) Adjust the Streamlit UI so that it will show the right FAQ data\r\n\r\nFor 1) if you already have a running ES instance, it would be good just to check that your instance doesn't get clash with or shutdown due to the ES instance that is created by `docker-compose up`. \r\n\r\nFor 2) you will need a yaml version of your FAQ pipeline. For this, you will need to use the `Pipeline.save_to_yaml()` function. This yaml file will then need to be moved to `haystack/rest_api/pipeline/pipelines.haystack-pipeline.yml`. You will also need to comment / uncomment the lines in `docker-compose.yml` (or `docker-compose-gpu.yml`) so that you are no longer using the default haystack-api image but rather your own custom pipeline.\r\n\r\nFor 3) I am not so familiar with how to make changes to Streamlit. But let us know when you get to this point. I believe that @TuanaCelik should be able to help you with this step.\r\n\r\nI hope this makes some sense! Please let us know if you need any help at any point.",
    "meta": {
      "name": "Are there any guides on customising the UI to use a custom elastic search document store?"
    },
    "answer": "Hi @danielduckworth, a Streamlit app for FAQ would be really cool! As I see it, this should be possible. I just want to set the scene here a bit first. So a Haystack pipeline can be deployed by wrapping it in a REST API application. The Streamlit UI communicates with the REST API in order to fire requests at the Haystack pipeline. By calling `docker-compose up` you can start up an ES instance, the Haystack REST API app and also the UI. I would highly recommend looking through our [REST API documentation](https://haystack.deepset.ai/guides/rest-api) which gives a detailed guide on how to interact with these components. But I can give a high level overview of the steps that I think we need to take:\r\n\r\n1) Ensure that your ES instance connects to Haystack\r\n2) Ensure that your FAQ pipeline is being used by the Haystack REST API\r\n3) Adjust the Streamlit UI so that it will show the right FAQ data\r\n\r\nFor 1) if you already have a running ES instance, it would be good just to check that your instance doesn't get clash with or shutdown due to the ES instance that is created by `docker-compose up`. \r\n\r\nFor 2) you will need a yaml version of your FAQ pipeline. For this, you will need to use the `Pipeline.save_to_yaml()` function. This yaml file will then need to be moved to `haystack/rest_api/pipeline/pipelines.haystack-pipeline.yml`. You will also need to comment / uncomment the lines in `docker-compose.yml` (or `docker-compose-gpu.yml`) so that you are no longer using the default haystack-api image but rather your own custom pipeline.\r\n\r\nFor 3) I am not so familiar with how to make changes to Streamlit. But let us know when you get to this point. I believe that @TuanaCelik should be able to help you with this step.\r\n\r\nI hope this makes some sense! Please let us know if you need any help at any point."
  },
  {
    "content": "Hello \r\n\r\nI am trying to use the MostSimiliarDocumentsPipeline and keep getting the following error: \r\n\"elasticsearch.exceptions.RequestError: RequestError(400, \"search_phase_execution_exception: Likely some of your stored documents don't have embeddings. Run the document store's update_embeddings() method.\", 'compile error')\"\r\n\r\nI also get this error when i try to run query_by_embedding directly on my document store (which is Elasticsearch).\r\n\r\nMy code is based on the example provided by bogdankostic in this discussion: https://github.com/deepset-ai/haystack/discussions/1496 \r\n\r\nI have tried the folllowing:\r\n- Made sure that my documents are not short as described in the link\r\n- Called document_store.update_embeddings\r\n- Pulled out all documents and printed their embeddings - they all have values / are not None (return_embedding=True)\r\n- Printed \"document_store.get_embedding_count()\" on the correct index - it is 10 (matching my number of documents)\r\n- Set document_store.return_embedding to True\r\n- Extracted the id of the first document i pull out, print its embeddings (not None) and use them for the search\r\n\r\nAll of these experiments seem to point to the fact that my documents actually have embeddings, but i still get the error.\r\nThe exception is thrown around line 1239-1245 in elasticsearch.py because \"self.embedding_count\" returns 0 but when i make the exact same call myself i get a non-zero value. \r\nAlso, if i use a random word as id in MostSimiliarDocumentsPipeline.run, it does not throw an exception but just returns an empty list of results.\r\n\r\nI have traced through the haystack source code and can't seem to find anywhere i am missing a parameter or anything like that. \r\nI have also searched through previous discussions to resolve my issue.\r\n\r\nAm i trying to use this the wrong way or is something else at fault?\r\nI am sorry if i missed something very obvious. \r\n\r\nI have attached a file with the code i am running.\r\n[haystack_post.txt](https://github.com/deepset-ai/haystack/files/8425290/haystack_post.txt)\r\n\r\nSebastian\r\nHi @kaep! Thanks for the detailed description of your problem and for providing code to reproduce this. Unfortunately, your code works just fine on my side. Neither `query_by_embedding` nor the `MostSimilarDocumentsPipeline`'s `run` method trigger an error on Elasticsearch side using current Haystack master and Elasticsearch version 7.9.2. What version of Haystack and Elasticsearch are you using?Hello @bogdankostic \r\nThank you for the fast reply! \r\nI am using the version 7.5.1 docker image of Elasticsearch as well as \"farm-haystack\" from pip. \r\nI must admit that i don't know why i am using 7.5.1, up until now i was sure that i was using the same version as in the docker-compose provided with your demo... Will try with ersion 7.9.2 asap.\r\n\r\nUpdate: \r\nI just tried again with Elasticsearch 7.9.2 and it seems to be working. \r\nThank you very much for the help.  \r\nI am quite embarrassed that i didn't even consider whether i was using the correct version of Elasticsearch.\r\n",
    "meta": {
      "name": "Missing embeddings in MostSimiliarDocumentsPipeline and query_by_embedding"
    },
    "answer": "Hello @bogdankostic \r\nThank you for the fast reply! \r\nI am using the version 7.5.1 docker image of Elasticsearch as well as \"farm-haystack\" from pip. \r\nI must admit that i don't know why i am using 7.5.1, up until now i was sure that i was using the same version as in the docker-compose provided with your demo... Will try with ersion 7.9.2 asap.\r\n\r\nUpdate: \r\nI just tried again with Elasticsearch 7.9.2 and it seems to be working. \r\nThank you very much for the help.  \r\nI am quite embarrassed that i didn't even consider whether i was using the correct version of Elasticsearch.\r\n"
  },
  {
    "content": "Hi , I was wondering  how integrate haystack on already exiting project on aws with opensearch , also my documents are on words format , so i want to know how to deploy it thanksHi @Yafaa! We recently added the functionality to convert your existing OpenSearch database to a list of Haystack-compatible Documents and write them into the DocumentStore of your choice. You can find the corresponding method [here](https://github.com/deepset-ai/haystack/blob/ae712fe6bf087c717f3e38e4e87d2347165fc12b/haystack/document_stores/utils.py#L283). I hope this will already help you, if not, please try to ask questions that are more specific :)",
    "meta": { "name": "Haystack with aws" },
    "answer": "Hi @Yafaa! We recently added the functionality to convert your existing OpenSearch database to a list of Haystack-compatible Documents and write them into the DocumentStore of your choice. You can find the corresponding method [here](https://github.com/deepset-ai/haystack/blob/ae712fe6bf087c717f3e38e4e87d2347165fc12b/haystack/document_stores/utils.py#L283). I hope this will already help you, if not, please try to ask questions that are more specific :)"
  },
  {
    "content": "Hi,\r\n\r\nI'm currently trying to fine tune a model for the task of end-to-end question generation in German. \r\nI used your published GermanQuAD and transformed it for the task, though it generated questions they weren't mostly suitable for the context (I haven't evaluated with automatic metrics yet). It may be that I need a larger pretrained LM (I currently use a german version of GPT2), a multilingual LM or more training data.\r\n \r\nSince I don't have exactly a clue how much training data for the fine-tuning is necessary and most approaches are using the English SQuAD dataset, I searched for an automatic translated SQuAD dataset and found something here https://huggingface.co/deepset/gelectra-base-germanquad#performance .\r\n\r\nMy question is now if you could share that automatic translation of SQuAD v1.1 and if not maybe give me an insight with which tools you approached this translation.\r\n\r\nThanks in advance and best regards\r\nHey @TiloMichel,\r\n\r\nThe German part of the [MLQA dataset](https://github.com/facebookresearch/MLQA) was used for the performance comparisons. Hope this helps :)As a coincidence, we just released a blogpost on NLP Resources Beyond English today \ud83d\ude01 https://www.deepset.ai/blog/nlp-resources-beyond-english",
    "meta": {
      "name": "Question regarding the german translation of SQuAD v1.1 mentioned for GELECTRA fine tuning"
    },
    "answer": "Hey @TiloMichel,\r\n\r\nThe German part of the [MLQA dataset](https://github.com/facebookresearch/MLQA) was used for the performance comparisons. Hope this helps :)"
  },
  {
    "content": "Hi, \r\n\r\nWhen I am using a new model in the `reader` variable like so:\r\n\r\n`reader = TransformersReader(tokenizer=\"deepset/roberta-base-squad2\", use_gpu=-1)`\r\n\r\nI notice there are files downloaded to `~/.cache/hugging/transformers/`. \r\n\r\n1. are these files static? \r\n2. what are these files exactly? \r\n\r\n\r\nI tried downloading the model locally from hugging face using these steps: \r\n\r\n1. https://huggingface.co/distilbert-base-uncased\r\n2. git lfs install\r\n3. git clone https://huggingface.co/distilbert-base-uncased\r\n\r\nBut I still need what is in the `/.cache` directory. How can I create a local hugging face model/ cache files for deployment in cloud? \r\nHi @asharm0662 \r\nthe files downloaded to `~/.cache/hugging/transformers/` are language models that interpret queries and find an answer in a text document (for example) and they include the corresponding tokenizers that split any arbitrary input strings, e.g., queries, into sequences of tokens. When you run `reader = TransformersReader(tokenizer=\"deepset/roberta-base-squad2\", use_gpu=-1)` the tokenizer is loaded from https://huggingface.co/deepset/roberta-base-squad2\r\nThese files are about 2 GB large, sometimes even larger. We cache these models inside the transformers library because it would take a long time to download them on-the-fly every time you want to run a query. The models won't change so they are static, yes.\r\n\r\nIf you would like to fill the cache, I recommend that you have a look at how we create our docker files with the models already cached: https://github.com/deepset-ai/haystack/pull/1978/files\r\nAnother way is to copy the cache from a different machine or to save the model on disk: https://stackoverflow.com/questions/62261602/downloading-transformers-models-to-use-offline\r\n\r\nHowever, when you run your cloud-deployed application for the first time (with internet connection) the cache will be filled automatically. Is there any particular reason why you want to make sure that the model is already in the cache even before using it for the first time?",
    "meta": {
      "name": "What are the files that haystack downloads to cache directory? and how can I create a local package?"
    },
    "answer": "Hi @asharm0662 \r\nthe files downloaded to `~/.cache/hugging/transformers/` are language models that interpret queries and find an answer in a text document (for example) and they include the corresponding tokenizers that split any arbitrary input strings, e.g., queries, into sequences of tokens. When you run `reader = TransformersReader(tokenizer=\"deepset/roberta-base-squad2\", use_gpu=-1)` the tokenizer is loaded from https://huggingface.co/deepset/roberta-base-squad2\r\nThese files are about 2 GB large, sometimes even larger. We cache these models inside the transformers library because it would take a long time to download them on-the-fly every time you want to run a query. The models won't change so they are static, yes.\r\n\r\nIf you would like to fill the cache, I recommend that you have a look at how we create our docker files with the models already cached: https://github.com/deepset-ai/haystack/pull/1978/files\r\nAnother way is to copy the cache from a different machine or to save the model on disk: https://stackoverflow.com/questions/62261602/downloading-transformers-models-to-use-offline\r\n\r\nHowever, when you run your cloud-deployed application for the first time (with internet connection) the cache will be filled automatically. Is there any particular reason why you want to make sure that the model is already in the cache even before using it for the first time?"
  },
  {
    "content": "Hi,\r\nI have trained a Q&A Model using FARMReader and now I have converted it into transformer model using transformer converter which returns prediction head. But I want to upload this trained model on to transformer hub so, I can achieve that. I am unable to save respective config files that are required when converter converts from FARM model to Transformer model.\r\n\r\nPlease help.\r\n\r\nThanksHi @karndeepsingh could you please describe in a bit more detail what you've tried and at what point it doesn't work for you? Am I correct that you have used Haystack's `Converter` class and its functionalities in the following way?\r\n```\r\nfrom haystack.modeling.conversion.transformers import Converter\r\ntransformers_models = Converter.convert_to_transformers(reader.inferencer.model)\r\n```Hi @karndeepsingh here are two different ways to save a loaded model. In the screenshot, you can see what files are stored. I hope that helps. \ud83d\udc4d \r\n```\r\nfrom haystack.nodes import FARMReader\r\nreader = FARMReader(model_name_or_path=\"bert-base-uncased\", use_gpu=True)\r\nreader.save(directory=\"my_model\")\r\n\r\nfrom haystack.modeling.conversion.transformers import Converter\r\ntransformers_models = Converter.convert_to_transformers(reader.inferencer.model)\r\ntransformers_models[0].save_pretrained(save_directory=\"my_model_transformers\")\r\n```\r\n<img width=\"286\" alt=\"image\" src=\"https://user-images.githubusercontent.com/4181769/153851467-ce8f6f25-5bec-40c4-b700-c424eebdd2f9.png\">\r\n",
    "meta": {
      "name": "How to save the model trained on FARMReader to Transformer model and upload it on Transformer Hub?"
    },
    "answer": "Hi @karndeepsingh here are two different ways to save a loaded model. In the screenshot, you can see what files are stored. I hope that helps. \ud83d\udc4d \r\n```\r\nfrom haystack.nodes import FARMReader\r\nreader = FARMReader(model_name_or_path=\"bert-base-uncased\", use_gpu=True)\r\nreader.save(directory=\"my_model\")\r\n\r\nfrom haystack.modeling.conversion.transformers import Converter\r\ntransformers_models = Converter.convert_to_transformers(reader.inferencer.model)\r\ntransformers_models[0].save_pretrained(save_directory=\"my_model_transformers\")\r\n```\r\n<img width=\"286\" alt=\"image\" src=\"https://user-images.githubusercontent.com/4181769/153851467-ce8f6f25-5bec-40c4-b700-c424eebdd2f9.png\">\r\n"
  },
  {
    "content": "I was trying to use the embedding retriever by using a sentence transformer model, when I set the similarity parameter as \"dot_product\" it works as expected. however if I try to use cosine similarity, I get the following error:\r\n\r\n![Screenshot from 2022-03-02 11-13-02](https://user-images.githubusercontent.com/45804540/156402113-0bbf9b31-4f79-44ea-9009-c9b9372e75dd.png)\r\n\r\n![Screenshot from 2022-03-02 11-13-36](https://user-images.githubusercontent.com/45804540/156402477-5030d04b-8b36-4653-8d44-919c34fd3e36.png)\r\n\r\n\r\ndoes anyone have any clue about this issue?\r\n\r\nthanks in advance.Hey @Saintes9601 ,\r\n\r\nthe bug you're describing was fixed in [this PR](https://github.com/deepset-ai/haystack/pull/1933).\r\n\r\nIf you update your Haystack version to 1.2, it should work as planned.\r\n\r\nHope this helps!",
    "meta": { "name": "Error using EmbeddingRetriever with cosine similarity" },
    "answer": "Hey @Saintes9601 ,\r\n\r\nthe bug you're describing was fixed in [this PR](https://github.com/deepset-ai/haystack/pull/1933).\r\n\r\nIf you update your Haystack version to 1.2, it should work as planned.\r\n\r\nHope this helps!"
  },
  {
    "content": "Hello. I am trying to use AWS OpenSearch Service (version: OpenSearch1.0) for my document store -- I am using JupyterLab with Python 3.7 and the latest Haystack version. When I try to create the document store in my notebook with OpenSearchDocumentStore(), I receive the following error: \r\n\r\n`RequestError(400, 'mapper_parsing_exception', 'No handler for type [flattened] declared on field [answer]')`.\r\n\r\n\r\nI am unsure if this is an AWS issue or if it's something the Haystack community can help me with. I have tried setting `index_type = 'hnsw' ` in the OpenSearchDocumentStore constructor and also tested different versions of ElasticSearch in AWS, but still get the same error. \r\n\r\nI appreciate any help, thanks! Hello! Sorry for the late reply. This might be related to a big change we've done recently to Haystack core classes (https://github.com/deepset-ai/haystack/issues/1590), so while I try to replicate your issue, can you try with the 0.10 release instead of the latest master, and see if the problem persists? That would help us a lot to narrow down the source of the problem.",
    "meta": { "name": "Haystack in AWS: Handler for type Flattened" },
    "answer": "Hello! Sorry for the late reply. This might be related to a big change we've done recently to Haystack core classes (https://github.com/deepset-ai/haystack/issues/1590), so while I try to replicate your issue, can you try with the 0.10 release instead of the latest master, and see if the problem persists? That would help us a lot to narrow down the source of the problem."
  },
  {
    "content": "Hi community,\r\nI have two questions regarding building a custom data set for the \"Question Answering\" task.\r\nI have very detailed user questions and their respective answers, but no context that the answers are referring to. What can be done to make use of the questions/answers within a training data set? Can this still be used to train a model? Can I \"synthetically\" embed the answers in a context? Or is there a different model where it is sufficient to input \"only\" questions and answers, without context and still get good results?\r\nIs there a rule of thumb about how much questions/answers are needed to do a reasonable training?\r\n  Hi @gabriead! To train an extractive QA model, you would need a context which contains the answer and the exact position of the answer inside this context. Therefore, you would need to map your question-answer pairs to a document containing the answer and extract the position of the answer. However, you might use your data to do open-domain evaluation, as this does not require to extract the exact position of an answer. Like this, you can check whether the existing models are already good enough for your use case such that you don't need to train a custom model. See [this blog post](https://www.deepset.ai/blog/how-to-evaluate-a-question-answering-system) for more information on evaluation.\r\n\r\nAs to how many labels are needed to do reasonable training: This depends highly on your domain and how much your use case diverges from SQuAD. We have seen that models trained on SQuAD show very strong general question answering capabilities. Therefore, we\u2019d recommend trying one of the off the shelf models before trying to adapt these models to your domain.",
    "meta": { "name": "How to build a custom data set for Question Answering" },
    "answer": "Hi @gabriead! To train an extractive QA model, you would need a context which contains the answer and the exact position of the answer inside this context. Therefore, you would need to map your question-answer pairs to a document containing the answer and extract the position of the answer. However, you might use your data to do open-domain evaluation, as this does not require to extract the exact position of an answer. Like this, you can check whether the existing models are already good enough for your use case such that you don't need to train a custom model. See [this blog post](https://www.deepset.ai/blog/how-to-evaluate-a-question-answering-system) for more information on evaluation.\r\n\r\nAs to how many labels are needed to do reasonable training: This depends highly on your domain and how much your use case diverges from SQuAD. We have seen that models trained on SQuAD show very strong general question answering capabilities. Therefore, we\u2019d recommend trying one of the off the shelf models before trying to adapt these models to your domain."
  },
  {
    "content": "Hi community,\r\nI want to use a custom data set that will contain technical documents. What are the most important text preprocessing steps that I should do? Should I indicate a headline (how would that be indicated?), start-end of a paragraph etc.? Currently I am using the entire text of an article with end of line tags as context value and the headline in as meta value. What meta information is important to improve search results?\r\n\r\nThank's a lot!Hi @gabriead first of all, the preprocessing and indexing is pretty straightforward and I'd say that the approach you describe sounds good so you don't need worry too much about it. You can find some tipps and examples in our tutorial on preproccessing, which you have probably already found: https://haystack.deepset.ai/tutorials/preprocessing\r\nWhat data to index as metadata depends on your search application and your use case. Having the headline as meta data isn't a bad idea, still I would also add it to the article text if it's not already in there. Meta data can help for filtering search results. We have an article on that topic here: https://www.deepset.ai/blog/metadata-filtering-in-haystack Happy reading! :) ",
    "meta": {
      "name": "What are the most important steps in preprocessingt text?"
    },
    "answer": "Hi @gabriead first of all, the preprocessing and indexing is pretty straightforward and I'd say that the approach you describe sounds good so you don't need worry too much about it. You can find some tipps and examples in our tutorial on preproccessing, which you have probably already found: https://haystack.deepset.ai/tutorials/preprocessing\r\nWhat data to index as metadata depends on your search application and your use case. Having the headline as meta data isn't a bad idea, still I would also add it to the article text if it's not already in there. Meta data can help for filtering search results. We have an article on that topic here: https://www.deepset.ai/blog/metadata-filtering-in-haystack Happy reading! :) "
  },
  {
    "content": "Hi, \r\n\r\nI am separating document parsing and question/answer prediction for the following the in-memory [tutorial](https://haystack.deepset.ai/tutorials/v0.9.0/without-elasticsearch) into separate scripts, so I can simply read in all only the dictionary object during time of prediction. \r\n\r\nFirst script for document parsing: \r\n\r\n```\r\nfrom haystack.utils import clean_wiki_text, convert_files_to_dicts, fetch_archive_from_http\r\nimport json\r\n\r\nfrom haystack.document_store.memory import InMemoryDocumentStore\r\ndocument_store = InMemoryDocumentStore()\r\n\r\ndoc_dir = \"data/article_txt_got\"\r\ns3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\"\r\nfetch_archive_from_http(url=s3_url, output_dir=doc_dir)\r\n\r\n# convert files to dicts containing documents that can be indexed to our datastore\r\n# You can optionally supply a cleaning function that is applied to each doc (e.g. to remove footers)\r\n# It must take a str as input, and return a str.\r\ndicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\r\n\r\nwith open('data.txt', 'w') as outfile:\r\n    json.dump(dicts, outfile)\r\n```\r\n\r\n\r\nSecond script to read in the parsed data from the JSON file from the first script:\r\n\r\n```\r\nimport json\r\nfrom haystack.document_stores import InMemoryDocumentStore\r\n\r\n\r\ndocument_store = InMemoryDocumentStore()\r\n\r\n\r\n\r\nwith open('data.txt') as jsonfile:\r\n    dicts = json.load(jsonfile)\r\n\r\ndocument_store.write_documents(dicts, duplicate_documents='overwrite')\r\n```\r\n\r\n\r\nThis line `document_store.write_documents(dicts, duplicate_documents='overwrite')`, returns back, \r\n`INFO - haystack.document_stores.base -  Duplicate Documents: Document with id '9e9a3181b6bc168b4a25429b641e8c86' already exists in index 'document'`, for all the documents every time I read them in. \r\n\r\n**Why is this the case?   How can I avoid this output when I am reading in the data from the JSON file that has already been parsed?**\r\n\r\n**Is there anyway to write out the `document_store` variable and read that back in to avoid the warning message?** Hello @asharm0662, I'll admit this sounds like a bug. I'm now trying to replicate this behavior, so I can have a closer look at what's going on.\r\n\r\nDo you mind if I convert this discussion into an issue? This way we can track it better, especially if it ends up being a real bug.",
    "meta": {
      "name": "How to read in parsed dictionary of document text without getting `Duplicate Documents` warning?"
    },
    "answer": "Hello @asharm0662, I'll admit this sounds like a bug. I'm now trying to replicate this behavior, so I can have a closer look at what's going on.\r\n\r\nDo you mind if I convert this discussion into an issue? This way we can track it better, especially if it ends up being a real bug."
  },
  {
    "content": "Hi, \r\n\r\nI am trying to create my own docker package for haystack/huggingface models. \r\n\r\nI have downloaded the model I want to use locally, and can point to the directory for the model on my local machine no problem for my reader. This is the code I am using on my local machine to point the reader transformer reader tokenizer, to local directory of the model: \r\n\r\n```\r\nfrom haystack.nodes import TransformersReader, FARMReader\r\nimport json\r\nfrom haystack.pipelines import ExtractiveQAPipeline, GenerativeQAPipeline\r\nfrom haystack.nodes import TfidfRetriever\r\nfrom haystack.document_stores import InMemoryDocumentStore\r\nfrom haystack.utils import print_answers\r\n\r\n\r\ndocument_store = InMemoryDocumentStore()\r\n\r\nwith open('data.txt') as jsonfile:\r\n    dicts = json.load(jsonfile)\r\n   \r\ndocument_store.write_documents(dicts, duplicate_documents='overwrite')\r\n# An in-memory TfidfRetriever based on Pandas dataframes\r\nretriever = TfidfRetriever(document_store=document_store)\r\nreader = TransformersReader(tokenizer=\"/path/to/model/distilbert-base-uncased\", use_gpu=-1)\r\npipe = ExtractiveQAPipeline(reader, retriever)\r\n\r\nprediction = pipe.run(\r\n    query=\"Who is the father of Arya Stark?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 2}}\r\n    )\r\n    \r\nprint_answers(prediction, details=\"minimal\")\r\n```\r\n\r\nHowever when I wrap this code for Docker/serverless and point this line to the folder path where my model is:\r\n\r\n`reader = TransformersReader(tokenizer=\"/var/task/distilbert-base-uncased\", use_gpu=-1)`\r\n\r\nHaystack is prefixing the URL like so:\r\n\r\n`reader = TransformersReader(tokenizer=\"https://huggingingface/var/task/distilbert-base-uncased\", use_gpu=-1)`\r\n\r\n\r\nWhen I change the same line with model_path attribute, I can pick up the model locally but I am not getting the right answer:\r\n\r\n`reader = TransformersReader(model_name_or_path=/var/task/distilbert-base-uncased, tokenizer=\"/var/task/distilbert-base-uncased\", use_gpu=-1)`\r\n\r\nHow do I prevent this behavior? Let me know what or if any files I can provide. Hello @asharm0662, actually the issue you're observing comes straight from the `transformer`'s library, which takes care of handling the models. I don't think we can help too much with it. I'd recommend:\r\n1. Checking the paths are correct\r\n2. Checking file permissions\r\n3. Have a look at the logs generated at startup and try to spot warnings from `transformers` (probably Haystack is not even noticing that there are issues finding the model, `transformers` seems to be handling it internally).\r\n\r\nIf you share the logs of you application I might be able to give you more hints.",
    "meta": {
      "name": "How to prevent Haystack from prefixing URL for transformers in model path?"
    },
    "answer": "Hello @asharm0662, actually the issue you're observing comes straight from the `transformer`'s library, which takes care of handling the models. I don't think we can help too much with it. I'd recommend:\r\n1. Checking the paths are correct\r\n2. Checking file permissions\r\n3. Have a look at the logs generated at startup and try to spot warnings from `transformers` (probably Haystack is not even noticing that there are issues finding the model, `transformers` seems to be handling it internally).\r\n\r\nIf you share the logs of you application I might be able to give you more hints."
  },
  {
    "content": "I am using ElasticsearchRetriever, with ElasticsearchDocumentStore.\r\nAnd my query and documents both are longer than 1024.\r\nSo when I query \r\n```python\r\ncandidate_documents = retriever.retrieve(\r\n    query=q,\r\n    top_k=10,\r\n)\r\n``` \r\nI get  this error\r\n```\r\nRequestError: RequestError(400, 'search_phase_execution_exception', 'failed to create query: maxClauseCount is set to 1024')\r\n```\r\nHow do I increase the maxClauseCount.  or what else can be a better-optimized approach, my query can be very long ranging from 10-5000 characters in length\r\n\r\n@tholor @tanaysoni  guys any clueHello @k-for-code, `maxClauseCount` is a parameter of your Elasticsearch instance. Have a look here: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-settings.html It can be set in the `elasticsearch.yml` file (inside the configuration folder of ES).\r\n",
    "meta": { "name": "How do i increase maxClauseCount in query" },
    "answer": "Hello @k-for-code, `maxClauseCount` is a parameter of your Elasticsearch instance. Have a look here: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-settings.html It can be set in the `elasticsearch.yml` file (inside the configuration folder of ES).\r\n"
  },
  {
    "content": "Hi, \r\n\r\nI am following this [tutorial](https://haystack.deepset.ai/tutorials/without-elasticsearch). Under the 'Preprocessing of Documents' section, there are some functions to clean documents and write them into a document store. \r\n\r\nInstead of writing the cleaned document text data to  document store, can I write the cleaned as dictionary object to text file and read that file into memory? If yes, what is the best format to write the data JSON/dictionary? \r\n\r\nThank you in advance. Hi @asharm0662! The `convert_files_to_dicts` method in the tutorial you linked will return a list of dictionaries. You can easily save them as a json-file like this:\r\n```python\r\nimport json\r\nwith open(FILE_NAME, \"w\") as json_file:\r\n    json.dump(dicts, json_file)\r\n```\r\n",
    "meta": {
      "name": "Can preprocessing of documents be saved as text file to be read back in?"
    },
    "answer": "Hi @asharm0662! The `convert_files_to_dicts` method in the tutorial you linked will return a list of dictionaries. You can easily save them as a json-file like this:\r\n```python\r\nimport json\r\nwith open(FILE_NAME, \"w\") as json_file:\r\n    json.dump(dicts, json_file)\r\n```\r\n"
  },
  {
    "content": "I've been using pair of facebook's `dpr-question_encoder-single-nq-base` and `dpr-ctx_encoder-single-nq-base` for dense passage retrieval but I feel like these are far from ideal for my domain related to legal+financial stuff. Do you suggest any other model that could be kind of better for that domain?Hi @kamilpz there are no domain-specific DPR models available for the legal domain or finance domain. On the model hub, you can search for DPR models with the help of filters: https://huggingface.co/models?other=dpr\r\n\r\nIf you're interested to learn what is needed to create a training dataset and train such a domain-specific model yourself, I would recommend these two papers: https://arxiv.org/abs/2004.04906 https://arxiv.org/abs/2104.12741\r\n",
    "meta": { "name": "DPR models for legal&financial documents" },
    "answer": "Hi @kamilpz there are no domain-specific DPR models available for the legal domain or finance domain. On the model hub, you can search for DPR models with the help of filters: https://huggingface.co/models?other=dpr\r\n\r\nIf you're interested to learn what is needed to create a training dataset and train such a domain-specific model yourself, I would recommend these two papers: https://arxiv.org/abs/2004.04906 https://arxiv.org/abs/2104.12741\r\n"
  },
  {
    "content": "Is there any documentation on how to add your own DocumentStore? Like if for example someone wants to add Weaviate or Vearch to store de representations from the Documents or something like that.Hi @sgaseretto a WeaviateDocumentstore is already available in Haystack: https://github.com/deepset-ai/haystack/blob/master/haystack/document_stores/weaviate.py \r\nIf you are thinking about implementing a custom Documentstore you can have a look at the base class to see the required methods: https://github.com/deepset-ai/haystack/blob/master/haystack/document_stores/base.py but there is no additional documentation on how to implement a custom document store. I think most users should be able to find everything they need in the currently available document stores. Out of curiosity: are there any particular features that you are currently missing?",
    "meta": { "name": "How to add your own custom DocumentStore" },
    "answer": "Hi @sgaseretto a WeaviateDocumentstore is already available in Haystack: https://github.com/deepset-ai/haystack/blob/master/haystack/document_stores/weaviate.py \r\nIf you are thinking about implementing a custom Documentstore you can have a look at the base class to see the required methods: https://github.com/deepset-ai/haystack/blob/master/haystack/document_stores/base.py but there is no additional documentation on how to implement a custom document store. I think most users should be able to find everything they need in the currently available document stores. Out of curiosity: are there any particular features that you are currently missing?"
  },
  {
    "content": "Tutorial 4 (FAQ-style QA) answers related to COVID19 but each question is followed by answer and then the same answer is quoted in HTML tags (see link below for reference)\r\n>https://raw.githubusercontent.com/deepset-ai/COVID-QA/master/data/faqs/faq_covidbert.csv\r\n\r\nDoes this mean accuracy will drop these tags are not available?Hi @superchargez! You don't need to provide the answers in HTML tags for FAQ-style QA. Given that we only embed the questions and not the answers and we try to find a question in our database that is similar to our query, the answer format does not have an impact on the performance of the `FAQPipeline`. ",
    "meta": { "name": "Are HTML tags necessary in FAQ style QA?" },
    "answer": "Hi @superchargez! You don't need to provide the answers in HTML tags for FAQ-style QA. Given that we only embed the questions and not the answers and we try to find a question in our database that is similar to our query, the answer format does not have an impact on the performance of the `FAQPipeline`. "
  },
  {
    "content": "Hi, I'm having a strange problem when installing farm-haystack with either \r\n`pip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html --user`\r\nor \r\n`pip install farm-haystack`\r\n\r\nIt appears everything correctly installs, but then this error appears in the terminal\r\n`ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behavior is the source of the following dependency conflicts.\r\nfarm 0.8.0 requires transformers==4.6.1, but you have transformers 4.7.0 which is incompatible.\r\ndatasets 1.15.2.dev0 requires huggingface-hub<1.0.0,>=0.1.0, but you have huggingface-hub 0.0.8 which is incompatible.`\r\n\r\nI've tried directly install the packages with the versions it wants, but that's only lead to more incompatibilities. Can anyone please help?Hi @Obsideaock! I was not able to replicate your issue on my MacOS machine. Which operating system are you using? Did you try to install haystack inside a new environment?> Hi, I'm having a strange problem when installing farm-haystack with either `pip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html --user` or `pip install farm-haystack`\r\n> \r\n> It appears everything correctly installs, but then this error appears in the terminal `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behavior is the source of the following dependency conflicts. farm 0.8.0 requires transformers==4.6.1, but you have transformers 4.7.0 which is incompatible. datasets 1.15.2.dev0 requires huggingface-hub<1.0.0,>=0.1.0, but you have huggingface-hub 0.0.8 which is incompatible.`\r\n> \r\n> I've tried directly install the packages with the versions it wants, but that's only lead to more incompatibilities. Can anyone please help?\r\n\r\nUpgrade pip. I had same issue for another framework but upgrading pip resolved this issue.\r\nYou may encounter problems upgrading pip (depending on 1> platform 2> already conda based environment)\r\n1a> If you are using windows just reinstall pip/python (after deleting older version, this is the fastest option)\r\n1b> If you are on linux install pyenv and install python that you need and THEN install/upgrade pip\r\n1b2> if above does not work then remove pyenv and install conda (I mean switch environment builder to another) and upgrade pip\r\n2> for conda environment users, create a new environment and just upgrade pip",
    "meta": { "name": "Installing farm-haystack causes weird errors" },
    "answer": "> Hi, I'm having a strange problem when installing farm-haystack with either `pip install farm-haystack -f https://download.pytorch.org/whl/torch_stable.html --user` or `pip install farm-haystack`\r\n> \r\n> It appears everything correctly installs, but then this error appears in the terminal `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behavior is the source of the following dependency conflicts. farm 0.8.0 requires transformers==4.6.1, but you have transformers 4.7.0 which is incompatible. datasets 1.15.2.dev0 requires huggingface-hub<1.0.0,>=0.1.0, but you have huggingface-hub 0.0.8 which is incompatible.`\r\n> \r\n> I've tried directly install the packages with the versions it wants, but that's only lead to more incompatibilities. Can anyone please help?\r\n\r\nUpgrade pip. I had same issue for another framework but upgrading pip resolved this issue.\r\nYou may encounter problems upgrading pip (depending on 1> platform 2> already conda based environment)\r\n1a> If you are using windows just reinstall pip/python (after deleting older version, this is the fastest option)\r\n1b> If you are on linux install pyenv and install python that you need and THEN install/upgrade pip\r\n1b2> if above does not work then remove pyenv and install conda (I mean switch environment builder to another) and upgrade pip\r\n2> for conda environment users, create a new environment and just upgrade pip"
  },
  {
    "content": "Hello :)\r\nI created a QA model that brings back 3 answers, with a confidence score,\r\nHowever, sometimes the answer with the lowest score is the correct one...\r\nHow can I know automatically what is the best answer without going manually over them?\r\n(And without fine-tuning or training a model from scratch? :) )\r\nHi @overg123 , glad to hear you managed to get it working! Unfortunately these models are not perfect and the correct answer might not always be your top answer. What you're after is an improvement in model performance. For this, the quickest improvement I could think of would be to try out some other models. In particular, large models will slow down your system but potentially improve performance.\r\n\r\nHere are two models that might be of interest to you:\r\nhttps://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad\r\nhttps://huggingface.co/deepset/xlm-roberta-large-squad2",
    "meta": { "name": "How to know which answer is the best from QA model" },
    "answer": "Hi @overg123 , glad to hear you managed to get it working! Unfortunately these models are not perfect and the correct answer might not always be your top answer. What you're after is an improvement in model performance. For this, the quickest improvement I could think of would be to try out some other models. In particular, large models will slow down your system but potentially improve performance.\r\n\r\nHere are two models that might be of interest to you:\r\nhttps://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad\r\nhttps://huggingface.co/deepset/xlm-roberta-large-squad2"
  },
  {
    "content": "I'm wondering how to set the b and k_1 parameters for BM25 in an ElasticSearchRetriever. Any suggestions?Hi @mgreenbe , I don't know the exact steps yet to making this happen but from a first quick search on the topic, I have some ideas. \r\n\r\nIn Haystack, when you initialize an `ElasticsearchDocumentStore`, you use something called a mapping to define how the ES index works (you can see the default mapping [here](https://github.com/deepset-ai/haystack/blob/4ca19377759d39604458e717086d3075eee369ec/haystack/document_stores/elasticsearch.py#L248)). You also have the option however to provide your own custom mapping when initializing the `ElasticsearchDocumentStore` via the `custom_mapping` argument. \r\n\r\nIn [this](https://stackoverflow.com/questions/27307291/bm25-similarity-tuning-in-elasticsearch) thread, the top answer provides a settings dictionary to define specific b and k_1 values. I think these could probably be inserted into a custom mapping. It also seems possible to make a PUT request to the ES instance directly.\r\n\r\nHope these pointers help. Please let us know how you get on with this!\r\n\r\n",
    "meta": { "name": "BM25 parameters" },
    "answer": "Hi @mgreenbe , I don't know the exact steps yet to making this happen but from a first quick search on the topic, I have some ideas. \r\n\r\nIn Haystack, when you initialize an `ElasticsearchDocumentStore`, you use something called a mapping to define how the ES index works (you can see the default mapping [here](https://github.com/deepset-ai/haystack/blob/4ca19377759d39604458e717086d3075eee369ec/haystack/document_stores/elasticsearch.py#L248)). You also have the option however to provide your own custom mapping when initializing the `ElasticsearchDocumentStore` via the `custom_mapping` argument. \r\n\r\nIn [this](https://stackoverflow.com/questions/27307291/bm25-similarity-tuning-in-elasticsearch) thread, the top answer provides a settings dictionary to define specific b and k_1 values. I think these could probably be inserted into a custom mapping. It also seems possible to make a PUT request to the ES instance directly.\r\n\r\nHope these pointers help. Please let us know how you get on with this!\r\n\r\n"
  },
  {
    "content": ">ModuleNotFoundError: No module named 'haystack.nodes'\r\n\r\n>ModuleNotFoundError: No module named 'haystack.document_stores'\r\n\r\nThese modules work fine on colab but when I try them on local machine the error occours. Kindly help.\r\nAdditional NOTE:\r\nI am using windows (though it should not matter) and have used EXACTLY the same notebook for jupyter that I used for colab.\r\nRef: Tutorial 12: LFQA_via_haystack, 3rd and 4th cellHi @noshila , we recently restructured the structure of the repository and so we have new import statements. `haystack.nodes` and `haystack.document_stores` are both in the new style suggesting you have a notebook from the latest master branch. If these are not working, this probably means that the package you are running is an older version of Haystack. \r\n\r\nCould you try installing the latest master branch of Haystack? To achieve this in the notebook you might need to modify the line that uses pip to something like this\r\n\r\n`!pip install --upgrade git+https://github.com/deepset-ai/haystack.git`",
    "meta": { "name": "Module Import Error" },
    "answer": "Hi @noshila , we recently restructured the structure of the repository and so we have new import statements. `haystack.nodes` and `haystack.document_stores` are both in the new style suggesting you have a notebook from the latest master branch. If these are not working, this probably means that the package you are running is an older version of Haystack. \r\n\r\nCould you try installing the latest master branch of Haystack? To achieve this in the notebook you might need to modify the line that uses pip to something like this\r\n\r\n`!pip install --upgrade git+https://github.com/deepset-ai/haystack.git`"
  },
  {
    "content": "Hi -\r\n\r\nBy default, elastic search returns only 10 results by default. What should be configured in haystack so that elastic search can return more than 10 results?\r\n\r\nThanks,\r\nSekhar H.Hi @sekh77 , I believe I answered this already in our Slack channel but I'll copy my response here in case someone else is having the same issue!\r\n\r\nThe argument that you are looking for is called `top_k`. You can supply it when you are initializing the Retriever, or you can provide it when calling `pipeline.run()` . Here's the [documentation on pipeline arguments](https://haystack.deepset.ai/components/pipelines#arguments) to help you implement the second option.",
    "meta": { "name": "Elastic search to return more than 10 results" },
    "answer": "Hi @sekh77 , I believe I answered this already in our Slack channel but I'll copy my response here in case someone else is having the same issue!\r\n\r\nThe argument that you are looking for is called `top_k`. You can supply it when you are initializing the Retriever, or you can provide it when calling `pipeline.run()` . Here's the [documentation on pipeline arguments](https://haystack.deepset.ai/components/pipelines#arguments) to help you implement the second option."
  },
  {
    "content": "Hi guys, I've just started playing with haystack and it's not splitting my docx files into paragraphs. Based on these lines I'm referencing looks like it's not possible. Am I missing something?\r\n\r\nhttps://github.com/deepset-ai/haystack/blob/bd823c9a6f97f5e34298ec28b8cbe6993914c2cd/haystack/file_converter/docx.py#L49-L53\r\n\r\nhttps://github.com/deepset-ai/haystack/blob/bd823c9a6f97f5e34298ec28b8cbe6993914c2cd/haystack/preprocessor/utils.py#L294-L300\r\n\r\nHi @kamilpz I can recommend two resources to read as starting points regarding splitting of documents.\r\n\r\n1. Our blog article on Parameter Tweaking has a section \"Increasing Pipeline Speed via Document Length Optimization\", which might be interesting to you: https://www.deepset.ai/blog/parameter-tweaking-get-faster-answers-from-your-haystack-pipeline\r\n2. We have a tutorial on preprocessing, where the `split_by=\"word\"` parameter is set but you can easily change it to `split_by=\"passage\"`: https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial8_Preprocessing.ipynb\r\n\r\nWhile the file converter converts your docx file into text in string format, the preprocessor handles splitting long text inputs into smaller chunks that can then be processed by the neural network models. The splitting can take into account word, sentence or paragraph (passage) boundaries. Within haystack, we call these smaller chunks \"documents\" and store them in document stores. So your original docx file will most likely be stored as multiple documents within haystack. \r\n\r\nI hope this answers your questions and if not feel free to describe in more detail what you would like to achieve/ what's your use case and I'd be happy to help. ",
    "meta": { "name": "Spliting docs into paragraphs" },
    "answer": "Hi @kamilpz I can recommend two resources to read as starting points regarding splitting of documents.\r\n\r\n1. Our blog article on Parameter Tweaking has a section \"Increasing Pipeline Speed via Document Length Optimization\", which might be interesting to you: https://www.deepset.ai/blog/parameter-tweaking-get-faster-answers-from-your-haystack-pipeline\r\n2. We have a tutorial on preprocessing, where the `split_by=\"word\"` parameter is set but you can easily change it to `split_by=\"passage\"`: https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial8_Preprocessing.ipynb\r\n\r\nWhile the file converter converts your docx file into text in string format, the preprocessor handles splitting long text inputs into smaller chunks that can then be processed by the neural network models. The splitting can take into account word, sentence or paragraph (passage) boundaries. Within haystack, we call these smaller chunks \"documents\" and store them in document stores. So your original docx file will most likely be stored as multiple documents within haystack. \r\n\r\nI hope this answers your questions and if not feel free to describe in more detail what you would like to achieve/ what's your use case and I'd be happy to help. "
  },
  {
    "content": "Between October 18, 2021 and October 19, 2021, something has changed and the docker returns the error: \"Connection Error. Is Haystack running?\" when we start running it.Also had an issue with testing Docker on a fresh download of the repo and Docker images (followed exact commands on README).\r\n\r\nLog files:\r\nhaystack-api_1   | ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\r\nhaystack-api_1   | [2021-10-19 19:16:54 +0000] [10] [INFO] Started server process [10]\r\nhaystack-api_1   | [2021-10-19 19:16:54 +0000] [10] [INFO] Waiting for application startup.\r\nhaystack-api_1   | [2021-10-19 19:16:54 +0000] [10] [INFO] Application startup complete.\r\nhaystack-api_1   | [2021-10-19 19:16:54 +0000] [10] [ERROR] Exception in ASGI application\r\nhaystack-api_1   | Traceback (most recent call last):\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 375, in run_asgi\r\nhaystack-api_1   |     result = await app(self.scope, self.receive, self.send)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/uvicorn/middleware/proxy_headers.py\", line 75, in __call__\r\nhaystack-api_1   |     return await self.app(scope, receive, send)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/fastapi/applications.py\", line 208, in __call__\r\nhaystack-api_1   |     await super().__call__(scope, receive, send)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/starlette/applications.py\", line 112, in __call__\r\nhaystack-api_1   |     await self.middleware_stack(scope, receive, send)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 181, in __call__\r\nhaystack-api_1   |     raise exc\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 159, in __call__\r\nhaystack-api_1   |     await self.app(scope, receive, _send)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/starlette/middleware/cors.py\", line 84, in __call__\r\nhaystack-api_1   |     await self.app(scope, receive, send)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/starlette/exceptions.py\", line 82, in __call__\r\nhaystack-api_1   |     raise exc\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/starlette/exceptions.py\", line 71, in __call__\r\nhaystack-api_1   |     await self.app(scope, receive, sender)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/starlette/routing.py\", line 656, in __call__\r\nhaystack-api_1   |     await route.handle(scope, receive, send)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/starlette/routing.py\", line 259, in handle\r\nhaystack-api_1   |     await self.app(scope, receive, send)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/starlette/routing.py\", line 61, in app\r\nhaystack-api_1   |     response = await func(request)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/fastapi/routing.py\", line 227, in app\r\nhaystack-api_1   |     dependant=dependant, values=values, is_coroutine=is_coroutine\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/fastapi/routing.py\", line 161, in run_endpoint_function\r\nhaystack-api_1   |     return await run_in_threadpool(dependant.call, **values)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\r\nhaystack-api_1   |     return await anyio.to_thread.run_sync(func, *args)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/anyio/to_thread.py\", line 29, in run_sync\r\nhaystack-api_1   |     limiter=limiter)\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/anyio/_backends/_asyncio.py\", line 805, in run_sync_in_worker_thread\r\nhaystack-api_1   |     return await future\r\nhaystack-api_1   |   File \"/usr/local/lib/python3.7/site-packages/anyio/_backends/_asyncio.py\", line 743, in run\r\nhaystack-api_1   |     result = func(*args)\r\nhaystack-api_1   |   File \"/home/user/rest_api/controller/search.py\", line 48, in query\r\nhaystack-api_1   |     result = _process_request(PIPELINE, request)\r\nhaystack-api_1   |   File \"/home/user/rest_api/controller/search.py\", line 66, in _process_request\r\nhaystack-api_1   |     result = pipeline.run(query=request.query, params=params)\r\nhaystack-api_1   |   File \"/home/user/haystack/pipeline.py\", line 300, in run\r\nhaystack-api_1   |     raise ValueError(f\"No node(s) or global parameter(s) named {', '.join(invalid_keys)} found in pipeline.\")\r\nhaystack-api_1   | ValueError: No node(s) or global parameter(s) named ESRetriever found in pipeline.\r\nui_1             | 2021-10-19 19:16:54.134 Expecting value: line 1 column 1 (char 0)\r\nui_1             | Traceback (most recent call last):\r\nui_1             |   File \"/home/user/webapp.py\", line 134, in main\r\nui_1             |     results, raw_json = retrieve_doc(question, top_k_reader=top_k_reader, top_k_retriever=top_k_retriever)\r\nui_1             |   File \"/usr/local/lib/python3.7/site-packages/streamlit/caching.py\", line 545, in wrapped_func\r\nui_1             |     return get_or_create_cached_value()\r\nui_1             |   File \"/usr/local/lib/python3.7/site-packages/streamlit/caching.py\", line 527, in get_or_create_cached_value\r\nui_1             |     return_value = func(*args, **kwargs)\r\nui_1             |   File \"/home/user/utils.py\", line 30, in retrieve_doc\r\nui_1             |     response_raw = requests.post(url, json=req).json()\r\nui_1             |   File \"/usr/local/lib/python3.7/site-packages/requests/models.py\", line 910, in json\r\nui_1             |     return complexjson.loads(self.text, **kwargs)\r\nui_1             |   File \"/usr/local/lib/python3.7/json/__init__.py\", line 348, in loads\r\nui_1             |     return _default_decoder.decode(s)\r\nui_1             |   File \"/usr/local/lib/python3.7/json/decoder.py\", line 337, in decode\r\nui_1             |     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\nui_1             |   File \"/usr/local/lib/python3.7/json/decoder.py\", line 355, in raw_decode\r\nui_1             |     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\nui_1             | json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n\r\nSeems to be some sort of JSON issue between the search.py and pipeline.py files with the UI?Hi @sieu-tran and @grahamschuckman, thanks for the bug report! I can see from your stacktrace that this is due to a new procedure for the validation of pipeline parameters tha we merged to master yesterday. I'm going to bugfix this soon and let you know when the fix is out.\r\n\r\nThanks @grahamschuckman for creating the issue (#1618) , let's continue the discussion there :)",
    "meta": { "name": "Connection Error. Is Haystack running?" },
    "answer": "Hi @sieu-tran and @grahamschuckman, thanks for the bug report! I can see from your stacktrace that this is due to a new procedure for the validation of pipeline parameters tha we merged to master yesterday. I'm going to bugfix this soon and let you know when the fix is out.\r\n\r\nThanks @grahamschuckman for creating the issue (#1618) , let's continue the discussion there :)"
  },
  {
    "content": "In many search engines, Faceted search is a very important aspect. \r\nUser may want to search for documents that only contains certain meta data tags, products in certain price range, content published in certain time window, etc.\r\n\r\nFrom the underlying DocumentStore perspective, some of them (eg Elasticsearch, Weaviate etc) already have their native api to perform this kind of filters. I am just wondering, in Haystack, is there a feasibility of applying these filters, get a subset of search space before throw it to the retriever?\r\nLooking at the documentation, I have not find it possible yet\r\n`document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")`\r\n\r\nThis is actually becoming a critical part in my project. One possible way is to do the search on all the docs first, then perform the filters on my downstream flow. However, this  is inefficient, and also, typically we are obtaining top K outputs from the search, if we apply filtering on theses K result, there is a good chance none of them meet the criteria.\r\n\r\nI know this is very search specific use case, but I think it is a big one.\r\nIf anyone knows how to do this, please suggest me. Much appreciated\r\n\r\nHey @NormanLYJ we think this is a quite common use case and some of our engineers worked on it in our hacky friday last week : ) \r\n\r\n@erendabanlioglu could you post some of the wireframes you created and see if our solutions could be a fit to the problem described here?Thanks @Timoeller \r\nThat is what I am looking for.\r\n\r\nI will definitely read more articles from this blog, they look insightful, surprisingly I didn't notice it yet...",
    "meta": { "name": "Search on a subset of documents from DocumentStore" },
    "answer": "Hey @NormanLYJ we think this is a quite common use case and some of our engineers worked on it in our hacky friday last week : ) \r\n\r\n@erendabanlioglu could you post some of the wireframes you created and see if our solutions could be a fit to the problem described here?"
  },
  {
    "content": "Hi guys, I think what you are doing is very interesting. I am currently struggling with data Preprocessing(Tutorial 8).\r\nWhen I open my own pdf file in function PDFToTextConverter, I get the following error:\r\n\r\n[WinError 2] The system can't find the specified file\r\n\r\nUnfortunately, I have not yet found a specific solution for it. Can you guide me?\r\nHi @ehsanVIP sorry for taking so long to reply to your question. Could this be a problem with read permissions? Or maybe the path to the file is wrong? Does your file have the file extension .pdf in the filename?Hey @ehsanVIP and also @Chance-Obondo \r\ncould you please post here \r\n1. how you defined the \"file_path\" parameter in PDFToTextConverter.convert() method\r\n2. the exact path where your pdf file is (e.g. right click + properties or so)\r\n\r\nMy guess would be you just did not specify the path correctly or possibly the pdf file is actually of the wrong format.\r\n @Chance-Obondo It seems your system have issue with pdftotext installation. Can you please try this https://pypi.org/project/pdftotext/ or use TIKA (https://github.com/deepset-ai/haystack/blob/master/haystack/file_converter/tika.py) to convert PDF to text.\r\n\r\n```\r\n'\"pdftotext -v\"' is not recognized as an internal or external command, operable program or batch file.\r\n```",
    "meta": {
      "name": "PDFToTextConverter: [WinError 2] The system can't find the specified file"
    },
    "answer": "@Chance-Obondo It seems your system have issue with pdftotext installation. Can you please try this https://pypi.org/project/pdftotext/ or use TIKA (https://github.com/deepset-ai/haystack/blob/master/haystack/file_converter/tika.py) to convert PDF to text.\r\n\r\n```\r\n'\"pdftotext -v\"' is not recognized as an internal or external command, operable program or batch file.\r\n```"
  },
  {
    "content": "I saw that you released GermanQuAD. (https://huggingface.co/datasets/deepset/germanquad)\r\nCould you share some details on how you prepared the dataset?\r\nI want to train a monolingual Dutch QA model. I plan to use an existing pre-trained Dutch BERT base model, and prepare some Dutch SQuAD samples myself. I have following ideas,\r\n1) machine translation of English SQuAD.\r\n2) human annotation of 500 Dutch question-passage-answer (Q-P-A) triplets in my domain.\r\n\r\nIf I skip 2), would 1) already be enough?\r\nIf I add 2), would 500 Q-P-A samples be enough?\r\nBy enough I mean at least better than a multilingual model.\r\n\r\nI've tried the multilingual model `deepset/xlm-roberta-large-squad2` on my Dutch samples. It works a bit however the performance is not satisfactory, which is expected. I feel that training a monolingual Dutch QA model will be much better. Your advice and tips are much appreciated!Hi @yingzwang! We released a [paper](https://arxiv.org/abs/2104.12741) where we summarize our learnings from creating the GermanQuAD dataset. We saw that using human annotations instead of machine-translated labels results in a better performance and also did some experiments on how the number of training samples affects the performance (see Figure 3).\r\nAlso, we released a [blogpost](https://medium.com/deepset-ai/going-beyond-squad-part-1-question-answering-in-different-languages-8eac6cf56f21) about QA on different languages than English.\r\n\r\nLet me know if you have further questions :)Thanks for sharing the paper @bogdankostic ! At a quick glance I see a lot of useful stuff that's exactly what I'm looking for.\r\nThe result of Table 4 is a bit surprising to me. The monolingual model trained on translated SQuAD is even worse than the XLM model trained on English SQuAD! Is the machine translated SQuAD that bad? I manually checked a few machine translated (English-to-Dutch) SQuAD samples, and they look pretty OK, i.e., the translated text is quite accurate and natural. What could seriously go wrong with the machine translated SQuAD, so that the monolingual model cannot learn well? \r\n\r\nThe performance boost by human annotated GermanQuAD is significant. I'm encouraged to invest more effort preparing more human annotated DutchQuAD :) Also need to explore a bit your annotation tool.\r\n\r\nAlso in the paper you mentioned about \"detailed labeling instructions\". Could you share your complete labeling instructions? Or is it already everything in Appendix A?Hi @yingzwang Appendix A contains all labeling instructions, yes. As we had a team of annotators, we did regular meetings during the annotation process where we discussed newly annotated samples. For example, we discussed the phrasing of the question and whether we can rephrase the question so that there is less lexical overlap with the answer string. Another example is that we discussed whether the question is self-sufficient or whether there should be some additional information in the question for open-domain question answering.\r\n\r\nEnglish-to-Dutch machine translation of SQuAD should already give you quite good results. At first glance, the translations look very good, I agree. However, sometimes there are small mistakes or the words used in the translation do not really fit the context. Sometimes machine-translation could increase the lexical overlap of the question and the answer and thus simplify the task for the model. That might be one reason why training on machine-translated data is not as good as training on carefully hand-annotated data. \r\n\r\nI think 500 hand-annotated samples could already be enough to see an improvement, yes. I would recommend that you try to make these 500 samples not too simple for the model.\r\n\r\nPlease keep us updated about the progress of your project and don't hesitate to contact us again if there are any questions coming up. Feel free to close the issue for now if there are no further questions at the moment and re-open it later. Good luck with training a monolingual Dutch QA model! \ud83d\udc4d Thanks for the elaborated answer @julian-risch ! You have really adopted a robust quality control workflow. How large is your annotation team? Not sure if I can afford a similar setup but I'll try.\r\n\r\nI found the manual of your annotation tool [here](https://haystack.deepset.ai/guides/annotation). There's one rule saying \"Don\u2019t create question that elaborate on context\", in page 13. Why is that? A user is very likely to ask a question in that way, especially in a conversation. Why should we avoid it in training data?\r\n\r\nYour labeling instructions are insightful. From ~50 annotated samples I've collected so far, some already violated the \"no lexical overlap rule\" and \"prefer short answer over long answer rule\". I read your paper just in time :)Great to hear that you find our documentation helpful! @yingzwang \r\nQuestions shouldn't have too many words in common with the context around the answer because that simplifies the question answering task and simple questions are not very helpful for the training. You are right, that users might ask such questions in an application. That's no problem for the model as it is a simple task to then find the corresponding document and answer. There is no need for training data that represents this relatively simple task.\r\nWe had a team of five annotators when we created the GermanQuAD dataset. Having a team of annotators does not only speed up the annotation process. It also helps at formulating more diverse questions and answers. For that reason I would recommend to have at least two annotators.There's also this recent article we did on annotation https://www.deepset.ai/blog/labeling-data-with-haystack-annotation-tool (and the latest one is about evaluation). Just in case it might be helpful in addition to the documentation pages.I just converted this issue to a discussion. @yingzwang you can mark the answer in this discussion rather than having to close the issue. That should also improve searchability for other community members.Another question, about preparing machine translated SQuAD.\r\n\r\nIn your paper you mentioned that you used Facebook's MLQA dataset to warm start the training. Unfortunately MLQA does not contain Dutch. It seems that I have to prepare the machine translated Dutch SQuAD myself. Do you have any suggestions or tips?\r\n\r\nI found this [repo](https://github.com/ccasimiro88/TranslateAlignRetrieve) implementing an translate-align-retrieve method. It seems useful?\r\n\r\n",
    "meta": { "name": "Prepare non-English SQuAD" },
    "answer": "Hi @yingzwang Appendix A contains all labeling instructions, yes. As we had a team of annotators, we did regular meetings during the annotation process where we discussed newly annotated samples. For example, we discussed the phrasing of the question and whether we can rephrase the question so that there is less lexical overlap with the answer string. Another example is that we discussed whether the question is self-sufficient or whether there should be some additional information in the question for open-domain question answering.\r\n\r\nEnglish-to-Dutch machine translation of SQuAD should already give you quite good results. At first glance, the translations look very good, I agree. However, sometimes there are small mistakes or the words used in the translation do not really fit the context. Sometimes machine-translation could increase the lexical overlap of the question and the answer and thus simplify the task for the model. That might be one reason why training on machine-translated data is not as good as training on carefully hand-annotated data. \r\n\r\nI think 500 hand-annotated samples could already be enough to see an improvement, yes. I would recommend that you try to make these 500 samples not too simple for the model.\r\n\r\nPlease keep us updated about the progress of your project and don't hesitate to contact us again if there are any questions coming up. Feel free to close the issue for now if there are no further questions at the moment and re-open it later. Good luck with training a monolingual Dutch QA model! \ud83d\udc4d "
  },
  {
    "content": "Hi, \r\n\r\nI am trying to zip and read in my own files from s3 bucket using the Tutorial 1 for Haystack [here](https://haystack.deepset.ai/tutorials/v0.5.0/first-qa-system). I can read in the zip file that is specified in the tutorial, no problem on my local setup: `s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\"`\r\n\r\nWhen I zip my own files and move to s3 bucket, and try reading in the zip file using this code, I get error: \r\n\r\n```\r\nfrom haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\r\ndoc_dir = \"Desktop/data/article_txt_data\"\r\ns3_url = \"https://s3.us-east-1.amazonaws.com/example_bucket/example_zipfile.zip\"\r\n\r\nError: `BadZipFile: File is not a zip file`\r\n```\r\n \r\n \r\nMachine specs:\r\nMac osx Big sur, M1 chip (if that makes a difference)\r\nPython3\r\n\r\nThe way I zipped the file was:\r\n\r\n1. create a directory on my desktop. \r\n2. put non zip file into directory\r\n3. open terminal\r\n4. cd into directory\r\n5. run `zip -r data.zip . -x \".*\" -x \"__MACOSX\" -x \".DS_Store\"` (remove any hidden files and output 'data.zip' to same directory)\r\n6. upload data.zip to s3. \r\n7. Test and get error `BadZipFile`\r\n\r\nThen I ran this function, i found [here](https://stackoverflow.com/questions/3083235/unzipping-file-results-in-badzipfile-file-is-not-a-zip-file) \r\n\r\n```\r\ndef fixBadZipfile(zipFile):  \r\n f = open(zipFile, 'r+b')  \r\n data = f.read()\r\n pos = data.find(b'\\x50\\x4b\\x05\\x06') # End of central directory signature  \r\n if (pos > 0):\r\n     print('yes')\r\n     #self._log(\"Trancating file at location \" + str(pos + 22)+ \".\")  \r\n     f.seek(pos + 22)   # size of 'ZIP end of central directory record' \r\n     f.truncate()  \r\n     f.close()  \r\n else:  \r\n     print('not working')\r\n     \r\nfixBadZipfile('path/to/zip/file.zip')\r\n```\r\n 8. and reuploaded the new zip, reran the same code but same error again. \r\n \r\nRegardless of what I try, I get the same error when I try to read in my own zip files. Is there something specific I need to do to zip my own files to be put on s3 so I dont get the `BadZipFile` error for Haystack specifically? \r\n\r\nThank you in advance. Hey @asharm0662! My guess is that your file is not publicly available by everyone and therefore, the zip-file is not downloaded. If you open the s3-url in your browser, will it download the file or do you get an \"AccessDenied\" error?",
    "meta": {
      "name": "Is there a proper way files are zipped to be put on s3, for Haystack to read them in?"
    },
    "answer": "Hey @asharm0662! My guess is that your file is not publicly available by everyone and therefore, the zip-file is not downloaded. If you open the s3-url in your browser, will it download the file or do you get an \"AccessDenied\" error?"
  },
  {
    "content": "[Novice]\r\nHi, \r\n\r\nI was successfully able to train model on my data , on Google Colab initially, and save a model to my Google drive and download that model to my local machine using Haystack tutorial [here](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb). I saved using this line:\r\n\r\n`reader.save(directory='path/to/directory')`\r\n\r\nNow that I have a saved/trained model, I am wanting to take the trained model and apply it locally on the same docs that were used for training in google colab, to the same docs on my local machine. \r\n\r\nI can read in the pretrained model in my local like so:\r\n\r\n`from haystack.reader.farm import FARMReader`\r\n`new_reader = FARMReader(model_name_or_path=\"Downloads/my_model/.\",use_gpu=False)`\r\n\r\n\r\nNow that I can read the model, I tried rerunning the prediction code like so:\r\n\r\n`prediction = pipe.run(query= \"is this working? \" ,params={\"retriever\": {\"top_k\": 10}, \"reader\": {\"top_k\": 5}})`\r\n\r\nHowever, this wont work and doesnt, because I dont have code on my local machine to run the `pipe.run` logic. My understanding was I wouldnt have to run the same training code again for predicting after training is done. \r\n\r\nMy question is, once I have trained a model on a set of documents in google colab, is there any easy way to pass the model to someone else and have them predicting without running all the code that I used for training, which includes the `pipe.run` code? Or what is  the easiest way to start predicting using a pretrained model? \r\n\r\nIf I need to process documents again, in my training code, can I save the text documents that are converted to paragraphs and use those for running the model on at a later date? It seems counter intuitive to have to reprocess documents as done in training. \r\n\r\nThank you in advance. \r\n\r\n-------\r\nLocal Machine Specs:\r\n1. Macbook pro M1 chip 2020\r\nHi @asharm0662 preprocessing of the documents is not needed again if you store the preprocessed documents in one of Haystack's document stores and re-use that document store. However, the document stores don't have something like an `export` method so we don't really support out of the box that you share a document store with somebody else using some other machine.\r\n\r\nYou're right that you don't need to run the training code again to make predictions after having trained a model. However, `pipe.run` needs to be executed to make predictions, for example, on an ExtractiveQAPipeline. I am bit confused about how you \"trained\" a model. Tutorial 1 does not do any training but you linked it in your issue. Maybe there is a misunderstanding? Could you please tell me what lines of code you used for training?Hi @asharm0662! Please let me try to help you. I think there is a misunderstanding, given that the tutorial you have linked does not contain any training but only the application of already trained QA models (= making predictions). You do not need to train a model in order to make predictions.\r\n\r\nYou should be able to load one of the existing Reader models (for example `\"deepset/roberta-base-squad2\"`) on your local machine just like in the tutorial and ask questions on your documents by following the steps in [Tutorial 1](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb) or [Tutorial 3](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb). In order to make predictions on your own documents instead of the Game-of-Thrones sample documents, just write your own documents to the document store.\r\n\r\nI hope this makes things a bit clearer, if not, I am happy to answer your questions. \r\n",
    "meta": {
      "name": "How to predict after loading in trained model in local?"
    },
    "answer": "Hi @asharm0662! Please let me try to help you. I think there is a misunderstanding, given that the tutorial you have linked does not contain any training but only the application of already trained QA models (= making predictions). You do not need to train a model in order to make predictions.\r\n\r\nYou should be able to load one of the existing Reader models (for example `\"deepset/roberta-base-squad2\"`) on your local machine just like in the tutorial and ask questions on your documents by following the steps in [Tutorial 1](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb) or [Tutorial 3](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb). In order to make predictions on your own documents instead of the Game-of-Thrones sample documents, just write your own documents to the document store.\r\n\r\nI hope this makes things a bit clearer, if not, I am happy to answer your questions. \r\n"
  },
  {
    "content": "Actually, I was going to be opening issue about this, but would like to discuss over here first to be sure if I am not making any mistake. Is the output json supposed to be using single-quotes(`'`) instead of double-quotes(`\"`)? \r\n\r\nI am trying to crawl this [webpage](https://www.businesswire.com/news/home/20200717005310/en/Global-8.5-Bn-Baobab-Powder-Market-Outlook-2020-2027---ResearchAndMarkets.com) using `Crawler()` module. But, the json that is getting outputted is corrupt in nature. I think the the use of single-quotes(`'`) in the key and values of the json is the reason. Also, if we replace them with double-quotes(`\"`), then the double-quotes(`\"`) in text need to be escaped manually (I guess, there is no other way) using `\\`.\r\n\r\nCode:\r\n```python\r\nfrom haystack.connector import Crawler\r\n\r\ncrawler = Crawler(output_dir=\"crawled_files\")\r\n# crawl Haystack docs, i.e. all pages that include haystack.deepset.ai/docs/\r\ndocs = crawler.crawl(urls=[\"https://www.businesswire.com/news/home/20200717005310/en/Global-8.5-Bn-Baobab-Powder-Market-Outlook-2020-2027---ResearchAndMarkets.com\"],\r\n                     filter_urls= [\"Baobab\"])\r\n```\r\nOutput:\r\n```json\r\n{'meta': {'url': 'https://www.businesswire.com/news/home/20200717005310/en/Global-8.5-Bn-Baobab-Powder-Market-Outlook-2020-2027---ResearchAndMarkets.com', 'base_url': 'https://www.businesswire.com/news/home/20200717005310/en/Global-8.5-Bn-Baobab-Powder-Market-Outlook-2020-2027---ResearchAndMarkets.com'}, 'text': 'Global $8.5 Bn Baobab Powder Market Outlook 2020-2027 - ResearchAndMarkets.com\\nJuly 17, 2020 09:20 AM Eastern Daylight Time\\nDUBLIN--(BUSINESS WIRE)--The \"Baobab Powder - Global Market Trajectory & Analytics\" report has been added to ResearchAndMarkets.com\\'s offering.\\n\u201cBaobab Powder - Global Market Trajectory & Analytics\u201d\\nTweet this\\nThe publisher brings years of research experience to this 7th edition of this report. The 276-page report presents concise insights into how the pandemic has impacted production and the buy side for 2020 and 2021. A short-term phased recovery by key geography is also addressed.\\nGlobal Baobab Powder Market to Reach US$8.5 Billion by the Year 2027\\nAmid the COVID-19 crisis, the global market for Baobab Powder, estimated at US$6 Billion in the year 2020, is projected to reach a revised size of US$8.5 Billion by 2027, growing at a CAGR of 5.1% over the period 2020-2027.\\nOrganic Baobab Powder, one of the segments analyzed in the report, is projected to grow at a 5.4% CAGR to reach US$7.2 Billion by the end of the analysis period. After an early analysis of the business implications of the pandemic and its induced economic crisis, growth in the Conventional Baobab Powder segment is readjusted to a revised 3.6% CAGR for the next 7-year period. This segment currently accounts for a 16.7% share of the global Baobab Powder market.\\nThe U.S. Accounts for Over 27.1% of Global Market Size in 2020, While China is Forecast to Grow at a 7.8% CAGR for the Period of 2020-2027\\nThe Baobab Powder market in the U.S. is estimated at US$1.6 Billion in the year 2020. The country currently accounts for a 27.09% share in the global market. China, the world second largest economy, is forecast to reach an estimated market size of US$1.8 Billion in the year 2027 trailing a CAGR of 7.8% through 2027.\\nAmong the other noteworthy geographic markets are Japan and Canada, each forecast to grow at 2.8% and 4.5% respectively over the 2020-2027 period. Within Europe, Germany is forecast to grow at approximately 3.1% CAGR while Rest of European market (as defined in the study) will reach US$1.8 Billion by the year 2027.\\nCompetitors identified in this market include, among others:\\nADUNA Ltd.\\nALAFFIA\\nAtacora Essential, Inc.\\nBaobab Foods, Inc.\\nBaobab Fruit Company Senegal\\nB\\'Ayoba (Pvt) Ltd.\\nEcoProducts\\nEcuadorian Rainforest LLC\\nFarafena\\nHolland & Barrett Retail Ltd.\\nIndigo Herbs Ltd.\\nKiki Ltd.\\nOrganic Africa\\nOrganic Burst UK Ltd.\\nOrganic Herb Trading Company\\nPowbab, Inc.\\nStern Ingredients, Inc.\\nSuperfruit Scandinavia AB\\nZ Natural Foods, LLC\\nTotal Companies Profiled: 42\\nFor more information about this report visit https://www.researchandmarkets.com/r/2hnl1y\\nContacts\\nResearchAndMarkets.com\\nLaura Wood, Senior Press Manager\\npress@researchandmarkets.com\\n\\nFor E.S.T Office Hours Call 1-917-300-0470\\nFor U.S./CAN Toll Free Call 1-800-526-8630\\nFor GMT Office Hours Call +353-1-416-8900\\n\\n\\n\\n\\n\\n\\nLog In\\nSign Up\\nMore from Business Wire\\nBlog\\nUK/Ireland\\nDeutschland\\nFrance\\nHong Kong\\nItaly\\nJapan\\nTradeshownews.com\\nContact Us\\nUK Tax Strategy\\nPrivacy\\nManage Cookies\\nTerms of Use\\n\u00a9 2021 Business Wire, Inc.\\nBy clicking \u201cAccept All Cookies\u201d, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. Cookie Policy\\nCookies Settings Accept All Cookies'}\r\n```\r\nThen, I used the [JsonFormatter Website](https://jsonformatter.curiousconcept.com/#), and the correct output json is:\r\n```\r\n{\r\n   \"meta\":{\r\n      \"url\":\"https://www.businesswire.com/news/home/20200717005310/en/Global-8.5-Bn-Baobab-Powder-Market-Outlook-2020-2027---ResearchAndMarkets.com\",\r\n      \"base_url\":\"https://www.businesswire.com/news/home/20200717005310/en/Global-8.5-Bn-Baobab-Powder-Market-Outlook-2020-2027---ResearchAndMarkets.com\"\r\n   },\r\n   \"text\":\"Global $8.5 Bn Baobab Powder Market Outlook 2020-2027 - ResearchAndMarkets.com\\nJuly 17, 2020 09:20 AM Eastern Daylight Time\\nDUBLIN--(BUSINESS WIRE)--The \\\"Baobab Powder - Global Market Trajectory & Analytics\\\" report has been added to ResearchAndMarkets.com\\\\'s offering.\\n\u201cBaobab Powder - Global Market Trajectory & Analytics\u201d\\nTweet this\\nThe publisher brings years of research experience to this 7th edition of this report. The 276-page report presents concise insights into how the pandemic has impacted production and the buy side for 2020 and 2021. A short-term phased recovery by key geography is also addressed.\\nGlobal Baobab Powder Market to Reach US$8.5 Billion by the Year 2027\\nAmid the COVID-19 crisis, the global market for Baobab Powder, estimated at US$6 Billion in the year 2020, is projected to reach a revised size of US$8.5 Billion by 2027, growing at a CAGR of 5.1% over the period 2020-2027.\\nOrganic Baobab Powder, one of the segments analyzed in the report, is projected to grow at a 5.4% CAGR to reach US$7.2 Billion by the end of the analysis period. After an early analysis of the business implications of the pandemic and its induced economic crisis, growth in the Conventional Baobab Powder segment is readjusted to a revised 3.6% CAGR for the next 7-year period. This segment currently accounts for a 16.7% share of the global Baobab Powder market.\\nThe U.S. Accounts for Over 27.1% of Global Market Size in 2020, While China is Forecast to Grow at a 7.8% CAGR for the Period of 2020-2027\\nThe Baobab Powder market in the U.S. is estimated at US$1.6 Billion in the year 2020. The country currently accounts for a 27.09% share in the global market. China, the world second largest economy, is forecast to reach an estimated market size of US$1.8 Billion in the year 2027 trailing a CAGR of 7.8% through 2027.\\nAmong the other noteworthy geographic markets are Japan and Canada, each forecast to grow at 2.8% and 4.5% respectively over the 2020-2027 period. Within Europe, Germany is forecast to grow at approximately 3.1% CAGR while Rest of European market (as defined in the study) will reach US$1.8 Billion by the year 2027.\\nCompetitors identified in this market include, among others:\\nADUNA Ltd.\\nALAFFIA\\nAtacora Essential, Inc.\\nBaobab Foods, Inc.\\nBaobab Fruit Company Senegal\\nB\\\\'Ayoba (Pvt) Ltd.\\nEcoProducts\\nEcuadorian Rainforest LLC\\nFarafena\\nHolland & Barrett Retail Ltd.\\nIndigo Herbs Ltd.\\nKiki Ltd.\\nOrganic Africa\\nOrganic Burst UK Ltd.\\nOrganic Herb Trading Company\\nPowbab, Inc.\\nStern Ingredients, Inc.\\nSuperfruit Scandinavia AB\\nZ Natural Foods, LLC\\nTotal Companies Profiled: 42\\nFor more information about this report visit https://www.researchandmarkets.com/r/2hnl1y\\nContacts\\nResearchAndMarkets.com\\nLaura Wood, Senior Press Manager\\npress@researchandmarkets.com\\n\\nFor E.S.T Office Hours Call 1-917-300-0470\\nFor U.S./CAN Toll Free Call 1-800-526-8630\\nFor GMT Office Hours Call +353-1-416-8900\\n\\n\\n\\n\\n\\n\\nLog In\\nSign Up\\nMore from Business Wire\\nBlog\\nUK/Ireland\\nDeutschland\\nFrance\\nHong Kong\\nItaly\\nJapan\\nTradeshownews.com\\nContact Us\\nUK Tax Strategy\\nPrivacy\\nManage Cookies\\nTerms of Use\\n\u00a9 2021 Business Wire, Inc.\\nBy clicking \u201cAccept All Cookies\u201d, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. Cookie Policy\\nCookies Settings Accept All Cookies\"\r\n}\r\n```\r\n\r\nWhen I try to pass the json output from crawler module (the corrupt one) into `PreProcesser` module:\r\n```python\r\nfrom haystack.preprocessor.preprocessor import PreProcessor\r\n\r\npreprocessor = PreProcessor(\r\n    clean_empty_lines=True,\r\n    clean_whitespace=True,\r\n    clean_header_footer=False,\r\n    split_by=\"word\",\r\n    split_length=100,\r\n    split_respect_sentence_boundary=True\r\n)\r\ndocs_default = preprocessor.process(docs)\r\nprint(f\"n_docs_input: 1\\nn_docs_output: {len(docs_default)}\")\r\n```\r\nIt gives an error:\r\n```\r\nTypeError: list indices must be integers or slices, not str\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/tmp/ipykernel_36003/2158995738.py in <module>\r\n      9     split_respect_sentence_boundary=True\r\n     10 )\r\n---> 11 docs_default = preprocessor.process(docs)\r\n     12 print(f\"n_docs_input: 1\\nn_docs_output: {len(docs_default)}\")\r\n\r\n~/miniconda3/envs/EWildlifeENV/lib/python3.8/site-packages/haystack/preprocessor/preprocessor.py in process(self, document, clean_whitespace, clean_header_footer, clean_empty_lines, split_by, split_length, split_overlap, split_respect_sentence_boundary)\r\n     95             split_respect_sentence_boundary = self.split_respect_sentence_boundary\r\n     96 \r\n---> 97         cleaned_document = self.clean(\r\n     98             document=document,\r\n     99             clean_whitespace=clean_whitespace,\r\n\r\n~/miniconda3/envs/EWildlifeENV/lib/python3.8/site-packages/haystack/preprocessor/preprocessor.py in clean(self, document, clean_whitespace, clean_header_footer, clean_empty_lines)\r\n    121         and empty lines. Its exact functionality is defined by the parameters passed into PreProcessor.__init__().\r\n    122         \"\"\"\r\n--> 123         text = document[\"text\"]\r\n    124         if clean_header_footer:\r\n    125             text = self._find_and_remove_header_footer(\r\n\r\nTypeError: list indices must be integers or slices, not str\r\n```\r\nSo, does the `PreProcesser.preprocess()` accept a json or a string only? Hi @prikmm thank you for your question. Maybe @DIVYA-19 can shed some light on this problem based on the contribution made in #775 ? Hi @prikmm, Crawler.crawl() returns list of paths. as @julian-risch said PreProcesser.preprocess() takes input as document in the form of dictionary or list of dictionaries as input `ex: {..., \"text\": ''}` see [preprocessing](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial8_Preprocessing.ipynb) colab notebook@julian-risch I have created a [PR](https://github.com/deepset-ai/haystack/pull/1436#issue-732014634) to add colab usecase to crawler as per the dicussion [here](https://github.com/deepset-ai/haystack/discussions/1420#discussioncomment-1311135)",
    "meta": { "name": "Why is Crawler outputting corrupt json?" },
    "answer": "@julian-risch I have created a [PR](https://github.com/deepset-ai/haystack/pull/1436#issue-732014634) to add colab usecase to crawler as per the dicussion [here](https://github.com/deepset-ai/haystack/discussions/1420#discussioncomment-1311135)"
  },
  {
    "content": "I am trying to get the docker compose demo to work dense retrieval(which worked for my usecase on colab). The poblem is when I run the docker compose file with only haystack_ui and haystack_api images, it gets into a weird loop where haystack_api worker keeps timing out and it runs all over again. I did change the necessary yaml pipeline file but being a complete newbie to docker, I'm probably making some major mistake. Attaching my docker-compse.yaml and pipelines.yaml below.\r\n\r\n[docker-compose.yml.txt](https://github.com/deepset-ai/haystack/files/7036833/docker-compose.yml.txt)\r\n[pipelines.yaml.txt](https://github.com/deepset-ai/haystack/files/7036842/pipelines.yaml.txt)\r\nHi @advaitdeshmukh, it seems that the docker image for the API is not using your custom `pipelines.yaml`.\r\n\r\nYou can replace `rest_api/pipeline/pipelines.yaml` with your own file. This custom file should then be mounted on the docker image. You can use the following docker-compose.yml:\r\n\r\n```YAML\r\nversion: \"3\"\r\nservices:\r\n  haystack-api:\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile\r\n    image: \"deepset/haystack-cpu:latest\"\r\n    pull_policy: always\r\n    volumes:\r\n      - ./rest_api/pipeline:/home/user/rest_api/pipeline\r\n    ports:\r\n      - 8000:8000\r\n    restart: always\r\n    command: \"/bin/bash -c 'sleep 15 && gunicorn rest_api.application:app -b 0.0.0.0 -k uvicorn.workers.UvicornWorker --workers 1 --timeout 180'\"\r\n  ui:\r\n    build:\r\n      context: ui\r\n      dockerfile: Dockerfile\r\n    ports:\r\n      - 8501:8501\r\n    environment:\r\n      - API_ENDPOINT=http://haystack-api:8000\r\n      - EVAL_FILE=eval_labels_example.csv\r\n```\r\n\r\nIf you still face issues, please share the complete logs of `docker-compose up`.Hi @advaitdeshmukh, I'm closing this thread. Let's continue the discussion on #1411. ",
    "meta": { "name": "Trouble using Documentstores other than elastic" },
    "answer": "Hi @advaitdeshmukh, I'm closing this thread. Let's continue the discussion on #1411. "
  },
  {
    "content": "We need to introduce a new model for the specific task.\r\nGoogle has already released the dataset (https://github.com/google-research-datasets/boolean-questions).\r\nHi @akkefa interesting idea. It could also make sense to combine it with a QueryClassifier that routes Yes/No questions to the new model and the rest of the questions to the models that we have so far.",
    "meta": { "name": "Question answering with Yes/No scope only. (BoolQ)" },
    "answer": "Hi @akkefa interesting idea. It could also make sense to combine it with a QueryClassifier that routes Yes/No questions to the new model and the rest of the questions to the models that we have so far."
  },
  {
    "content": "I have this following codeset which takes 30 seconds to return a response. I'm not sure how to make it fast\r\n\r\n document_store = ElasticsearchDocumentStore(host=\"server.lan\", username=\"\", password=\"\",\r\n                                                index=\"insurance\", embedding_dim=768,\r\n                                                embedding_field=\"embedding\")\r\n\r\n    retriever = ElasticsearchRetriever(document_store=document_store)\r\n\r\n\r\n    print(retriever.retrieve(query=query))\r\n\r\n    if modelname == \"roberta\":\r\n\r\n        reader = FARMReader(\r\n            model_name_or_path=\"deepset/roberta-base-squad2\",\r\n            use_gpu=False,\r\n            num_processes=8\r\n\r\n        )\r\n\r\n    else:\r\n        reader = FARMReader(\r\n            model_name_or_path=\"my_model\" + \"/\" + modelname,\r\n            use_gpu=False,\r\n            num_processes=8\r\n        )\r\n\r\n\r\n    pipeline = ExtractiveQAPipeline(reader, retriever)\r\n\r\n    result = pipeline.run(query=query, top_k_retriever=10,top_k_reader=3)\r\n\r\n    return resultHi @kamaljai,\r\n\r\nWe've recently published a series of articles on Haystack's performance here: https://medium.com/deepset-ai/parameter-tweaking-get-faster-answers-from-your-haystack-pipeline-b94f0f528111\r\n\r\nIn general, without a GPU, inference could be pretty slow.\r\n\r\nHope this helps.",
    "meta": { "name": "Slow search" },
    "answer": "Hi @kamaljai,\r\n\r\nWe've recently published a series of articles on Haystack's performance here: https://medium.com/deepset-ai/parameter-tweaking-get-faster-answers-from-your-haystack-pipeline-b94f0f528111\r\n\r\nIn general, without a GPU, inference could be pretty slow.\r\n\r\nHope this helps."
  },
  {
    "content": "Hi folks, Love what you are doing here. We are playing around w/ the tutorial https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.ipynb\r\n\r\nFor some quick prototyping, we are trying to produce some great Q&A'ing with a corpus of legal opinions. These materials vary in size, but are often multiple pages of text. We've had problems w/ other experiments w/ longer documents. As you probably know, legal texts in particular pose particular challenges in re summarization and Q&A due to the importance of deep-in-the-weeds text, so we'd hope to avoid truncation.\r\n\r\nHopefully Haystack can help us out w/ support fo Q&A of longer texts?I was able to answer my own question by reading the documentation :)",
    "meta": { "name": "Support for longer text documents" },
    "answer": "I was able to answer my own question by reading the documentation :)"
  },
  {
    "content": "Hi Guys,\r\n\r\nThanks for your help with this lib! I am just finding my way around it. I want to get the the (top k) responses to the question from _every_ document in the document store instead of the retriever taking the top instances across all documents. My use case is more in the nature of data retrieval from periodic documents.\r\n\r\nWould appreciate some pointers around this. Thanks!\r\nBCTagging @tholor @brandenchan @oryx1729 Hi @hepbc, cool to hear you are using Haystack this way! This Information extraction style of QA is something we see a lot of potential in! \r\n\r\nSo in this style of QA, you obviously want to iterate over each Q in your set of Qs. In terms of Docs, you also want to iterate over each Doc but make sure you're performing the query on just that single document and not your full set of Docs!\r\n\r\nTo do this, you need to perform some metadata filtering (see [this](https://haystack.deepset.ai/docs/latest/optimizationmd#Metadata-Filtering) for a basic intro) . The idea is that you attach a unique piece of meta data to Document, and at query time, you isolate a single document to perform the query on by setting a filter.\r\n\r\nIn a very rough sketch, you will want something like this\r\n```\r\n# Document preparation and indexing\r\ndocs = [\r\n    {\r\n        'text': \"some text here...\",\r\n        'meta': {'doc_id': 12345, ...}\r\n    }, ...\r\n]\r\ndocument_store.write_documents(docs)\r\n```\r\n```\r\n# Query time\r\nfor q in queries:\r\n    for doc_id in doc_ids:\r\n        result = pipeline.run(\r\n            query=q,\r\n            filters={\"doc_id\":[doc_id]}\r\n        )\r\n```\r\nLet me know how this goes for you!\r\n\r\nAll the best,\r\nFrom a fellow BC",
    "meta": {
      "name": "Search every document in doc store for the ques vs the top semantic matches"
    },
    "answer": "Hi @hepbc, cool to hear you are using Haystack this way! This Information extraction style of QA is something we see a lot of potential in! \r\n\r\nSo in this style of QA, you obviously want to iterate over each Q in your set of Qs. In terms of Docs, you also want to iterate over each Doc but make sure you're performing the query on just that single document and not your full set of Docs!\r\n\r\nTo do this, you need to perform some metadata filtering (see [this](https://haystack.deepset.ai/docs/latest/optimizationmd#Metadata-Filtering) for a basic intro) . The idea is that you attach a unique piece of meta data to Document, and at query time, you isolate a single document to perform the query on by setting a filter.\r\n\r\nIn a very rough sketch, you will want something like this\r\n```\r\n# Document preparation and indexing\r\ndocs = [\r\n    {\r\n        'text': \"some text here...\",\r\n        'meta': {'doc_id': 12345, ...}\r\n    }, ...\r\n]\r\ndocument_store.write_documents(docs)\r\n```\r\n```\r\n# Query time\r\nfor q in queries:\r\n    for doc_id in doc_ids:\r\n        result = pipeline.run(\r\n            query=q,\r\n            filters={\"doc_id\":[doc_id]}\r\n        )\r\n```\r\nLet me know how this goes for you!\r\n\r\nAll the best,\r\nFrom a fellow BC"
  },
  {
    "content": "\r\n\r\nI am trying Haystack tutorial 1 - https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial1_Basic_QA_Pipeline.ipynb on Google Colab and I am getting the following incompatibility errors during the installation\r\n\r\nCommand\r\n\r\n```\r\n!pip install git+https://github.com/deepset-ai/haystack.git\r\n!pip install urllib3==1.25.4\r\n```\r\n\r\nErrors\r\n```\r\nSuccessfully built farm-haystack langdetect python-multipart python-docx tika seqeval databricks-cli prometheus-flask-exporter alembic\r\nERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\r\nERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\r\nERROR: pytest 3.6.4 has requirement pluggy<0.8,>=0.5, but you'll have pluggy 0.13.1 which is incompatible.\r\nERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\r\nERROR: botocore 1.20.84 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\r\nERROR: grpcio-tools 1.37.1 has requirement grpcio>=1.37.1, but you'll have grpcio 1.34.1 which is incompatible.\r\n```\r\nHi @eboraks try resetting the runtime and then using this:\r\n```\r\n!pip install grpcio-tools==1.34.1\r\n!pip install git+https://github.com/deepset-ai/haystack.git\r\n```\r\nThe `pip install urllib3==1.25.4`is not needed anymore and the rest of the notebook should run fine even if some of the error messages suggest otherwise.",
    "meta": { "name": "Tutorials 1 incompatible libraries" },
    "answer": "Hi @eboraks try resetting the runtime and then using this:\r\n```\r\n!pip install grpcio-tools==1.34.1\r\n!pip install git+https://github.com/deepset-ai/haystack.git\r\n```\r\nThe `pip install urllib3==1.25.4`is not needed anymore and the rest of the notebook should run fine even if some of the error messages suggest otherwise."
  }
]
