{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "doc_dir = \"combinedSemanticDataSet.json\"\n",
    "\n",
    "json_files = json.loads(open(doc_dir).read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am looking for best practices as I know that there could be multiple ways to solve the problem. I just like to understand if there is a lightning approach that should be preferred.\n",
      "\n",
      "In my `LightningModule` I initialize a `CrossEntropyLoss` with specific `weight` to handle imbalanced classes: `torch.nn.CrossEntropyLoss(weight=my_weights)`. The weight for each class is defined as the `1 / number_of_samples_in_the_class`.\n",
      "\n",
      "In order to do this, I need to supply my `LightningModule` instance with the number of samples per class. However, usually you would load the data (and therefore count the number of samples per class in the `setup` function of the `LightningDataModule` instance. So here's the problem: usually, when you initialize the `LightningModule` you haven't loaded yet the data.\n",
      "\n",
      "Example:\n",
      "```\n",
      "class MyDataModule(LightningDataModule):\n",
      "    def __init__(self):\n",
      "        self.number_of_samples_per_class = None\n",
      "\n",
      "     def setup(self, stage):\n",
      "         self.number_of_samples_per_class: Dict[str, int] = ....\n",
      "\n",
      "class MyModel(LightningModule):\n",
      "    def __init__(number_of_samples_per_class: Dict[str, int]):\n",
      "        weights = self._compute_weights(number_of_samples_per_class)\n",
      "        self.loss = CrossEntropyLoss(weights)\n",
      "    \n",
      "    def _compute_weights(self, number_of_samples_per_class: Dict[str, int]):\n",
      "         .... compute weights here ...\n",
      "         return weights\n",
      "```\n",
      "```\n",
      "my_data_module = MyDataModule()\n",
      "my_model = MyModel(my_data_module.number_of_samples_per_class) # Error! my_data_module.number_of_samples_per_class is still None because my_data_module.setup() has not been called yet!\n",
      "```\n",
      "\n",
      "As possible solutions, I could manually call `my_data_module.setup()` or I could compute the number of samples inside the `__init__` function of `MyDataModule` but both ways seem not to follow torch lightning philosophy. What would be the cleanest way to solve this? \n",
      "Thank you!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(json_files[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_text(texts):\n",
    "    \"\"\"\n",
    "    Clean wikipedia text by removing multiple new lines, removing extremely short lines,\n",
    "    adding paragraph breaks and removing empty paragraphs\n",
    "    \"\"\"\n",
    "    newArray = []\n",
    "\n",
    "    for text in texts: \n",
    "        content = text['content'] \n",
    "\n",
    "        while \"\\n\" in content:\n",
    "            content = content.replace(\"\\n\", \"\")\n",
    "        \n",
    "        while \"\\r\" in content:\n",
    "            content = content.replace(\"\\r\", \"\")\n",
    "\n",
    "        while \"```\" in content:\n",
    "            content = content.replace(\"```\", \" \")\n",
    "\n",
    "        while \"  \" in content:\n",
    "            content = content.replace(\"  \", \" \")\n",
    "        \n",
    "        newArray.append({'content': content, 'meta': text['meta']})\n",
    "        \n",
    "\n",
    "    return newArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = clean_json_text(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import PreProcessor\n",
    "\n",
    "document_store = FAISSDocumentStore(\n",
    "    faiss_index_factory_str=\"Flat\", \n",
    "    return_embedding=True, \n",
    "    duplicate_documents='overwrite'\n",
    ")\n",
    "\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=50,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Some weights of the model checkpoint at facebook/rag-sequence-nq were not used when initializing RagTokenForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.weight', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RagTokenForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagTokenForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RagTokenForGeneration were not initialized from the model checkpoint at facebook/rag-sequence-nq and are newly initialized: ['rag.generator.lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import DensePassageRetriever\n",
    "from haystack.nodes import RAGenerator\n",
    "\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    use_gpu=True,\n",
    "    embed_title=True,\n",
    "    )\n",
    "\n",
    "generator = RAGenerator(\n",
    "    model_name_or_path=\"facebook/rag-sequence-nq\",\n",
    "    retriever=retriever,\n",
    "    top_k=1,\n",
    "    min_length=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "p = Pipeline()\n",
    "p.add_node(component=retriever, name=\"Retriever\", inputs=['Query'])\n",
    "p.add_node(component=generator, name=\"RAGenerator\", inputs=['Retriever'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb373be28264f4999caf5f22d952f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing Documents:   0%|          | 0/1827 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc267ecc8a9452e827c48b1945520fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Embedding:   0%|          | 0/1826 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c2edf794dd49c3822539aad757a8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/1840 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_store.write_documents(json_files)\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the document store\n",
    "document_store.save(\"my_faiss_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = FAISSDocumentStore.load(\"my_faiss_index.faiss\")\n",
    "assert document_store.faiss_index_factory_str == \"Flat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what comes after a Preprocessor in a pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what comes after a Preprocessor in a pipeline',\n",
       " 'answers': [<Answer {'answer': ' data accessor', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_id': None, 'meta': {'doc_ids': ['d31382234c0249763ca77babbf19477d', '681f259a0a88b1dbee6cb4f3963d4468', '4b171c865a0882a9790e559a4c50836d', '4b5bb9d8b4fe8618ebe397bcdca374a8', '11f3a35bdc738eae5c76e54aa2741675'], 'doc_scores': [0.6619975155599239, 0.6615594470192911, 0.6609649402339938, 0.6608263232300359, 0.6602357254224744], 'content': ['Whenever I index a document I see this in my console:Batches: 100%|██████████| 1/1 [00:00<00:00, 42.84it/s]I set `progress_bar=False` on every pipeline component yet I\\'m still seeing this: text_converter = TextConverter(valid_languages=[\\'en\\'], progress_bar=False)preprocessor = PreProcessor(progress_bar=False) p = Pipeline()p.add_node(component=text_converter, name=\"TextConverter\", inputs=[\"File\"])p.add_node(component=preprocessor, name=\"PreProcessor\", inputs=[\"TextConverter\"])p.add_node(component=document_store, name=\"DocumentStore\", inputs=[\"PreProcessor\"]) Doe anyone know where this is coming from?Also this output doesn\\'t appear to be going via my normal logging setup. Is this output going via the logging architecture or simply a print statement?ThanksHi @thobson,Which document_store are you using and which haystack version?Also just for completeness could you post the full code?', 'Hey, as suggested in the [wandb docs](https://docs.wandb.ai/ref/python/log), I would like to log metrics like pythonself.log(\\'val\\', {\\'acc\\': 0.9})# orself.log_dict({\\'val\\': {\\'acc\\': 0.9}}) And then be able to access them for e.g. early stopping like pythoncallbacks = [ EarlyStopping(\\'val.Corpus_F1\\', patience=opt.early_stop, mode=\"max\")]trainer = pl.Trainer(callbacks=callbacks) Is something similar possible? I keep getting this error.`pytorch_lightning.utilities.exceptions.MisconfigurationException: ModelCheckpoint(monitor=\\'val.Corpus_F1\\') not found in the returned metrics: [\\'train\\', \\'val\\'].`', \"When I use `IterableDataset` without `__len__`，the `estimated_stepping_batches` is always `-1`.So how to get correct `estimated_stepping_batches`?it's not possible to determine that since no one can really tell how many iterations will happen in such a case.\", 'When I use an `IterableDataset`, the `Trainer.fit()` gives something like: Epoch 0: : 21it [02:41, 7.69s/it, loss=0.663, v_num=18] Which does not display a progress. Even if I specify a validation interval, there is no progress shown. Is there a way I can specify an \"epoch length\", or something like that for `IterableDataset`?HiIterableDatasets don\\'t have a lengh defined. They can either be infinite in size or raise StopIteration at an arbitrary point. Hence, there is no notion of epochs and we cannot anticipate when it finishes iterations. If you really need to have epochs, I suggest you try to rewrite your dataset logic into a regular Dataset.', 'The device of the metric return by validation_step is GPU, related code is pythondef validation_step(self, batch, batch_idx): x, y = batch if y.device != self.device: y = y.to(self.device) y_hat = self(x) loss = self.loss(y_hat, y) # loss.device is cuda. self.log(\\'valid loss\\', loss.item()) return loss After an epoch of validation compeleted when using earlystopping, follow error occured: File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 871, in run_train self.train_loop.run_training_epoch() File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\training_loop.py\", line 584, in run_training_epoch self.trainer.run_evaluation(on_epoch=True) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 1011, in run_evaluation self.evaluation_loop.on_evaluation_end() File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\evaluation_loop.py\", line 102, in on_evaluation_end self.trainer.call_hook(\\'on_validation_end\\', *args, **kwargs) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 1228, in call_hook trainer_hook(*args, **kwargs) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\callback_hook.py\", line 227, in on_validation_end callback.on_validation_end(self, self.lightning_module) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\callbacks\\\\early_stopping.py\", line 173, in on_validation_end self._run_early_stopping_check(trainer) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\callbacks\\\\early_stopping.py\", line 193, in _run_early_stopping_check should_stop, reason = self._evalute_stopping_criteria(current, trainer) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\callbacks\\\\early_stopping.py\", line 226, in _evalute_stopping_criteria elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! This error didn\\'t appear until I updated the version of pytorch_lightning.Looking into it in #8295'], 'titles': ['Where are these log statements coming from?', 'Early stopping on nested metrics', 'How to get correct `estimated_stepping_batches` in `IterableDataset`', 'How to display progress on IterableDataset?', 'Earlystopping callback metrics in different devices with single gpu']}}>],\n",
       " 'documents': [<Document: {'content': 'Whenever I index a document I see this in my console:Batches: 100%|██████████| 1/1 [00:00<00:00, 42.84it/s]I set `progress_bar=False` on every pipeline component yet I\\'m still seeing this: text_converter = TextConverter(valid_languages=[\\'en\\'], progress_bar=False)preprocessor = PreProcessor(progress_bar=False) p = Pipeline()p.add_node(component=text_converter, name=\"TextConverter\", inputs=[\"File\"])p.add_node(component=preprocessor, name=\"PreProcessor\", inputs=[\"TextConverter\"])p.add_node(component=document_store, name=\"DocumentStore\", inputs=[\"PreProcessor\"]) Doe anyone know where this is coming from?Also this output doesn\\'t appear to be going via my normal logging setup. Is this output going via the logging architecture or simply a print statement?ThanksHi @thobson,Which document_store are you using and which haystack version?Also just for completeness could you post the full code?', 'content_type': 'text', 'score': 0.6619975155599239, 'meta': {'name': 'Where are these log statements coming from?', 'vector_id': '1473'}, 'embedding': '<embedding of shape (768,)>', 'id': 'd31382234c0249763ca77babbf19477d'}>,\n",
       "  <Document: {'content': 'Hey, as suggested in the [wandb docs](https://docs.wandb.ai/ref/python/log), I would like to log metrics like pythonself.log(\\'val\\', {\\'acc\\': 0.9})# orself.log_dict({\\'val\\': {\\'acc\\': 0.9}}) And then be able to access them for e.g. early stopping like pythoncallbacks = [ EarlyStopping(\\'val.Corpus_F1\\', patience=opt.early_stop, mode=\"max\")]trainer = pl.Trainer(callbacks=callbacks) Is something similar possible? I keep getting this error.`pytorch_lightning.utilities.exceptions.MisconfigurationException: ModelCheckpoint(monitor=\\'val.Corpus_F1\\') not found in the returned metrics: [\\'train\\', \\'val\\'].`', 'content_type': 'text', 'score': 0.6615594470192911, 'meta': {'name': 'Early stopping on nested metrics', 'vector_id': '709'}, 'embedding': '<embedding of shape (768,)>', 'id': '681f259a0a88b1dbee6cb4f3963d4468'}>,\n",
       "  <Document: {'content': \"When I use `IterableDataset` without `__len__`，the `estimated_stepping_batches` is always `-1`.So how to get correct `estimated_stepping_batches`?it's not possible to determine that since no one can really tell how many iterations will happen in such a case.\", 'content_type': 'text', 'score': 0.6609649402339938, 'meta': {'name': 'How to get correct `estimated_stepping_batches` in `IterableDataset`', 'vector_id': '472'}, 'embedding': '<embedding of shape (768,)>', 'id': '4b171c865a0882a9790e559a4c50836d'}>,\n",
       "  <Document: {'content': 'When I use an `IterableDataset`, the `Trainer.fit()` gives something like: Epoch 0: : 21it [02:41, 7.69s/it, loss=0.663, v_num=18] Which does not display a progress. Even if I specify a validation interval, there is no progress shown. Is there a way I can specify an \"epoch length\", or something like that for `IterableDataset`?HiIterableDatasets don\\'t have a lengh defined. They can either be infinite in size or raise StopIteration at an arbitrary point. Hence, there is no notion of epochs and we cannot anticipate when it finishes iterations. If you really need to have epochs, I suggest you try to rewrite your dataset logic into a regular Dataset.', 'content_type': 'text', 'score': 0.6608263232300359, 'meta': {'name': 'How to display progress on IterableDataset?', 'vector_id': '475'}, 'embedding': '<embedding of shape (768,)>', 'id': '4b5bb9d8b4fe8618ebe397bcdca374a8'}>,\n",
       "  <Document: {'content': 'The device of the metric return by validation_step is GPU, related code is pythondef validation_step(self, batch, batch_idx): x, y = batch if y.device != self.device: y = y.to(self.device) y_hat = self(x) loss = self.loss(y_hat, y) # loss.device is cuda. self.log(\\'valid loss\\', loss.item()) return loss After an epoch of validation compeleted when using earlystopping, follow error occured: File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 871, in run_train self.train_loop.run_training_epoch() File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\training_loop.py\", line 584, in run_training_epoch self.trainer.run_evaluation(on_epoch=True) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 1011, in run_evaluation self.evaluation_loop.on_evaluation_end() File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\evaluation_loop.py\", line 102, in on_evaluation_end self.trainer.call_hook(\\'on_validation_end\\', *args, **kwargs) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\trainer.py\", line 1228, in call_hook trainer_hook(*args, **kwargs) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\trainer\\\\callback_hook.py\", line 227, in on_validation_end callback.on_validation_end(self, self.lightning_module) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\callbacks\\\\early_stopping.py\", line 173, in on_validation_end self._run_early_stopping_check(trainer) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\callbacks\\\\early_stopping.py\", line 193, in _run_early_stopping_check should_stop, reason = self._evalute_stopping_criteria(current, trainer) File \"E:\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\pytorch_lightning\\\\callbacks\\\\early_stopping.py\", line 226, in _evalute_stopping_criteria elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! This error didn\\'t appear until I updated the version of pytorch_lightning.Looking into it in #8295', 'content_type': 'text', 'score': 0.6602357254224744, 'meta': {'name': 'Earlystopping callback metrics in different devices with single gpu', 'vector_id': '15'}, 'embedding': '<embedding of shape (768,)>', 'id': '11f3a35bdc738eae5c76e54aa2741675'}>],\n",
       " 'root_node': 'Query',\n",
       " 'params': {'Retriever': {'top_k': 5}},\n",
       " 'node_id': 'RAGenerator'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.run(query, params={'Retriever': {'top_k': 5}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a4a5fa04fb9ecdb6f6657916fb57a7287f257c6a31d8a9ca980ca1e813e1f67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
